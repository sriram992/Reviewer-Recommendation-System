Received July 28, 2016, accepted August 30, 2016, date of publication September 13, 2016, date of current version October 15, 2016. Digital Object Identifier 10.1109/ACCESS.2016.2608847 An Evaluation of Background Subtraction for Object Detection Vis-a-Vis Mitigating Challenging Scenarios SUMAN KUMAR CHOUDHURY, PANKAJ KUMAR SA, (Member, IEEE), SAMBIT BAKSHI, (Member, IEEE), AND BANSHIDHAR MAJHI, (Member, IEEE) Department of Computer Science and Engineering, National Institute of Technology Rourkela, Rourkela 769008, India Corresponding author: P. K. Sa (PankajKSa@nitrkl.ac.in) This work was supported by the Science and Engineering Research Board, Department of Science & Technology, Government of India, under Grant SB/FTP/ETA-0059/2014. ABSTRACT Background subtraction is a popular technique for detecting objects moving across a xed camera view. The performance of this paradigm is in uenced by various challenges, such as object relocation, illumination change, cast shadows, waving background, camera shake, bootstrapping, camou age, and so on. In this paper, we present a synopsis on the evolution of the background subtraction techniques over the last two decades. The different ways of mathematical modeling are taken into consideration to categorize the methods. We also evaluate the performance of some of the state-of-the-art techniques vis-a-vis the challenges associated. Eleven different algorithms of background subtraction have been simulated on thirty-four image sequences collected from ve benchmark datasets. For each image sequence, seven performance metrics are evaluated and an exhaustive comparative analysis has been made to derive inferences. The potential ndings in the result analysis are presented for future exploration. The obtained image and video results are uploaded at https://sites.google.com/site/soaBSevaluation. INDEX TERMS Video surveillance, object detection, background subtraction, background modeling, foreground extraction, background maintenance, shadow removal. I. INTRODUCTION Computational inef ciency, for the last few decades, has been a major bottleneck in processing videos in reasonable time. The recent advancement in parallel architectures has made feasible live-analysis of video data. It has also stimu- lated the researchers to develop more sophisticated and robust models that can deliver output under challenging conditions. The objective of video surveillance is to extract essential information from a set of image sequences by automatically detecting and tracking the objects of interest followed by recognizing the relevant activities. Video Surveillance has enormous applications both in public and private sectors such as theft avoidance, crime hindrance, site visitors monitoring, combating in opposition to act of terrorism, land security, accident prediction, and many others. A generic surveillance framework comprises a set of cameras placed at strategic locations that are connected to digital computers to ana- lyze the ongoing activities. This article concentrates on the very rst phase of an automated surveillance framework: separating moving objects through background subtraction. Figure 1 illustrates few sample objects extracted as fore- grounds from their respective frames and background model. Background subtraction has been the most widely used approach to detect moving objects over the last two decades. In general, this framework is a three-stage process as shown in Figure 2. In the rst stage, i.e. background initializa- tion, either the rst frame or few initial frames are taken into consideration to estimate a model of the background. Each successive frame is then compared with the established background to extract the moving objects during foreground extraction. The nal stage, background maintenance, keeps updating the model to adapt any changes that may occur in the observed scene over the time. However, this framework is susceptible to various challenges such as cast shadow illumi- nation, bootstrapping movement during initialization, back- ground oscillation with varying periodicity across the view, the chromatic similarity between a foreground to its underly- ing background, gradual change in sunlight illumination over VOLUME 4, 2016 2169-3536 2016 IEEE. Translations and content mining are permitted for academic research only. Personal use is also permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. 6133 S. K. Choudhury et al.: Evaluation of Background Subtraction for Object Detection Vis-a-Vis Mitigating Challenging Scenarios FIGURE 1. First row: four frames of the Fountain video; Second Row: Corresponding output, white pixels indicate foreground object. the time, rapid varying illumination with cloud movement and switching the lights on/off. The subtle elements on each of these dif culties with their varying solutions are expounded in Section III. FIGURE 2. Primitive steps of background subtraction. Quite a signi cant amount of work on background subtraction are available in literature [1] [4]. In this paper, we provide an overview of the existing meth- ods along with their solution strategy towards miti- gating the challenges. We simulate eleven state-of-the- art methods on various image sequences collected from ve benchmark datasets. An in-depth analysis of the results reveal some key ndings in background subtraction methodologies. All results of this study are available at https://sites.google.com/site/soaBSevaluation. The rest of this paper is organized as follows. Section II outlines the evolution of various mathematical models. Section III details the challenges as well as their solution strategies proposed over the years. Simulation statistics alongside the selected datasets, state-of-the-art methods, and performance measures are enumerated as well as the obtained results are analyzed in Sections IV and V. Finally, Section VI presents the concluding remarks. II. THE GENESIS OF BACKGROUND SUBTRACTION Effective background modeling and its periodic update are very much essential for accurate object detection. There exists plenty of literature in this eld over the last two decades [5], [6]. However, there is no unique way to cat- egorize these methods. In this section, we enumerate the evolution of various mathematical models of background subtraction to detect moving objects. A. BASIC MODEL The simplest way is to set the rst frame as the background and subtract all subsequent frames to extract the foregrounds. However, an oscillating background cannot be adapted using a single frame. Furthermore, the very rst frame of the sequence may contain moving objects that may falsely appear as background. The frame difference method uses the previ- ous frame rather than the initial frame for subtraction pur- pose. This method well adapts the slow varying illumination; however, fails to update the background, when a moving object ceases its motion abruptly. Lai and Yung modeled the background using the arithmetic mean of pixel values over few temporal sequence [7]. The W4 system considers three tuples for subtraction over the initialization sequence; the minimum gray value, the maximum gray value, and the max- imum intensity difference between two adjacent frames [8]. However, the inherent noise during image acquisition may substantially alter the intensity gap that leads to false pos- itives and false negatives. All these methods are unimodal and therefore, an associate oscillatory background cannot be tailored employing a single-valued model. B. STATISTICAL MODELS: SINGLE GAUSSIAN, MIXTURE OF GAUSSIANS, CODEBOOK The initialization pixel sequence along the temporal axis is modeled using a univariate Gaussian distribution [9]. The multivariate distribution, for R G B color channels, is mod- eled as a product of three independent univariate Gaussian distribution, where each distribution is parametrized by the 6134 VOLUME 4, 2016 S. K. Choudhury et al.: Evaluation of Background Subtraction for Object Detection Vis-a-Vis Mitigating Challenging Scenarios sample mean and standard deviation . The standard Z-score labeling is applied to extract the foreground pixels against the motion parameters. However, the single Gaus- sian model is unimodal and hence fails to accommodate the oscillating background. As an alternative, Stauffer and Grim- son incorporate the mixture of Gaussians (MoG) to create a multi-modal background. Each pixel location is classi ed into J 1 classes, where the unknown parameter J is chosen arbitrarily. The learning rate parameter is introduced to cope with varying illumination. The choice of learning rate parameter plays a major role; low learning rate fails to tackle sudden illumination variations, whereas higher rate includes the slow-moving objects in the background [10], [11]. Zivkovic and Hayden, in their work, proposed a solu- tion to choose the correct number of background classes at each location based on their sample variation over the frames [12], [13]. In another work, Kaewtrakulpong and Bowden changed the background update equations in the original MoG model to address the rapid illumination variations [14]. The codebook scheme uses few statisti- cal attributes to encode each background location such as the minimum and maximum intensity for a pixel over the frames, frequency-of-occurrence for each codeword, frame number at which the codeword has rst time occurred, frame number at which the codeword last appears, and the maximum frame gap during which the codeword remains missing [15], [16]. Fernandez-Sanchez et. al. follow the same principle to initialize the background; moreover, they incorporate the depth cue as an additional model parame- ter to strengthen the discriminating ability of the developed model [17]. C. NON-PARAMETRIC MODEL The temporal pixel sequence at any location might not fol- low the default Gaussian distribution. The kernel density estimation (KDE) techniques are applied in those scenarios, where the underlying distribution is unknown. In particular, these algorithms take ample training samples to converge to the underlying density function [18], [19]. The KDE based methods strongly depend on the suitable choice of kernel bandwidth that must have nite local support. Moreover, the bandwidth is inversely related to the number of frames adapted for background initialization. A narrow bandwidth results in a jagged density estimation, whereas an exten- sive one leads to over-smoothed distribution [20]. Piccardi and Zen, in their work, apply the median of the absolute difference between adjacent frames to estimate the kernel bandwidth [21]. The varying waving periodicity in the case of oscillating background appeals to approximate kernel band- width for each model location across each of R, G, B color channel. In another work, the mean-shift paradigm is chosen to estimate the kernel bandwidth with fewer training sam- ples [22]. Elgammal et al. apply a fast Gauss transform to reduce the overall response time of density computation [23]. Mittal and Paragios, in their work, model the dynamic back- ground using optical ow, and the feature uncertainty is resolved using a KDE technique [24]. Parag et al. suggested a boosting based ensemble learning to select appropriate features for the KDE based methods [25]. D. NON-RECURSIVE BUFFER BASED SUBTRACTION Lo and Velastin store the recent pixel history in a nite buffer to represent the model location [26]. The signi - cant difference between the current pixel and buffer median decides if it were a foreground; else, the new background is enqueued inside the buffer. The rst-in- rst-out strategy is applied to tackle the situation when the over ow condition is encountered. Subsequently, Cucchiara et al. prefer the medoid rather than the median statistics to take the appro- priate decision [27], [28]. In another work, the background is modeled using a linear predictive model through Wiener ltering [29]; the covariance of pixel sequence estimates the lter coef cients. This work is further extended in a relevant subspace via PCA [30], [31]. Wang and Suter use the notion of consensus to model the background. Additionally, two algorithms are suggested to deal with rapid varying illumi- nation and background relocation [32], [33]. E. FUZZY MODEL Fuzzy principle can be incorporated to address the deci- sion uncertainty during foreground extraction [34]. Zhang and Xu incorporate the fuzzy Sugeno integral to model the underlying scene under observation [35]. Subsequently, the Choquet integral is preferred over the former one that yields comparatively better accuracy [36]. Azab et al. fuse the edge information along with the color and texture features to model the background, where the Choquet integral is applied to extract the foreground pixels. Bouwmans et al. proposed another type-2 fuzzy technique to take the classi cation decision [37], [38]. Kim and Kim apply the fuzzy color histogram to model the waving background [39]. F. LEARNING MODEL A classi er (for example, neural network) is used to train the underlying density distribution of the pixel sequence and decide the nature of the next picture element. Culibrk et al. train a probabilistic, multi-layered, feed-forward neural net- work with 124 neurons to create a background model. A Bayesian classi er is then employed to separate the non-stationary pixels [40]. Maddalena and Petrosino design a self-organization map network in which each back- ground location is represented by a set of learned weight vectors [41], [42]. Moreover, a spatial coherence paradigm is introduced to reduce false alarms. G. LOW-RANK SPARSE DECOMPOSITION Subspace learning models are also introduced in the eld of background subtraction. The Robust Principal Component Analysis (RPCA) is applied to decompose the video frames into a low-rank background matrix and a sparse foreground matrix [43], [44]. Wright et. al., in their work, incorporated a L1-norm on the sparse matrix such that the background VOLUME 4, 2016 6135 S. K. Choudhury et al.: Evaluation of Background Subtraction for Object Detection Vis-a-Vis Mitigating Challenging Scenarios frames are linearly correlated to each other [45]. In another work, the Total Variation regularization constraint is incor- porated to handle the noisy data [46]. The low-rank matrix (background model) assumes to capture any variation that has been observed at the underlying scene over the time. However, The RPCA paradigm considers the entire image sequence as a vectorized data matrix and therefore has high memory overhead [47]. Sobral et. al. suggested an incremen- tal tensor subspace learning that builds the low-rank matrix using few initial temporal sequence only, and periodically update the model with subsequent frames [48]. In another work, a Sparse Outlier Iterative Removal (SOIR) algorithm is employed to model the background scene, where a cyclic iteration process is suggested to separate foreground pixels [49], [50]. H. SHADOW REMOVAL MODEL Shadow darkens the scene illumination, and hence, the under- lying region falsely appears as foreground. The literature includes two different ways to tackle this situation. The for- mer group suggests various invariant color models [27], [51] to nullify the shadow effects, whereas another set of algo- rithms prefer the texture feature that remains indifferent in the presence of shadow [52], [53]. Wang and Suter, in their work, apply the normalized RGB color space to model the background; however, it has been observed that the normal- ized RGB is very much noisy in case of low intensity [54]. Cucchiara et al. apply the HSI model to suppress the shadow illumination and use a median lter to selectively update the established model [27]; a second validation is further applied using both invariant color and texture pattern of the underlying scene [28]. Huang et al. incorporate the color and color co-occurrence features to model the static and waving background respectively [55]. Huerta et al. suggest a two- stage approach to counter the shadow illumination [56]. The rst stage combines the gradient details and color information to detect the probable shadow pixels. A second validation is further applied based on the temporal and spatial analysis of chrominance measure, brightness content, texture distortion, and diffused bluish effect. Zhou et al. consider multiple cues such as motion details, object location, its shape, and color feature to detect the objects in motion [57]. I. POST-PROCESSING REFINEMENT Foreground extraction may be erroneous owing to the clut- tered background and the inherent sensor noise during image acquisition. It may so happen that a few portion of foreground pixels may be wrongly identi ed as the background and vice- versa. A post improvisation module should be incorporated to minimize such false alarms [58]. The median ltering is a suitable tool to reduce such false positives. Again, some methods apply connected component analysis to attach the disjointed regions. The size constraint as per the objects of interest can be incorporated to eliminate small foreground pixels. Many authors prefer morphological post- ltering for such improvisation. Morphological Opening is applied to reduce the scattered noise pixels. The closing operation con- nects the disjointed pixels. Moreover, the morphological ll- ing can be applied to ll the camou age gap. The literature includes a number of articles on the use of background subtraction in identifying the moving entities. Parametric models are based on their underlying assump- tions; the appropriate parameter selection can be cumbersome and moreover, it may vary with different scene structures. On the other hand, non-parametric models are more reli- able, however, requires a long pixel history to estimate the underlying density function. Pixel-based methods usually apply the color feature to compare the pixel intensities at the same location over the frame sequence, whereas block- based methods consider the inter-pixel neighborhood char- acteristics, partition the image into several blocks, and apply both color and texture cues to decide the pixel behavior. The unimodal background outputs signi cant false positives in case of uninteresting waving motion that can be tackled by the multi-modal background at the price of higher space com- plexity. The recursive models update the model parameters in an iterative fashion, and thereby fast enough to deploy in real time applications. On the contrary, the non-recursive techniques store the recent pixel information inside a buffer to model the background. The latter one well adapts the gradual illumination variations at the cost of high memory overhead. III. CHALLENGES AND MITIGATIONS Moving object detection through background subtraction is usually challenged by a number of factors. The strength and weakness of any background subtraction algorithm are assessed by observing the ef ciency with which it coun- ters the challenges. In this section, we detail the possi- ble challenges along with their solutions proposed over the years. A. GRADUAL ILLUMINATION CHANGE Visibility of outdoor scenes is, in general, affected by the problem of slow illumination change. The sunlight illumi- nation varies gradually over the day and thereby inducing a deviation in the modeled background. As a result, faulty fore- ground pixels appear, even though, no real object movement has occurred. Earlier schemes apply the recursive paradigm to model the underlying background. Motion parameters, such as mean and variance, are recursively updated with each forthcoming background pixel. In this principle, only two parameters per location are required to model the entire scene. Such recursive update over a longer period may include the distant past pixel contribution. The so formed model parameters may get more skewed towards the old values. It may so happen that a true background pixel may signi cantly differ against such biased distribution. Such misclassi cation adversely impacts the ensuing decisions for a more drawn out period. The non-recursive methods, on the other hand, store only the recent pixel history in a nite buffer to model each location. The subsequent pixels are then compared with the 6136 VOLUME 4, 2016 S. K. Choudhury et al.: Evaluation of Background Subtraction for Object Detection Vis-a-Vis Mitigating Challenging Scenarios buffer statistics such as median, medoid etc, to take necessary decision. A forthcoming background pixel is included in the buffer as long as the over ow condition is not encountered, or replaced by an existing buffer value when its size is full. B. UNINTERESTING BACKGROUND OSCILLATION AND CAMERA SHAKE The waving of leaves, vacillating of banners, the uttering of ags, water stream in wellsprings are few real life instances which are usually independent of motions under considera- tion. In addition, the camera may also undergo motion due to external forces. Earlier background models are inherently unimodal as each background location is de ned either by a single value or by a nite range. As an alternative, multi-label background models are in use for the last one and half decade. In such modeling, multiple classes are assigned for the background coordinates so that the waving patterns can be sampled with- out any loss. The earlier multi-modal systems assign an equal number of classes for each pixel location. However, the waving pattern may not be uniform across the background. As a result, the equal class distribution, without any knowledge of scene structures, may fail to accommodate all possible variations. The recent state-of-the-art methods rst learn the oscillation periodicity for each pixel location with suf cient training samples, and then, assign the required number of classes across the model. C. SHADOW AND REFLECTION Shadow darkens the underlying region, whereas diffuse re ection brightens the scene visibility. In other words, a shadow can be interpreted as a scaled down value of lumi- nance, whereas diffuse illumination as a scaled up value. In either case, the resulting deviation yields unnecessary false positives. These problems are usually suffered by methods that are solely based on luminance features. In case of RGB color space, each of the channels is a linear combination of both luminance and chrominance component. The shadow removal strategy demands an invariant pixel representation that should be independent of luminance channel. In other words, the so formed pixel data structure should be a function of the chrominance measure only. There exist two different ways of addressing the shadow illumination issues. Chro- matic channels such as Hue from HSV, a, b from Lab, Cb, Cr from YCbCr color channel, and so forth, are applied to nullify the illumination factor. Another set of methods applies a battery of texture features that remain invariant to illumination. D. BOOTSTRAPPING In the best case, there would be no object movement at the observed scene during background modeling. Foreground movements during initialization create faulty classes in the developed model, known as the bootstrapping problem. Once, the foreground objects leave their location, the rearward background appears as non-stationary in rest of the frames. On the contrary, the same foreground object, or look-alike objects, when pass across the underlying region, may remain undetected due to the match with the faulty background class. Usually, the appearance frequency of a background class is taken into consideration to gure out the bootstrapping prob- lem. A moving object cannot remain stationary at the same location for a longer period. In other words, the appearance frequency of such faulty classes is very low as compared to that of a true background class. Once the background modeling is over, a suitable outlier labeling method can be applied to remove the low-frequency classes. E. BACKGROUND DISPLACEMENT A background model might change after initial training owing to various scenarios such as parking a vehicle, replacing an old refrigerator with a new one with varying shape and size, shifting chairs to another room etc. The above scenarios can be generalized into three different categories, as given below. Case 1: When a new background object is introduced into the scene, it is quite apparent that the number of objects in the foreground increases by one. Case 2: When an existing background object is removed, it creates a void space whose pixel distribution is not at par with the prevailing background, and thus introducing a new foreground object that actually does not exist. Case 3: The relocation of existing background objects could be interpreted as a combination of the above two cases. An existing background object is removed from its current position (Case 2) and placed at another location (Case 1), and creating an illusion of two new objects, which is in actual one object. The developed system should be robust enough to update such changed location as background. Usually, the visibility duration of each object is taken into consideration to alleviate this issue. In particular, a foreground object that remains stationary at a location for a longer time period will be incorporated into the background model. On the contrary, an existing background class that remains absent for suf cient frame period will be removed from the model. F. CAMOUFLAGE Foregrounds signi cantly deviate from the modeled back- ground in terms of their visual appearance. Most of the exist- ing schemes incorporate the intensity and color difference as a measure to identify the non-stationary pixels. However, color alone cannot solve the detection task. It may so happen that the color of a moving object may match to its rearward background. As a result, the pixel difference lies within the set threshold that yields false negatives. Recent schemes incorporate the texture or gradient information along with the color feature to strengthen its ability to detect the foreground objects more accurately. The only case when it may fail lies with same color and texture values at both foreground and background points; however, such scenarios are very rare. VOLUME 4, 2016 6137 S. K. Choudhury et al.: Evaluation of Background Subtraction for Object Detection Vis-a-Vis Mitigating Challenging Scenarios G. SUDDEN ILLUMINATION VARIATION Usually, the number of background pixels at any instant of time exceeds the number of foreground pixels. However, the rapid illumination variation alters the entire visibility of the scene, and maximum background pixels signi cantly deviate from their original value. The indoor illumination is usually disturbed by the lights on/off at times, whereas the outdoor environment uctuates in the presence of cloud. The simplest solution is to reinitialize the model as soon as the fast variation is observed. The usual background update strategy discussed in Section III-E can also be applied at the expense of few successive frames so that the model can be relabeled automatically. Detection result during this frame interval may be faulty; however, the outcome is more reliable at the expense of this small frame gap. Methods based on Gaussian mixture model update the background using a learning rate parameter. Another set of algorithms models the variation transformation as regression polynomial of probable background coordinates. IV. SIMULATION SETUP In our work, we simulate some state-of-the-art algorithms on several image sequences. The outcomes consequently acquired, are investigated and the ndings are summarized in the next section. In this section, we detail the standard datasets and various performance measures used for the evaluation. A. DATASETS Benchmark datasets alongside their ground-truth annotations are crucial for both qualitative and quantitative analysis of any algorithm. In this work, we simulate several image sequences collected from ve benchmark datasets namely, Wall ower, I2R, Carnegie Mellon, Change detection (CDW), and Background Models Challenge (BMC). The selected image sequences alongside the underlying challenges are listed in Table 1. Wall ower: We simulate ve out of eight image sequences from Wall ower dataset [29]. Besides, one ground-truth image is provided with each sequence, which is compared on a pixel-by-pixel basis. Camou age: In this video, a man strolls over the screen of a computer. The color of his shirt matches to the moving interlacing bars on the computer monitor. Towards the end, the person casts shadow on the side wall. Bootstrapping: This video is recorded inside a cafeteria. Foreground movements can be observed from the very rst frame of the sequence. TimeOfDay: The gradual variation of daylight illumination over a day is portrayed in the TimeOfDay sequence. The video demonstrates a moderately lit vacant room being brightened gradually and uncovering various items present in it. Towards the end, a man enters the room and sits on a couch. LightSwitch: The lights are initially off with no moving objects inside the room. After a while, a person comes, switches the light on, and leaves the room. TABLE 1. Selected videos and the underlying challenges. WavingTrees: It demonstrates an oscillating background that includes a person walking across a swaying tree. I2R: We simulate seven out of nine image sequences from I2R dataset [55]. Each sequence is again associated with 20 hand segmented images. Accordingly, the result is com- puted across these 20 ground-truth annotations unlike single ground-truth in Wall ower dataset. Lobby: This video is captured inside a room with ve light sources illuminating the scene. It can be observed that switching different lights on/off at various times alters the visibility of the prevailing background. Campus: This sequence delineates an open air scene where the waving trees yield uninteresting background movement. Fountain: Pedestrians are walking in front of a water foun- tain. Any detection algorithm should incorporate the water ow in the background model, otherwise, it results in false positives. WaterSurface: This is another instance of uninteresting background movement, where the sea waves, if not dealt properly, will give rise to signi cant false positives. Curtain: It portrays an instance of both camou age and waving background. The uttering curtain should be absorbed in the model. Furthermore, the disguise issue can 6138 VOLUME 4, 2016 S. K. Choudhury et al.: Evaluation of Background Subtraction for Object Detection Vis-a-Vis Mitigating Challenging Scenarios be observed owing to the attire similarity between the fore- ground to that of underlying background. Escalator: This video is recorded in a subway station. The pedestrians, as well as the escalators, are in motion from the very rst frame itself. It illustrates the issue of bootstrapping as well as background oscillation. In addition, the variation in light illumination can be observed over the time. Hall: It delineates pedestrian motion from the very rst frame alongside their cast shadow. Furthermore, the cam- ou age issue comes into picture owing to the similarity in pedestrian clothing with various parts of the background. Carnegie Mellon: This dataset [59] has only one image sequence that contains 500 TIF frames along with their hand segmented annotations. The scene is recorded with nominal camera movement, which is of 18 pixels on average. Background Models Challenge (BMC): This dataset is partitioned into three different categories namely, Learning mode, Synthetic videos and Real videos. In this work, we simulate six out of eight sequences from the Real videos. WonderingStudents: The bootstrapping movement is well re ected from the very rst frame of the sequence. BewareOfTrains: This sequence portrays a number of chal- lenges over the frames. The swaying leaves yield background oscillation, whereas the chromatic match between foreground cars and rearward background raises the camou age issue. Furthermore, the underlying shadow of the moving vehicle alters the scene appearance, and the trains journey, running steady running, appeals to update the back- ground model. BigTrucks: The truck is big in size, homogeneous in color, and very close to the camera. The interior pixels, while the truck is in motion, may appear as stationary due to pixel homogeneity across the neighborhood. Movement during ini- tialization, cast shadow, and the chromatic match between the truck and side wall are few other issues. RabbitInNight: The quantization noise in this low- resolution video appears as a continuous motion over the temporal sequence. In addition, the rapid change in light illumination and the shadow of the walking pedestrian are other concerns need to be taken care. Traf cDuringWindyDay: The camera shake and waving trees are the reasons for uninteresting background oscilla- tion, whereas the car movement during initialization raises the bootstrapping issue. Furthermore, the cloud movement changes the appearance of the prevailing view. TrainInTunnel: The bootstrapping motion of an individual and his shadow can be well visualized in this sequence. Change Detection.Net (CDW): This dataset is parti- tioned into eleven categories with several image sequences in each category [60], [61]. Furthermore, each video is associated with their hand segmented annotations. In some sequences, the ground-truth contains binary results from part of an image only rather than the entire frame; in those videos, the results are generated across the interest region only. In particular, we simulate thirteen videos from CDW dataset. Parking, WinterDriveAway: In both sequences, the cars are initially parked for some time and therefore behave as station- ary against the developed model. After a while, one person drives a car away from the scene and thereby creating a ghost space that may falsely appear as foreground. Furthermore, the cloud movement in the Parking sequence alters the scene visibility in terms of rapid varying illumination. Sofa: The primary focus in this video is a sofa and the various items placed on or near by it. Over the time, few background objects are either shifted to another location or taken away from the camera view. A background model should take care of all such movements. Backdoor, BusStation, Bungalows: Pedestrian motion in the rst and second sequences, and moving vehicle in the third sequence cast shadow that darkens the true intensity. Alongside, the waving tree in the rst video demands a multi- modal background. Canoe, Fountain01, Fountain02, Overpass: The river ow in the rst video, the fountain water in the second and third videos, and the waving trees in the fourth video need to be absorbed with the background. Sidewalk, Traf c: The camera oscillation in both sequences result in a dynamic background that is actually independent of motion under consideration. B. STATE-OF-THE-ART COMPARISON In our work, we simulate eleven state-of-the-art meth- ods for comparison analysis (i) Block-based classi er cascade with probabilistic decision integration (BCCPDI, [62]), (ii) P nder: real-time tracking of the human body (Pfinder, [9]), (iii) Fuzzy integral for moving object detection (FuzzyIntegral, [63]), (iv) Self organizing background subtraction (SOBS, [41]), (v) Improved adap- tive background mixture model (IABMM, [14]), (vi) Multi- layer background subtraction based on color and texture (MultiLayer, [64]), (vii) Pixel-based adaptive segmenter (PBAS, [65]), (viii) Fast principal component pursuit via alternating minimization (FPCP, [43]), (ix) GoDec: Random- ized low-rank & sparse matrix decomposition (GoDec, [44]), and two variants of ViBe [66]: (x) ViBeRGB, based on RGB color space and (xi) ViBeGray, based on gray color space. C. PERFORMANCE MEASURES Background subtraction can be interpreted as a binariza- tion process in which each pixel of the current frame can either be labeled as background (black) or foreground (white). The ef cacy of such algorithms are evaluated by computing the number of correctly identi ed motion pixels (true positives TP), the number of correctly labeled back- ground pixels (true negatives TN), the number of pixels that are incorrectly detected as foreground (false positives FP) or wrongly labeled as background (false negatives FN). A con- fusion matrix is created using these four parameters as shown in Table 2. The following seven evaluation metrics are computed using the above four parameters. VOLUME 4, 2016 6139 S. K. Choudhury et al.: Evaluation of Background Subtraction for Object Detection Vis-a-Vis Mitigating Challenging Scenarios TABLE 2. Confusion matrix for background subtraction. PCC, percentage of correct classi cation, is the proportion of correctly detected pixels over total image pixels under consideration. PCC = TP + TN TP + TN + FP + FN 100 (1) Speci city, known as true negative rate, measures the percentage of correctly identi ed negative samples over the actual number of negatives present in the ground-truth. Speci city = TN TN + FP 100 (2) False Positive Rate (FPR), known as fall-out, outputs the proportion of false positives that are retrieved by any algorithm, out of the total number of negative samples in the ground-truth. FPR = FP FP + TN 100 (3) False Negative Rate (FNR), known as miss rate, outputs the proportion of false negatives that are retrieved by any algorithm over the total number of positive samples in the ground-truth. FNR = FN FN + TP 100 (4) Recall, known as detection rate, is the ratio of number of true positives to the total number of positive examples annotated in the ground-truth. Recall = TP TP + FN 100 (5) Precision, known as positive prediction, is the ratio of num- ber of true positives to the total number of foreground pixels detected by any algorithm. Precision = TP TP + FP 100 (6) F1 measure, known as gure of merit, is the harmonic mean of Precision and Recall. Higher is the score, better is the ef cacy. F1 = 2 Precision Recall Precision + Recall (7) These seven measures lay the basis of our analysis that we discuss in the next section. V. RESULTS AND DISCUSSIONS We simulate eleven state-of-the-art algorithms (listed in Section IV-B) on 34 image sequences collected from ve benchmark datasets (enumerated in Section IV-A). The tab- ular results of the above seven evaluation metrics are listed in Tables 3, 4, 5, 6, 7, 8, and 9. Furthermore, the obtained binary images and video results of the test sequences are uploaded at https://sites.google.com/site/soaBSevaluation. For the readers perusal, the variation in results distribu- tion over the simulated image sequences are compared in Figures 3, 4, 5, 6, 7, 8, and 9 for all state-of-the-art algo- rithms; the vertical red bar demonstrates the results varia- tion across the simulated videos, whereas a green rectangle in each red bar depicts the average performance of the corresponding approach. The following paragraphs sum- marize the in-depth analysis of the obtained results with the perspective of various challenges and performance metrics. A. ANALYSIS FROM CHALLENGE PERSPECTIVE 1) ANALYSIS ON BACKGROUND DISPLACEMENT We simulate three image sequences that depict the back- ground relocation scenario, namely Parking, Sofa, and Win- terDriveAway. It has been observed that SOBS, FPCP, GoDec, and BCCPDI have good recall rate whereas IABMM, Multilayer, Pfinder, VibeRGB, VibeGray have good precision rate. Background displacement strongly depends on the scene under observation. It is quite imprac- tical to set a prede ned threshold of time beyond which all stationary foregrounds can be absorbed into the background. On the contrary, it is very tough to strict an absence duration threshold beyond which an existing background class will be removed from the developed model. These two parameters have to be varied with respect to the underlying environment. The scene knowledge along with the information of possible stationary objects have to be learned to reduce such false alarms. 2) ANALYSIS ON BOOTSTRAPPING Both Bootstrapping and WonderingStudents videos well re ect the bootstrapping scenario. All methods except IABMM have satisfactory output. Bootstrapping can be con- sidered as a special case of background relocation wherein the knowledge of possible objects, their size, average halt dura- tion etc, have to be learned over the initialization sequence to remove the faulty background classes from the developed model. 3) ANALYSIS ON CAMERA SHAKE Camera oscillation can be observed in Boulevard, SideWalk, Traf c, and Carnegie Mellon sequences. IABMM has very poor recall rate, whereas the Pfinder, FPCP, GoDec marginally drop the precision rate. The oscillation periodicity owing to camera-shake needs to be learned with suf cient initialization frames. 6140 VOLUME 4, 2016 S. K. Choudhury et al.: Evaluation of Background Subtraction for Object Detection Vis-a-Vis Mitigating Challenging Scenarios TABLE 3. Comparative analysis of percentage of correct classification (PCC). TABLE 4. Comparative analysis of specificity. VOLUME 4, 2016 6141 S. K. Choudhury et al.: Evaluation of Background Subtraction for Object Detection Vis-a-Vis Mitigating Challenging Scenarios TABLE 5. Comparative analysis of false positive rate. TABLE 6. Comparative analysis of false negative rate. 6142 VOLUME 4, 2016 S. K. Choudhury et al.: Evaluation of Background Subtraction for Object Detection Vis-a-Vis Mitigating Challenging Scenarios TABLE 7. Comparative analysis of recall. TABLE 8. Comparative analysis of precision. VOLUME 4, 2016 6143 S. K. Choudhury et al.: Evaluation of Background Subtraction for Object Detection Vis-a-Vis Mitigating Challenging Scenarios TABLE 9. Comparative analysis of F1-score. FIGURE 3. PCC Distribution (%) across various methods. 4) ANALYSIS ON CAMOUFLAGE The attire similarity between the foreground and background can be observed in the Camou age and Curtain sequence. All methods except IABMM, FPCP, GODec possess com- paratively better output. Complementary cues, i.e, texture features along with color cues need to be incorporated to tackle this disguise issue. In addition, the morphological processing and other low pass ltering can be applied as a post improvisation module to minimize the camou age gap. 5) ANALYSIS ON GRADUAL ILLUMINATION VARIATION The varying sunlight illumination, over the time, can be seen in the TimeOfDay sequence. Multilayer is the only method that produces acceptable results. Recursive models often fail to tackle such eventual variations because their underlying model parameters are skewed towards the long past data. On the other hand, non-recursive methods ef - ciently handle the problem at the cost of high memory over- head in terms of a nite buffer at each pixel location. 6144 VOLUME 4, 2016 S. K. Choudhury et al.: Evaluation of Background Subtraction for Object Detection Vis-a-Vis Mitigating Challenging Scenarios FIGURE 4. Specificity Distribution (%) across various methods. FIGURE 5. False positive rate Distribution (%) across various methods. 6) ANALYSIS ON SUDDEN ILLUMINATION VARIATION The rapid variation in illumination can be observed in the Lobby and LightSwitch sequence. To the best of our knowl- edge, the literature still lags any immediate foolproof solution to tackle such rapid variation. MultiLayer and BCCPDI produce comparatively better results. Such rapid variation completely alters the color and intensity characteristics of the underlying scene. One time-consuming yet reliable solution is to re-initialize the model as soon as such rapid variation is observed. The usual background update strategy also adapts the changed pixel values in the model with few successive frames. 7) ANALYSIS ON SHADOW Shadow effect can be visualized in the Backdoor, Bunga- low, and BusStation sequences. BCCPDI and Multilayer ef ciently suppress the shadow illumination as compared to other algorithms. Shadow is the scaled down value of illu- mination. Methods based on RGB or gray color space miss- classify shadow as foreground. Gradient or texture features along with invariant color models are suitable candidates to counter this phenomenon. 8) ANALYSIS ON UNINTERESTING BACKGROUND OSCILLATION We simulate eight sequences (four from CDW, three from I2R, one from Wall ower) that demonstrate various real world instances of background oscillation. BCCPDI has the most promising detection rate over others. Unimodal methods fail to incorporate dynamic background in the model. Multi- modal systems usually assign equal number of classes, and therefore fail in situations, where the waving periodicity dif- fers across the scene. The obvious strategy is to learn sample variation of pixel sequence at each location to determine the oscillation periodicity. Then, a suitable clustering method can distribute the input sequence into the required number of classes. B. ANALYSIS FROM METRIC PERSPECTIVE The above analyses are useful in evaluating an algorithm against an underlying challenge. However, a desired algo- rithm should be capable of countering many challenges at a time. In our simulation, we have considered those image sequences that depict more than one challenge simultaneously. The overall results distribution of each VOLUME 4, 2016 6145 S. K. Choudhury et al.: Evaluation of Background Subtraction for Object Detection Vis-a-Vis Mitigating Challenging Scenarios FIGURE 6. False negative rate Distribution (%) across various methods. FIGURE 7. Recall Distribution (%) across various methods. simulated algorithm on the image sequences, in terms of the seven evaluation metrics, are summarized below. 1) Correct classi cation rate: PCC measures the percent- age of correctly detected samples over the entire sample space. Almost all methods possess high PCC as shown in Figure 3. 2) Speci city rate: Speci city measures the proportion of correctly detected background pixels out of total background pixels present in the ground-truth. It can be realized from Figure 4 that all methods except FPCP and GoDec have an average of more than 90% speci- city rate. The variation in results across the image sequences is minimum (the average speci city rate is maximum and reliable) in the case of IABMM followed by MultiLayer and BCCPDI. Usually, the num- ber of foreground pixels in any frame is very less as compared to that of the background pixels. Thereby, the ratio of true negatives to the total negatives will be obviously very high unless the underlying scene is highly multi-modal or the foreground density is very high. Therefore, this metric is not very much well suited to rank the simulated methods. 3) False positive rate: It measures the proportion of incor- rectly labeled foreground pixels out of the total back- ground samples available in the ground-truth. False positives in the case of background subtraction occur due to an oscillating background, bootstrapping move- ment, relocating stationary objects, varying illumina- tion, shadow impression etc. The distribution of false positive rate is plotted in Figure 5. All methods except FPCP and GoDec have comparatively low false pos- itive rate. Again, the variation in the distribution is minimal in case of IABMM, MultiLayer and BCCPDI. 4) False negative rate: It measures the percentage of incor- rectly labeled background pixels out of the total fore- ground samples in the binary ground-truth. Almost all methods have a high false negative rate that can be visualized in Figure 6. The disguise issue owing to attire similarity between the foreground and rearward background yields signi cant false negatives. Another major factor is the selection of deviation threshold that acts as a decision boundary between the fore- ground and background regions; a large threshold may 6146 VOLUME 4, 2016 S. K. Choudhury et al.: Evaluation of Background Subtraction for Object Detection Vis-a-Vis Mitigating Challenging Scenarios FIGURE 8. Precision Distribution (%) across various methods. FIGURE 9. F1 Score Distribution (%) across various methods. incorrectly include the foreground pixels in the back- ground regions resulting in false negatives. 5) Recall rate: Recall measures the percentage of true foreground pixels detected by any algorithm over the actual collection of foregrounds in the ground-truth. The Recall distribution is depicted in Figure 7. FPCP, BCCPDI, and SOBS have comparatively better results over their counterparts. On the contrary, the average recall rate in case of IABMM is even less than 50%. 6) Precision rate: It measures the percentage of correctly labeled foreground samples out of all positive samples detected by any algorithm. The corresponding distri- bution result is shown in Figure 8. MultiLayer and IABMM have maximum recall rate, whereas FPCP, GoDec, SOBS possess the least among others. 7) F1 Score distribution: Neither Recall nor Precision alone may accurately measure the ef ciency of the sim- ulated algorithms; rather their combination is a better choice to select the superior methods. The distribution of F1 Score, plotted in Figure 9, ranks BCCPDI and MultiLayer as the two best methods. C. PARAMETER SELECTION One major concern in background subtraction is the choice of appropriate parameters. The effect of various parameters applied across different phases of background subtraction is enumerated below. 1) Accurate modeling of a scene is directly related to the number of frames (say M) adapted to create the background model. The parameter M should be varied depending on the scene structure. Higher is the fore- ground density or bootstrapping movements, the more is the number of initialization frames required to cap- ture all background locations precisely. Furthermore, the periodicity of waving background at all locations as well as the camera oscillation need to be learned with suf cient training frames that are again directly relative to the number of initialization frames M. 2) Foreground extraction phase requires detailed knowl- edge of the size, speed, halt duration of pos- sible mobile objects to formulate an appropri- ate deviation threshold to separate the foreground pixels. VOLUME 4, 2016 6147 S. K. Choudhury et al.: Evaluation of Background Subtraction for Object Detection Vis-a-Vis Mitigating Challenging Scenarios 3) Background maintenance phase requires the scene knowledge to decide (i) the immobile duration thresh- old beyond which a foreground object will be absorbed in the model, and (ii) the absence duration threshold beyond which an existing background will be removed from the model. 4) The shadow illumination may increase or decease depending on the intensity of source illumination. Accordingly, the shadow removal threshold should be modeled as a function of light illumination rather than a constant value. 5) Another major concern is the temporal buffer length in case of non-recursive modeling that holds the recent pixel history. A small sized buffer may fail to appro- priately model a background location. On the contrary, a large-sized buffer may include the long past obser- vation that may result in false negatives together with higher memory overhead. VI. CONCLUSION This paper includes a detailed evaluation of various back- ground subtraction framework for detecting objects moving across a scene. The principles adopted by the reported meth- ods for mathematical modeling is taken into consideration to classify their evolution: parametric vs. non-parametric model, unimodal vs. multi-modal background, pixel-based vs. region-based segmentation, recursive paradigm vs. non- recursive architecture etc. We have enumerated the possible challenges that come into picture during background sub- traction along with their varying mitigation strategies over the years. Some of the state-of-the-art methods are simulated on thirty-four benchmark image sequences, in which each sequence portrays either a single challenge or a number of challenges at a time. A set of seven benchmark evaluation measures is selected to compare the output sequence with the supplied ground-truth. The variation in result distribution across the simulated image sequences gives an idea to select the suitable method depending upon the requirement. The underlying scene knowledge along with more prior details regarding the foreground movements and available background objects are very much helpful in formulating (1) the absence duration beyond which the existing back- ground class(es) will be removed from the background model, and (2) the appearance interval after which an immo- bile foreground will be relabeled as background. The varying oscillating pattern across the background (at each model loca- tion) has to be learned with suf cient initialization frames to address the uninteresting background movement and camera motion. Complementary texture details need to be incorpo- rated along with invariant color features to tackle the prob- lem with camou age and shadow illumination. Finite queue (with recently accessed background pixels only) driven non- recursive background modeling is proven effective to cope with slow varying sunlight illumination. No solution to the problem of rapid light illumination variation, to the best of our knowledge, is found in the literature; however, the back- ground update automatically reinitializes the model at the cost of few successive frame delays. It can be observed that the false positive rate is very low for the simulated methods, however, such attempt sometimes substantially increases the false negative rate. It can be realized that the selection of appropriate parameters at each stage of background subtrac- tion demands the prior scene knowledge and more infor- mation regarding the possible stationary and non-stationary movements. REFERENCES [1] I. Setitra and S. Larabi, Background subtraction algorithms with post- processing: A review, in Proc. Int. Conf. Pattern Recognit., 2014, pp. 2436 2441. [2] A. Sobral and A. Vacavant, A comprehensive review of background subtraction algorithms evaluated with synthetic and real videos, Comput. Vis. Image Understand., vol. 122, pp. 4 21, May 2014. [3] A. Elgammal, Background subtraction: Theory and practice, Synthesis Lectures Comput. Vis., vol. 5, no. 1, pp. 1 83, 2014. [4] K. K. Hati, P. K. Sa, and B. Majhi, Intensity range based background subtraction for effective object detection, IEEE Signal Process. Lett., vol. 20, no. 8, pp. 759 762, Aug. 2013. [5] S. Brutzer and B. H ferlin, and G. Heidemann, Evaluation of back- ground subtraction techniques for video surveillance, in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., Jun. 2011, pp. 1937 1944. [6] T. Bouwmans, Recent advanced statistical background modeling for foreground detection A systematic survey, Recent Patents Comput. Sci., vol. 4, no. 3, pp. 147 176, Sep. 2011. [7] A. H. S. Lai and N. H. C. Yung, A fast and accurate scoreboard algorithm for estimating stationary backgrounds in an image sequence, in Proc. Int. Symp. Circuits Syst., vol. 4. May 1998, pp. 241 244. [8] I. Haritaoglu, D. Harwood, and L. Davis, W/sup 4/: Real-time surveil- lance of people and their activities, IEEE Trans. Pattern Anal. Mach. Intell., vol. 22, no. 8, pp. 809 830, Aug. 2000. [9] C. R. Wren, A. Azarbayejani, T. Darrell, and A. Pentland, P nder: Real- time tracking of the human body, IEEE Trans. Pattern Anal. Mach. Intell., vol. 19, no. 7, pp. 780 785, Jul. 1997. [10] C. Stauffer and W. E. L. Grimson, Adaptive background mixture models for real-time tracking, in Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit., vol. 2. Jun. 1999, pp. 246 252. [11] C. Stauffer and W. E. L. Grimson, Learning patterns of activity using real-time tracking, IEEE Trans. Pattern Anal. Mach. Intell., vol. 22, no. 8, pp. 747 757, Aug. 2000. [12] Z. Zivkovic, Improved adaptive Gaussian mixture model for background subtraction, in Proc. 17th Int. Conf. Pattern Recognit., vol. 2. 2004, pp. 28 31. [13] Z. Zivkovic and F. van der Heijden, Ef cient adaptive density estimation per image pixel for the task of background subtraction, Pattern Recognit. Lett., vol. 27, no. 7, pp. 773 780, 2006. [14] P. KaewTraKulPong and R. Bowden, An improved adaptive back- ground mixture model for real-time tracking with shadow detection, in Video-Based Surveillance Systems. New York, NY, USA, Springer, 2002, pp. 135 144. [15] K. Kim, T. H. Chalidabhongse, D. Harwood, and L. Davis, Real-time foreground-background segmentation using codebook model, Real- Time Imag., vol. 11, no. 3, pp. 172 185, Jun. 2005. [16] M. Wu and X. Peng, Spatio-temporal context for codebook-based dynamic background subtraction, AEU Int. J. Electron. Commun., vol. 64, no. 8, pp. 739 747, Aug. 2010. [17] E. Fernandez-Sanchez, J. Diaz, and E. Ros, Background subtraction based on color and depth using active sensors, Sensors, vol. 13, no. 7, pp. 8895 8915, 2013. [18] M. Heikkil and M. Pietik inen, A texture-based method for modeling the background and detecting moving objects, IEEE Trans. Pattern Anal. Mach. Intell., vol. 28, no. 4, pp. 657 662, Apr. 2006. [19] J. M. McHugh, J. Konrad, V. Saligrama, and P.-M. Jodoin, Foreground- adaptive background subtraction, IEEE Signal Process. Lett., vol. 16, no. 5, pp. 390 393, May 2009. [20] R. O. Duda, P. E. Hart, and D. G. Stork, Pattern Classi cation, 2nd ed. New York, NY, USA: Wiley, 2000. 6148 VOLUME 4, 2016 S. K. Choudhury et al.: Evaluation of Background Subtraction for Object Detection Vis-a-Vis Mitigating Challenging Scenarios [21] A. Elgammal, D. Harwood, and L. Davis, Non-parametric model for background subtraction, in Proc. Eur. Conf. Comput. Vis., 2000, pp. 751 767. [22] M. Piccardi and T. Jan, Mean-shift background image modelling, in Proc. Int. Conf. Image Process., vol. 5. Oct. 2004, pp. 3399 3402. [23] A. Elgammal, R. Duraiswami, and L. S. Davis, Ef cient non-parametric adaptive color modeling using fast gauss transform, in Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit., vol. 2. Dec. 2001, pp. 563 570. [24] A. Mittal and N. Paragios, Motion-based background subtraction using adaptive kernel density estimation, in Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit., vol. 2, Jun./Jul. 2004, pp. 302 309. [25] T. Parag, A. Elgammal, and A. Mittal, A framework for feature selection for background subtraction, in Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit., vol. 2. 2006, pp. 1916 1923. [26] B. P. L. Lo and S. A. Velastin, Automatic congestion detection system for underground platforms, in Proc. IEEE Int. Symp. Intell. Multimedia, Video Speech Process., May 2001, pp. 158 161. [27] R. Cucchiara, C. Grana, M. Piccardi, and A. Prati, Detecting moving objects, ghosts, and shadows in video streams, IEEE Trans. Pattern Anal. Mach. Intell., vol. 25, no. 10, pp. 1337 1342, Oct. 2003. [28] S. Calderara, R. Melli, A. Prati, and R. Cucchiara, Reliable background suppression for complex scenes, in Proc. ACM Int. Multimedia Conf. Exhibit., 2006, pp. 211 214. [29] K. Toyama, J. Krumm, B. Brumitt, and B. Meyers, Wall ower: Prin- ciples and practice of background maintenance, in Proc. 7th IEEE Int. Conf. Comput. Vis., vol. 1. Sep. 1999, pp. 255 261. [30] J. Zhong and S. Sclaroff, Segmenting foreground objects from a dynamic textured background via a robust Kalman lter, in Proc. IEEE Int. Conf. Comput. Vis., vol. 1. Oct. 2003, pp. 44 50. [31] N. M. Oliver, B. Rosario, and A. P. Pentland, A Bayesian computer vision system for modeling human interactions, IEEE Trans. Pattern Anal. Mach. Intell., vol. 22, no. 8, pp. 831 843, Aug. 2000. [32] H. Wang and D. Suter, Background subtraction based on a robust con- sensus method, in Proc. Int. Conf. Pattern Recognit., vol. 1. Aug. 2006, pp. 223 226. [33] H. Wang and D. Suter, A consensus-based method for tracking: Modelling background scenario and foreground appearance, Pattern Recognit., vol. 40, no. 3, pp. 1091 1105, Mar. 2007. [34] T. Bouwmans, Background subtraction for visual surveillance: A fuzzy approach, Handbook Soft Comput. Video Surveill., 2012, pp. 103 134. [35] Z. Hongxun and X. De, Fusing color and texture features for background model, Fuzzy Syst. Knowl. Discovery (Lecture Notes in Computer Science), vol. 4223. Berlin, Germany, Springer, 2006, pp. 887 893, doi: 10.1007/11881599_110 [36] F. El Baf, T. Bouwmans, and B. Vachon, Foreground detection using the Choquet integral, in Proc. Int. Workshop Image Anal. Multimedia Interact. Services, May 2008, pp. 187 190. [37] T. Bouwmans and F. El Baf, Modeling of dynamic backgrounds by type- 2 fuzzy Gaussians mixture models, MASAUM J. Basic Appl. Sci., vol. 1, no. 2, pp. 265 276, 2009. [38] F. El Baf, T. Bouwmans, and B. Vachon, Fuzzy statistical modeling of dynamic backgrounds for moving object detection in infrared videos, in Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. Workshops, Jun. 2009, pp. 60 65. [39] W. Kim and C. Kim, Background subtraction for dynamic texture scenes using fuzzy color histograms, IEEE Signal Process. Lett., vol. 19, no. 3, pp. 127 130, Mar. 2012. [40] D. Culibrk, O. Marques, D. Socek, H. Kalva, and B. Furht, Neural network approach to background modeling for video object segmentation, IEEE Trans. Neural Netw., vol. 18, no. 6, pp. 1614 1627, Nov. 2007. [41] L. Maddalena and A. Petrosino, A self-organizing approach to back- ground subtraction for visual surveillance applications, IEEE Trans. Image Process., vol. 17, no. 7, pp. 1168 1177, Jul. 2008. [42] L. Maddalena and A. Petrosino, A fuzzy spatial coherence-based approach to background/foreground separation for moving object detec- tion, Neural Comput. Appl., vol. 19, no. 2, pp. 179 186, 2010. [43] P. Rodriguez and B. Wohlberg, Fast principal component pursuit via alternating minimization, in Proc. IEEE Int. Conf. Image Process., Sep. 2013, pp. 69 73. [44] T. Zhou and D. Tao, GoDec: Randomized low-rank & sparse matrix decomposition in noisy case, in Proc. 28th Int. Conf. Mach. Learn., Jun. 2011, pp. 33 40 [45] J. Wright, A. Ganesh, S. Rao, Y. Peng, and Y. Ma, Robust principal component analysis: Exact recovery of corrupted low-rank matrices via convex optimization, in Proc. Adv. Neural Inf. Process. Syst., 2009, pp. 2080 2088. [46] X. Zhou, C. Yang, and W. Yu, Moving object detection by detecting contiguous outliers in the low-rank representation, IEEE Trans. Pattern Anal. Mach. Intell., vol. 35, no. 3, pp. 597 610, Mar. 2013. [47] C. Qiu and N. Vaswani, Real-time robust principal components pursuit, in Proc. 48th Annu. Allerton Conf. Commun. Control Comput. (Allerton), Sep./Oct. 2010, pp. 591 598. [48] A. Sobral, C. G. Baker, T. Bouwmans, and E.-H. Zahzah, Incremen- tal and multi-feature tensor subspace learning applied for background modeling and subtraction, in Proc. Int. Conf. Image Anal. Recognit., Oct. 2014, pp. 94 103. [49] S. Javed, S. H. Oh, A. Sobral, T. Bouwmans, and S. K. Jung, OR-PCA with mrf for robust foreground detection in highly dynamic backgrounds, in Proc. Asian Conf. Comput. Vis., Nov. 2014, pp. 284 299. [50] L. Li, P. Wang, Q. Hu, and S. Cai, Ef cient background modeling based on sparse representation and outlier iterative removal, IEEE Trans. Circuits Syst. Video Technol., vol. 26, no. 2, pp. 278 289, Feb. 2016. [51] T. Horprasert, D. Harwood, and L. S. Davis, A statistical approach for real-time robust background subtraction and shadow detection, in Proc. Int. Conf. Comput. Vis., vol. 99, 1999, pp. 1 19. [52] W. Zhang, X. Z. Fang, and X. Yang, Moving cast shadows detection based on ratio edge, in Proc. Int. Conf. Pattern Recognit., vol. 4. Aug. 2006, pp. 73 76. [53] S. J. McKenna, S. Jabri, Z. Duric, A. Rosenfeld, and H. Wechsler, Tracking groups of people, Comput. Vis. Image Understand., vol. 80, no. 1, pp. 42 56, Oct. 2000. [54] H. Wang and D. Suter, A re-evaluation of mixture of Gaussian back- ground modeling, in Proc. IEEE Int. Conf. Acoust., Speech Signal Process., vol. 2. Mar. 2005, pp. 1017 1020. [55] L. Li, W. Huang, I. Gu, and Q. Tian, Foreground object detection from videos containing complex background, in Proc. ACM Int. Multimedia Conf. Exhibit., 2003, pp. 2 10. [56] I. Huerta, M. Holte, T. Moeslund, and J. Gonz lez, Detection and removal of chromatic moving shadows in surveillance scenarios, in Proc. Int. Conf. Comput. Vis., Sep./Oct. 2009, pp. 1499 1506. [57] Q. Zhou and J. K. Aggarwal, Tracking and classifying moving objects from video, in Proc. IEEE Workshop Perform. Eval. Tracking Surveill., Honolulu, HI, USA, 2001. [58] D. H. Parks and S. S. Fels, Evaluation of background subtraction algo- rithms with post-processing, in Proc. Int. Conf. Adv. Video Signal Based Surveill., Sep. 2008, pp. 192 199. [59] Y. Sheikh and M. Shah, Bayesian modeling of dynamic scenes for object detection, IEEE Trans. Pattern Anal. Mach. Intell., vol. 27, no. 11, pp. 1778 1792, Nov. 2005. [60] N. Goyette, P.-M. Jodoin, F. Porikli, J. Konrad, and P. Ishwar, Changede- tection.net: A new change detection benchmark dataset, in Proc. Com- put. Vis. Pattern Recognit. Workshops (CVPRW), Jun. 2012, pp. 1 8. [61] Y. Wang, P.-M. Jodoin, F. Porikli, J. Konrad, Y. Benezeth, and P. Ishwar, CDnet 2014: An expanded change detection benchmark dataset, in Proc. Comput. Vis. Pattern Recognit. Workshops, 2014, pp. 387 394. [62] V. Reddy, C. Sanderson, and B. C. Lovell, Improved foreground detec- tion via block-based classi er cascade with probabilistic decision integra- tion, IEEE Trans. Circuits Syst. Video Technol., vol. 23, no. 1, pp. 83 93, Jan. 2013. [63] F. El Baf, T. Bouwmans, and B. Vachon, Fuzzy integral for mov- ing object detection, in Proc. IEEE Int. Conf. Fuzzy Syst., Jun. 2008, pp. 1729 1736. [64] J. Yao and J.-M. Odobez, Multi-layer background subtraction based on color and texture, in Proc. Conf. Comput. Vis. Pattern Recognit., Jun. 2007, pp. 1 8. [65] M. Hofmann, P. Tiefenbacher, and G. Rigoll, Background segmentation with feedback: The pixel-based adaptive segmenter, in Proc. Comput. Vis. Pattern Recognit. Workshops, Jun. 2012, pp. 38 43. [66] O. Barnich and M. Van Droogenbroeck, ViBe: A universal background subtraction algorithm for video sequences, IEEE Trans. Image Process., vol. 20, no. 6, pp. 1709 1724, Jun. 2011. VOLUME 4, 2016 6149 S. K. Choudhury et al.: Evaluation of Background Subtraction for Object Detection Vis-a-Vis Mitigating Challenging Scenarios SUMAN KUMAR CHOUDHURY received the M.Tech. degree from the National Institute of Technology Rourkela, India, in 2013. He is cur- rently pursuing the Ph.D. degree in computer vision with the Department of Computer Science and Engineering, National Institute of Technol- ogy Rourkela. His research interest includes video surveillance, image processing, and pattern recog- nition. His excellence in research has brought him laurels from the academia. PANKAJ K. SA received the Ph.D. degree in Com- puter Science in 2010. He is currently serving as an Assistant Professor with the Department of Com- puter Science and Engineering, National Institute of Technology Rourkela, India. His research inter- est includes computer vision, biometrics, visual surveillance, and robotic perception. He has co- authored a number of research articles in various journals, conferences, and book chapters. He has co-investigated some Research and Development projects that are funded by SERB, DRDO-PXE, DeitY, and ISRO. He is the recipient of prestigious awards and honors for his excellence in academics and research. Apart from research and teaching, he conceptualizes and engineers the process of institutional automation. SAMBIT BAKSHI received the Ph.D. degree in computer science in 2015. He is currently with the Centre for Computer Vision and Pattern Recogni- tion, National Institute of Technology Rourkela, India. He also serves as an Assistant Profes- sor with the Department of Computer Science and Engineering, National Institute of Technol- ogy Rourkela. He serves as an Associate Editor of International Journal of Biometrics (2013). He is a Technical Committee Member of the IEEE Computer Society Technical Committee on Pattern Analysis and Machine Intelligence. He received the Prestigious Innovative Student Projects Award- 2011 from Indian National Academy of Engineering for his master s thesis. He has more than 30 publications in journals, reports, and conferences. BANSHIDHAR MAJHI is a Professor with the Department of Computer Science and Engineer- ing, National Institute of Technology Rourkela, India. He has successfully executed various Research and Development projects being funded by agencies, such as MHRD, ISRO, DRDO, and DeitY. He has authored hundreds of articles in reputed journals and conferences. His current research interests include image processing, com- puter vision, biometric security, and pattern recog- nition. He has been conferred with prestigious awards and honors for his contribution toward scienti c research and academic excellence. 6150 VOLUME 4, 2016