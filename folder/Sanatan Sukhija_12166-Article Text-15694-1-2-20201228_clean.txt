Label Space Driven Heterogeneous Transfer Learning with Web Induced Alignment Sanatan Sukhija Department of Computer Science and Engineering Indian Institute of Technology Ropar, Punjab, India sanatan@iitrpr.ac.in Abstract Heterogeneous Transfer Learning (HTL) algorithms lever- age knowledge from a heterogeneous source domain to per- form a task in a target domain. We present a novel HTL al- gorithm that works even where there are no shared features, instance correspondences and further, the two domains do not have identical labels. We utilize the label relationships via web-distance to align the data of the domains in the pro- jected space, while preserving the structure of the original data. Introduction Traditional supervised algorithms require suf cient labeled data to learn a computational model for making reliable pre- dictions. Often, obtaining labeled training data is expensive and time consuming. Transfer learning algorithms overcome this limitation for a target domain by leveraging labeled knowledge from related domains (often termed as the source domains) that can have different distributions, different fea- ture spaces and even different label spaces (Sukhija, Krish- nan, and Singh 2016). Knowledge transfer between domains with heterogeneous feature spaces is widely known as Het- erogeneous Transfer Learning (HTL). As the feature spaces are heterogeneous, the rst task of any HTL approach is to identify a common feature space for the source and target domain that can be used for adaptation. Based on this com- mon space, the HTL literature can be categorized into two groups. The HTL methods belonging to the rst group (also known as Feature Space Remapping (FSR) methods) learn a single transformation that maps source features to target features. With the help of this learned transformation, the data from the source domain can be projected to the target domain and vice-versa. The approaches associated with the second group (also known as Latent Space Transformation (LST) methods) learn a pair of mappings, one for each do- main, to project the data onto a shared subspace for adapta- tion. The second task is to bridge the gap between the data differences that arise when the data from both the domains is projected onto the common space. For compensating these differences, some shared information can be leveraged to maximize the similarity of the source and target domain Copyright c 2018, Association for the Advancement of Arti cial Intelligence (www.aaai.org). All rights reserved. data in the common space. The shared information can be present in the form of instance correspondences, overlapping features, shared label space, common meta-features/latent space or any task speci c/independent information. Some HTL approaches also leverage domain-speci c knowledge by utilizing external sources such as oracles/dictionary, so- cial media or web to reduce the domain differences. The proposed algorithm utilizes the inter-label space se- mantic similarities to improve the joint alignment of the data from the source and target domains in the common space. Our approach is motivated by the cross-domain ac- tivity recognition task where the label spaces are semanti- cally related. Learning a robust activity recognition model requires manually annotating large amounts of sensor ob- servations, which is an expensive task. Cross-domain activ- ity recognition leverages labeled data from existing smart homes to a target smart home to circumvent the annotation effort. The presence of different sensor modalities across the different smart home layouts leads to heterogeneous feature spaces. Differences in the daily routine of the residents in different smart homes results in differences in the marginal and conditional distribution of the data. However, the daily activities of different smart home residents lead to seman- tically related label spaces. The proposed approach learns a mapping between the heterogeneous sensors to enable knowledge transfer. We conducted experiments to test the transfer ef ciency of our proposed approach on three single- resident smart homes from the CASAS datasets (Cook et al. 2013). Proposed approach The primary limitation with state-of-the-art LST approaches is that they rely on implicit feature relationships to nd a lower dimensional optimal shared subspace that will be used for adaptation. Learning the optimal shared subspace is an expensive task as it involves a grid-search on the dimension of the shared subspace. In order to circumvent this inherent limitation of LST approaches, the proposed HTL algorithm is conceived as a FSR minimization objective that bridges the heterogeneous feature and output spaces of the source and target domains without relying on instance or feature correspondences. Our novel approach, Label Space Driven Heterogeneous Transfer Learning with Web Induced Align- ment (LSDHTL-WIA), aligns the data from the source and The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18) 8165 Table 1: Transfer performance comparison of state-of-the-art algorithms is shown in terms of mean error and standard deviation (%) over 4 folds. The best performance has been highlighted in bold. CASAS horizon house datasets Baseline Results Transfer Results S T BRF SVM ECOC SHFR ECOC HFA HeMap SHDA-RF SHFR-RF Co-HTL LSDHTL WIA hh102 hh113 29.67 2.51 34.58 2.20 28.25 2.86 36.31 2.35 35.51 3.50 26.37 2.23 27.98 2.51 28.32 2.38 25.19 2.49 hh102 hh118 36.41 2.42 43.51 3.01 33.14 2.82 32.99 3.41 41.08 3.33 31.50 2.01 32.01 2.09 31.09 2.66 28.30 2.80 hh113 hh102 36.70 1.95 41.23 2.93 31.58 2.41 41.28 2.65 38.05 3.71 29.88 1.76 32.81 2.38 33.52 2.47 28.97 2.78 hh113 hh118 32.35 2.56 39.41 1.89 31.02 2.16 38.52 2.07 38.04 2.68 28.0 2.02 30.15 2.44 30.06 1.97 27.28 2.46 hh118 hh102 38.95 2.57 41.80 2.21 36.51 2.31 43.51 2.75 41.08 2.72 33.02 2.13 35.40 2.35 36.32 2.28 32.54 2.10 hh118 hh113 31.01 1.80 34.73 3.39 29.27 3.44 31.09 3.67 35.89 3.61 27.73 1.08 30.46 2.95 31.53 2.55 27.42 2.73 target domain in the projected space taking into considera- tion the semantic relationship between the labels while pre- serving the original structure of data. The proposed minimization objective is shown in Equa- tion 1. Given the source domain data S RnS dS and target domain data T RnT dT , the proposed optimiza- tion framework iteratively minimizes the overall loss J(.) incurred by jointly aligning the data of the source and tar- get domain for learning the optimal transformation P RdS dT , the optimal projected source data BS RnS dT and the optimal projected target data BT RnT dS. J(.) = min BS,BT ,P || S BSP ||2 + || T BT P ||2 + ( nS  i=1 nT  j=1 Wij || xS i BTj ||2 + nS  i=1 nT  j=1 Wij || BSi xT j ||2) + (|| BS ||2 + || BT ||2 + || P ||2) (1) The rst two terms represent the individual reconstruction loss functions for the source and target domain. A linear re- construction helps to preserve the original structure of data in the projected space. However, there can still be signi - cant distribution differences in the projected space even af- ter preserving the original topology. Hence, in order to add more discriminating information to the learned transforma- tion, the proposed optimization framework constrains the in- stances from the source and target domains with the same or related labels to be closer to each other in the projected space. This can be viewed as minimizing the conditional dis- tribution differences in the projected space. The similarity between the labeled data across the domains is de ned using the Normalized Google Distance (NGD) (Cilibrasi and Vi- tanyi 2007) on the associated labels. NGD is a well known semantic similarity measure that returns the web distance between any two keywords. We transform NGD into a sim- ilarity measure W [0, 1]nS nT for every labeled instance pair in the source and target domain. Using the similarity matrix W, the third term induces se- mantic co-alignment in our framework by penalizing for se- mantic data misalignment between the source and target data points in the projected space. While minimizing the inter- domain differences using the label information, there is a signi cant risk of over- tting on source training data. Hence, we adopt a regularizer in the objective function (the fourth term) to penalize over- tting. The hyper-parameter regu- lates the importance of label space induced alignment while controls the importance of the regularization term. The proposed optimization problem is not jointly convex with respect to the three variables BS, BT and P. However, it is convex with respect to any one of them while the other two have been xed. Consequently, we utilize an alternating algorithm for solving the unconstrained optimization, by it- eratively xing two out of the three variables to estimate the remaining one until convergence. Results and Discussion We report the performance of LSDHTL-WIA against sev- eral baselines and state-of-the-art transfer approaches on six cross-domain activity recognition tasks in Table 1. It can be seen that our algorithm outperforms all the baseline and transfer approaches on all transfer tasks. Kindly refer the supplementary material1 for detailed experimental results. A limitation of the proposed approach is that it requires some amount of labeled data in the target domain. Consequently, if labeled data is absent in the target domain, annotating relatively small number of unlabeled data becomes an in- escapable task. Acknowledgment Narayanan Chatapuram Krishnan is the research adviser for this work. This research is partially supported by the YSS/2015/001206 grant from Department of Science and Technology, India and ISIRD grant from IIT Ropar. References Cilibrasi, R. L., and Vitanyi, P. M. B. 2007. The google similarity distance. IEEE Trans. on Knowl. and Data Eng. 19(3):370 383. Cook, D. J.; Crandall, A. S.; Thomas, B. L.; and Krishnan, N. C. 2013. Casas: A smart home in a box. Computer 46(7):62 69. Sukhija, S.; Krishnan, N. C.; and Singh, G. 2016. Super- vised heterogeneous domain adaptation via random forests. In Proceedings of the 25th International Joint Conference on Arti cial Intelligence, 193 200. 1https://drive.google.com/open?id=1Ay7oF2NYz58ZgN- OrdvFFpjR5dR DRUW 8166