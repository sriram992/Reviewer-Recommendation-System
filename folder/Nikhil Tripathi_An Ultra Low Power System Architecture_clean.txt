An Ultra Low Power System Architecture for Sensor Network Applications Mark Hempstead, Nikhil Tripathi, Patrick Mauro, Gu-Yeon Wei, David Brooks Division of Engineering and Applied Sciences Harvard University {mhempste, nikhil, mauro, guyeon, dbrooks}@eecs.harvard.edu Abstract Recent years have seen a burgeoning interest in em- bedded wireless sensor networks with applications rang- ing from habitat monitoring to medical applications. Wire- less sensor networks have several important attributes that require special attention to device design. These include the need for inexpensive, long-lasting, highly reliable de- vices coupled with very low performance requirements. Ul- timately, the holy grail of this design space is a truly untethered device that operates off of energy scavenged from the ambient environment. In this paper, we describe an application-driven approach to the architectural design and implementation of a wireless sensor device that recognizes the event-driven nature of many sensor-network workloads. We have developed a full-system simulator for our sensor node design to verify and explore our architecture. Our simulation results suggest one to two orders of magnitude reduction in power dissipation over existing commodity- based systems for an important class of sensor network ap- plications. We are currently in the implementation stage of design, and plan to tape out the rst version of our system within the next year. 1. Introduction Wireless sensor networks are poised to transform the way society interacts with the physical world, driven by an explosion of systems research in sensor networks. Sen- sor networks have been proposed and deployed for a wide variety of applications such as habitat monitoring [13, 23], structural monitoring, and emergency medical response [7, 12]. While the application space seems limitless, it is actu- ally limited by the operating lifetime of the battery-operated wireless sensor nodes. Current deployments rely on com- mercially available wireless sensor network devices (e.g., Mica2 [4]). Such devices typically consist of a basic mi- crocontroller, a radio, and a variety of (often MEMS-based) sensors. One of the main limitations of these platforms is that they are built using commodity chips, which them- selves are not speci cally designed for wireless sensor net- works. As a result, they suffer several inef ciencies that lead to high power consumption and limited operational life- times. To address this limitation, this paper presents the de- sign and analysis of an ultra low power device speci cally for sensor network applications. In these systems, the CPU, radio, and sensor devices are responsible for the majority of the total system power and we show that the general- purpose nature of commodity microcontrollers results in in- ef cient power usage, presenting an opportunity to signi - cantly reduce its power. This paper outlines our design approach, which studies all levels of the system design from the applications down to the circuits and even the choice of process technology. This holistic approach enables us to uncover architectural and circuit-level design tradeoffs that guide design deci- sions in order to meet our low power goals and long life- time requirements. The power target of the system architec- ture is 100 W for normal workloads. We chose this power level with the ultimate objective of implementing a truly un- tethered device that can operate inde nitely off of energy scavenged from the environment. The intermittent, event-driven nature of sensor network application workloads motivates several architectural de- sign features. We optimize the architecture for frequent, repetitive behavior that is characteristic of sensor network applications. These optimizations include hardware accel- eration and of oading immediate event handling from the general purpose computing component. Features such as event-driven computation improve performance (thus per- mitting a slower system clock) and reduce power consump- tion by eliminating unnecessary operating system overhead. In order to meet the long-lifetime demands of many wire- less sensor network deployments, our architecture enables ne-grain power management to minimize extraneous dy- namic and static power consumption. Efforts to minimize idle (or leakage) power has led us to investigate tradeoffs between process technology generations. Given the rela- tively low performance requirements of the sensor nodes, Device/Mode Current Device/Mode Current CPU Radio Active 8.0mA Rx 7.0 mA Idle 3.2mA Tx (-20 dBm) 3.7mA ADC Acquire 1.0mA Tx (-8dBm) 6.5mA Extended Standby 0.223mA Tx (0 dBm) 8.5mA Standby 0.216mA Tx (10 dBm) 21.5mA Power-save 0.110mA Sensors Power-down 0.103mA Typical Board 0.7 mA Table 1. Mica2 platform current draw measured with a 3V power supply. we argue that the most advanced process technology is not necessarily the lowest power solution. The rest of the paper is organized as follows: Section 2 presents a brief background of wireless sensor network de- vices and related work. Then, Section 3 presents our study of wireless sensor network applications. Section 4 details our system architecture and explains several architectural optimizations made to reduce power consumption. Subse- quently, our study of process technologies and circuit design optimizations are presented in Section 5. Section 6 presents system-level simulation results (supported by power data extracted from key circuit-level simulations) that validate our architecture. 2. Background and Related Work We leverage active systems research in wireless sensor networks to provide insights and details of power consump- tion in currently available hardware platforms. We rst set out to determine whether efforts to reduce power in the computational component of a sensor node is warranted, dictated by Amdahl s law. Power consumed for radio com- munication is generally recognized to be signi cant and is the focus of much research effort to reduce power consumed by the circuitry itself and to minimize radio usage [16, 26]. While radio power is indeed signi cant, power required for computation can also be appreciable. The PowerTOSSIM project [21] studied the power consumption of the widely used Mica2 mote available from Crossbow [4]. A summary of power consumed by the CPU, sensors, and radio is pre- sented in Table 1. The table shows that active CPU and radio power numbers are comparable. Given the ability to oper- ate the CPU in both active and idle (or lower power) modes, these numbers do not present the complete picture. The computational demands of the application determine the CPU s actual activity and radio usage. The PowerTOSSIM paper provides a detailed breakdown of energy consumed by different components for a variety of applications. In these results, the CPU power ranges from 28% to 86% of the total power consumed and roughly 50% on average. Fur- thermore, data ltering and more ef cient communication protocols can shift activity from the radio to the CPU. Al- though there may be ways to reduce CPU power in existing hardware platforms, there is an innate inef ciency associ- ated with using general purpose CPUs for sensor network workloads. Simulation results in Section 6 reveal the poten- tial for signi cantly reducing power consumption when the computational unit uses an architecture designed to lever- age the event-driven nature of sensor networks. Several organizations are actively involved in design- ing hardware for sensor network devices. The devices that have been used widely for research and in some commer- cial deployments, such as the Mica2 [4] and Telos [17] motes, employ general-purpose microcontrollers that do not ef ciently handle interrupt processing. However, the pri- mary task of a sensor network device is to handle timer and external interrupts since their applications are inher- ently event driven [9]. Therefore, these devices must run an event-driven operating system (TinyOS [10]) to mask the de ciencies of the hardware platforms that have not been designed speci cally for sensor networks. The rst custom device for sensor networks is the Spec architecture [9], which includes hardware accelerators for tasks such as message start-symbol detection. In fact, the newer genera- tion radio chips incorporate some of these features [3]. In our architecture, we intend to use accelerators not merely to improve performance, but also as a power-saving mea- sure. The SNAP architecture, which is an asynchronous de- sign initiative described in [5], is the only example of an event-driven architecture for sensor network devices that we have come across in literature. However, the SNAP archi- tecture does not exploit the powerful event-driven paradigm apart from getting rid of the TinyOS overhead. In other words, its primary computing engine is still a general pur- pose microcontroller that must remain powered on all the time, even when events occur rarely, thereby incurring leak- age power. The Smart Dust project out of UC Berkeley de- veloped a general-purpose microcontroller with low-power design techniques for use in a sensor network device [25]. All of the existing architectures for wireless sensor network devices fail to optimize common-case behavior of appli- cations, because they all suffer from the overly general- purpose nature of the primary computing engines. Our design approach seeks to fully leverage the event-driven nature of applications as we pursue the design of next- generation low-power sensor network nodes. Scavenging energy from the environment and using this energy to power the sensor network device would greatly in- crease the effective lifetime of a wireless sensor node. There are many sources of energy available in the environment such as solar, vibration, and electro-magnetic radiation, and researchers have developed techniques to harness this en- ergy [20]. For example, vibrational energy can be trans- lated into electrical energy through piezoelectric materials that induce an open circuit voltage when placed under me- chanical stress. While using vibration as an energy source is promising, the power output is limited to the order of a hun- dred W (for mote-size devices). The PicoRadio project out of UC Berkeley built a proof-of-concept transmitter that op- erates at very low duty cycles while powered off of solar and vibrational energy [19]. Based on these demonstrations and the belief that energy-harvesting technology will improve, we have set the design target of our device at 100 W. 3. Sensor Network Applications During the initial phase of our design process, we stud- ied the wireless sensor network application space to un- derstand the computational needs of sensor network work- loads. Hardware requirements vary widely depending on the projected lifetime, computational complexity, and commu- nication needs of the deployment. We found that the mon- itoring class of applications, characterized by low duty cy- cles, long deployment lifetimes, and regularity of operation provide well-de ned and interesting constraints for sensor node design. Typical monitoring applications can be broken down into a clear set of regular tasks. Nodes typically complete sev- eral data generation tasks that include taking sensor sam- ples, preparing messages containing data, and sending radio messages. Nodes also complete ad-hoc routing tasks such as receiving messages, looking up routing information, and sending radio messages. The interval of sensor readings depends on the phe- nomenon being measured and these rates are typically very low. UC Berkeley s Great Duck Island (GDI) application measured all sensors every 70 seconds, then transmitted a packet [13, 23]. Harvard s deployment of sensor nodes to measure infrasound on the Tungurahua volcano measured samples at 100 Hz and sent 4 radio messages a second with 25 samples per packet [8]. While both of these applications transmitted to a base station that was one hop from the sen- sor nodes, other deployments may require nodes to also serve as communication relays due to large physical sep- aration between nodes. The ultimate goal of a monitoring deployment is to pro- vide continuous sensing for years to decades without be- ing touched. Past deployments of sensor networks for en- vironmental monitoring had limited lifetimes (a few weeks or months) due to the relatively high power consumption of commodity hardware. Therefore, an ultra low power sys- tem is required to achieve these deployment goals. The next section describes the goals and implementation of our ar- chitecture, which is designed speci cally for this applica- tion class. 4. System Design and Architecture 4.1. Motivation and Goals Our architecture replaces most of the functionality of a general purpose microcontroller with an event-driven sys- tem speci cally optimized for monitoring applications. A summary of our design goals for the system architecture are presented below and detailed discussions of goals and how the architecture meets these goals follows in the next sub- section. 1. Event-Driven Computation: Eliminate unneces- sary event-processing overhead with an event-driven hardware platform. 2. Hardware Acceleration to Improve Performance and Power: Build a system composed of several compo- nents that are optimized for speci c tasks. 3. Exploiting Regularity of Operations within an Appli- cation: Optimize the common-case behavior within an application. 4. Optimization for a Particular Class of Applications: Optimize the common-case behavior of monitoring applications to reduce power, while still providing general-purpose processing capability to enable broad functionality. 5. Modularity: Provide an easily extensible system archi- tecture that allows different sets of hardware compo- nents to be combined into a larger system targeting a particular application. 6. Fine-grain Power Management Based on Computa- tional Requirements: Provide explicit programmer- accessible commands for ne-grain resource and power control. 4.2. Architecture Description To ful ll our design goals, we seek to replace the ba- sic functionality of a general-purpose microcontroller with a modularized, event-driven system described in this sec- tion. The system architecture is illustrated in Figure 1. There are two distinct divisions within the system in terms of the positions of the components with respect to the system bus. We refer to the components to the right of the bus as slave components and those to the left as master components (ex- cept the memory, which is a slave). The system bus has three divisions data, interrupt, and power control. The slaves compete for the interrupt bus using centralized arbitration if more than one slave has an interrupt to signal. The slaves also respond to read or write requests from the master side on the data bus, thus allowing the masters to read their in- formation content and control their execution. Power con- trol lines are explained later. Bus Arbiter Power lines for Memory Segments Memory uController General Processing (Irregular Events) Bus Signals Bus Signals POWER CONTROL DATA ADDRESS INTERRUPT Event Processor Interrupt Processing Power Management (Regular Events) Interrupt Signals Unhandled Interrupts Power Enable Lines Figure 1. Block Diagram of System Architecture The features of the architecture are best understood in the context of design goals listed in the previous subsec- tion. 4.2.1. Event-Driven System We propose an event-driven system in which all of the master components are involved with event handling, and the slaves assist the master com- ponents in their tasks and signal the occurrence of events to trigger the master components. All external events, such as the beginning of radio packet reception, are expressed as in- terrupts by an appropriate slave component. The slave com- ponents also raise interrupts for their internal events, such as completion of an assigned task. To the master components, there is no distinction between external and internal events. Also, since the occurrence of all events is signaled by inter- rupts, we will use the terms event and interrupt interchange- ably. The system idles until one of the slaves signals the presence of an event, and when all outstanding events have been processed, the system returns to its idle mode. Since all the system does is respond to events, there is no soft- ware overhead for interrupt handling. 4.2.2. Hardware Acceleration to Improve Performance and Power There is a general-purpose microcontroller in our system. However, unlike other sensor network device architectures, the intent of our design is for the microcon- troller to be the last resort for any computation, i.e. the mi- crocontroller should be called upon to perform a task only if the rest of the system does not have the requisite func- tionality. Speci c tasks that are considered common to a wide variety of application are of oaded to hardware accel- erators, which can be more power and cycle ef cient than the general-purpose microcontroller. Hence, the microcon- troller can usually be powered down by gating the supply voltage. This not only reduces active power but also leak- age, which can be a very signi cant source of power con- sumption for low duty cycle operation. Some questions that arise are: How are the hardware accelerators con gured for their tasks? How are interrupts handled while the micro- controller is asleep? The answers to these questions are de- ferred to the discussion of how the architecture exploits reg- ularity of operations. All of the hardware accelerators in the system are slave components. There is a timer block that sets alarm events, which may be used to sample data from the sensors, in a Time-Division Multiple Access (TDMA) radio scheme, or for any tasks to be performed at regular intervals. In the ab- sence of a hardware timer, a software timer would have to be implemented in the microcontroller, requiring the micro- controller to always be active. There is a generic lter slave for basic data processing. In our architecture, this block is a simple threshold lter with a programmable threshold. We also implement a message processor block to of oad packet processing and avoid waking up the microcontroller for common events such as packet forwarding and transmit- ting packets of collected samples. The slave components also include essential sensor network device components such as the radio, and a block of sensors and Analog-to- Digital Converters (ADCs). 4.2.3. Exploiting Regularity of Operations within an Application All immediate interrupt handling is of oaded to the event processor block while the microcontroller is powered down. The event processor is a simple state ma- chine that can be programmed to handle an interrupt by transferring data blocks between the slave devices and set- ting up control information for these devices to complete their tasks. The event processor can also be programmed to wake up the microcontroller if the requisite functionality for pro- cessing the interrupt is not otherwise available. To some ex- tent, the event processor can be perceived as an intelligent DMA controller. Thus, there are two levels of interrupt ser- vice routines (ISRs) to handle an interrupt: at the event pro- cessor level and at the microcontroller level. ISRs for both the event processor and the microcontroller are stored in the main memory, which is a uni ed instruction and data mem- ory connected to the bus. We now elaborate further on the notion of regular and irregular events. A regular event is one that can be pro- cessed wholly by the event processor and the slave com- ponents. An irregular event is one that requires the micro- controller. One of the tasks involved in mapping an applica- tion to our system is to determine the partitioning of events into regular and irregular events. The regularity of an event is determined by the functionality present in the slaves and the event processor. For a typical application, events such as sampling, transmitting samples, and forwarding packets would ideally be regular while application or network re- con gurations would often be classi ed as irregular. 4.2.4. Optimization for Monitoring Applications The system does not seek to satisfy any real-time requirements and only one outstanding interrupt is possible. As a result, slave devices may continue to write their interrupts to the in- terrupt bus. However, if the system begins to be overloaded, events will simply be dropped. Outstanding events cannot preempt either the microcontroller or the event processor. The system bus, the microcontroller, and the event proces- sor are all non-pipelined. All of these simpli cations give rise to a light-weight system that is well suited to handle monitoring applications while consuming very little power. 4.2.5. Modularity All of the slave devices are attached to the system bus and are memory-mapped. Both control and data are communicated to and from the slaves by simply reading from and writing to appropriate addresses in the memory. Thus, the event processor is not aware that data is being transferred between separate slaves, or that control information is being written to the slaves. This memory- mapped interface allows the system design to be extremely modular and new components (and hence new functional- ity) can be added on to the system bus without modi cation of the event processor or the microcontroller. 4.2.6. Fine-grain Power Management Based on Com- putational Requirements Since the master components are triggered by interrupts, the ISRs for each interrupt can con gure the system according to its computational require- ments for handling the interrupt. To suf ciently curb leak- age power, special instructions within the event processor are used to gate the supply voltages of system components. Note that the system does not infer the resource usage for an event; rather, the ISR programmer selects the compo- nents to turn on depending on the needs of the application. Individual power enable lines are required for each compo- nent under direct control. Vdd-gating and power down im- plementations will vary depending on the circuit-level de- sign of the individual slave components. Such power con- trol may not only be exercised over the slave components, but also over segments within the main memory that contain temporary data, such as application scratch space. We be- lieve that event-driven programmable resource usage is one of our most signi cant innovations. It allows con guration of system power consumption with very little logic over- head, as opposed to a technique that attempts to infer re- source usage. Also, it allows the addition of several speci c components to the system as slaves that can be used in vary- ing combinations to provide the functionality required by an application. Any component unused in an application can be turned off (i.e., supply voltage gated) and is nearly invisible during the entire lifetime of the application. Therefore, the system can satisfy the general-purpose requirements of ap- plications by providing a broad range of slave components, enabling an on-demand functionality that imposes negligi- ble overhead when a component is not required. 4.3. System Components We now discuss some of the more interesting system components in greater detail. 4.3.1. System Bus As discussed earlier, the system bus comprises the data bus, the interrupt bus, and power con- trol lines. The data bus has address, data, and control sig- nals indicating read and write operations. In our current im- plementation the address bus has 16 lines, the data bus has 8 lines, and there is one control signal each for read and write operations. The address space for our memory-mapped ar- chitecture is therefore 64K. The address and control lines can be driven only by the event processor and the micro- controller in mutual exclusion as determined by the bus ar- biter, which is currently just a mux. The data lines are driven by the slave that determines that the current request lies in its address range, and are demultiplexed to the originator of the request, i.e., the event processor or the microcontroller. The interrupt bus has 6 address lines and control signals for arbitrating the writing of interrupts by slaves. The sys- tem is therefore capable of handling 64 interrupts in the cur- rent model. The event processor has control signals in the interrupt bus to indicate when it has read the current inter- rupt address. The power control lines are handshake pairs for each slave or memory segment controlled. The handshake is rel- evant only when a component is turned on, to determine the time when the component can be used. The system cur- rently makes no assumptions about the time taken to wake up for the components over which explicit power control is exercised. 4.3.2. Microcontroller The microcontroller is used to handle irregular events, as discussed in previous sec- tions, such as system initialization and reprogramming. The microcontroller is a simple non-pipelined microcon- troller. It implements an 8-bit Instruction Set Architecture (ISA). We plan to leverage currently available computa- tional cores with necessary modi cations for our low-power features. 4.3.3. Event Processor The event processor is essentially a programmable state machine designed to perform the repetitive task of interrupt handling. Figure 2 illustrates a READY No Interrupt is available WAIT_ BUS Interrupt to process and bus is available Interrupt to process and bus is not available Bus is availabe LOOKUP Lookup is not complete FETCH Fetched instruction is not complete Lookup is complete EXECUTE Execution is not complete Fetch is complete Execution complete: Instruction was not WAKEUP, TERMINATE Execution complete: Instruction was WAKEUP, TERMINATE Figure 2. Event Processor State Machine simpli ed version of the actual state machine within the event processor. Because the event processor is an impor- tant component of our architecture we now explain its func- tionality in detail. The event processor idles in the READY state until there is an interrupt to process. When an interrupt is signaled, the event processor transitions to the LOOKUP state if the data bus is available, i.e., the microcontroller is not awake. If not, the event processor transitions to the WAIT BUS state and waits until the microcontroller relin- quishes the data bus. In the LOOKUP state, the event pro- cessor looks up the ISR address corresponding to the inter- rupt. The lookup table is stored in memory, and the starting location of the table, offset by an amount proportional to the interrupt address, contains the address of the event proces- sor ISR. When the lookup is complete, the event processor transitions to the FETCH state, in which the rst instruc- tion at the ISR address discovered in the LOOKUP state is fetched. The event processor stays in the FETCH state un- til all the words of the current instruction have been fetched, and then it transitions to the EXECUTE state. The instructions within an event processor ISR can be one of the following SWITCHON, SWITCHOFF, READ, WRITE, WRITEI, TRANSFER, TERMINATE, or WAKEUP. Table 2 provides a summary of the opera- tions corresponding to the instructions. The event processor has one register used to store temporary data. The op- codes are each 3 bits and the instructions vary in the num- ber of words they span. The EXECUTE state holds until the instruction has been completely executed, e.g., the complete transfer has been completed for a TRANSFER instruction. A component is completely powered on for the SWITCHON instruction. If the instruction is not a WAKEUP or TERMINATE instruc- tion, the event processor returns to the FETCH state and fetches the next instruction in the ISR for execution. For WAKEUP or TERMINATE instructions, the event proces- sor returns to the READY state and waits for the next inter- rupt. 4.3.4. Timer Subsystem The timer subsystem consists of a set of four 16-bit timers in our current implementation. Each timer is essentially a counter that counts down to zero from a pre-con gured value, and then generates an alarm event. The timers can be chained to allow alarm events to be generated for larger intervals of time. Each timer can be paused, disabled, and recon gured. 4.3.5. Message Processor Our architecture enables hard- ware accelerators designed for speci c tasks. For example, our architecture uses a message processor block to handle regular message processing tasks, including message prepa- ration and routing. Simple tasks such as table lookup and check-sum calculations can be sped up using hardware im- plementations (with low power overhead). Currently, the message processor interface has two mem- ory blocks for each message as well as memory-mapped control words. Data is transfered to the message processor from sensor devices and once the message has been pre- pared the message processor res an interrupt and the mes- sage is sent to the radio. All incoming messages are trans- fered from the radio to the message processor. If the mes- sage is a regular message, the message processor looks up whether the message should be forwarded. If the message is an irregular message, then an interrupt is red and the event processor wakes up the microcontroller. Our message pro- cessor model handles standard 802.15.4 packets [28]. 4.3.6. Radio Like the new Telos mote, our architecture in- terfaces with the low-power CC2420 802.15.4 radio from ChipCon [3]. This radio provides hardware support for tasks such as start-symbol detection, error detection, etc., and we plan to take advantage of these features as they are con- sistent with our system design approach. A simple radio model enables us to fully test our system architecture con- cepts without having to explicitly build a transceiver. 5. Process Technology and Circuit Techniques In addition to architectural innovations, circuit tech- niques and the choice of process technology can signi - cantly impact the power consumed by the sensor nodes. This section presents the results of a process technol- ogy simulation study and the architecture and circuit design of a low-power SRAM. Traditionally, the choice of pro- cess technology has been straight forward. To push the envelope of performance, the most advanced technol- ogy with the smallest feature size and smallest parasitic capacitance should be used. However, subthreshold leak- age current is becoming a signi cant fraction of the total power in designs that use advanced deep-submicron pro- cess technologies [1]. When choosing a process technology for sensor network hardware one must choose the technol- Instruction Size Description SWITCHON One word Turn on a component and wait for acknowledgment that the component is ready to proceed. SWITCHOFF One word Turn off a component READ Three words Read a location in the address space and store to the register WRITE Three words Write a location in the address space from the register WRITEI Three words Write an immediate value to a location in the address space TRANSFER Five words Transfer a block of data within the address space TERMINATE One words Terminate the ISR without waking up the microcontroller WAKEUP Two words Terminate the ISR and wake up the microcontroller at a microcontroller ISR address Table 2. Event Processor Instruction Set Figure 3. Analysis of process technology for low-duty cy- cle sensor node applications ogy that considers both active and leakage power in the context of low duty cycle operation. 5.1. Simulation Study To study the power and performance tradeoffs of differ- ent technologies, we ran a comprehensive set of HSPICE simulations for several eleven-stage ring oscillators com- prised of various static CMOS gates. Simulations were run across a wide range of temperatures, supply voltages, and process technologies. Transient simulation results of the os- cillators generated active power data. Leakage power was simulated by disabling the feedback in the ring. Given the characteristically low workload requirements of sensor network applications, leakage power is a ma- jor concern. Several researchers have studied and modeled leakage power, but they do not compare different process technologies [2, 6, 14, 18]. Our simulation results show that even with aggressive voltage scaling, deep sub-micron tech- nologies incur higher leakage current penalties, which dis- courage their use for sensor network applications. Older technologies with higher threshold voltages ex- hibit lower leakage current than newer, faster technolo- gies that utilize lower threshold voltages to enable aggres- sive voltage scaling. However, advanced deep sub-micron technologies consume less active power. We assume a syn- chronous design that operates off of a globally distributed clock.1 To account for both sources of power, we used Equa- tion 1 to model the active and leakage power tradeoff, Ptotal = (T/Ttarget)Pactive+(1 (T/Ttarget))Pleakage (1) where is the activity factor and Ttarget is the maximum expected cycle time required to accommodate all applica- tions. We chose 30 s, which is the time a typical 802.15.4 radio takes to transmit one byte of data [28]. T, Pactive, and Pleakage are the measured period of oscillations, ac- tive power, and leakage power, respectively, for each tech- nology node, temperature, device, and voltage simulated. Figure 3 presents a 3D illustration of Equation 1, which compare total power across Vdd and activity for different technology nodes. Supply voltage was scaled to the lowest value that was still less than Ttarget. Notice that more ad- vanced deep sub-micron technologies consume much less power for high activity factors compared to older technolo- gies. However, for low activity factors, expected for sensor network applications, leakage power dominates. Hence, the most advanced technologies are less desirable due to their higher leakage characteristics. The process decision should balance active energy and leakage power for the activity fac- tor ranges of the applications detailed in Section 3. 5.2. Low Power Memory Design In order to optimize the power consumed by our architec- ture, we designed a 2-kilobyte custom on-chip SRAM. The architecture s overarching paradigm of switching off un- needed circuit elements is present in the memory. For exam- ple, the SRAM is divided into banks of 256 bytes to allow unused portions of memory to be Vdd-gated. The SRAM architecture is illustrated in Figure 4. Both leakage power and active power can be reduced through this banked archi- tecture. By partitioning the memory, the voltage supply to 1 While an asynchronous implementation may bene t from the event- driven nature of sensor network applications, we do not feel the poten- tial to reduce active power is justi ed in light of higher design com- plexity and circuit overhead required for asynchronous operation. Fur- thermore, asynchronous designs do not reduce leakage power. Miscellaneous Control Circuits Precharge Circuitry Decoders Bitline Read/Write Circuits Memory Bank Indicates parts of the sRAM that draw active power. All other elements draw idle power One wordline One byte wide Figure 4. Power usage characteristics of a 1KB SRAM block. Active Power Idle Power Gated Power 1.93 W 409 pW 342 pW Table 3. Power for a Single 256B Bank and All Associ- ated Control Circuitry (Vsupply = 1.2V) unused banks can be shut off through Vdd-gating, result- ing in over a 98% reduction in power drawn by the mem- ory bank when not Vdd-gated the bank draws 66.5 pW of power, compared to less than 1 pW when gated. It takes 950ns (or less than one clock cycle) to power up a bank af- ter it has been gated. The SRAM was layed out in a 0.25 m technology and the extracted netlist (with parasitics) was simulated using Nanosim[22]. The power characteristics for a single bank of memory with all associated control circuitry are summa- rized in Table 3. The 2-kilobyte SRAM design consumes 2.07 W while operating at 100kHz and 1.2V. Future revisions of the memory will also include an in- telligent precharging scheme. Precharging each bitline con- sumes the most power when a bank is active, so we envision reducing this power by only precharging the bitlines of the cells that will be accessed. In order to do this we will have to include additional decoder and precharge control circuitry. However, we believe this cost will be offset by a 35% reduc- tion in total active power when this scheme is implemented. 6. Proof of Concept Results 6.1. Performance Methodology and Estimates While the performance requirements of wireless sensor nodes are very low, the performance of our architecture rel- ative to general-purpose microcontrollers is an important issue, because it determines the minimum required clock rate of our system. In this section, we present our perfor- mance modeling methodology and cycle-level comparisons between our architecture and the Atmel Atmega128 micro- controller used in the Mica2 sensor node. 6.1.1. Performance Modeling - SystemC Simulator We used a cycle-accurate simulator written in SystemC to char- acterize the cycle-level behavior of our architecture. Sys- temC is a set of C/C++ libraries that is used to model high- level architectural behavior [15]. Currently, the simulator has 8000 lines of code (excluding SystemC library code). We implemented a modular design to which models of slave components can be easily attached. The SystemC model al- lowed us to explore several design choices rapidly until we arrived at the current version of our architecture. We also utilized this model to provide component utilization statis- tics that allowed us to perform power analysis for various workloads. The simulator can currently take in assembly code for both the microcontroller and the event processor, as well as other simulation data required such as received data pack- ets and data sampled by the sensor block. Thus, it is pos- sible to specify complete applications. A few applications were mapped to the simulator by hand (we are considering compilation from higher-level languages). The same appli- cations were also ported to a simulator for the MICA plat- form, and the cycle counts for both platforms were collected and compared. 6.1.2. Test Application In order to evaluate our architec- ture, we began with the simplest application that is repre- sentative of existing real-world applications such as habi- tat monitoring [23]. We then added complexity to this ap- plication in stages and the nal application is fairly com- plex, including standard sensing and transmission of data, multi-hop routing, and remote application recon guration. We describe the four application versions according to the complexity added in each stage: 1. Periodically collect samples and transmit packets con- taining the samples. 2. Periodically collect samples and transmit packets con- taining the samples if it is above a certain threshold. 3. Receive and forward incoming messages from other sensor nodes. 4. Receive and handle incoming recon guration mes- sages. (These messages include changes to the sam- pling period and the sensor threshold value.) The base application collects samples and transmits the packets. For our architecture, the processing of a sample is initiated by the timer ring an interrupt. The event processor responds to this interrupt by sampling the ADC and trans- ferring the value to the message processor. The message processor prepares the message and signals an event that causes the event processor to transfer the packet to the radio <timer intaddr>: SWITCHON <sensor> READ <sensor:mem> SWITCHOFF <sensor> SWITCHONN <message proc> WRITE <message proc> WRITEI <ctrl_wrd> <message proc> TERMINATE; <message proc mesg. ready intaddr>: SWITCHON <radio> TRANSFER <msg size> <message proc> <radio> SWITCHOFF <message proc> WRITEI <ctrl_wrd> <radio> TERMINATE; <radio, message sent intaddr> SWITCHOFF <radio> TERMINATE; Timer Interrupt Collect Sensor Data Prepare Message Send Radio Message Figure 5. Diagram and Code of Monitoring Application. The code displayed are the ISR routines for the event pro- cessor. Actual address values have been omitted to make the code easy to read. block and setup the radio for transmission. The pseudo-code for the program that runs on the event processor for this ap- plication is shown in Figure 5. Similarly, the second version of the application includes a very simple threshold ltering operation. In a multi-hop routing environment, message forwarding is expected to be a fairly frequent activity and we, therefore, map it as a regular event in our architecture. When a mes- sage arrives, an interrupt is red by the radio block to indi- cate that a packet has been received. The event processor re- sponds by transferring the packet to the message processor, which signals whether the message has been previously re- ceived (this is performed by searching for the packet ID in the routing table). If the message has been previously re- ceived, the packet is dropped, otherwise the event processor sets up the radio to forward the packet. The last version of the test application contains two ir- regular events that require intervention from our general- purpose microcontroller. In this case, message handling is the same as in the preceding case until the message proces- sor receives the packet. If the message processor determines that the message is not a simple forwarding request, then it signals an interrupt indicating that intervention by the mi- crocontroller is required. The event processor wakes up the microcontroller in response to an irregular event signaled by the message processor. The microcontroller decodes the message to determine whether the timer needs to be recon- gured or whether the lter threshold needs to be modi ed. 6.1.3. Cycle Performance Estimates Cycle count results using our SystemC simulator and the Mica2 cycle simula- tor, Atemu [11], for each application task are shown in Ta- ble 4. Each row represents the measurement of a particular segment of code. The rst two rows provide measurements of the send path of our application as described in the pre- Our Measurement Mica2 System Speedup Total send path w/out lter 1522 102 14.9 Total send path w/ lter 1532 127 12.1 Process regular message 429 165 2.6 Process irregular message Timer change 234 136 1.7 Threshold change 11 114 0.096 Units Cycles Cycles Table 4. Comparison of cycle count for the test applica- tion written on our architecture and on TinyOS for the Mica Platform. vious section. The next two rows display cycle comparisons of the receive path for both regular and irregular messages. As explained in Section 4.3.6, our architecture assumes the use of 802.15.4 compatible radios like the CC2420 from ChipCon [3], which implements the radio stack in hard- ware. Therefore, to ensure an accurate comparison we did not count the cycles for the instructions in the TinyOS ra- dio stack run on the Mica2. For the Mica2 platform, processing a sample includes the software-equivalent implementation of our test appli- cations, in addition to the overhead of TinyOS, required for context switching and task scheduling. Our architecture handles task scheduling natively in the design and, there- fore, we see a large difference in cycle counts. Because our architecture is optimized for regular events, it does not show improvements for irregular tasks that require the general- purpose microcontroller. It is clear that the emphasis of our proposed architecture, for typical events seen within a sensor node, has signi cant cycle-count advantages over commodity systems. These ad- vantages enable our architecture to operate at signi cantly lower clock rates while maintaining suf cient performance to keep up with the 802.15.4 radio standard and process sen- sor data requests at a level required by typical applications. For the Mica2 platform, the applications were written us- ing the TinyOS component library. Because the test appli- cations can be created using typical TinyOS components, programming these applications is straight forward. How- ever, the code size of the nal application incorporating all components was 11558 bytes for instructions when ported to the Mica2 platform. This is signi cant compared to the 180 byte memory footprint required for our system. Ideally, one would like to compare our results with other designs speci cally tailored for sensor networks such as the SNAP architecture [5]. Unfortunately, this is complicated by the fact that the SNAP paper and results assume the older radio chip and software stack, and we do not have access to their simulation environment. Hence, we can only com- pare two relatively simple applications that were reported in the paper: blink, which sets a timer to periodically inter- rupt the processor to blink an LED; and sense, which peri- odically samples data from the ADC and computes a run- ning average. From the published results, SNAP takes 41 cycles and 261 cycles respectively, while our architecture can complete these operations within 12 cycles and 24 cy- cles. For comparison, the Mica2 requires 523 cycles to run blink and 1118 cycles for sense. As can be seen from Table 4, the number of cycles taken to process one timer event for the sample, lter, and transmit application takes 127 cycles. The cycle count at 100 KHz gives us a maximum sample rate of roughly 800 samples/second. This maximum rate seems very reason- able considering the fact that most documented sensor net- work applications have sample rates less than a 100 sam- ples/second. It should also be noted that the clock rate was chosen to accommodate the radio communication data rate of the 802.15.4 standard, 250 Kbits/second [28]. 6.2. Power Estimation Methodology and Results 6.2.1. Methodology Since the power consumption of our system can be accurately characterized only after a fabri- cated chip has been measured, we restrict ourselves to ob- taining a conservative estimate based on the active power consumption of the system for its most frequent activities, i.e. the processing of regular events. Again, since we ex- pect to use commodity radio and sensor components, we do not consider these components in our estimates. Because we have not completed the oorplan for our system, we also do not currently include power estimates for global routing sig- nals, buses, and clocks, although we do consider local clock driver circuitry. The event processor is the largest power consumer in the system since this is a component that must always be pow- ered. Moreover, this block has the most complex micro- architecture of all of the components involved with regu- lar event processing. Hence, for this block, we have ob- tained conservative estimates by going through the com- plete process of synthesizing a VHDL model, performing placement, routing, and simulating the nal netlist. For the other components, we have broken them down into common substructures such as incrementers, comparators, buffers, etc., and estimated the power consumption numbers for all of these components by simulating netlists synthesized for these sub-structures and combining the results. For all of the memories used (including the main memory block), we use the estimates described in Section 5.2. 6.2.2. Power Estimates The power estimates for the main components of the system are presented in Table 5. The power numbers are shown for active and idle modes (gated clock) of each component at a supply voltage of 1.2V and a clock frequency of 100KHz. In the subsection on work- load analysis (Section 6.3), the power estimates are corre- Idle/Active Vdd 1.2(V) Event Processor Active 14.25 W Idle 0.018 W Timer Active 5.68 W Idle 0.024 W Message Processor Active 2.57 W Idle 0.025 W Threshold Filter Active 0.42 W Idle 0.0 W Memory Active 2.07 W Idle 0.003 W System Active 24.99 W Idle 0.070 W Table 5. Power Estimates for Regular Event Processing in the System lated with duty cycle values for sample application work- loads to provide a better understanding of the actual power consumption of the system operating under practical situa- tions. The active power consumption corresponds to a situation in which the event processor always has an outstanding in- terrupt to handle. Therefore, the event processor is always switching in this mode because it begins to process a new interrupt the moment it gets done with the current one. The idle power corresponds to a duration in which the event pro- cessor is not provided an interrupt to handle. Both of these situations are extreme cases that we do not expect in nor- mal situations. Implementation details of each block were required to provide accurate power estimates. The message processor block is comprised of a CAM (Content Addressable Mem- ory) structure for the routing table, a counter for keeping track of the packets transmitted, and two 32-byte buffers to allow packet processing and transfer to/from the message processor to take place in parallel. The timer block con- sists of four decrementers with registers to store the cur- rent count, zero-detect logic to re interrupts, and a small buffer to store current timer con guration. The threshold l- ter consists mainly of a comparator and a register to store the threshold value. Active power estimates are obtained for cases in which the relevant sub-structures within a slave component are always switching, and idle power estimates are obtained for settings in which none of the sub-structures are switching. Finally, the system active and idle power es- timates were obtained by summing up the active and idle powers for the components. It is important to note that the idle power numbers do not re ect Vdd-gating as this fea- ture has not been fully characterized for these components. 0 5 10 15 20 25 1 0.1 0.01 0.001 0.0001 Duty Cycle Power (Micro-Watts) Total System Event Processor Memory Timer Message Threshold Figure 6. Estimated power varying node duty cycle sam- ple application A duty cycle of 1.0 is roughly 800 tasks per second. 6.3. Workload Analysis The active power estimates presented in the previous sec- tion are conservative for practical applications since compo- nent activity patterns due to application workload were not taken into account. We now correlate the performance and power estimates obtained in Sections 6.1.3 and 6.2.2 for a real application to obtain power numbers that take into ac- count the utilization of each component of the system. The application we consider for this analysis is the second appli- cation described in Section 6.1.2, i.e. a sample is collected periodically, ltered, and a packet is transmitted contain- ing the sample if it passes ltering. For the sake of sim- plicity, we assume that the sample is always transmitted, i.e. the sample passes the threshold check. This case is the more conservative one, because it is when the system has to do more work and all components are active sometime dur- ing the processing of one sample. An upper bound on the sample rate is obtained by as- signing a utilization ratio of 1 to the event processor, i.e. the event processor is always active. At this utilization ratio for the event processor, the system is processing the max- imum rate of 800 samples per second as described in Sec- tion 6.1.3. The power consumption of each component (and the system) is calculated considering the utilization of the component within the system for several event processor utilization ratios, beginning from the limiting ratio of 1. As a point of reference, the volcano [8] deployment has a duty cycle of 0.12 and the GDI experiment [23] has a duty cy- cle of approximately 0.0001. The resulting curves for each component and the sys- tem total are shown in Figure 6. For this application, one of the 4 timers in the timer subsystem is always on while the rest are idle. Also, the threshold lter is used for 3 cycles out of the total system 127 cycles per sample, and the mes- sage processor is used for 70 cycles per sample. The system power consumption drops to below 2 W for even reason- ably high sample rates. For the purpose of comparison, the power consumption of the Atmel microcontroller is also estimated for the sam- ple rates corresponding to the utilization points of the event processor. The idea is to compare the power numbers for the same work done for both systems, with the utilization of the Atmel microcontroller normalized to those of the event pro- cessor in our system. Again, we use the cycle counts pre- sented in Table 4 for the sample- lter-transmit application. For the active and idle power estimates, we use the mea- sured current values for the Atmel microcontroller in Ta- ble 1. We found that the trends are similar to Figure 6 but with a power consumption of a little over two orders of mag- nitude higher than our architecture. With the advent of the TI-MSP430 next-generation general-purpose microcontroller used in the Telos mote, the energy consumption difference will likely shrink [17, 24]. For example, the TI-MSP430 reports an active mode power dissipation of between 616 W and 693 W at 1MHz and 2.2V. It has been reported for other sensor network applica- tions that the 32KHz idle mode, which dissipates power be- tween 44 W and 123 W, is the most practical low-power mode for the TI-MSP430 because of the ability to man- age peripherals and service interrupts in this mode [27]. If we assume equal cycle-level performance with the At- mel processor, with a sampling rate corresponding to the 0.1 utilization point for our system, the MSP430 will con- sume between 113 W and 192 W. At this time we are unable to compare power results with the SNAP proces- sor because the published results do not include enough data for us to make an accurate comparison. 7. Conclusion and Future Directions This work describes a holistic approach to the design of a wireless sensor network device. Employing an application- driven design philosophy, this work describes the selec- tion of process technology, circuit design considerations, and a novel system architecture for sensor devices. In order to provide ef cient operation and enable ne-grain power control, our architecture provides explicit support for the event-driven nature of sensor network applications and pro- vides key functionality in separate hardware blocks. Our es- timates for the key components of the system include a to- tal active power of 25 W and idle power is 70nW. With a duty cycle of 0.1 or less, the average power drops to less than 2 W. These results represent a substantial savings over existing systems. This project is currently in the circuit-level implementa- tion phase of the architecture described in this paper, and we expect to tape out a chip implementing key components of our architecture to validate design decisions within the next year. We plan to utilize the lessons learned from this im- plementation work to guide the development of future op- timizations to our base architecture, and we plan to con- sider additional slave devices to expand the space of well- optimized applications. Acknowledgments We would like to thank the Motes group at Harvard, speci cally Matt Welsh, Victor Shnayder, Bor-rong Chen, and Geoff Werner-Allen for their insight into sensor net- work applications. This material is based upon work supported by the Na- tional Science Foundation under grant number 0330244. References [1] International technology roadmap for semiconductors. Semiconduc- tor Industry Association, 2004. [2] S. Borkar. Design challenges of technology scaling. IEEE Micro, pages 23 29, July-Aug 1999. [3] Chipcon AS. CC2420 2.4GHz IEEE 802.15.4/ZigBee-ready RF Transceiver. http://www.chipcon.com. [4] Crossbow Technology Inc. Mica2 sensor node. http://www. xbow.com. [5] V. Ekanayake, I. Clinton Kelly, and R. Manohar. An ultra low-power processor for sensor networks. In Proc. ASPLOS, Oct 2004. [6] D. J. Frank, R. H. Dennard, E. Dowak, P. M. Solomon, Y. Taur, and H.-S. P. Wong. Device Scaling Limits of Si MOSFETs and Their Ap- plication Dependencies. Proceedings of the IEEE, 89(3):259 288, March 2001. [7] T. R. F. Fulford-Jones, G.-Y. Wei, and M. Welsh. A Portable, Low- Power, Wireless Two-Lead EKG System. In In Proceedings of the 26th IEEE EMBS Annual International Conference, San Francisco, CA, Sept 2004. [8] Geoffrey Werner-Allen and Matt Welsh. Monitoring volcanic erup- tions with a wireless sensor network. http://www.eecs. harvard.edu/ werner/projects/volcano/. [9] J. Hill. System Architecture for Wireless Sensor Networks. PhD the- sis, UC Berkeley, May 2003. [10] J. Hill, R. Szewczyk, A. Woo, S. Hollar, D. E. Culler, and K. S. J. Pister. System architecture directions for networked sensors. In Ar- chitectural Support for Programming Languages and Operating Sys- tems, pages 93 104, 2000. [11] M. Karir, J. Polley, D. Blazakis, J. McGee, D. Rusk, and J. S. Baras. ATEMU: A Fine-grained Sensor Network Simulator. In Proceedings of First IEEE International Conference on Sensor and Ad Hoc Com- munication Networks (SECON 04), Santa Clara, CA, Oct 2004. [12] K. Lorincz, D. Malan, T. R. F. Fulford-Jones, A. Nawoj, A. Clavel, V. Shnayder, G. Mainland, S. Moulton, and M. Welsh. Sensor net- works for emergency response: Challenges and opportunities. IEEE Pervasive Computing, Oct-Dec 2004. [13] A. Mainwaring, J. Polastre, R. Szewczyk, D. Culler, and J. Ander- son. Wireless sensor networks for habitat monitoring. In ACM In- ternational Workshop on Wireless Sensor Networks and Applications (WSNA 02), Atlanta, GA, Sept. 2002. [14] S. Narendra, V. De, S. Borkar, D. Antoniadis, and A. P. Chan- drakasan. Full-chip sub-threshold leakage power prediction model for sub-0.18 m cmos. In Proc. ISLPED, Aug 2002. [15] Open SystemC Initiative. SystemC. http://www.systemc. org. [16] J. Polastre, J. Hill, and D. Culler. Versatile Low Power Media Access for Wireless Sensor Networks. In Proceedings of the Second ACM Conference on Embedded Networked Sensor Systems (SenSys 04), Baltimore, MD, Nov 2004. [17] J. Polastre, R. Szewczyk, C. Sharp, and D. Culler. The Mote Revolu- tion: Low Power Wireless Sensor Network Devices. In In Hot Chips 16: A Symposium on High Performance Chips, Aug 2004. [18] R. Rao, A. Strivastava, D. Blaauw, and D. Sylvester. Statistical anal- ysis of subthreshold leakage current for vlsi circuits. IEEE Transac- tions On VLSI Systems, 12(2):131 139, Feb 2004. [19] S. Roundy, B. P. Otis, Y.-H. Chee, J. M. Rabaey, and P. Wright. A 1.9GHz RF Transmit Beacon using Environmentally Scavenged En- ergy. In Proc. ISLPED, Aug 2003. [20] S. Roundy, P. K. Wright, and J. Rabaey. A study of low level vibra- tions as a power source for wireless sensor nodes. Computer Com- munications, 26(1):1131 1144, July 2003. [21] V. Shnayder, M. Hempstead, B.-R. Chen, G. W. Allen, and M. Welsh. Simulating the Power Consumption of LargeScale Sensor Network Applications. In Proceedings of the Second ACM Conference on Em- bedded Networked Sensor Systems (SenSys 04), Baltimore, MD, Nov 2004. [22] Synopsys Corporation. NanoSim - High capacity and high per- formance circuit simulation. http://www.synopsys.com/ products/mixedsignal/nanosim/nanosim.html. [23] R. Szewczyk, J. Polastre, A. Mainwaring, and D. Culler. Lessons from a sensor network expedition. In Proc. the First European Work- shop on Wireless Sensor Networks (EWSN), January 2004. [24] Texas Instruments. TI MSP430F149 Ultra-Low Power Microcon- troller. http://www.ti.com. [25] B. A. Warneke and K. S. Pister. An ultra-low energy microcontroller for smart dust wireless sensor networks. In Proc. ISSCC, Jan 2004. [26] Y. Xu, J. Heidemann, and D. Estrin. Geography-informed Energy Conservation for Ad Hoc Routing. In Proceedings of the Seventh Annual ACM/IEEE International Conference on Mobile Computing and Networking (ACM MobiCom), Rome, Italy, July 2001. [27] P. Zhang, C. Sadler, S. Lyon, and M. Martonosi. Hardware Design Experiences in ZebraNet. In Proceedings of the Second ACM Con- ference on Embedded Networked Sensor Systems (SenSys 04), Balti- more, MD, Nov 2004. [28] ZigBee Alliance, http://www.zigbee.org. IEEE 802.15.4 Standard.