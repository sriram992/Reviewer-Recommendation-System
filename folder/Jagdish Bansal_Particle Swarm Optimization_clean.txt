See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/325628393 Particle Swarm Optimization Chapter in Studies in Computational Intelligence January 2019 DOI: 10.1007/978-3-319-91341-4_2 CITATIONS 102 READS 242 1 author: Some of the authors of this publication are also working on these related projects: SocProS 2012 View project liverpool hope University View project Jagdish Chand Bansal South Asian University 164 PUBLICATIONS 3,984 CITATIONS SEE PROFILE All content following this page was uploaded by Jagdish Chand Bansal on 12 January 2023. The user has requested enhancement of the downloaded file. Noname manuscript No. (will be inserted by the editor) Particle Swarm Optimization Jagdish Chand Bansal the date of receipt and acceptance should be inserted later Abstract Particle Swarm Optimization (PSO) is a swarm intelligence based numer- ical optimization algorithm, introduced in 1995 by James Kennedy, a social psychol- ogist, and Russell Eberhart, an electrical engineer. PSO has been improved in many ways since its inception. This chapter provides an introduction to the basic particle swarm optimization algorithm. For better understanding of the algorithm, a worked- out example has also been given. 1 Particle Swarm Optimization Particle Swarm Optimization (PSO) is a swarm intelligent algorithm, inspired from birds ocking or sh schooling for the solution of nonlinear, nonconvex or combina- torial optimization problems that arise in many science and engineering domains. In [4] Feng et al. used PSO to optimize parameters in radical basis function (RBF) net- work. In [2] PSO is modi ed to solve the two-sided U-type assembly line balancing (TUALB) problem. The results obtained by proposed approach showed that design- ing a two-sided assembly line in U-shaped layout provides shorter lines. Mousavi et al. applied PSO to optimize a supply chain network for a seasonal multi-product in- ventory system with multiple buyers, multiple vendors and warehouses with limited capacity owned by the vendors [8]. Apart from continuous version of PSO, binary PSO has also been applied extensively. Bansal et al. [1] proposed a modi ed binary PSO and applied it to solve various forms of knapsack problems. Jain et al. [5] pro- posed an improved-Binary Particle Swarm Optimization (iBPSO) for cancer classi- cation. The next subsection presents the motivation and general framework of PSO procedure. Jagdish Chand Bansal Department of Mathematics, South Asian University, New Delhi, India E-mail: jcbansal@gmail.com 2 Jagdish Chand Bansal 1.1 Motivation Many bird species are social and form ocks for various reasons. Flocks may be of different sizes, occur in different seasons and may even be composed of different species that can work well together in a group. More eyes and ears mean increased op- portunities to nd food and improved chances of detecting a predator in time. Flocks are always bene cial for survival of its members in many ways [10]: Foraging: A socio-biologist E. O. wilson said that, In theory at least, individual members of the school (swarm) can pro t from the discoveries and previous expe- rience of all other members of the school during the search for food [11]. If for a group of birds, the food source is the same then some species of birds form ock in a non-competing way. In this way, more birds take advantage of discoveries of other birds about the location of the food. Protection against Predator: A ock of birds have number of advantages in pro- tecting themselves from the predator: More ears and eyes means more chances of spotting a predator or any other po- tential threat. A group of birds may be able to confuse or overwhelm a predator through mob- bing or agile ights. In case of a group, large availability of prays reduces the danger for any single bird. Aerodynamics: When birds y in ocks, they often arrange themselves in speci c shapes or formations. Those formations take advantage of the changing wind patterns based on the number of birds in the ock and how each bird s wings create differ- ent currents. This allows ying birds to use the surrounding air in the most energy ef cient way. However, the development of PSO requires simulation of some advantages of birds ock, in order to understand an important property of swarm intelligence and therefore of PSO, it is worth mentioning some disadvantages of the birds ocking. When birds form ock they also create some risk for them. More ears and more eyes means more wings and more mouths which result more noise and motion. In this sit- uation, more predators can locate the ock causing a constant threat to the birds. A larger ock will also require a greater amount of food which causes more competition for food. This may result in death of some weaker birds of the group. It is important to mention here that PSO does not simulate the disadvantages of the birds ocking Particle Swarm Optimization 3 behavior and therefore, during the search process killing of any individual is not al- lowed as in Genetic Algorithms where some weaker individuals die out. In PSO, all individuals remain alive and try to make themselves stronger throughout the search process. The improvement in potential solutions in PSO is due to cooperation while in evolutionary algorithms it is due to competition. This concept makes swarm intel- ligence different from evolutionary algorithms. In short, in evolutionary algorithms a new population is evolved in every generation / iteration while in swarm intelligent algorithms in every generation / iteration individuals make themselves better. Identity of the individual does not change over the iterations. Mataric [7] gave the following rules for birds ocking: 1. Safe Wandering: When birds y they are not allowed to collide with each other and with obstacles. 2. Dispersion: Each bird will maintain a minimum distance with any other. 3. Aggregation: Each bird will also maintain a maximum distance with any other. 4. Homing: All birds will have potential to nd a food source or the nest. All these four rules have not been adopted in the simulation of birds ocking behavior while designing the PSO. In basic PSO model developed by Kennedy and Eberhart, safe wandering and dispersion rules are not followed for the movement of agents. In other words, during the movement in basic PSO agents are allowed to come closer to each other as they can. While aggregation and homing are valid in the PSO model. In PSO, agents have to y within a particular region so that they can maintain a maximum distance with any other agent. This is equivalent to the fact that throughout the process, search remains within or at the boundaries of the search space. The fourth rule, homing says that any agent in the group may reach to the global optima. For the development of PSO model, Kennedy and Eberhart followed ve fun- damental principles which determine whether a group of agents is a swarm or not [12]: 1. Proximity Principle: the population should be able to carry out simple space and time computations. 2. Quality Principle: the population should be able to respond to quality factors in the environment. 3. Diverse Response Principle: the population should not commit its activity along excessively narrow channels. 4. Stability Principle: the population should not change its mode of behaviour every time the environment changes. 5. Adaptability Principle: the population should be able to change its behaviour mode when it is worth the computational price. 1.2 Particle Swarm Optimization Process Considering these ve principles Kennedy and Eberhart developed a PSO model for function optimization. In PSO, the solution is obtained through a random search 4 Jagdish Chand Bansal equipped with swarm intelligence. In other words, PSO is a swarm intelligent search algorithm. This search is done by a set of randomly generated potential solutions. This collection of potential solutions is known as swarm and each individual poten- tial solution is known as a particle. In PSO, the search is in uenced by two types of learning by the particles. Each particle learns from other particles and it also learns from its own experience during the movement. The learning from others may be referred as social learning while the learning from own experience as cognitive learning. As a result from social learning, the particle stores in its memory the best solution visited by any particle of the swarm which we call as gbest. As a result of cognitive learning, the particle stores in its memory the best solution visited so far by itself, called pbest. Change of the direction and the magnitude in any particle is decided by a factor called velocity. This is the rate of change in the position with respect to the time. With reference to the PSO, time is the iteration. In this way, for PSO, the velocity may be de ned as the rate of change in the position with respect to the iteration. Since iteration counter increases by unity, the dimension of the velocity v and the position x becomes the same. For a D dimensional search space, the ith particle of the swarm at time step t is represented by a D- dimensional vector, xt i = (xt i1,xt i2,...,xt iD)T. The velocity of this particle at time step t is represented by another D dimensional vector vt i = (vt i1,vt i2,...,vt iD)T. The previously best visited position of the ith particle at time step t is denoted as pt i = (pt i1, pt i2,..., pt iD)T . g is the index of the best particle in the swarm. The velocity of the ith particle is updated using the velocity update equation in (1). Velocity Update Equation: vt+1 id = vt id +c1r1(pt id xt id)+c2r2(pt gd xt id) (1) The position is updated using position update equation in (2). Position Update Equation: xt+1 id = xt id +vt+1 id (2) where d = 1,2,...,D represents the dimension and i = 1,2,...,S represents the par- ticle index. S is the size of the swarm and c1 and c2 are constants, called cognitive and social scaling parameters, respectively or simply acceleration coef cients. r1, r2 are random numbers in the range [0,1] drawn from a uniform distribution. It appears from equations (1) and (2) that every particle s each dimension is updated indepen- dently from the others. The only link between the dimensions of the problem space is introduced via the objective function, i.e., through the locations of the best positions found so far gbest and pbest [9]. Equations (1) and (2) de ne the basic version of PSO algorithm. An algorithmic approach of PSO procedure is given in algorithm 1: Particle Swarm Optimization 5 Algorithm 1: Basic Particle Swarm Optimization Create and Initialize a D-dimensional swarm, S and corresponding velocity vectors ; for t= 1 to the maximum bound on the number of iterations do for i=1 to S do for d=1 to D do Apply the velocity update equation 1; Apply position update equation 2; end Compute tness of updated position; If needed, update historical information for pbest and gbest; end Terminate if gbest meets problem requirements; end 1.3 Understanding Update Equations The right hand side in the velocity update equation (1), consists of three terms [3] : 1. The previous velocity v, which can be thought of as a momentum term and serves as a memory of the previous direction of movement. This term prevents the par- ticle from drastically changing direction. 2. The second term is known as the cognitive or egoistic component. Due to this component, the current position of a is attracted towards its personal best position. In this way, throughout the search process, a particle remembers its best position and thus prohibits itself from wandering. Here, it should be noted that (pid xid) (superscript t is dropped just for simplic- ity) is a vector whose direction is from xid to pid which results the attraction of current position towards the particle s best position. This order of xid and pid must be maintained for attraction of current position towards the particle s best posi- tion. If we write the second term using vector (xid pid) then the current position will repel from the particle s best position. 3. The third term is called social component and is responsible for sharing informa- tion throughout the swarm. Because of this term a particle is attracted towards the best particle of the swarm, i.e. each particle learns from others in the swarm. Again the same reason stands here also to keep the order of xid and pgd in the vector (pgd xid). It is clear that cognitive scaling parameter c1 regulates the maximum step size in the direction of the personal best position of that particle while social scaling parame- ter c2 regulates the maximum step size in the direction of global best particle. Figure 1 presents a typical geometric illustration of a particle s movement in a 2-Dimensional space 6 Jagdish Chand Bansal Fig. 1: Geometric Illustration of Particle s Movement in PSO Process From the update equations, it is also clear that the PSO design of Kennedy and Eberhart follows ve basic principals of PSO, described in section 1.1. In the PSO process, calculations are carried out over a series of time steps in a D-dimensional space. Population at any time step, follows the direction guided by gbest and pbest, i.e. the population is responding to the quality factors and thus the quality principal is adhered to. Because of uniformly distributed random numbers r1 and r2 in the velocity update equation, a random allocation of current position between pbest and gbest justi es the diverse response principle. Principle of stability is also justi ed in PSO process because no particle of the swarm moves randomly but only when it receives a better information from gbest. The swarm changes when gbest changes and therefore adaptability principle is adhered to. 2 Particle Swarm Optimization Parameters The convergence speed and the ability of nding optimal solution of any popula- tion based algorithm is greatly in uenced by the choice of its parameters. Usually, a general recommendation for the setting of parameters of these algorithms is not possible as it is highly dependent upon the problem parameters. However, theoretical and/or experimental studies have been carried out to recommend the generic range for parameter values. Likewise other population based search algorithms, tuning of parameters for a generic version of PSO has always been a challenging task due the presence of stochastic factors r1 and r2 in the search procedure. The basic version of Particle Swarm Optimization 7 PSO enjoys the luxury of very few parameters. This chapter discusses parameters of only the basic version of PSO introduced in [6]. One radical parameter is the swarm size which is often set empirically on the basis of the number of decision variables in the problem and problem complexity. In general, 20-50 particles are recommended. Another parameters are scaling factors, c1 and c2. As mentioned earlier, these parameters decide the step size of the particle for the next iteration. In other words, c1 and c2 determine the speed of particles. In basic version of PSO, c1 = c2 =2 were chosen. With this choice, particle s speed increases without control which is good for faster convergence rate but harmful for better exploitation of the search space. If we set c1 = c2 > 0 then particles will attract towards the average of pbest and gbest. c1 > c2 setting will be bene cial for multimodal problems while c2 > c1 will be bene cial for unimodal problems. Small values of c1 and c2 will provide smooth particle trajectories during the search procedure while larger values of c1 and c2 will be responsible for abrupt movements with more acceleration. Adaptive acceleration coef cients have also been proposed by the researchers [? ]. Stopping criterion is also a parameter not only for PSO but for any population based meta-heuristic algorithm. Popular stopping criteria are usually based on maxi- mum number of function evaluations or iterations which are proportional to the time taken by the algorithm and acceptable error. A more ef cient stoping criteria is based on the available search capacity of the algorithm. If an algorithm does not improve the solution with a signi cant amount upto a certain number of iterations, search should be stopped. 3 A Worked-Out Example In this section, a numerical example is explained for better understanding of the work- ing of PSO. For simplicity, following sphere function in two dimension is considered to minimize using PSO. Minf(x1,x2) = x2 1 +x2 2; where x1,x2 ( 5,5). First, we generate swarm of size 5, randomly using uniform distribution in the range (-5, 5): The position matrix x = xij 1 2 1 4.7059 -0.7824 2 4.5717 4.1574 3 -0.1462 2.9221 4 3.0028 4.5949 5 -3.5811 1.5574 As mentioned in the PSO Algorithm 1, initialization of velocity vectors is also required at this stage. Velocity corresponding to a particle is initialized in the range 8 Jagdish Chand Bansal [ Vmax,+Vmax]. Here Vmax, the maximum velocity bound is a PSO parameter and is usually set Vmax = Xmax. Therefore, in this example velocity vector is generated uniformly in the range [ 5,5]. The velocity matrix V = vij 1 2 1 4.0579 -2.215 2 -3.7301 0.4688 3 4.1338 4.5751 4 1.3236 4.6489 5 -4.0246 -3.4239 Next step is the objective function evaluation for the current position matrix. The tness is usually the value of the objective function in the optimization problem being solved. A solution with better objective function value represents a better t solution. Since the considered problem is a minimization problem, we will consider a solution better t if it has small objective function value. Substituting x11 = 4.7059 and x12 = 0.7824 in the objective function f = x2 1 +x2 2, we get 22.7576. Similarly, calculating objective function value for other position vectors, we get the following initial tness matrix: It can be observed that the minimum of these 5 objective function values f1 22.7576 f2 38.1844 f3 8.5600 f4 30.1299 f5 15.2497 corresponding to 5 particles is 8.5600. Therefore, the most t solution of this swarm is x3 = ( 0.1462,2.9221) which we call gbest . Since this is the rst iteration, no previous iteration exists for the comparison and therefore every particle s current position is also the pbest position. Now we proceed to the next iteration using PSO update equations. It should be noted here that all calculations for velocity and position update are carried out com- ponent wise. Let us consider to update the rst particle x1 = (4.7059, 0.7824). First we will update its rst component x11 = 4.7059. The velocity component corresponding to x11 is v11 = 4.0579. Therefore, we will apply velocity update equation to v11 as fol- lows: (considering c1 = c2 = 2 and r1 = 0.34,r2 = 0.86) v11 = 4.0579+2 0.34 (4.7059 4.7059)+2 0.86 ( 0.1462 4.7059) = 4.2877 Since the updated velocity component 4.2877 lies in the range [ 5,5], we accept the value for updating position. In case of updated velocity component value goes Particle Swarm Optimization 9 beyond the pre-speci ed range, we will consider the nearest boundary value. For example, suppose the updated velocity component is 5.8345 then we will set it to 5 because the maximum bound of velocity on this side is 5. Now if the component is 6.8976, the updated velocity will be set equal to 5 because of the similar reason. Now the position update equation for x11 is x11 = 4.7059+( 4.2877) = 0.4182 Since the updated solution component lies in the search space ( 5,5), we accept the solution. If the updated position does not lie within the given search space, there are many methods suggested by researchers to deal with the situation, some of them will be discussed in the chapter 2. For this example, we will randomly re-initialize the particle if the updated value falls outside the search space boundary. Similarly, we will update the second component x12 = 0.7824. To update this, we will rst apply velocity update equation on v12 = 2.215. v12 = 2.215+2 0.47 ( 0.7824 ( 0.7824))+2 0.91 (2.9221 ( 0.7824)) = 4.5272 Updated v12 = 4.5272 is in the range [ 5,5] and therefore we will use this value to update x12. x12 = 0.7824+4.5272 = 3.7448 Updated x12 is again within the search space ( 5,5) so we accept the solution. Thus the rst particle after applying the PSO update equations becomes: x1 = (0.4182,3.7448) We update all the particles using the same procedure. Second Particle: v21 = 3.7301+2 0.34 (4.5717 4.5717)+2 0.86 ( 0.1462 4.5717) = 11.8449 x21 = 4.5717+( 11.8449) = 7.2732 Since the updated value of x21 is out of the search space, we re-initialize this x21 in the range ( 5,5). Let x21 = 3.4913. v22 = 0.4688+2 0.12 (4.1574 4.1574)+2 0.06 (2.9221 4.1574) = 0.3206 x22 = 4.1574+0.3206 = 4.4780 10 Jagdish Chand Bansal Thus the second particle after PSO updating becomes: x2 = (3.4913,4.4780) Third Particle: v31 = 4.1338+2 0.69 ( 0.1462 ( 0.1462))+2 0.34 ( 0.1462 ( 0.1462)) = 4.1338 x31 = 0.1462+4.1338 = 3.9876 v32 = 4.5751+2 0.69 (2.9221 2.9221)+2 0.34 (2.9221 2.9221) = 4.5751 x32 = 2.9221+4.5751 = 7.4972(exceeding the search space bounds.) x32 = random value in the range (-5, 5) = 4.3399 Thus the updated third particle is (3.9876,4.3399). Fourth Particle: v41 = 1.3236+2 0.18 (3.0028 3.0028)+2 0.23 ( 0.1462 3.0028) = 0.1249 x41 = 3.0028+( 0.1249) = 2.8779 v42 = 4.6489+2 0.61 (4.5949 4.5949)+2 0.94 (2.9221 4.5949) = 1.5040 x42 = 4.5949+1.5040 = 6.0989(falls outside the search space bounds.) x42 = random value in the range (-5, 5) = 2.5774 Updated fourth particle is (2.8779,2.5774). Fifth Particle: v51 = 4.0246+2 0.09 ( 3.5811 ( 3.5811))+2 0.39 ( 0.1462 ( 3.5811)) = 1.3454 x51 = 3.5811+( 1.3454) = 4.9265 v52 = 3.4239+2 0.65 (1.5574 1.5574)+2 0.10 (2.9221 1.5574) = 3.1510 x52 = 1.5574+( 4.0246) = 2.4672 Particle Swarm Optimization 11 Updated fth particle is ( 4.9265, 2.4672). Therefore, after this initial iteration, the updated velocity matrix v is shown in Table 1, the updated position matrix x and the tness matrix are shown in Table 2 and Table 3, respectively. Table 1: Updated velocity matrix vij 1 2 1 -4.2877 4.5272 2 -11.8449 0.3206 3 4.1338 4.5751 4 -0.1249 1.504 5 -1.3454 -3.151 Table 2: Updated position matrix xij 1 2 1 0.4182 3.7448 2 3.4913 4.4780 3 3.9876 4.3399 4 2.8779 2.5774 5 -4.9265 -2.4672 Table 3: Updated tness values f1 14.1984 f2 32.2416 f3 34.7356 f4 14.9252 f5 30.3574 Clearly, it can be seen that the minimum objective function value is 14.1984 which corresponds to the rst particle. Therefore, gbest for the updated swarm is the rst particle x1. Now we compare this gbest with the previous gbest, obviously updated gbest is not better than the previous one so for the carrying out the next iteration, we consider the gbest of previous iteration ( 0.1462,2.9221). Now for each particle, we observe the selection of pbest. It should be noted that gbest is for the whole swarm and pbest is for a particular particle. For the rst particle: Fitness in the previous swarm = 22.7576 Fitness in the current swarm = 14.1984 12 Jagdish Chand Bansal Clearly, the tness of current swarm is better than that of its previous, so we set pbest1 = (0.4182,3.7448). On the other hand, if the tness of current swarm would not be better than that of its previous then the current pbest and old pbest would be the same. Similarly, for the second particle: pbest2 = (3.4913,4.4780); for the third particle: pbest3 = (2.2534, 3.1379); for the fourth particle: pbest4 = (1.6400, 1.3202) and for the fth particle: pbest5 = (2.2668, 2.0009). The same procedure is continued until the termination criterion is attained. As a nal note to the chapter, PSO is a dynamic population of active, interactive agents with no inherent intelligence . In PSO each individual teaches its neighbor, each individual learns from its neighbors. During the search procedure, potential so- lutions make better than random guesses using Collaborative Trial and Error strate- gies. These guesses are better than random search because they are informed by social learning. Since its inception, PSO has seen many changes which made it a strong can- didate for numerical optimization. Researchers have applied PSO to almost all kind of problems where a numerical optimization technique is expected to work. Particle Swarm Optimization is quite exible for modi cations according to the problem requirements. Therefore, even after 23 years of its invention, there is enough scope to modify PSO and apply it to new complex optimization problems. References 1. Jagdish Chand Bansal and Kusum Deep. A modi ed binary particle swarm optimization for knapsack problems. Applied Mathematics and Computation, 218(22):11042 11061, 2012. 2. Y lmaz Delice, Emel K z lkaya Aydo gan, U gur Ozcan, and Mehmet S tk Ilkay. Balancing two-sided u-type assembly lines using modi ed particle swarm opti- mization algorithm. 4OR, 15(1):37 66, 2017. 3. Andries P Engelbrecht. Computational intelligence: an introduction. Wiley. com, 2007. 4. Jingwei Feng, Fengchun Tian, Pengfei Jia, Qinghua He, Yue Shen, and Shu Fan. Improving the performance of electronic nose for wound infection detection us- ing orthogonal signal correction and particle swarm optimization. Sensor Review, 34(4):389 395, 2014. 5. Indu Jain, Vinod Kumar Jain, and Renu Jain. Correlation feature selection based improved-binary particle swarm optimization for gene selection and cancer clas- si cation. Applied Soft Computing, 62:203 215, 2018. 6. Kennedy James and Eberhart Russell. Particle swarm optimization. In Proceed- ings of 1995 IEEE International Conference on Neural Networks, pages 1942 1948, 1995. 7. Maja J Mataric. Interaction and intelligent behavior. Technical report, DTIC Document, 1994. Particle Swarm Optimization 13 8. Seyed Mohsen Mousavi, Ardeshir Bahreininejad, S Nurmaya Musa, and Farazila Yusof. A modi ed particle swarm optimization for solving the integrated lo- cation and inventory control problems in a two-echelon supply chain network. Journal of intelligent manufacturing, 28(1):191 206, 2017. 9. Ioan Cristian Trelea. The particle swarm optimization algorithm: convergence analysis and parameter selection. Information processing letters, 85(6):317 325, 2003. 10. webpage. http://birding.about.com/od/birdbehavior/a/why-birds- ock.htm. 11. Edward Wilson. 0.(1975) sociobiology: The new synthesis, 1980. 12. Bin Yang. Modi ed particle swarm optimizers and their application to robust design and structural optimization. PhD thesis, Mu?nchen, Techn. Univ., Diss., 2009, 2009. View publication stats