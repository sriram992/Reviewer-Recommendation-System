Noname manuscript No. (will be inserted by the editor) A Hinge-Loss based Codebook Transfer for Cross-Domain Recommendation with Nonoverlapping Data Sowmini Devi Veeramachaneni Arun K Pujari Vineet Padmanabhan Vikas Kumar Received: date / Accepted: date Abstract Recommender systems(RS), especially collaborative ltering(CF) based RS, has been playing an impor- tant role in many e-commerce applications. As the information being searched over the internet is rapidly increasing, users often face the di culty of nding items of his/her own interest and RS often provides help in such tasks. Recent studies show that, as the item space increases, and the number of items rated by the users become very less, issues like sparsity arise. To mitigate the sparsity problem, transfer learning techniques are being used wherein the data from dense domain(source) is considered in order to predict the missing entries in the sparse domain(target). In this paper, we propose a transfer learning approach for cross-domain recommendation when both domains have no overlap of users and items. In our approach the transferring of knowledge from source to target domain is done in a novel way. We make use of co-clustering technique to obtain the codebook (cluster-level rating pattern) of source domain. By making use of hinge loss function we transfer the learnt codebook of the source domain to target. The use of hinge loss as a loss function is novel and has not been tried before in transfer learning. We demonstrate that our technique improves the approximation of the target matrix on benchmark datasets. Keywords Matrix Factorisation Collaborative Filtering Codebook Transfer Learning Cross-Domain Recommendation 1 Introduction Recent years have witnessed a clear explosion in the amount of e-commerce data available. As a result, the quantity of information that needs to be searched for suggesting items of interest to users is becoming more challenging. Recommender systems (1)(2)(21) can be used for predicting items that the users will be interested in and has become a core part of several e-commerce applications. Machine learning techniques (19) have become an integral part in the process of building recommender systems and in the last decade or so there has been a surge in this area of research. In general, the existing algorithms for recommender systems can be broadly categorized into three major categories namely content-based (CB), collaborative (CF) and hybrid recommendation. In content-based method, the recommendation is made by rst calculating a similarity score between a user pro le and the pro le information of an item. Based on the similarity score, a set of top k items are then recommended to a user. For instance, in movie recommendation task, the pro le information of a user contains his/her interest over the genre. A movie is then can be recommended by nding out the similarity between the user interest for the genre and the movie description (genre). In the case of collaborative ltering, the preferences of the target user are matched with that of other users having similar tastes and a recommendation is made. For example, if a set of users likes the same Sowmini Devi Veeramachaneni, Vineet Padmanabhan School of Computer and Information Sciences, University of Hyderabad, Hyderabad, India Tel.: +91-8985061848 E-mail: sowmiveeramachaneni@gmail.com, vineetcs@uohyd.ernet.in Arun K Pujari Mahindra University Ecole Centrale School of Engineering (MEC), Hyderabad, India E-mail: arun.k.pujari@gmail.com Vikas Kumar University of Delhi, Delhi, India E-mail: vikas007bca@gmail.com arXiv:2108.01473v1 [cs.IR] 2 Aug 2021 2 Sowmini Devi Veeramachaneni et al. movies as that of user A (say target user), then there is a chance that user A will like the movies liked by the set of users which the user A has not seen. The hybrid method combines both the content-based and collaborative-based models. The major shortcoming with content based recommendation is the situations where the item features are not meaningful or situations where there is a need to capture the change in user interests over time. On the other hand, collaborative ltering handles the above mentioned situations as it only requires the preference (implicit or explicit) information for recommendation. The rating matrix is memorised in the memory-based methods and the missing entries are predicted based on the relation (ratings) between similar users/items. Among memory-based methods, neighbourhood-based methods are prominent wherein users/items that are similar to target user/item are considered for prediction. The idea is that any two users having similar ratings for few items will also tend to have similarity in ratings for the remaining items too. In addition to the assumption, it is also assumed that if rating similarity is there for two items among some users then other users will also give the same rating to the items. Identifying similar users as that of the target user and using the mean of their ratings for prediction is the crux of user-based CF methods. In the case of item-based CF it is the other way around. Depending on how the weighted average is computed (Pearson correlation, Vector Cosine, Mean-squared-di erence etc.), neighbourhood methods may vary. In the case of model based methods, missing ratings can be predicted more quickly by making use of a parametric model which is built from the training data. Low-rank matrix factorisation, which is a latent factor model, is a successful and an e ective model-based CF approach. In the Matrix Factorisation (MF) approach as proposed in (8) and one of the most widely-used CF technique, a small set of user and item latent factors are learnt based on the user-item rating matrix. By making use of the matrix factorisation method as outlined in (8) for each user (or, each item) one latent vector is constructed by extracting the latent factors of users/items and this vector captures the user s (or, item s) characteristics. To obtain the user ratings for an item, the product of a user latent vector with that of an item is made use through a process called low-dimensional embedding. Maximum Margin Matrix Factorisation (MMMF) (24) is perhaps the most widely used and successful matrix factorisation technique. Recommender systems often face with the problem of sparsity which results from less amount of information being available of the data in hand. Machine learning techniques like transfer learning (12) has been applied in collaborative ltering based recommender systems to avoid the problems related to sparsity. Transfer learning involves the construction of a predictive model across multiple domains wherein relevant knowledge from one domain is extracted and transferred to another domain. Transfer learning in collaborative ltering-based recommender systems is otherwise called as cross-domain collaborative ltering. When multiple domains are involved it is necessary for transfer learning to address crucial issues related to the presence/absence of aligned users/items during knowledge transfer. It is often the case that the presence of common users/items across domains do not exist and even if it does exist it is di cult to map their correspondence. Transfer learning in the absence of common users/items in the two domains is often hard to comprehend and CodeBook transfer (CBT) (9) has been proposed as a transfer learning approach to address these type of problems. Codebook construction is based on the idea that the abundance of information in one domain is to be made use to predict the missing information in another domain. Since it is assumed that certain kind of rating pattern is involved in any kind of data, the codebook tries to capture this rating pattern in a summarised form. The belief/hypothesis is that across domains these rating patterns are unvarying. Not only in recommender systems, but also in many machine learning algorithms, loss functions plays a signi cant role in empirical risk minimization and computational complexities (10; 22). So, choice of loss function is very important. Among all the loss functions, hinge loss is more suitable for discrete classi cation task. Also, among all the matrix factorisation techniques, maximum margin matrix factorisation(MMMF) treats the collaborative prediction problem as a classi cation task and takes full advantage of the hinge loss. Until now MMMF has been used in single domain recommender systems and in our proposed approach, we adapt it to work for cross domain recommendation by taking advantage of the hinge loss function. This paper proposes a codebook based transfer method for collaborative ltering in cross domain recommender systems. Our proposed approach consists of two steps: in the rst, we construct the codebook by making use of the source data and apply a technique called co-clustering on it. In the second step, the idea is to transfer the learned codebook in a novel way to the target domain by making use of hinge loss as the loss function. In previous works, researchers have made use of squared loss while transferring the source knowledge to the target domain in order to predict the missing ratings of target domain. The proposed codebook based transfer method for collaborative ltering outperforms MMMF (i.e., when MMMF is directly used on target data) and other major codebook based transfer learning approaches for CF as can be seen from the experimental results. A Hinge-Loss based Codebook Transfer for Cross-Domain Recommendation with Nonoverlapping Data 3 The remainder of the paper is structured in the following manner: Section 2 presents a quick review of current work on transfer learning in recommendation systems and the proposed approach is explained in Section 3. The experimental results are given in Section 4, and Section 5 provides the conclusion of the work. 2 Related work Matrix Factorisation has been very popular and successful in predicting the missing values of the user-item rating matrix even when the data is too sparse. The advantage of matrix factorisation technique is that for a matrix which is very sparse and has less information available, the matrix can be lled up accurately to a satisfactory level. Although this is quite good as far as MF techniques for recommender systems are concerned, the major drawback with having more sparsity is the failure in the accuracy of the prediction levels. To overcome such scenarios researchers have come up with the idea of using information from diverse domains so that information of one domain could be utilized in another. Transfer learning (12; 13) is one such technique proposed, which makes use of cross domain collaborative ltering in the domain of recommendation systems to alleviate the sparsity problem. Transfer learning intents to transfer the information(knowledge) from a rich(dense) source domain to the sparse target domain. For example, suppose that a particular user has watched many movies and have rated the same. The same user has very less ratings in another domain related to books but wants a book to be recommended, then by using his ratings from the movie domain book can be recommended. Formally, given a dense source rating matrix (source domain), and a sparse target rating matrix (target domain), the goal is to predict the missing entries of target domain by utilizing the information available in the source domain. The major question that was unanswered is to determine whether both domains are suitable for knowledge transfer. This is necessary because transfer of knowledge can take place only if a correspondence can be established among two domains. Certain assumptions need to be made for applying transfer learning strategy which includes having either common subset of items/users or attribute similarity in items/users. Domains can be linked and the transfer can happen explicitly via inter-domain similarities, common item attributes, etc. The transfer can happen also implicitly via shared user latent features or item latent features or by rating patterns which can be transferred between the domains. In (3), a framework was proposed in which the pertinent items in the source domain are picked based on their common attributes with the target domain (user interested domain). The inter domain links were created utilising the common item attributes, however there is no overlap of users/items required between the domains. On the other hand the transfer of knowledge by the shared latent features (of users/items) is addressed in (18). In this work the latent features of users and items of source domain are learnt and tted to a target domain by integrating the features into the factorisation of target rating matrix via regularization. However, it require either common users or items among the two domains. In (14), the latent characteristics(features) of source and target are shared in a collective way. In this paper a method called matrix factorisation is employed wherein features of both domains are learned simultaneously rather than learning from source rst and later applying in the target domain. The only constrain is that in both domains the users and items needs to be identical. The main assumption that is made in this method is that the change of domain does not a ect the rating behaviour of the same user/item or di erent users/items with similar attributes. In order to maximise knowledge transfer, in (26; 27) the authors presume that across di erent domains there is a correspondence between users/items. Here rather than starting from source domain, matrix factorisation technique is applied rst in the target rating data so as to learn the latent features of the target. After that based on certain criteria, random users/items gets selected from the target domain and is matched with the corresponding users/items from source domain. In order to ensure that the latent factors of the selected users/items of both the target and source domain are same, the authors make use of some kind of regularisation function. The methods as outlined in (15; 16; 17) takes care of scenarios wherein the feedback data of target and source are of di erent types. The users and items in the target and auxiliary(source) domains are assumed to be the same in (15). Two sets of source data are taken into consideration in (23), one of which shares a common set of users with the target data and the other of which shares a common set of items with the target data. The source data s latent factors are extracted, and similarity graphs are constructed from these latent factors. Both latent factors and similairty graphs are then transferred to the target data. In collaborative ltering approach that is based on codebook transfer(CBT) (9) the idea is to capture the hidden correlations that exist between the ratings given by groups of users and groups of items. This pattern is usually denoted by the name codebook or cluster-level rating pattern. In codebook transfer(CBT) what happens is that initially, the codebook of the source domain is extracted by analysing the rating matrix and thereafter this codebook is used for the prediction of the target domain. Regardless of the domains, the way the users and items cluster 4 Sowmini Devi Veeramachaneni et al. Fig. 1 Illustration of the proposed method together it so happens that across domains the cluster level rating behaviour remains invariant. One assumption that is made in this connection is regarding the source matrix being a full matrix. Cases wherein the rating matrix is not full, the average rating of a particular user is used for lling-in missing entries of that user. Thereafter the rows (users) and columns (items) of the lled-in rating matrix are clustered using co-clustering (9). The way the codebook is generated is such that each user group is assigned a row and each item group has a column, and each user group and item group has a rating. The average rating of users and items in a particular group is taken into consideration for giving a rating for user-group/item-group. In order to use the codebook in the targt domain it is necessary to select the users/items in the target domain that are more appropriate with the user-group/item-group of the transferred codebook. A linear combination of codebooks from multiple domains is shown in (11) wherein using an optimisation technique the coe cient of linear combinations is learned. The assumption of having a fully dense rating matrix is relaxed as demonstrated in (5; 6). A di erent way of generating the codebook has been outlined in (7; 25). In this method, by making use of the technique of matrix factorisation the user and item latent factors are generated from the source domain and thereby tends to avoid any pre-processing and co-clustering of the rating matrix. The user and item latent factors thus generated are used to obtain the user latent facor group and item latent factor group. Codebook is generated by multiplying the mean latent vecores of the group. In (9), the authors focus on extracting the group level behavior of users on items by assuming that, though the users/items are di erent across systems, the groups (groups - based on age, interest etc) of them behave similarly. All the missing ratings in the source rating matrix are lled in a priori by a preprocessing step and then co- clustering is applied on a lled-in source rating matrix to directly get the cluster-level rating pattern (codebook). The cluster-level rating pattern is then transferred to another domain (target). In the target domain, the user and item membership to clusters encode in the codebook so that a user (or, an item) is member of one user-cluster (or, item-cluster) represented in the codebook. In our approach, we do not use a separate preprocessing stage in the source domain, and we use hinge loss instead of squared loss when transferring the learned codebook from the source to the target domain. 3 Proposed Method Given a dense source user-item rating matrix X Rm n and a sparse target user-item rating matrix Y Rm n where m, m are the number of users in source and target domain, and n, n are the number of items in source and target data, the goal is to predict the missing entries (as numerous users do not rate many items) in target domain by leveraging the data from the source domain. Prediction of missing entries should take place in such a way that the existing ratings must be approximated with less error rate. The illustration of the proposed method is shown in Figure 1. Initially (Step-1), the users (rows) and items (columns) of the source rating matrix need to be simultaneously clustered (co-clustering) to construct the codebook. We need the cluster indicators of users and items, and in order to get the cluster indicators we can choose any of the co-clustering algorithms. We employed the Orthogonal nonnegative matrix tri-factorisation technique (ONMTF) (4) to get the cluster indicators, which has been shown to be equivalent to a two-way k-means clustering approach. The source rating matrix X can be tri-factorised as follows. min P,Q,S 0 ||[X PSQT ] W||2 F + ||P1 1||2 F + ||Q1 1||2 F (1) where W is an indicator matrix and the size of W is m n. The entry of W is 1 if the rating exists in X and 0 for the rest. ||.||F represents the Frobenius norm. The dimension of P is m k1, S is k1 k2, and Q is n k2. A Hinge-Loss based Codebook Transfer for Cross-Domain Recommendation with Nonoverlapping Data 5 P1 1 ensures the row sum of P to be one, similarly with Q. Maximum value of each row of P and Q becomes the cluster indicator for the user/item in that row. These P and Q are not in a recognizable form in terms of user/item membership matrices. To represent P and Q, we use binary values by setting the maximum valued entry in each row to be 1 and the others to be 0. As a result, these binary user/item cluster indicators form (membership) matrices, denoted by Ps and Qs, for the source rating matrix. From these matrices, we can construct the codebook B as follows. B = [P T s XQs] [P T s 11T Qs] (2) indicates element-wise division. Averaging of all the ratings in each of the user/item co-cluster takes place in Eq. 2 Once the codebook from the source domain is constructed, transfer (Step-2) it to the target domain (Y ) by substituting it in Eq. 3 and solve the objective function (J ) in order to get U and V of target data. We use Hinge loss to learn U and V instead of squared loss. As the data of ours is discrete, apart from U and V , r 1 thresholds ic (1 c r 1) for each user i has to be learned to classify the prediction value into r discrete values. Here, r is the maximum rating (say 5). J (U, V, ) = X (i,j) r 1 X c=1 h(T c ij( ic UBV T )) + 2 (||U||2 F + ||V ||2 F ) (3) where, T c ij = ( 1 if c < yij +1 if c yij h( ) is a smoothed hinge-loss function de ned as, h(d) = 0 if d 1 1 2(1 d)2 if 0 < d < 1 1 2 d otherwise. is the set of observed entries, > 0 is regularization parameter. Gradient based approach can be used to solve the optimization function (Eq. 3) and following are the gradients. J Uik1 = Uik1 r 1 X c=1 X j|ij T c ij.h (T c ij( ic UBV T ))V BT J Vjk2 = Vjk2 r 1 X c=1 X i|ij T c ij.h (T c ij( ic UBV T ))UB J ir = X j|ij T c ij.h (T c ij( ic UBV T )) where, h (d) = 0 if d 1 d 1 if 0 < d < 1 1 otherwise. In gradient descent algorithms, we start with random U, V , and iteratively update them using the following update rules. U t+1 ik1 = U t ik1 c J U t ik1 V t+1 jk2 = V t jk2 c J V t jk2 t+1 ia = t ia c J t ia c is the trade-o parameter. Once U, V of target data are learnt, product of U, B, and V T (i.e., UBV T ) mapped with the threshold matrix ( ) becomes the approximation (prediction {1, 2, ..., r} say r = 5) of the target rating matrix (Step-3). This is illustrated in Algorithm 1. 6 Sowmini Devi Veeramachaneni et al. Algorithm 1: Codebook transfer via hinge loss 1: Input: A source rating matrix X of size m n and a sparse target rating matrix Y of size m n with yij known for (i, j) 2: Output: Y (yij for (i, j) / ) 3: Tri-factorise X using Eq. 1 inorder to get P, Q and S. 4: Calculate codebook B using Eq. 2. 5: Transfer the codebook to the target data by substituting in Eq. 3. 6: Solve the optimization (Eq. 3) using gradient descent technique inorder to get the target U, V and . 7: Product of U, B, V T (i.e., UBV T ) by mapping with the threshold matrix gives Y . 4 Experimental Results and Analysis To evaluate the performance of our method we have experimented the method with di erent datasets. The datasets used are MovieLens 100K1, MovieLens 1M1, Goodbooks2, Douban Music3, Douban Book3. From the Goodbooks dataset we have considered the rst 5000 users and 3000 items. From the Douban Music and Douban Book data, we have considered the rst 2000 users and 2000 items. The entries of all the datasets fall in {0,1,2,3,4,5}, where 0 indicates the missing value, 1 indicates that the item is leastly liked and 5 indicates that the item is heavily liked. The statistics of the datasets are given in Table 1. In all the experiments, we have divided the observed data into 80% and 20%, in which 80% is used for training, and 20% is used for testing. Table 1 Datasets statistics Dataset # of Users # of Items % of Observed entries MovieLens 100K 945 1682 6.29 MovieLens 1M 6040 3952 3.77 Goodbooks (Subset) 5000 3000 1.08 Douban Music (Subset) 2000 2000 11.26 Douban Book (Subset) 2000 2000 5.51 4.1 Metrics used for Evaluation As far as collaborative ltering algorithms are concerned the computation of the prediction accuracy is crucial for assessing the performance of the algorithms. Root Mean Square Error (RMSE) (Eq. (4)) and Mean Absolute Error (MAE) (Eq. (5)) are the two most widely used measures for computing the prediction accuracy. The di erence between the predicted rating and the true(actual) rating forms the basis for these measures and it is natural that better performance means smaller values for RMSE and MAE. The values stated in the tables are the average of ve runs. RMSE = v u u t X (i,j) (yij yij)2 | | (4) MAE = X (i,j) |(yij yij)| | | (5) where yij is the original rating and yij is the predicted rating, | | is the count of test ratings. 4.2 Comparison Techniques Some of the methods we consider for evaluating our proposed method are outlined below; 1 https://grouplens.org/datasets/movielens/ 2 https://github.com/zygmuntz/goodbooks-10k 3 https://github.com/hezi73/TRACER/blob/master/douban.rar A Hinge-Loss based Codebook Transfer for Cross-Domain Recommendation with Nonoverlapping Data 7 Fig. 2 Impact of number of clusters (k1, k2) on RMSE of MovieLens 1M data when MovieLens 100K is considered as source MMMF (24; 20): Maximum Margin Matrix Factorisation (MMMF) is the dominant factorisation technique used in collaborative ltering. MMMF is usually applied on the input rating matrix consisting of the user-item ratings. The idea is to nd the user and item latent-factor vectors which are of low rank by making use of the existing ratings. MMMF can be applied on a single domain only, and hence in our experiments, we applied it on the target domain directly. MINDTL (6): In MINDTL, codebook is constructed by taking into consideration the data from all the incom- plete source domains. Here codebook for each domain is constructed. Following that, the constructed codebooks are linearly integrated and transferred to the target domain, and the absent(missing) values of the target rat- ing matrix gets predicted. As far as our experimental setup is concerned only a single domain is taken into consideration. TRACER (28): In TRACER, data from multiple domains are accounted for and based on this, ratings (which includes missing ratings) for all the source matrices are predicted. Thereafter the predicted knowledge is utilized by transferring it into the target domain. By making use of consensus regularisation during the knowledge transfer process, all the predicted values are forced to be similar. In a way it can be said that in TRACER at the same time learning and transferring happens. We have considered a single domain in our experiments, therefore there is no necessity for consensus regularisation. We thank the authors for giving the code4 on the web. CBT (9): In this approach, the dense part of the source user-item rating matrix is considered, and the missing values of the rows of the dense matrix get imputed using the average of the ratings of particular row (user). The codebook is obtained from the dense user-item matrix by applying the technique of co-clustering. In our experiments, unlike in (9), which consider only the dense part of the input data, the codebook is constructed by making use of the whole source data and the learnt codebook is transferred to the target domain. We have conducted the experiments with varying number of clusters (k1, k2 - 25, 50, 75, 100, 125, 150, 175, 200) and Fig. 2 and Fig. 3 shows the impact of number of clusters on RMSE and MAE of MovieLens 1M data when MovieLens 100K data is considered as source. By observing the gures we can say that the best performance for this scenario (source - MovieLens 100K, target - MovieLens 1M) is achieved when the number of clusters are in between 100 and 150 and hence we have xed both k1, k2 to 125. In the similar way, when MovieLens 1M is used as source and Goodbooks is the target, the number of clusters is xed to 150. Similarly, when Douban music data is used as source and Douban book is the target, we set the number of clusters to 100. 4 https://github.com/hezi73/TRACER 8 Sowmini Devi Veeramachaneni et al. Fig. 3 Impact of number of clusters (k1, k2) on MAE of MovieLens 1M data when MovieLens 100K is considered as source Table 2 Values of Root Mean Square Error and Mean Absolute Error of baseline methods and proposed method on MovieLens 1M data, Goodbooks data, Douban Book data Dataset Method RMSE MAE MovieLens 1M MMMF 0.9361 0.6402 MINDTL 0.9948 0.7965 TRACER 0.9800 0.8039 CBT 0.9676 0.7746 Proposed 0.9123 0.6134 Goodbooks MMMF 0.9604 0.6524 MINDTL 1.2818 0.9224 TRACER 0.9617 0.7772 CBT 0.9634 0.7886 Proposed 0.9470 0.6381 Douban Book MMMF 0.7976 0.5414 MINDTL 1.1970 0.9054 TRACER 0.8047 0.6600 CBT 0.7828 0.6130 Proposed 0.7785 0.5022 Table 2 shows the values of RMSE and MAE of MovieLens 1M dataset, Goodbooks dataset, Douban Book dataset. For MovieLens 1M data, MovieLens 100K is considered as source and for Goodbooks data, MovieLens 1M is considered as source, and for Douban Book data, Douban music is considered as source data. The rst column gives di erent datasets considered, whereas second column gives the methods considered for comparison along with the proposed method. The third and the fourth columns gives the RMSE and MAE values of the considered methods on datasets. By observing the table, we can say that the proposed method is performing well on any of the datasets considered. We have also experimented our method by considering the same datasets as source and target data. It is nothing but reconstructing the same matrix by using the extracted knowledge. Table 3 gives the results on MovieLens 100k data when the same is used as source, and also on MovieLens 1M data when itself is used as source. By observing the metric values (RMSE, MAE) in the table, we can claim that our method is outperforming all the comparision methods on the datasets considered. A Hinge-Loss based Codebook Transfer for Cross-Domain Recommendation with Nonoverlapping Data 9 Table 3 Values of Root Mean Square Error and Mean Absolute Error of baseline methods and proposed method on MovieLens data by considering same dataset as source and target Dataset Method RMSE MAE MovieLens 100K MMMF 0.9828 0.6808 MINDTL 1.9026 1.6538 TRACER 1.0027 0.8213 CBT 1.0264 0.8239 Proposed 0.9653 0.6600 MovieLens 1M MMMF 0.9349 0.6389 MINDTL 1.8545 1.5984 TRACER 0.9892 0.8145 CBT 1.0729 0.8725 Proposed 0.9138 0.6175 5 Conclusions and Future work A novel method based on codebook based transfer learning for cross domain recommendation has been proposed in this paper. Maximum margin matrix factorisation technique is used to construct the codebook from the source user-item rating matrix. By making use of the hingeloss function in a novel way, the constructed codebook is transferred to the target domain. The experimental results show that the sparse target matrix is approximated well by the proposed method. Rather than considering only ratings of items, in the future it would be worthwhile to consider social tags as well as other types of data. Applying transfer learning in other real world scenarios other than recommender systems could be another direction in which this research could be taken forward. References 1. Aggarwal, C. C. Recommender Systems: The Textbook, 1st ed. Springer Publishing Company, Incorporated, 2016. 2. Bobadilla, J., Ortega, F., Hernando, A., and Guti errez, A. Recommender systems survey. Knowledge-Based Systems 46 (2013), 109 132. 3. Chung, R., Sundaram, D., and Srinivasan, A. Integrated personal recommender systems. In Proceedings of the Ninth International Conference on Electronic Commerce (New York, NY, USA, 2007), ICEC 07, ACM, pp. 65 74. 4. Ding, C., Li, T., Peng, W., and Park, H. Orthogonal nonnegative matrix tri-factorizations for clustering. In Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (New York, NY, USA, 2006), KDD 06, ACM, pp. 126 135. 5. He, M., Zhang, J., Yang, P., and Yao, K. Robust transfer learning for cross-domain collaborative ltering using multiple rating patterns approximation. In Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining (New York, NY, USA, 2018), WSDM 18, ACM, pp. 225 233. 6. He, M., Zhang, J., and Zhang, J. Mindtl: Multiple incomplete domains transfer learning for information recommendation. China Communications 14 (11 2017), 218 236. 7. Ji, K., Sun, R., Li, X., and Shu, W. Improving matrix approximation for recommendation via a clustering-based reconstructive method. Neurocomput. 173, P3 (Jan. 2016), 912 920. 8. Koren, Y., Bell, R., and Volinsky, C. Matrix factorization techniques for recommender systems. Computer 42, 8 (Aug. 2009), 30 37. 9. Li, B., Yang, Q., and Xue, X. Can movies and books collaborate?: Cross-domain collaborative ltering for sparsity reduction. In Proceedings of the 21st International Jont Conference on Arti cal Intelligence (San Francisco, CA, USA, 2009), IJCAI 09, Morgan Kaufmann Publishers Inc., pp. 2052 2057. 10. Masnadi-Shirazi, H., and Vasconcelos, N. On the design of loss functions for classi cation: Theory, robustness to outliers, and savageboost. In Proceedings of the 21st International Conference on Neural Information Processing Systems (USA, 2008), NIPS 08, Curran Associates Inc., pp. 1049 1056. 11. Moreno, O., Shapira, B., Rokach, L., and Shani, G. Talmud: Transfer learning for multiple domains. In Proceedings of the 21st ACM International Conference on Information and Knowledge Management (New York, NY, USA, 2012), CIKM 12, ACM, pp. 425 434. 12. Pan, S. J., and Yang, Q. A survey on transfer learning. IEEE Trans. on Knowl. and Data Eng. 22, 10 (Oct. 2010), 1345 1359. 10 Sowmini Devi Veeramachaneni et al. 13. Pan, W. A survey of transfer learning for collaborative recommendation with auxiliary data. Neurocomputing 177 (2016), 447 453. 14. Pan, W., Liu, N. N., Xiang, E. W., and Yang, Q. Transfer learning to predict missing ratings via heterogeneous user feedbacks. In Proceedings of the Twenty-Second International Joint Conference on Arti cial Intelligence - Volume Three (2011), IJCAI 11, AAAI Press, pp. 2318 2323. 15. Pan, W., and Ming, Z. Interaction-rich transfer learning for collaborative ltering with heterogeneous user feedback. IEEE Intelligent Systems 29 (11 2014), 48 54. 16. Pan, W., Nan Liu, N., Wei Xiang, E., and Yang, Q. Transfer learning to predict missing ratings via heterogeneous user feedbacks. pp. 2318 2323. 17. Pan, W., Xia, S., Liu, Z., Peng, X., and Ming, Z. Mixed factorization for collaborative recommendation with heterogeneous explicit feedbacks. Information Sciences 332 (2016), 84 93. 18. Pan, W., Xiang, E. W., Liu, N. N., and Yang, Q. Transfer learning in collaborative ltering for sparsity reduction. In Proceedings of the Twenty-Fourth AAAI Conference on Arti cial Intelligence (2010), AAAI 10, AAAI Press, pp. 230 235. 19. Portugal, I., Alencar, P., and Cowan, D. The use of machine learning algorithms in recommender systems: A systematic review. Expert Systems with Applications 97 (11 2015). 20. Rennie, J. D. M., and Srebro, N. Fast maximum margin matrix factorization for collaborative prediction. In Proceedings of the 22Nd International Conference on Machine Learning (New York, NY, USA, 2005), ICML 05, ACM, pp. 713 719. 21. Ricci, F., Rokach, L., Shapira, B., and Kantor, P. B. Recommender Systems Handbook, 1st ed. Springer-Verlag, Berlin, Heidelberg, 2010. 22. Rosasco, L., De Vito, E., Caponnetto, A., Piana, M., and Verri, A. Are loss functions all the same? Neural Comput. 16, 5 (May 2004), 1063 1076. 23. Shi, J., Long, M., Liu, Q., Ding, G., and Wang, J. Twin bridge transfer learning for sparse collaborative ltering. In Advances in Knowledge Discovery and Data Mining (Berlin, Heidelberg, 2013), J. Pei, V. S. Tseng, L. Cao, H. Motoda, and G. Xu, Eds., Springer Berlin Heidelberg, pp. 496 507. 24. Srebro, N., Rennie, J. D. M., and Jaakola, T. S. Maximum-margin matrix factorization. In Advances in Neural Information Processing Systems 17 (2005), MIT Press, pp. 1329 1336. 25. Veeramachaneni, S. D., Pujari, A. K., Padmanabhan, V., and Kumar, V. A maximum margin matrix factor- ization based transfer learning approach for cross-domain recommendation. Appl. Soft Comput. 85 (2019). 26. Zhao, L., Pan, S. J., Xiang, E. W., Zhong, E., Lu, Z., and Yang, Q. Active transfer learning for cross-system recommendation. In Proceedings of the Twenty-Seventh AAAI Conference on Arti cial Intelligence (2013), AAAI 13, AAAI Press, pp. 1205 1211. 27. Zhao, L., Pan, S. J., and Yang, Q. A uni ed framework of active transfer learning for cross-system recommen- dation. Arti cial Intelligence 245 (2017), 38 55. 28. Zhuang, F., Zheng, J., Chen, J., Zhang, X., Shi, C., and He, Q. Transfer collaborative ltering from multiple sources via consensus regularization. Neural Networks 108 (2018), 287 295.