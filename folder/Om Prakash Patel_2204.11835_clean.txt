Noname manuscript No. (will be inserted by the editor) A Novel Scalable Apache Spark Based Feature Extraction Approaches for Huge Protein Sequence and their Clustering Performance Analysis Preeti Jha* Aruna Tiwari Neha Bharill Milind Ratnaparkhe Om Prakash Patel Nilagiri Harshith Mukkamalla Mounika Neha Nagendra Received: date / Accepted: date Abstract Genome sequencing projects are rapidly in- creasing the number of high-dimensional protein sequence datasets. Clustering a high-dimensional protein sequence dataset using traditional machine learning approaches poses many challenges. Many different feature extraction methods exist and are widely used. However, extracting features from millions of protein sequences becomes impractical because they are not scalable with current algorithms. Therefore, there is a need for an ef cient feature extrac- tion approach that extracts signi cant features. We have proposed two scalable feature extraction approaches for ex- tracting features from huge protein sequences using Apache Spark, which are termed 60d-SPF (60-dimensional Scalable Protein Feature) and 6d-SCPSF (6-dimensional Scalable Co-occurrence-based Probability-Speci c Feature). The proposed 60d-SPF and 6d-SCPSF approaches capture the statistical properties of amino acids to create a xed-length numeric feature vector that represents each protein sequence P. Jha Indian Institute of Technology Indore E-mail: jha.preeti07@gmail.com A. Tiwari Indian Institute of Technology Indore N. Bharill Mahindra Ecole Centrale, Hyderabad M. Ratnaparkhe ICAR-Indian Institute of Soybean Research Indore O. P. Patel Mahindra Ecole Centrale, Hyderabad N. Harshith Mahindra Ecole Centrale, Hyderabad N. Nagendra Indian Institute of Technology Indore M. Mounika Indian Institute of Technology Indore in terms of 60-dimensional and 6-dimensional features, respectively. The preprocessed huge protein sequences are used as an input in two clustering algorithms, i.e., Scalable Random Sampling with Iterative Optimization Fuzzy c-Means (SRSIO-FCM) and Scalable Literal Fuzzy C-Means (SLFCM) for clustering. We have conducted extensive experiments on various soybean protein datasets to demonstrate the effectiveness of the proposed feature extraction methods, 60d-SPF, 6d-SCPSF, and existing feature extraction methods on SRSIO-FCM and SLFCM clustering algorithms. The reported results in terms of the Silhouette index and the Davies-Bouldin index show that the proposed 60d-SPF extraction method on SRSIO-FCM and SLFCM clustering algorithms achieves signi cantly better results than the proposed 6d-SCPSF and existing feature extraction approaches. Keywords Apache Spark Cluster Big Data Feature Extraction Fuzzy Clustering Huge Protein Sequences Scalable Algorithms 1 Introduction Bioinformatics has become a major eld of research and is gaining the constant attention of biologists and computer scientists. The main goal of bioinformatics is to collect and manage biological data and to develop data mining tech- niques that are useful for the analysis and understanding of biological processes. With the fast improvement of Next- Generation Sequencing (NGS) innovation, a huge number of genomic datasets have been created, representing a great challenge to customary bioinformatics tools [1, 2]. The sur- prising development of protein sequences is a problem in managing these sequences to a large extent. The number of unique sequences in the protein database together now sur- passes a million. A protein family contains a large number arXiv:2204.11835v1 [q-bio.QM] 21 Apr 2022 2 Preeti Jha* et al. of sequences that are progressively related [3]. The cluster- ing of protein sequences aims to provide meaningful parti- tioning from a huge protein dataset. The protein sequences are arranged into clusters based on their similarity in pro- tein sequences [4]. Clustering protein sequences predicted from sequencing reads can impressively reduce the excess of sequence sets and the expense of downstream analysis and storage [5, 6]. Many researchers have worked on the K- means clustering algorithm to create high-quality sequence clusters [7, 8]. However, the K-means algorithm calculates the distance between the data samples with exact precision. When the distance function is not properly de ned, the K- means method may fail to disclose the sequence-to-structure relationship. As a result, many clusters have poor protein sequence matching. Hence, due to poor protein sequence clustering, the fuzzy concept has emerged as a new research frontier [9, 10, 11]. A data sample in fuzzy clustering might belong to many clusters with varying degrees of membership. The Fuzzy c-Means (FCM) technique was introduced by Bezdek [12] and employs iterative optimization to minimize an objective function using a similarity measure on feature space. Many researchers have worked with the FCM algorithm for protein sequence clustering [13, 14, 15]. Due to the rapid evolution of clustering algorithms, these techniques have gained less recognition for dealing with Big Data challenges. This is because these methods are not scalable [16]. Big Data environments are required to build scalable methods for dealing with enormous amounts of data generated from many sources [17]. To handle large amounts of data, several fuzzy clustering methods, such as the FCM extension, also known as random sampling plus extension Fuzzy c-Means (rseFCM), are employed. Still, the cluster overlapping is an issue with rseFCM. The overlap is eliminated by Random Sampling with Iterative Optimization Fuzzy c-Means (RSIO-FCM) [18]. However, RSIO-FCM, suffers from a rapid rise in multiple iterations during clustering. To address the shortcomings of RSIO-FCM, a scalable incremental fuzzy clustering technique known as SRSIO-FCM was designed [18]. The SRSIO-FCM inherently makes use of the Scalable Literal Fuzzy c-Means (SLFCM) algorithm during the clustering process. In the literature survey, it is found that the SRSIO-FCM algorithm generates clusters of better quality in comparison with the RSIO-FCM method. Therefore, in this work, we have used the SRSIO-FCM and SLFCM algorithms to cluster huge protein sequences. The SRSIO-FCM method is implemented in Apache Spark [18]. It splits large amounts of data into smaller subsets. For the rst subset, the membership degree belonging to the cluster centers is generated by applying the SLFCM algorithm [18]. The rst subset cluster centers are assigned to the second subset for clustering. But unlike RSIO-FCM, it does not use these cluster centers as an input for the clustering of the third subset. Instead, it combines the membership information of the rst and second subset to compute the new cluster centers. The membership matrices from the rst two subsets are merged, and the cluster centers are fed into the third subset. This procedure is repeated for the next grouping of all subgroups. In RSIO-FCM, for clustering of the next subset, the cluster center of one subset is fed as an input next subset. Hence, it will result in slow convergence by taking the higher number of iterations for that subset. This problem has been overcome in the SRSIO-FCM, because this approach uses the cluster centers obtained with the combined membership matrices of all the processed subsets for the clustering of the current subset. Before applying any machine learning algorithm to pro- tein sequences for analysis and modeling of huge sequences, it is important to encode the protein sequences into feature vectors. Protein sequences are made up of characters from the 20-letter amino acid alphabets ={A, C, D, E, F, G, H, I, K, L, M, N, P, Q, R, S, T, V, W, Y}. The amino acids of a sequence can be concatenated in any order, and the protein sequences can be of any length. The high dimensionality of protein data presents many critical challenges while apply- ing any machine learning methods [19]. Therefore, repre- sentation of protein sequences in terms of feature vectors is an important problem to be addressed. For the correct cat- egorization of protein sequences, a suitable input represen- tation (feature extraction) is required. Wang et al. [20] de- veloped a new feature extraction approach that attempts to capture both global and local similarity of sequences. Lo- cal similarity refers to frequently occurring sub-strings in the sequences, whereas global similarity refers to the overall similarity among numerous sequences. It has considered the 2-gram method as discussed by Wu et al. [21] to compute the global similarity of sequences and adopted a method called 6-letter exchange groups to represent a sequence [22]. Then it uses a sequence mining tool to compute the local similar- ity. Das et al. [23] developed a mapping sequence to feature vector using a numerical representation of codons targeted to amino acids for alignment-free sequence analysis. Bandy- opadhyay [24] proposes another feature extraction strategy that employs a 1-gram algorithm for feature encoding. The feature size consists of 20 amino acids. The extracted fea- tures are taken into account the probability of occurrences of the amino acids in the various positions of the sequences. G Mansoori et al. [25] suggested a novel feature extraction approach. To extract important characteristics from protein sequences, the occurrences of six exchange groups [22] in each sequence are counted. Chou [26] proposed two types of models widely used to illustrate protein models: sequen- tial models and descriptive models, although both the mod- els suffer certain disadvantages. The sequential model fails when the query protein has no substantial sequence resem- blance to any known protein. As a result, Chou offers a dis- 60d-SPF and 6d-SCPSF 3 tinct model known as the pseudo-amino acid composition model [26, 27]. Gupta et al. [28] used the general version of Chou [29] pseudo amino acid composition, which is a sixty- dimensional numerical feature vector of protein sequences, to develop an alignment-free approach for nding similar- ity across protein sequences. Bharill et al. [30] developed an approach to extract six-dimensional numerical feature vec- tors from a protein sequence. Likewise, many feature ex- traction techniques [23, 20, 24, 31, 25, 30, 28] have been introduced in the past, but, none of them is scalable. A scal- able approach for selecting statistically relevant characteris- tics from a large sequence is required. Many Big Data processing frameworks exist to make the approaches scalable for processing huge protein data. In this work, to make the proposed feature extraction approaches scalable, we have used the Apache Spark Big Data processing framework [32, 33]. Apache Spark is a general-purpose distributed processing system used for Big Data framework built on top of the Hadoop Distributed File System (HDFS). It utilizes in-memory computation and optimized query execution for fast analytic queries against data of any size [34, 35, 36]. We propose two scalable preprocessing algorithms for feature extraction of huge protein sequences using the Apache Spark framework; a 60-dimensional Scalable Protein Feature extraction approach named 60d-SPF and a 6-dimensional Scalable Co-occurrence based Probability Speci c Feature extraction approach named 6d-SCPSF. After extracting xed length numeric feature vectors from huge protein sequences using these approaches, the numeric feature vectors are passed as input to the scalable clustering algorithms, i.e., SRSIO-FCM and SLFCM, to create clusters for huge protein datasets. Finally, the validation measures are used to validate the results obtained from the SRSIO-FCM and SLFCM algorithms, which helps to analyze the performance of the proposed 60d-SPF and 6d-SCPSF feature extraction methods. Additionally, we have compared the clustering results of the proposed 60d-SPF and 6d-SCPSF feature extraction methods with other existing feature extraction methods [24, 31]. This paper is broadly standardized as follows: Section 2 describes feature extraction methods for protein sequences. The Apache Spark framework is discussed in Section 3. In Section 4, the implementation of the proposed 60d-SPF and 6d-SCPSF extraction algorithms using Apache Spark is ex- plained in detail. In Section 5, we have reported the results of the experimental study obtained from the proposed 60d-SPF and 6d-SCPSF methods, as well as other existing methods when applied to the SRSIO-FCM and SLFCM algorithms for clustering of huge protein sequences. The results are re- ported in terms of the Silhouette index and Davies Bouldin index. Finally, the conclusions of our work are drawn in Sec- tion 6. 2 Preliminaries In this section, we present a detailed description of the ex- isting feature extraction techniques applied to protein se- quences that form the basis for the discussion of the pro- posed feature extraction approaches. A 60-dimensional feature vector corresponding to each protein sequence is formed by considering three properties of each amino acid from 20 amino acids [29, 26, 27, 37, 28]. The numerical construction of the vector is based on the following: 1. Amino acid count: The twenty amino acids for protein sequences are ={A, C, D, E, F, G, H, I, K, L, M, N, P, Q, R, S, T, V, W, Y}. The total length of 20 amino acids de ned as lA,lC,lD,lE,lF,lG,lH,lI,lK,lL,lM,lN,lP,lQ,lR,lS,lT,lV,lW, lY. The total number of amino acids in a protein se- quence is determined by the length of the sequence. This criterion is insuf cient since sequences with the same number of amino acids in various places differ substantially. As a result, in a 60-dimensional vector, it contributes to the rst 20 values [28]. 2. Total distance: The sum of the distances between each amino acid and the rst amino acid in the protein se- quence is the total distance. The other 20 values in the 60-dimensional vectors are in uenced by these 20 val- ues. However, for identical protein sequences, this pa- rameter may appear to be the same. The total distance of Ti is de ned as follows: Ti = li j=1 Tj (1) where, i = A,C,D,E,F,G,H,I,K,L,M,N,P,Q,R,S,T,V, W,Y ; tj is the distance from the rst amino acid to the jth amino acid of i in the protein sequence. The other twenty feature vectors of the total distances denoted as TA,TC,TD,TE,TF,TG,TH,TI,TK,TL,TM,TN,TP,TQ,TR,TS, TT,TV,TW,TY. 3. Distribution: Two parameters, as seen above, are unable to accurately differentiate the similarity/dissimilarity of the two sequences. As a result, the third parameter, Di, represents the distribution of 20 amino acids throughout the protein sequence. Even if two protein sequences have the same content and total distance of 20 amino acids, their amino acid distribution are different. As a result, one-third of the 60-dimensional vector is made up of the 20 amino acid distribution. Each amino acid distribution is determined as follows: Di = li j=1 (tj di)2 li (2) where, i = A,C,D,E,F,G,H,I,K,L,M,N,P,Q,R,S,T,V, W,Y ; tj is the distance from the rst amino acid to the 4 Preeti Jha* et al. jth amino acid of i in the protein sequence and di = Ti li . We can then say that all these three characteristics make up the 60-dimensional vector that characterizes the pro- tein sequence. The 60-dimensional equal length vectors were constructed for unequal length protein sequences. So, the feature vector, which contains 60-dimensional data, is given as follows: < lA,TA,DA,lC,TC,DC,lD,TD,DD,lE,TE,DE,lF,TF,DF, lG,TG,DG,lH,TH,DH,lI,TI,DI,lK,TK,DK,lL,TL,DL,lM, TM,DM,lN,TN,DN,lP,TP,DP,lQ,TQ,DQ,lR,TR,DR,lS,TS, DS,lT,TT,DT,lV,TV,DV,lY,TY,DY > The 60-dimensional approach extracts 60 feature vectors from a protein sequence. The detailed description of the six-dimensional feature extraction method are presented next. A 6-dimensional feature extraction approach corre- sponding to each protein sequence is formed by considering three stages [30, 22, 38]. The 6-dimensional feature ex- traction approach captures the protein sequences statistical properties together with the amino acid position informa- tion to generate a vector of xed-length numerical features for each protein sequence. The Co-occurrence-based Probability-Speci c Feature (CPSF) extraction approach is a 6-dimensional feature extraction approach that extracts features from the protein dataset in three steps: protein se- quence encoding (PSE), global similarity measure (GSM), and local similarity measure (LSM). The rst step encodes protein sequences, representing each protein sequence as exchange groups. In the second stage, the overall GSM is calculated, considering the probability that each amino acid appears at a particular position within the total number of protein sequences present in a speci c species. In the third stage, the LSM calculates each amino acid weight concerning each protein sequence, taking into account the GSM. In this way, the 6-dimensional feature extraction approach represents each protein sequence by a xed- length numerical vector consisting of only six dimensional numeric feature vectors. 2.0.1 Stage One: Protein Sequence Encoding (PSE) In the rst step of the CPSF approach [30], each protein sequence is encoded and presented in terms of six exchange groups. According to Dayhoff and Schwartz [22], the amino acids in the protein sequence belong to six exchange groups. This is because, within each exchange group, there is a high evolutionary similarity between these amino acids. Exchange groups are ef cient amino acid equivalent classes, formally represented by {e1,e2,e3,e4,e5,e6}, where e1={H,R,K}, e2={D,E,N,Q}, e3={C}, e4={S,T,P,A,G}, e5={M,I,L,V} and e6={F,Y,W} [20]. The protein se- quences belong to different species, and within each species, the protein sequences share some structural sim- ilarities. Once the sequence of proteins is encoded by the exchange groups, the total similarity between the encoded exchange groups is calculated. A detailed description of the subsequent stages is presented next. 2.0.2 Stage Two: Global Similarity Measure (GSM) In the second step of the CPSF method, we calculate the GSM by estimating the instance probability of all exchange groups at each position relative to the total number of protein sequences in the species. The GSM is calculated as follows: (Probability)ij = (Instance)ij/ (3) Where (Probability)ij denotes the probability of instance of the ith exchange group at jth position, (instance)ij represents the frequency at which the ith exchange group appear at jth position and represents the total number of sequences in a particular species. After that, LSM is calculated, which de- termines the speci c weight of each exchange group s posi- tion. A detailed description of it is given in the subsequent section. 2.0.3 Stage Three: Local Similarity Measure (LSM) In the third stage of the CPSF approach [30], the LSM is calculated, which determines the location-speci c weight of each exchange group within the sequence considering the weight factors. These weight factors ultimately represent the numeric feature vectors for each protein sequence. The weight of each exchange group is calculated as follows: (Weight)SEQk i = j j=1 (Probability)ij (PW)SEQk ij (4) where (Weight)SEQk i represents the weight of ith ex- change group corresponding to the kth protein sequence, (Probability)ij denotes the probability of occurrence of the ith exchange group at jth position and (PW)SEQk ij is the positional weight assigned to the ith exchange group based on the presence of kth protein sequence at jth position. The CPSF approach extracts a numeric feature vector from the protein sequence. The extracted numeric feature vector consists of only six dimensions. We have adopted the 60-dimensional approach and pro- pose the scalable feature extraction approach named 60d- SPF which has been discussed in detail in Section 4.1. Fur- thermore, we have proposed and designed a scalable version of the CPSF feature extraction method named 6d-SCPSF. A detailed description of the 6d-SCPSF approach has been given in Section 4.2. Before introducing the proposed algo- rithms, we give the details of a well-known framework for Big Data processing in Section 3. 60d-SPF and 6d-SCPSF 5 Fig. 1 Apache Spark cluster stack. 3 Big Data Framework: Apache Spark The proposed 6d-SCPSF and 60d-SPF extraction algorithms can be scalable using Apache Spark clusters to handle large- scale protein data. To speed up implementation of the feature extraction approach in this paper, we choose Apache Spark to implement the proposed algorithms. Apache Spark con- sists of several main components, including the Spark core and higher-level libraries. Figure 1 shows the Apache Spark clusters stack used in our experiment, and the details for the same are explained below. Spark core: The Spark core runs on different cluster man- agers and can access data from any Hadoop data source. It provides a simple programming interface for large-scale processing datasets, Resilient Distributed Dataset (RDD). Spark Core is embedded in Scala, but it comes with APIs in Scala, Java, Python, and R. Python in our case. Besides, the Spark core provides a key function for in-memory cluster computing, including memory management, job scheduling, data shuf ing, and error recovery [39]. Upper-level libraries: Spark SQL has been created to man- age various workloads on the Spark core. Cluster managers and data source: A cluster manager is used to obtain cluster resources to run a job. The Spark Engine works with the built-in Spark cluster manager (i.e., standalone). The cluster manager manages resource sharing between Spark applications. On the other hand, Spark can access data from the Hadoop Distributed File System (HDFS) [40]. Resilient Distributed Datasets: Spark Core is built on the RDD abstraction. An RDD is a read-only distribution of records. RDDs provide a false tolerance, a parallel data structure that allows users to explicitly store data on disk or memory, control its partitions, and manipulate it using a full set of operators. This enables us to ef ciently distribute the data in the calculation of the necessary requirements for different workloads. An RDD can be created from external data sources or from other RDDs. Spark-application: The Spark application execution con- sists of ve main units, a controller program, a cluster administrator, workers, executors, and tasks. The Apache Spark clusters application is shown in Figure 2. A driver program is an application that uses Spark as a library and de nes the high-level ow of control for the target calcula- tion. While a worker provides CPU, memory, and storage resources to a Spark application, an executor is a Java Virtual Machine (JVM) process that Spark creates on each worker for that application. A job is a set of calculations that the Spark controller performs on a cluster to get results in the program. A Spark application can start multiple jobs. Spark divides the work into steps of a directed acyclic chart (DAG), where each phase is a collection of tasks. A task is the smallest unit of work that a spark sends to an executor. The main entry point for spark functions is a spark context, through which the driver program uses Spark. A Spark context represents a connection to a computing cluster. The proposed algorithms were implemented using the Apache Spark clusters framework, where Hadoop is used as a data storage. The proposed 60d-SPF and 6d-SCPSF ex- traction methods are explained in Section 4. Fig. 2 Apache Spark cluster Application. 4 Proposed Work This section describes the proposed scalable algorithms implemented on the Apache Spark cluster. To propose a scalable protein preprocessing algorithm, we followed the pseudo-amino acid composition approach discussed in [28] and proposed the design of a scalable feature extraction approach named 60d-SPF. The proposed 60d-SPF approach, 6 Preeti Jha* et al. when applied to huge protein data, extracts 60-dimensional numeric feature vectors. In addition to this, we propose the design of another scalable feature extraction named 6d-SCPSF, which is inspired by the CPSF approach [30]. To make the proposed feature extraction approaches scal- able, we implemented them on the Apache Spark cluster. The output obtained from the scalable feature extraction approaches is passed as an input to the SRSIO-FCM and SLFCM [18] clustering algorithms to form clusters of the huge protein sequence data. A detailed description of the proposed scalable feature extraction approaches is presented next. 4.1 Scalable 60-dimensional Feature Extraction Approach To represent all protein sequences in terms of 60- dimensional numeric feature vectors, the proposed scalable protein feature extraction technique is implemented using the Apache Spark framework. The architecture of the proposed 60d-SPF extraction approach is shown in Figure 3. Algorithm 1 summarizes the steps of the proposed Fig. 3 Work ow of 60d-SPF Architecture. 60d-SPF approach. The input given is a raw protein dataset containing 20 amino acids ={A, C, D, E, F, G, H, I, K, L, M, N, P, Q, R, S, T, V, W, Y}. The output is a feature vector of the given input dataset, which is a le containing 60-dimensional numeric feature vectors. In Line 1 of Algorithm 1 , the data is read into an RDD from the Hadoop. Then a map function is used to accom- plish the preprocessing task. In Line 2 of Algorithm 1, the map function calls the Algorithm 2: Protein Preprocess, where each sequence is mapped into RDD and then applied Protein Preprocess algorithm for that sequence. Finally, the obtained RDD after map function is saved using Line Algorithm 1 60d-SPF Input: Raw protein data : raw Protein.txt Output: Processed protein data : proc Protein.txt 1: Read the protein sequences from the fasta le (raw Protein.txt) and parallelize them with the help of Spark RDD. 2: Map each sequence in RDD to Protein Preprocess() algorithm for that particular sequence. 3: Save feature vectors to text le: proc Protein.txt. Algorithm 2 Protein Preprocess Input: Grid of values; z: [A, C, D, E, F, G, H, I, K, L, M, N, P, Q, R, S, T, V, W, Y] Output: li,Ti,Di 1: Let i denote amino ={A, C, D, E, F, G, H, I, K, L, M, N, P, Q, R, S, T, V, W, Y}. 2: for x in z do 3: if x is i then 4: Increase the count of amino i i.e., li ++; 5: end if 6: end for 7: Calculate total distance Ti using Eq. 1. 8: Calculate the variance of distance Di using Eq. 2. 3 of Algorithm 1. The proposed scalable feature extraction approaches extracts features of protein sequences in three sets of the numerical parameter. The rst parameter is a calculation of the length of sequences. The second param- eter is the total distances of each amino base to the rst amino. The third parameter is the variance of distance for each amino acid [28]. Each set of numerical parameters is not suf cient to denote a speci c protein sequence. Thus, a combination of all the three sets of numerical parameters will represent each protein sequence with a 60-dimensional numeric feature vector. The Algorithm 2 discusses the Protein Preprocess algorithm, which is called by 60d-SPF given in Algorithm 1. The proposed scalable protein preprocessing algorithm (60d-SPF) has the signi cant characteristic that it takes raw protein sequences as input and produces 60-dimensional numeric feature vectors as an output which is feed as an input to SRSIO-FCM and SLFCM clustering algorithms to produce output in terms of clusters of protein sequences. The working of the Protein Preprocess algorithm is shown in various steps by using a toy illustration presented next. A Toy Illustration for 60d-SPF Approach Here, we give a toy illustration of the ve protein sequences as shown in Fig. 4 to explain the Protein Preprocess algo- rithm. The toy problem is solved using the following steps. Step-1: Calculation of length of sequence The Protein Preprocess algorithm (given in Algorithm 2) working is being described using a rst step in which the length of each amino acid sequence is calculated as shown in Line 1-6. Here an illustration is presented by consider- ing an example of ve sequences are shown in Fig. 4. The 60d-SPF and 6d-SCPSF 7 output obtained after preprocessing of these ve sequences are shown in Fig. 5, where the rst column is the sequence number and the other information presents in the form of 60-dimensional numeric feature vectors are as follows: < lA,TA,DA,lC,TC,DC,lD,TD,DD,lE,TE,DE,lF,TF,DF,lG, TG,DG,lH,TH,DH,lI,TI,DI,lK,TK,DK,lL,TL,DL,lM,TM, DM,lN,TN,DN,lP,TP,DP,lQ,TQ,DQ,lR,TR,DR,lS,TS,DS, lT,TT,DT,lV,TV,DV,lY,TY,DY > The sequence1 of Fig. 4 is represented in terms of fea- ture vectors are [2,2,1,2,1,3,2,2,2,4,3,0,2,4,1,1,1,4,4,4, 1,0], respectively. Likewise, the value of the other four sequences are shown in Fig. 5. Fig. 4 Example of protein sequences. Step-2: Total distances of each amino acid to the rst amino acid The second numerical parameter, i.e., the total distances of each amino acid-base to the rst amino acid Ti, which is cal- culated using Eq. 1 in Line 7 of Protein Preprocess algo- rithm. Add the position values corresponding to each amino acid as shown in Fig. 4, and amino acid G appears at 5, 7, and 16 (index starts from 0). So, the value of TG is calculated as follows: TG= 5+7+16 = 28 Likewise, the value of TA,TC,TD,TE,TF,TH,TI,TK,TL,TM,TN, TP,TQ,TR,TS,TT,TV,TW,TY for sequence1 and the total dis- tances of each amino acid to the rst amino acid for all other four sequences are shown in Fig. 5. Step-3: Variance of distance for each amino acid The third numerical parameter Di is the variance of distance for each amino acid, which is calculated using Eq. 2 in Line 8 of Protein Preprocess algorithm. The rst step is to com- pute di using di = Ti li . The value of dG in sequence1 is calcu- lated as follows: dG = TG lG = 31 3 =10.33 The second step is to compute the variance of distance for each amino acid-base, i.e., Di, for example, the value of DG in sequence1 is 22.88, calculations of DG is as follows: DG = [(6 10.33)2+(8 10.33)2+(17 10.33)2] 3 = 22.88 Hence, the result obtained from sequence1, which contains 60-dimensional numeric feature vectors is as follows: [2,28,169.0,2,76,1.0,1,19,0.0,2,47,6.25,1,34,0.0,3,28, 22.8,2,18,36.0,2,28,16.0,4,64,127.5,3,100,64.8,0,0,0,2, Algorithm 3 6d-SCPSF algorithm Input: Raw protein data : raw Protein.txt Output: Preprocessed protein data : pre Protein.txt 1: Call Scalable Protein Sequence Encoding (SPSE) algorithm. 2: Call Global Similarity Matrix (GSM) algorithm. 3: Call Scalable Local Similarity Measures (SLSM) algorithm. 4: Save as text le (Feature Vectors.txt) 45,110.25,4,75,69.6,1,36,0.0,1,32,0.0,1,30,0.0,4,31, 35.1,5,106,108.2,1,23,0.0,0,0,0] The results of all the other four sequences are shown in Fig. 5. 4.2 Scalable 6-dimensional Feature Extraction Approach The proposed scalable feature extraction approach 6d- SCPSF is being implemented using Apache Spark frame- work to represent all the protein sequences in 6-dimensional numeric feature vectors. The architecture of the proposed 6d-SCPSF extraction approach is shown in Figure 6. Algorithm 3 summarizes the steps of 6d-SCPSF approach. The input given is a raw protein dataset containing 20 amino acids ={A, C, D, E, F, G, H, I, K, L, M, N, P, Q, R, S, T, V, W, Y}. The output is a feature vector of the given input dataset, which is a le containing 6-dimensional numeric feature vectors. The 6d-SCPSF extraction approach is explained in Algorithm 3. Line 1 of Algorithm 3, calls the SPSE algorithm, which distributes raw Protein.txt protein dataset on Apache Spark clusters. In Line 2, the GSM algorithm is called, which calculates the probability matrix of the sequences on a master machine without distributing the dataset in spark clusters. The output obtained from the Scalable Protein Sequence Encoding (SPSE) algorithm is used as an input to the GSM algorithm. In Line 3, the Scalable Local Similarity Measures (SLSM) algorithm is called, which again distributes the output obtained from the GSM algorithm using the Apache Spark cluster. Finally, the 6-dimensional numerical feature vectors are saved in a le using Line 4. The subsequent section explains the SPSE, GSM, and SLSM algorithms. 4.2.1 SPSE Algorithm The PSE algorithm encodes each amino acid into particular encoding groups, as discussed in Section 2.0.1. This section presents the scalable version of PSE (SPSE). Algorithm 4 shows the steps of the SPSE approach. In Line 1, data is read into a resilient distributed dataset (RDD) from Hadoop. In Line 2, the le (raw Protein.txt) is passed to the Encode function and store as the return values in other Spark RDD. Then the map function distributes Encode() method to ev- ery worker node for parallel execution of the task in Line 3. In Line 4, we save the data of the encoded RDD to the text 8 Preeti Jha* et al. Fig. 5 Preprocessed result using proposed 60d-SPF extraction method for the protein sequences given in Fig. 4. Fig. 6 6d-SCPSF architecture embedded using SRSIO-FCM and SLFCM with performance measure evaluation. le. From Line 5-12, the work of Encode() function is given. The amino acids are mapped with the exchange groups from Line 7-12. The DataFrame is used to store the computation of each step. In Spark, a DataFrame is a distributed collec- Algorithm 4 Scalable Protein Sequence Encoding (SPSE) Input: Raw protein sequence: raw Protein.txt Output: Encoded protein sequence: enc Protein.txt 1: Read the protein sequences from the le and parallelize them with the help of Spark RDD. 2: Pass the le (raw Protein.txt) to Encode function and store the return values in other Spark RDD. 3: Map Encode function. 4: Save data of encoded RDD to text le. 5: Function Encode(z){ 6: Store the data in the DataFrame and split each letter in the se- quence to different columns. 7: Replace the H , R , K with e1. 8: Replace the D , E , N , Q with e2. 9: Replace the C with e3. 10: Replace the S , T , P , A , G with e4. 11: Replace the M , I , L , V with e5. 12: Replace the F , Y , W with e6. 13: Returns the encoded protein sequence: enc Protein.txt } tion of data organized into named columns. In Line 13, the output of the SPSE algorithm is saved in the enc Protein.txt le. After that, the enc Protein.txt le is taken as input to the GSM algorithm, which is presented next. 4.2.2 GSM Algorithm The GSM is used to calculate the instance probability of all exchange groups at each position relative to the total number of protein sequences. Section 2.0.2 discusses the steps used to nd global similarity measures of the protein-encoding sequence. Algorithm 5 discusses the steps of the GSM ap- proach. The input of this algorithm is enc Protein.txt, which is obtained from Algorithm 4. Line 1 reads the data from a le and saves it to a DataFrame. Line 2 nds the number of columns in the DataFrame, and Line 3 locates the number of rows in the DataFrame. An empty DataFrame is created for the estimation of the probability DataFrame of sequences in Line 4. Then, calculate the occurrence of the exchange Algorithm 5 Global Similarity Measures (GSM) Input: Encoded protein sequences: enc Protein.txt Output: Probability DataFrame of sequences: prob Protein.txt 1: Read the encoded sequences data from the le and store it in the DataFrame. 2: Find the number of columns of the DataFrame(col). 3: Find the number of rows of the DataFrame(row). 4: Create an empty probability DataFrame with index names e1,e2,e3,e4,e5,e6. 5: for column in range 0 to col do 6: Create an empty list. 7: for exchange group do 8: Get the occurrence of exchange group in that column. 9: Get the probability of the exchange group using Eq. (3). 10: Add probability to the list. 11: end for 12: Add list as a new column in the probability DataFrame. 13: end for 14: Save probability DataFrame as text le (prob Protein.txt). group for each column and add this to the DataFrame in Line 8. In Line 9, the value of the probability of the ex- change group is computed using Eq. (3). Line 10 adds the probability to the list. In Line 12, a list is added as a new column in the probability DataFrame. Line 14 saves the re- sult to prob Protein.txt le used as input to the SLSM algo- rithm. After that, enc Protein.txt and prob Protein.txt les are taken as input to the next stage in the SLSM algorithm, which is presented next. 4.2.3 SLSM Algorithm This section presents the scalable version of LSM (SLSM). The SLSM determines the location-speci c weight of each 60d-SPF and 6d-SCPSF 9 exchange group within the sequence and produces the weight factors. These weight factors ultimately represent the numeric feature vectors for each protein sequence. Section 2.0.3 discusses the steps used to nd local similarity measures of the protein-encoding sequence. Algorithm 6 discusses the steps of the SLSM approach. Algorithm 6 Scalable Local Similarity Measures (SLSM) Input: Probability matrix of sequences: prob Protein.txt, encoded protein sequences: enc Protein.txt Output: Feature Vectors: Feature Vectors.txt 1: Read the sequences from the le (enc Protein.txt) and parallelize using Spark RDD. 2: Read the data from the prob Protein.txt and store it in DataFrame (probability). 3: Map FeatureVector function. 4: Save data of FeatureVector RDD to text le. 5: Function FeatureVector(probability, data) 6: Create an empty featureVector DataFrame with column names as e1,e2,e3,e4,e5,e6. 7: Create DataFrame using data. 8: Find the number of columns of the DataFrame(cols). 9: Find the number of rows of the DataFrame(rows). 10: for row in range of 0 to rows do 11: Create a dictionary as featureVector = e 1 : 0, e 2 : 0, e 3 : 0, e 4 : 0, e 5 : 0, e 6 : 0. 12: for col in range of 0 to cols do 13: Get the exchange group from the position of rows and cols in DataFrame. 14: Modify the key value of the exchange group from fea- tureVector by adding previous value with the value of proba- bility at the position of exchange group and cols from prob- ability DataFrame. 15: end for 16: Add the featureVector to the featureVector DataFrame. 17: end for 18: Save the featureVector DataFrame to text le (Feature Vectors.txt). This algorithm takes input from two les, i.e., enc Protein.txt and prob Protein.txt, which are ob- tained from Algorithm 4 and Algorithm 5, respectively. The data (enc Protein.txt) is read into an RDD from Hadoop in Line 1. Then, the data from the le (prob Protein.txt) is accessed and stored in DataFrame (probability) in Line 2. The map function distributes FeatureVector() method to every worker node for parallel execution of the task in Line 3. In Line 4, we save the data obtained from the featureVector RDD to a text le. From Line 5-17, the working of the FeatureVector() function is given, where the DataFrame is used to store the computation of each step. An empty DataFrame is created with the given column names in Line 6. Then, a DataFrame is created using the data in Line 7. Line 8 nds the number of cols, and Line 9 locates the number of rows of the DataFrame. Line 11 creates a dictionary of feature vectors. The exchange group of rows and columns from DataFrame is acquired in line 13. In Line 14, we modify the key value of the exchange group from featureVector. This is done by adding previous value with the value of probability at the position of the exchange group and columns from the probability DataFrame. Line 10-17 is computed using Eq. (4). Finally, the results are saved in the Feature Vectors.txt le on Line 18. The proposed scalable feature extraction algorithm (6d-SCPSF) has the signi cant characteristic that it takes raw protein sequences as input and produces 6-dimensional numeric feature vectors as output using the Apache Spark framework. The output produced from the proposed feature extraction approaches in terms of 60-dimensional and 6-dimensional feature vectors is passed as an input to the SRSIO-FCM and SLFCM clustering algorithms to produce output in terms of clusters consisting of protein sequences. The output produced by the clustering algorithms is reported in terms of clusters. 5 Experimental results In the experiments, we analyzed the performance of the proposed 60d-SPF and 6d-SCPSF extraction methods, which are applied to both the SRSIO-FCM and SLFCM algorithms. Their performance is measured in terms of the Silhouette index (SI) and Davies Bouldin index (DBI). Furthermore, feature vectors using Bandyopadhyay [24] and Mansoori [31] methods have been computed. Then, the clustering of these feature vectors is carried out using the SRSIO-FCM and SLFCM algorithms. The performance of the proposed 60d-SPF and 6d-SCPSF is compared with that of Bandyopadhyay and Mansoori in terms of SI and DBI, respectively. 5.1 Experimental environment We have implemented the proposed algorithms on the Apache Spark cluster. The Spark cluster consists of one master and ve worker nodes. The master node is equipped with 32 GB of RAM, 8 cores, and 3 TB of storage. Each worker node is equipped with 16 GB of RAM, 8 cores, and 1 TB of storage. 5.2 Dataset description We have investigated the performance of the proposed 60d-SPF and 6d-SCPSF approaches compared with those of Bandyopadhyay and Mansoori on SRSIO-FCM and SLFCM algorithms on the following protein datasets. A detailed description of the protein datasets used for the experimental analysis is present in Table 1. 10 Preeti Jha* et al. Table 1 Description of soybean protein datasets. Parameters Datasets Lee Williams82 PI483463 W05 sequences 71358 73320 62102 89477 size 44 MB 53.4 MB 38.9 MB 50.7 MB Table 2 Comparison of the number of features extracted with different feature extraction approaches. Authors Feature Extraction Method Features Bandyopadhyay 1-gram feature encoding method 20 Mansoori Exchange group encoding method 6 Proposed 6d-SCPSF Global and local similarity 6 Proposed 60d-SPF Pseudo amino acid composition 60 5.2.1 Lee The Lee strain, which crosses between the Chinese lines CNS and S-100, is widely used as a parent in many breeding projects in the southern United States and Brazil. Diversity is characterized by resistance to bacteria from Phytophthora rot, Peanut Mottle Virus, and bacterial pustule [41]. 5.2.2 Williams82 Williams 82, a soybean cultivar used to construct the refer- ence genome sequence, was created by reversing the Phy- tophthora root rot resistance locus from the donor parent Kingwa to the recurrent Williams parent [42]. 5.2.3 PI483463 Glycine Soja is the closest wild soybean of Glycine max. Species remain interfertile, and specimens of G. soja are used in breeding projects to introduce traits such as resis- tance to certain diseases or environmental stress. Glycine swelling accession PI483463 is known to be abnormally tol- erable. The genome of this entry is sequential and is partly based on this salt tolerance [43]. 5.2.4 W05 Glycine Soja accession W05 is a salt-tolerant wild soybean whose genome is designed to serve as a reference genome assembly. W05 af liation has been used for genetic studies of various traits including uncertainty, seed size, number of pods per plant, and seed color [44]. All the protein datasets discussed in this section are available at the following URL: https://soybase.org/dlpages/. 5.3 Performance evaluation 5.3.1 Silhouette index (SI) SI is a metric that compares how similar a data sample is to its cluster to other clusters. The Silhouette value is limited to a number between -1 and 1. A negative number implies poor clustering quality, whereas a positive value suggests excellent clustering quality [45]. Thus SI is characterized as: S(i) = a2(i) a1(i) max[a1(i),a2(i)] (5) Where a1(i) is the average distance between ith sample from all other data samples within the same cluster, a2(i) is the lowest average distance of ith sample to all the data samples in any other cluster, of which i is not a member. 5.3.2 Davies-Bouldin index (DBI) DBI divides a single record into two measures, one for the dispersion of individual clusters and the other for the parti- tioning of distinct clusters [46]. The DBI is not constrained inside a particular range, thus a lower DBI implies higher clustering quality. Thus, DBI is characterized as: DBI = 1 c c i=1max j =i diam(Ci)+diam(Cj) d(Ci,Cj)  (6) Where d(Ci,Cj) correlates to the distance between the cen- ter of clusters Ci and Cj, diam (Ci) is the maximum distance between all the data samples of cluster Ci and c is the num- ber of clusters. 5.4 Results and discussion In this section, we discuss the effectiveness of the proposed 60d-SPF compared with the proposed 6d-SCPSF extraction and existing methods like Bandyopadhyay and Mansoori when applied to SRSIO-FCM and SLFCM algorithms on protein data, and the results are investigated in terms of SI and DBI. The SRSIO-FCM algorithm partitions the protein dataset into three subsets, where the subsets are the subsets of the entire data. However, SLFCM works over the whole data. The clustering is performed on clusters 5, 10, 15, 20, 25, and 30, respectively. The performance of the proposed feature extraction approach is evaluated in comparison with other feature extraction approaches discussed in the literature. The feature encoding techniques used here for comparison are reported along with the number of features extracted by each approach in Table 2. The comparison of proposed 60d-SPF and 6d-SCPSF, existing feature extraction methods like Bandyopadhyay and Mansoori 60d-SPF and 6d-SCPSF 11 Table 3 SI values of LEE protein dataset applied on SRSIO-FCM and SLFCM algorithms. clusters Algorithms SRSIO-FCM SLFCM 60d-SPF 6d-SCPSF Bandyopadhyay Mansoori 60d-SPF 6d-SCPSF Bandyopadhyay Mansoori 5 0.6678 0.4501 0.1321 0.1327 0.7232 0.4506 0.1683 0.1469 10 0.4826 0.3361 0.0461 0.0772 0.4802 0.3361 0.0572 0.0501 15 0.3922 0.2721 0.0129 0.0703 0.4039 0.2706 0.0184 0.0556 20 0.3522 0.2511 -3.6947 0.0447 0.3505 0.2513 0.0007 0.0464 25 0.3074 0.2303 -0.0067 0.0369 0.3118 0.2329 -0.0006 0.0516 30 0.2131 0.199 -0.0143 0.0357 0.2769 0.2164 -0.0088 0.0482 Table 4 SI values of Williams82 protein dataset applied on SRSIO-FCM and SLFCM algorithms. clusters Algorithms SRSIO-FCM SLFCM 60d-SPF 6d-SCPSF Bandyopadhyay Mansoori 60d-SPF 6d-SCPSF Bandyopadhyay Mansoori 5 0.6672 0.4607 0.0348 0.1234 0.8021 0.4999 0.0279 0.1278 10 0.5055 0.3391 -0.026 0.089 0.5758 0.3728 -0.0272 0.0689 15 0.4174 0.2706 -0.047 0.0612 0.4676 0.293 -0.0277 0.0467 20 0.3918 0.2538 -0.0835 0.0475 0.4122 0.2569 -0.0681 0.0429 25 0.331 0.2252 -0.1142 0.0398 0.3916 0.2399 -0.066 0.0346 30 0.2868 0.2201 -0.1153 0.0341 0.3548 0.2267 -0.0714 0.0259 Table 5 SI values of PI48346 protein dataset applied on SRSIO-FCM and SLFCM algorithms. clusters Algorithms SRSIO-FCM SLFCM 60d-SPF 6d-SCPSF Bandyopadhyay Mansoori 60d-SPF 6d-SCPSF Bandyopadhyay Mansoori 5 0.6329 0.4492 0.0498 0.1154 0.6283 0.4493 0.0594 0.1818 10 0.478 0.3358 0.0203 0.0665 0.4766 0.3358 0.0166 0.0863 15 0.3894 0.2533 -0.0349 0.0593 0.3919 0.2557 -0.0036 0.0605 20 0.3471 0.24 -0.0759 0.0533 0.3393 0.2477 -0.075 0.0445 25 0.3063 0.2244 -0.1174 0.039 0.3065 0.2275 -0.0306 0.0509 30 0.2699 0.2013 -0.1556 0.0366 2.0357 0.2219 -0.1008 0.0561 applied to SRSIO-FCM, and SLFCM algorithms on all four protein datasets is shown in tables reported subsequently. Tables 3-6 highlight SI values for the Lee, Williams82, PI483463, and W05 soybean protein datasets, which are computed after supplying the features extracted from the proposed 60d-SPF and 6d-SCPSF extraction and existing methods like Bandyopadhyay and Mansoori into the SRSIO-FCM and SLFCM clustering algorithms. SI demonstrates the quality of clustering. The higher SI value indicates good clustering results. Table 3 shows the SI values for the Lee dataset. The SRSIO-FCM and SLFCM algorithms have obtained a lower value for the 6d-SCPSF extraction method than the 60d-SPF extraction method. On the other hand, the existing methods (Bandyopadhyay and Mansoori) have signi cantly lower values than those proposed. Observing the SI values, the SRSIO-FCM and SLFCM algorithms have obtained negative values for most of the clusters for Bandyopadhyay and extremely low for Mansoori. Moreover, for the 60d-SPF approach, the SRSIO-FCM and SLFCM achieved the highest value of SI for cluster 5. Table 4 shows the SI values for the Williams82. The SRSIO-FCM and SLFCM algorithms have obtained a lower value for the 6d-SCPSF extraction method than the 60d-SPF extraction method. On the other hand, the existing methods (Bandyopadhyay and Mansoori) have signi cantly lower values than those proposed. Observing the SI values, the SRSIO-FCM and SLFCM algorithms have obtained negative values for most of the clusters for Bandyopadhyay and extremely low for Mansoori. Additionally, for the 60d-SPF approach, the SRSIO-FCM and SLFCM achieved the highest value of SI for cluster 5. Table 5 shows the SI values for the PI483463 dataset. The SRSIO-FCM and SLFCM algorithms have obtained a 12 Preeti Jha* et al. Table 6 SI values of W05 protein dataset applied on SRSIO-FCM and SLFCM algorithms. clusters Algorithms SRSIO-FCM SLFCM 60d-SPF 6d-SCPSF Bandyopadhyay Mansoori 60d-SPF 6d-SCPSF Bandyopadhyay Mansoori 5 0.6505 0.4526 0.1706 0.0567 0.7219 0.4676 0.1668 0.1658 10 0.4895 0.3341 0.066 0.0384 0.51031 0.3409 0.0825 0.1707 15 0.4063 0.2732 0.025 0.0383 0.4519 0.3136 0.0371 0.1298 20 0.36365 0.2494 0.0078 0.0286 0.3898 0.2618 0.0175 0.0947 25 0.3087 0.2358 0.0103 0.0334 0.3447 0.2467 0.0029 0.0692 30 0.2719 0.217 0.0004 0.0309 0.3023 0.2325 -0.0081 0.0486 Table 7 DBI values of LEE protein dataset applied on SRSIO-FCM and SLFCM algorithms. clusters Algorithms SRSIO-FCM SLFCM 60d-SPF 6d-SCPSF Bandyopadhyay Mansoori 60d-SPF 6d-SCPSF Bandyopadhyay Mansoori 5 0.6654 0.7227 2.125 1.443 0.536 0.7223 1.8179 1.7073 10 0.8207 0.9299 3.5887 1.7254 0.8214 0.9332 4.1065 1.8161 15 1.1391 1.1504 5.445 1.7718 1.1412 1.1535 5.8585 1.8317 20 1.382 1.1829 6.5559 1.8216 1.4467 1.1468 6.4553 1.9362 25 1.7125 1.1861 7.6284 1.8225 0.6562 1.1641 7.2828 1.8729 30 1.7456 1.2772 8.7204 1.8378 1.8213 1.1876 7.9131 1.8648 Table 8 DBI values of Williams82 protein dataset applied on SRSIO-FCM and SLFCM algorithms. clusters Algorithms SRSIO-FCM SLFCM 60d-SPF 6d-SCPSF Bandyopadhyay Mansoori 60d-SPF 6d-SCPSF Bandyopadhyay Mansoori 5 0.603 0.7409 3.2884 1.6421 0.6196 0.691 2.9909 1.7052 10 0.7689 1.0339 5.2884 1.7618 0.6086 0.842 7.5041 1.9261 15 1.0555 1.2841 6.9123 1.9205 0.8371 1.1241 6.7961 1.9288 20 1.1908 1.2863 8.4169 1.9681 1.0746 1.1763 7.7635 1.8761 25 1.141 1.2663 8.8731 1.8905 0.9778 1.2336 8.1169 1.8983 30 1.4429 1.3602 8.0659 1.8505 1.0743 1.2645 7.6 1.9418 higher value for the 60d-SPF extraction method than the 6d- SCPSF extraction method. On the other hand, the existing methods (Bandyopadhyay and Mansoori) have signi cantly lower values than those proposed. Observing the SI values, the SRSIO-FCM and SLFCM algorithms have obtained neg- ative values for most of the clusters for Bandyopadhyay and extremely low for Mansoori. Additionally, for the 60d-SPF approach, the SRSIO-FCM and SLFCM achieved the high- est value of SI for cluster 5. Table 6 shows the SI values for the W05 dataset. The SRSIO-FCM and SLFCM algorithms have obtained a higher value for the 60d-SPF extraction method than the 6d-SCPSF extraction method. On the other hand, the existing methods (Bandyopadhyay and Mansoori) have signi cantly lower values than those proposed. Observing the SI values, the SRSIO-FCM and SLFCM algorithms have obtained extremely low values for most of the clusters for Bandyopadhyay and Mansoori. Additionally, for the 60d-SPF approach, the SRSIO-FCM and SLFCM achieved the highest value of SI for cluster 5. Table 7-10 highlights DBI values for the Lee, Williams82, PI483463, and W05 soybean protein datasets. Conversely to the SI, the DBI is not bounded within a given range. As a general rule, the lower the DBI value, the better the clustering result. Table 7 shows the DBI values for the Lee dataset. The value achieved by SRSIO-FCM and SLFCM is much better for the 60d-SPF than 6d-SCPSF. As we can see, for 60d-SPF, the DBI values are lower than 6d-SCPSF when clustered using SRSIO-FCM and SLFCM on almost all the clusters. On the other hand, the existing methods (Bandyopadhyay and Mansoori) have extremely higher values than those proposed. Moreover, SRSIO-FCM and SLFCM attained a very low value in cluster 5. 60d-SPF and 6d-SCPSF 13 Table 9 DBI values of PI48346 protein dataset applied on SRSIO-FCM and SLFCM algorithms. clusters Algorithms SRSIO-FCM SLFCM 60d-SPF 6d-SCPSF Bandyopadhyay Mansoori 60d-SPF 6d-SCPSF Bandyopadhyay Mansoori 5 0.6434 0.7243 2.6845 1.7117 0.6439 0.7249 2.8005 1.494 10 0.8636 0.9302 5.2577 1.7012 0.864 0.9311 5.3841 1.7239 15 1.117 1.2587 5.8198 1.8244 1.1296 1.2434 7.4839 1.7644 20 1.1223 1.2611 7.2405 1.8055 1.5365 1.1831 5.918 1.7856 25 1.176 1.244 6.934 1.8702 1.8109 1.1824 7.2622 1.7718 30 1.7778 1.3 6.6071 1.8825 0.2883 1.2268 8.4102 1.8205 Table 10 DBI values of W05 protein dataset applied on SRSIO-FCM and SLFCM algorithms. clusters Algorithms SRSIO-FCM SLFCM 60d-SPF 6d-SCPSF Bandyopadhyay Mansoori 60d-SPF 6d-SCPSF Bandyopadhyay Mansoori 5 0.6563 0.7259 1.867 1.8148 0.5976 0.7317 1.8314 1.6565 10 0.8644 1.0193 4.1775 2.0667 0.7805 0.9628 3.5178 1.4468 15 1.0727 1.1937 6.2512 1.9356 0.9015 1.0197 4.8952 1.5505 20 1.3559 1.2546 7.3401 1.9127 1.0317 1.1793 4.4736 1.9088 25 1.7617 1.1998 8.0035 2.0381 1.4231 1.1975 6.3209 1.8746 30 1.7758 1.2694 9.0045 1.9626 1.7899 1.1922 7.5768 1.8836 Table 8 shows the DBI values for the Williams82 dataset. The value achieved by SRSIO-FCM and SLFCM is much better for the 60d-SPF than 6d-SCPSF. As we can see, for the 6d-SCPSF approach, the DBI values are higher than the 60d-SPF approach when clustered using SRSIO-FCM and SLFCM on all the clusters except cluster 30 for SRSIO-FCM. On the other hand, the existing methods (Bandyopadhyay and Mansoori) have extremely higher values than those proposed. Moreover, SRSIO-FCM attained a low value on cluster 5 for 60d-SPF. Table 9 shows the DBI values for the PI483463 dataset. The value achieved by SRSIO-FCM and SLFCM is much better for the 60d-SPF than 6d-SCPSF. As we can see, for 6d-SCPSF, DBI values are higher than 60d-SPF when clus- tered using SRSIO-FCM and SLFCM on almost all the clus- ters. On the other hand, the existing methods (Bandyopad- hyay and Mansoori) have extremely higher values than those proposed. Moreover, SRSIO-FCM attained a low value in cluster 5 for 60d-SPF. Table 10 shows the DBI values for the W05 dataset. The value achieved by SRSIO-FCM and SLFCM is much bet- ter for the 60d-SPF than 6d-SCPSF. As we can see, for 6d- SCPSF, DBI values are higher than 60d-SPF when clustered using SRSIO-FCM and SLFCM on almost all the clusters. On the other hand, the existing methods (Bandyopadhyay and Mansoori) have extremely higher values than those pro- posed. Moreover, SRSIO-FCM and SLFCM attained a low value in cluster 5 for 60d-SPF. Finally, we can conclude that 60d-SPF performs bet- ter than the proposed 6d-SCPSF extraction and existing methods (Bandyopadhyay and Mansoori) when applied to SRSIO-FCM and SLFCM algorithms. The performance is reported in SI and DBI values for Lee, Williams82, PI483463, and W05 soybean protein datasets. 6 Conclusion and Future Work In this paper, two scalable feature extraction approaches, i.e., 60d-SPF and 6d-SCPSF, have been proposed to extract a numerical feature vector from huge protein sequences using the Apache Spark cluster. After that, preprocessed numer- ical feature vectors are applied to the scalable fuzzy clus- tering algorithms. In this case, we have used the SRSIO- FCM and SLFCM algorithms as the scalable fuzzy cluster- ing approaches to cluster huge soybean protein datasets. One of the most signi cant characteristics of the proposed scal- able feature extraction approach is that it takes raw protein sequences of variable length as input and produces xed- length numeric feature vectors as an output. Thus, both the algorithms are scalable and can handle huge numbers of pro- tein sequences. However, when obtained feature vectors are applied to scalable fuzzy clustering algorithms, the 60d-SPF extraction method excels over the 6d-SCPSF and existing feature extraction approaches. One distinctive characteris- tic of the proposed 60d-SPF approach is that it computes pseudo amino acid composition parameters on the Apache 14 Preeti Jha* et al. Spark cluster to consider all possible position-speci c vari- ations of amino acids in a protein sequence. We focused on the exact evaluation of both the proposed feature extraction methods applied to the SRSIO-FCM and SLFCM clustering algorithms. The performance was investigated on distinct soybean protein datasets. The investigated results demon- strate the potential bene ts of adopting our feature extrac- tion technique for massive protein datasets. In the future, we are interested in applying and analyzing the performance of the proposed scalable feature extraction techniques on other plant species like rice and wheat data. Acknowledgements This work is supported by National Supercom- puting Mission, HPC Applications Development Funded Research Project by DST in collaboration with the Ministry of Electronics and Information Technology (MeiTY). Compliance with ethical standards Con ict of interest All authors declare that there are no con icts of interests. References 1. Guo R, Zhao Y, Zou Q, Fang X, Peng S (2018) Bioin- formatics applications on apache spark. GigaScience 7(8):giy098 2. Alawneh L, Shehab MA, Al-Ayyoub M, Jararweh Y, Al-Sharif ZA (2020) A scalable multiple pairwise pro- tein sequence alignment acceleration using hybrid cpu gpu approach. Cluster Computing 23(4):2677 2688 3. Krause A, Stoye J, Vingron M (2005) Large scale hier- archical clustering of protein sequences. BMC bioinfor- matics 6(1):15 4. Zou Q, Lin G, Jiang X, Liu X, Zeng X (2020) Sequence clustering in bioinformatics: an empirical study. Brief- ings in bioinformatics 21(1):1 10 5. Steinegger M, S oding J (2018) Clustering huge protein sequence sets in linear time. Nature communications 9(1):1 8 6. Zeng M, Zhang F, Wu FX, Li Y, Wang J, Li M (2020) Protein protein interaction site prediction through com- bining local and global features with deep neural net- works. Bioinformatics 36(4):1114 1120 7. Han KF, Baker D (1995) Recurring local sequence motifs in proteins. Journal of molecular biology 251(1):176 187 8. Bystroff C, Thorsson V, Baker D (2000) Hmmstr: a hidden markov model for local sequence-structure correlations in proteins. Journal of molecular biology 301(1):173 190 9. Jha P, Tiwari A, Bharill N, Ratnaparkhe M, Mounika M, Nagendra N (2020) A novel scalable kernelized fuzzy clustering algorithms based on in-memory computation for handling big data. IEEE Transactions on Emerging Topics in Computational Intelligence 10. Jha P, Tiwari A, Bharill N, Ratnaparkhe M, Mounika M, Nagendra N (2021) Apache spark based kernelized fuzzy clustering framework for single nucleotide poly- morphism sequence analysis. Computational Biology and Chemistry 92:107454 11. Jha P, Tiwari A, Bharill N, Ratnaparkhe M, Nagendra N, Mounika M (2021) Scalable incremental fuzzy con- sensus clustering algorithm for handling big data. Soft Computing pp 1 17 12. Bezdek JC, Ehrlich R, Full W (1984) Fcm: The fuzzy c- means clustering algorithm. Computers & Geosciences 10(2-3):191 203 13. Zhang CT, Chou KC, Maggiora G (1995) Predicting protein structural classes from amino acid composition: application of fuzzy clustering. Protein Engineering, Design and Selection 8(5):425 435 14. Lu T, Dou Y, Zhang C (2013) Fuzzy clustering of cpp family in plants with evolution and interaction analyses. BMC bioinformatics 14(S13):S10 15. Farhangi E, Ghadiri N, Asadi M, Nikbakht MA, Pitre S (2017) Fast and scalable protein motif sequence cluster- ing based on hadoop framework. In: 2017 3th Interna- tional Conference on Web Research (ICWR), IEEE, pp 24 31 16. Chunduri RK, Cherukuri AK (2018) Scalable formal concept analysis algorithms for large datasets using spark. Journal of Ambient Intelligence and Humanized Computing pp 1 21 17. Oussous A, Benjelloun FZ, Lahcen AA, Belfkih S (2018) Big data technologies: A survey. Journal of King Saud University-Computer and Information Sciences 30(4):431 448 18. Bharill N, Tiwari A, Malviya A (2016) Fuzzy based scalable clustering algorithms for handling big data using apache spark. IEEE Transactions on Big Data 2(4):339 352 19. Vipsita S, Rath SK (2013) Two-stage approach for pro- tein superfamily classi cation. Computational Biology Journal 2013 20. Wang JTL, Ma Q, Shasha D, Wu CH (2001) New tech- niques for extracting features from protein sequences. IBM Systems Journal 40(2):426 441 21. Wu C, Whitson G, McLarty J, Ermongkonchai A, Chang TC (1992) Protein classi cation arti cial neural system. Protein Science 1(5):667 677 22. Dayhoff M, Schwartz R, Orcutt B (1978) 22 a model of evolutionary change in proteins. In: Atlas of Protein Sequence and Structure, vol 5, National Biomedical Re- 60d-SPF and 6d-SCPSF 15 search Foundation Silver Spring, MD, pp 345 352 23. Das JK, Sengupta A, Choudhury PP, Roy S (2021) Mapping sequence to feature vector using numerical representation of codons targeted to amino acids for alignment-free sequence analysis. Gene 766:145096 24. Bandyopadhyay S (2005) An ef cient technique for su- perfamily classi cation of amino acid sequences: fea- ture extraction, fuzzy clustering and prototype selec- tion. Fuzzy Sets and Systems 152(1):5 16 25. G Mansoori E, Zolghadri MJ, Katebi SD, Mohabatkar H, Boostani R, Sadreddini MH (2008) Generating fuzzy rules for protein classi cation. Iranian Journal of Fuzzy Systems 5(2):21 33 26. chou KC (2011) Some remarks on protein attribute pre- diction and pseudo amino acid composition. Journal of theoretical biology 273(1):236 247 27. Chou kC (2005) Using amphiphilic pseudo amino acid composition to predict enzyme subfamily classes. Bioinformatics 21(1):10 19 28. Gupta M, Niyogi R, Misra M (2013) An alignment- free method to nd similarity among protein sequences via the general form of chou s pseudo amino acid com- position. SAR and QSAR in Environmental Research 24(7):597 609 29. Chou KC (2001) Prediction of protein cellular attributes using pseudo-amino acid composition. Proteins: Struc- ture, Function, and Bioinformatics 43(3):246 255 30. Bharill N, Tiwari A, Rawat A (2015) A novel technique of feature extraction with dual similarity measures for protein sequence classi cation. Procedia Computer Sci- ence 48:795 801 31. Mansoori EG, Zolghadri MJ, Katebi SD (2009) Protein superfamily classi cation using fuzzy rule-based classi- er. IEEE Transactions on NanoBioscience 8(1):92 99 32. Veiga J, Exp osito RR, Pardo XC, Taboada GL, Touri- o J (2016) Performance evaluation of big data frame- works for large-scale data analytics. In: 2016 IEEE In- ternational Conference on Big Data (Big Data), IEEE, pp 424 431 33. Li R, Hu H, Li H, Wu Y, Yang J (2016) Mapreduce par- allel programming model: a state-of-the-art survey. In- ternational Journal of Parallel Programming 44(4):832 866 34. Le Nir Y (2020) Spark and machine learning library. TORUS 1 Toward an Open Resource Using Services: Cloud Computing for Environmental Data pp 229 243 35. Zaharia M, Chowdhury M, Das T, Dave A, Ma J, Mc- Cauley M, Franklin MJ, Shenker S, Stoica I (2012) Re- silient distributed datasets: A fault-tolerant abstraction for in-memory cluster computing. In: Proceedings of the 9th USENIX conference on Networked Systems De- sign and Implementation, USENIX Association, pp 2 2 36. Tang S, He B, Yu C, Li Y, Li K (2018) A survey on spark ecosystem for big data processing. arXiv preprint arXiv:181108834 37. Chou KC (2005) Using amphiphilic pseudo amino acid composition to predict enzyme subfamily classes. Bioinformatics 21(1):10 19 38. Dayhoff MO (1972) A model of evolutionary change in proteins. Atlas of protein sequence and structure 5:89 99 39. Salloum S, Dautov R, Chen X, Peng PX, Huang JZ (2016) Big data analytics on apache spark. International Journal of Data Science and Analytics 1(3-4):145 164 40. Borthakur D, et al. (2008) Hdfs architecture guide. Hadoop Apache Project 53(1-13):2 41. Wysmierski PT, Vello NA (2013) The genetic base of brazilian soybean cultivars: evolution over time and breeding implications. Genetics and molecular Biology 36(4):547 555 42. Sedivy EJ, Wu F, Hanzawa Y (2017) Soybean domes- tication: the origin, genetic architecture and molecular bases. New Phytologist 214(2):539 553 43. Lee JD, Shannon JG, Vuong TD, Nguyen HT (2009) In- heritance of salt tolerance in wild soybean (glycine soja sieb. and zucc.) accession pi483463. Journal of Hered- ity 100(6):798 801 44. Xie M, Chung CYL, Li MW, Wong FL, Wang X, Liu A, Wang Z, Leung AKY, Wong TH, Tong SW, et al. (2019) A reference-grade wild soybean genome. Nature communications 10(1):1 12 45. Bolshakova N, Azuaje F (2003) Cluster validation tech- niques for genome expression data. Signal processing 83(4):825 833 46. Coelho GP, Barbante CC, Boccato L, Attux RR, Oliveira JR, Von Zuben FJ (2012) Automatic feature se- lection for bci: an analysis using the davies-bouldin in- dex and extreme learning machines. In: The 2012 inter- national joint conference on neural networks (IJCNN), IEEE, pp 1 8 47. Shen HB, Yang J, Liu XJ, Chou KC (2005) Using su- pervised fuzzy clustering to predict protein structural classes. Biochemical and biophysical research commu- nications 334(2):577 581