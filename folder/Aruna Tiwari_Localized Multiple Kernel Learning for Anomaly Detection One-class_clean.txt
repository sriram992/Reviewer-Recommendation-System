Localized Multiple Kernel Learning for Anomaly Detection: One-class Classi cation Chandan Gautama, , Ramesh Balajia, Sudharsan K.a, Aruna Tiwaria, Kapil Ahujaa aIndian Institute of Technology Indore, Indore, Simrol, India Abstract Multi-kernel learning has been well explored in the recent past and has exhibited promising outcomes for multi-class classi cation and regression tasks. In this paper, we present a multiple kernel learning approach for the One-class Classi cation (OCC) task and employ it for anomaly detection. Recently, the basic multi-kernel approach has been proposed to solve the OCC problem, which is simply a convex combination of di erent kernels with equal weights. This paper proposes a Localized Multiple Kernel learning approach for Anomaly Detection (LMKAD) using OCC, where the weight for each kernel is assigned locally. Proposed LMKAD approach adapts the weight for each kernel using a gating function. The parameters of the gating function and one-class classi er are optimized simultaneously through a two-step optimization process. We present the empirical results of the performance of LMKAD on 25 benchmark datasets from various disciplines. This performance is evaluated against existing Multi Kernel Anomaly Detection (MKAD) algorithm, and four other existing kernel-based one-class classi ers to showcase the credibility of our approach. LMKAD achieves signi cantly better Gmean scores while using a lesser number of support vectors compared to MKAD. Friedman test is also performed to verify the statistical signi cance of the results claimed in this paper. Keywords: One-Class Classi cation, Anomaly Detection, S VM, OCS VM, Multiple kernel learning. 1. Introduction The Anomaly Detection is a problem of nding instances of the input data that do not conform to the general pattern or behavior exhibited by the majority of the data points. This problem has been well explored and addressed in the past using One-class Classi cation [1, 2, 3, 4, 5, 6, 7, 8]. The One-class Classi cation (OCC) problem is unlike the conventional binary and multi-class classi cation problems in that in OCC, one has data about only one of the many Corresponding author Email addresses: chandangautam31@gmail.com (Chandan Gautam), ee140002025@iiti.ac.in (Ramesh Balaji), cse140001014@iiti.ac.in (Sudharsan K.), artiwari@iiti.ac.in (Aruna Tiwari), kahuja@iiti.ac.in (Kapil Ahuja) Preprint submitted to Elsevier July 17, 2018 arXiv:1805.07892v4 [cs.LG] 17 Jul 2018 classes that could constitute the input space. Outliers, on the other hand, could belong to any class or even be isolated anomalies. Various models to handle OCC problems have been developed in the past [1]. Out of these, SVM based methods have gotten more attention from researchers due to their e ciency, kernel learning ability, and generalization capability. In SVM based methods, the OCC problem is rede ned as the task of devising a boundary around the given data of target class points, such that most of the target class points lie within the de ned boundary. Two types of SVM based methods have been developed viz., Support Vector Data Description (S VDD) [9] and One-class SVM (OCS VM) [10]. Tax and Duin [9] developed SVDD by nding a hyper-sphere of minimum radius around the target class data such that it encloses almost all points in the target class data set. Scholkopf et al. [10] extend the idea of S VM for binary classi cation [11] to the domain of anomaly detection by proposing OCS VM. They construct a hyper-plane such that it separates all the data points from the origin and the hyper-plane s distance from the origin is maximum. We choose Scholkopf s OCS VM as the classi er for our developments because of its robust performance as reported by other researchers [12]. It also guaranteed convergence and the exibility of kernel methods in general [8, 13]. The general idea of kernel-based methods such as S VM is to project the input space to a higher dimension where they become linearly separable. Kernel-based methods have received considerable attention over the last decade due to their success in classi cation problems. An advance in kernel-based methods for classi cation problems is to use many di erent kernels or di erent parameterization of kernels instead of a single xed kernel [14] to get better performance. Using multiple kernels gives two advantages. Firstly, this provides exibility to select for an optimal kernel or parameterizations of kernels from a larger set of kernels, thus reducing bias due to kernel selection and at the same time allowing for a more automated approach [15, 16, 17]. Secondly, multiple kernels are also re ective of the need to combine knowledge from di erent data sources (such as images and sound in video data). Thus they accommodate the di erent notions of similarity in the di erent features of the input space. A further advance in multiple kernel learning is to localize the kernel selection process for various task like, binary classi cation [18], image recognition problems [19], regression [20], and clustering [21]. Gonen et al. [18, 19, 20] achieve this localization by using a gating function which helps in selection of the appropriate kernel locally. Further this concept is explored by various researchers [22, 23, 24, 25]. There have been few attempts to transfer the idea of multiple kernel learning to the domain of One-class Clas- si cation and Anomaly Detection. Das et al. [26] propose a simple weighted sum of two kernels, each of which describes the discrete and continuous streams in aviation data, respectively. This method takes advantage of multiple kernel learning to incorporate the di erent notions of similarity in the two streams for the task of detecting anomalies in a heterogeneous system. Although, the method takes advantage of the ability of multi-kernel learning to combine 2 knowledge from di erent data sources and exhibited better performance compared to single kernel learning. How- ever, kernels are combined using only xed combination rule, i.e., assign equal weight to each kernel over whole input space. In this paper, the proposed method provides more exibility to the existing MKAD algorithm by taking advantage of local information presents among data and extend it to a localized formulation of multiple kernel-based OCS VM for anomaly detection, which is named as LMKAD. The intuition behind this work is same as discussed above and also by Jacobs et al. [27] and Gonen et al. [18] for multi-class classi cation. In this paper, we are extending this concept for anomaly detection using One-class Classi cation. The main contributions of this paper are as follows: 1. This paper presents an optimization problem for data-driven anomaly detection in which a convex combination of kernels is used, with weights assigned locally. This optimization problem is analogous to the conventional OCS VM and can be solved similarly. 2. Our algorithm achieves signi cantly better Gmean scores compared to conventional OCS VM and MKAD, and at the same time uses a lesser number of support vectors compared to MKAD. For demonstrating the credibility of our algorithm, we perform extensive testing using ve-fold Cross-validation (CV) over 5 runs on 13 small and 12 medium sized benchmark datasets. 3. LMKAD performance is compared with 5 state-of-the-art kernel-based methods available in the literature. Fi- nally, a Friedman test [28] is conducted to verify the statistical signi cance the experimental outcomes of LMKAD classi er and it rejects the null hypothesis with 95% con dence level. The rest of the paper is organized as follows. Sections 2 and 3 describe the OCS VM and Multiple Kernel Anomaly Detection (MKAD) algorithms, respectively. In Section 4, we propose OCS VM based Localized Multiple Kernel Anomaly Detection (LMKAD). Section 5 describes the experimental setup and evaluates the proposed (LMKAD) and existing One-class Classi ers (OCS VM and MKAD) against 25 benchmark datasets. The paper concludes in Section 6. 2. One-Class SVM One-class SVM was proposed by Scholkopf et al. [10] for extending the utility o ered by S VM to One-class classi cation. Given a set of training vectors xi Rn, i = 1, . . . , N, where, all training vectors belong to the same class. OCS VM constructs a hyperplane that basically separates all the target class data points from the origin and maximizes 3 the distance of this hyperplane from the origin. This is done by solving the following optimization problem. min , , 1 2 T + 1 N N X i=1 i s.t. T (xi) i i = 0, . . . , N, i 0, i = 0, . . . , N (1) The dual of which can be written as min 1 2 TQ s.t. 0 i 1 N i = 0, . . . , N, N X i i = 1 (2) where Qi j = K(xi, xj) = (xi)T (x j) In the above two equations, w is the weight coe cients, (.) is the mapping in the feature space, K is the kernel matrix, i is the Lagrange multiplier, N is the total number of training samples provided, is a parameter that lets the user de ne the fraction of target class points rejected, and is the bias term. This results in a binary function which returns +1 or 1 for target class and outliers, respectively, and is called the decision function. The decision function f(x) thus obtained is as follows: f(x) = sign( N X i=1 iK(xi, x) ) (3) Based on the formulation OCSVM, Das et al. [26] proposed anomaly detection for more than one kernel, which is described in the next section. 3. Multiple Kernel Anomaly Detection Das et al. [26] proposed MKAD to detect anomalies in aviation data. Aviation data consists of features that can be grouped into two categories - (i) Real-valued data such as ight velocity, altitude, ap angle, etc and (ii) Binary valued data such as cockpit switch positions. Single-kernel OCS VM cannot capture the di erent notions of similarity in the Real and Binary valued data. Instead, a composite kernel K is used. This composite kernel can be any valid convex combination of individual kernels. This is the method used by Das et al. [26]. Here the composite kernel is a simple weighted sum of the individual 4 kernels computed over all or a subset of the features, i.e., K(xi, x j) = p X m=1 mkm(xi, x j), (4) where m 0 and Pp m=1 m = 1. Here km(xi, xj) represents the mth kernel computed for data points xi and xj, and m denotes assigned weight to individual kernels. The dual of this optimization problem is similar to that of OCS VM, with the kernel replaced by the composite kernel. Note that here, the advantage of the multiple kernel learning approach is to incorporate knowledge of the di ering notions of similarity in the decision process. Thus, we are able to achieve an improvement in detecting anomalies in a system that involves various data sources. A xed combination rule (like a weighted summation or product) assigns the same weight to a kernel which remains xed over the entire input space. However, this does not take into account the underlying localities in the data. Assigning di erent weights to a kernel in a data-dependent way may lead to a further improvement in detecting the anomalies. We explore this possibility in the next section. 4. Localized Multiple Kernel Anomaly Detection In this section, we propose Localized Multiple Kernel Anomaly Detection (LMKAD). By assigning weights in a data-dependent way we intend to give more weights to kernel functions which best match the underlying locality of the data in di erent regions of the input space. We modify the decision function in the previous sections to the following: f(x) = p X m=1 m(x) m, m(x) (5) where m(x) is the weight corresponding to each kernel and is assigned by the gating function. The value of m(x) is a function of the input x and is de ned by the parameters of the gating function. These parameters are in turn learned from the data during optimization as shown later in this section. We rewrite the conventional OCS VM optimization 5 problem with our new decision function to get the following primal optimization problem min m, m(x), , 1 2 p X m=1 T m m + 1 N N X i=1 i s.t. p X m=1 m(x) m, m(x) i i, i 0 i (6) where is the rate of rejection, N is the total number of training samples, and i are the slack variables as usual. We now need to solve this optimization problem for the above parameters. However, we do not solve this optimization problem directly, but use a two-step alternate optimization scheme inspired by Rakotomamonjy et al. [29] and Gonen et al. [18] to nd the values of the parameters of the gating function ( m(x)) and the parameters of the decision function. Before starting the optimization procedure, we initialize the value of m(x). Then, in the rst step of the procedure we treat m(x) as a constant and solve the optimization problem (6) for , and . Note that if we treat m(x) as a constant, this step is essentially the same as solving conventional OCS VM under certain conditions as we will explain shortly. Solving the conventional OCS VM returns the optimal value of the Objective function and the Lagrange multipliers. In the second step we update the value of the parameters of m(x) using gradient descent on the Objective function. The updated parameters de ne a new m(x) which is used for the next iteration. The above two steps are repeated until convergence. From the Objective function in (6) and the constraints, for xed m(x) the Lagrangian of the primal problem is written as: LD = 1 2 p X m=1 T m m + N X i=1  1 N i i  i N X i=1 i  p X m=1 m(xi) m, m(xi)  (7) and taking the derivatives of the Lagrangian LD with respect to the variables in (6) gives : LD m m = N X i=1 i m(xi) m(xi) m (8) LD N X i=1 i = 1 (9) LD i 1 N = i + i (10) Substituting, (8), (9), and (10) into (7), we obtain the dual problem, 6 max J( ) = 1 2 TQ s.t. 0 i 1 i = 0, . . . , l, N X i i = 1 (11) where Qi j = K (xi, xj). Here, the kernel matrix is de ned as: K (xi, xj) = p X m=1 m(xi) m(xi), m(xj) m(x j) (12) The objective function of the dual is termed as a function of J( ). The dual formulation is exactly the same as the conventional OCS VM formulation with kernel function K (xi, xj). Multiplying the kernel matrix with a non-negative value will still give a positive de nite matrix [30]. Note that the locally combined kernel function will, therefore, satisfy the Mercer s condition if the gating function is non-negative for both input instances. This can be easily ensured by picking a non-negative m(x). In order to assign the weight to the di erent kernels based on the training data, a gating function is used. In above formulations of LMKAD, objective value of the dual formulation (11) is equal to the objective value of the primal (6). By using the dual formulation in (11), gating function is trained and m(x) is computed. For training, the gradient of the objective function of the dual (i.e. J( )) is computed with respect to the parameters of the gating function. Now substituting the new value of m(x) in (12) gives us the new K (xi, x j) which we use to solve the dual formulation (11). We again update the value of the gating function parameters and repeat until convergence. The convergence of the algorithm is determined by observing the change in the objective function in (11). The entire algorithm can be summarized as follows: Algorithm 1 LMKAD algorithm 1: Initialize the values of vm and vm0 by random values for each mth kernel, where m = 1, 2, . . . , p 2: do 3: Calculate m using vt m and vt m0 4: Calculate K (xi, x j) using the gating function 5: Solve conventional one-class SVM with K (xi, x j) 6: v(t+1) m v(t) m (t) J( ) vm m 7: v(t+1) m0 v(t) m0 (t) J( ) vm0 m 8: while Not converged Once the algorithm converges and the nal m(x) and the Lagrange multipliers are obtained, the decision function 7 can be rewritten as f(x) = N X i=1 p X m=1 i m(x)Km(x, xi) m(xi) (13) The sign of this decision function tells us whether the given input is target or outlier. Also, we compute the average error using the sum over the (target value - predicted value) on the training data. Then we set the bias term to this mean value. In the above discussion, the use case of gating function is explained in detail. Three types of gating functions are used in this paper for our experiments, which are de ned as follows: 4.1. Softmax Function m(x) = exp( vm, x + vm0) Pp k=1 exp( vk, x + vk0) (14) This function is characterized by the parameters vm0 and vm. The above function is called the Softmax function and ensures that m(x) is non-negative. Note that if we use a constant gating function, the algorithm reduces to that of MKAD [26], and assigns xed weights over the entire input space. As per the above discussion, gradient of the objective function J( ) needs to be calculated with respect to the parameters of the Softmax gating function (vm0 and vm), which is mentioned as follows: J( ) vm0 = 1 2 N X i=1 N X j=1 p X k=1 i j k(xi)Kk(xi, xj) k(xj)( k m m(xi) + k m m(xj)) (15) J( ) vm = 1 2 N X i=1 N X j=1 p X k=1 i j k(xi)Kk(xi, x j) k(x j)(xi( k m m(xi)) + xj( k m m(x j))) (16) where, k m = 1, if m = k 0, otherwise Once we obtain the updated values of the parameters vm0 and vm, we calculate the new value of m(x) using the gating function. 8 4.2. Sigmoid Function m(x) = 1 1 + exp( vm, x vm0) (17) Again the parameters vm0 and vm characterize the above gating function. The gradients of the objective function J( ) with respect to the parameters of the sigmoid gating function (vm0 and vm) are: J( ) vm0 = 1 2 N X i=1 N X j=1 i j m(xi)Km(xi, x j) m(xj)(1 m(xi) + 1 m(xj)) (18) J( ) vm = 1 2 N X i=1 N X j=1 i j m(xi)Km(xi, x j) m(xj)(xi(1 m(xi)) + xj(1 m(xj))) (19) 4.3. Radial Basis Function (RBF) m(x) = e x m 2 2 2m Pp k=1 e x k 2 2 2 k (20) where m is the center and m gives the spread of the local region. Similar as above, the gradients of the objective function J( ) with respect to the parameters of the RBF gating function ( m and m) are: J( ) m = N X i=1 N X j=1 p X k=1 i j k(xi)Kk(xi, xj) k(xj)((xi m)( k m m(xi)) + (xj m)( k m m(xj)))/ 2 m (21) J( ) m = N X i=1 N X j=1 p X k=1 i j k(xi)Kk(xi, xj) k(xj)( xi m 2 2( k m m(xi)) + x j m 2 2( k m m(xj)))/ 3 m (22) where, k m = 1, if m = k 0, otherwise 5. Performance Evaluation In this section, experiments are conducted to evaluate the performance of the proposed classi er over 25 datasets. These datasets are obtained from University of California Irvine (UCI) repository [31] and mainly belong to two disciplines, i.e., medical and nance. Description of the datasets can be found in Table 1. Many of the datasets are 9 slightly imbalanced. Class imbalance ratio of both of the classes are approximately 1 : 2 in case of 9 datasets viz., Iris, Iono(1), Iono(2), Pima(1), Pima(2), German(1), German(2), Wave(1), Wave(2), and. These datasets were originally generated for the binary or multi-class classi cation task. For our experiments, we have made it compatible with OCC task in the following ways. If a dataset has two classes then we use each of the classes in the binary dataset alternately as the target class and the remaining one as outlier. If a dataset has more than two classes then we use one of the classes in the dataset as the target class and the remaining ones as representative of the outlier class. In this way, we construct 25 one-class datasets from 13 multi-class datasets. Table 1: Datasets S. No. Name #Targets #Outliers #Features #samples Small Size Datasets 1 Iris 50 100 4 150 2 heart(1) 160 137 13 297 3 heart(2) 137 160 13 297 4 Iono(1) 225 126 34 351 5 Iono(2) 126 225 34 351 6 bupa(1) 145 200 6 345 7 bupa(2) 200 145 6 345 8 Japan(1) 294 357 15 651 9 Japan(2) 357 294 15 651 10 Australia(1) 307 383 14 690 11 Australia(2) 383 307 14 690 12 pima(1) 500 268 8 768 13 pima(2) 268 500 8 768 Medium Size Datasets 14 German(1) 700 300 24 1000 15 German(2) 300 700 24 1000 16 Park(1) 520 520 29 1040 17 Park(2) 520 520 29 1040 18 Space(1) 1541 1566 6 3107 19 Space(2) 1566 1541 6 3107 20 Abalone(1) 2096 2081 8 4177 21 Abalone(2) 2081 2096 8 4177 22 Spam(1) 1813 2788 57 4601 23 Spam(2) 2788 1813 57 4601 24 Wave(1) 1692 3308 40 5000 25 Wave(2) 3308 1692 40 5000 10 5.1. Experimental Setup All the experiments1 have been conducted on MATLAB 2016a in Windows 7 (64 bit) environment with 64 GB RAM, 3.00 GHz Intel Xeon processor. For implementing existing One-class classi ers, LIBSVM package [32] is used. For every dataset 5 fold Cross-validation (CV) indices are generated and this procedure is repeated 5 times each time constituting a run. These indices are kept same throughout the experiment for all the classi ers. In 5-fold CV, 4 folds are used for training and 1 fold is used for testing. However, out of the 4-folds used for training, only samples from one of the classes (i.e., target class) are used for training the model. Samples from the other classes are used as validation samples to nd optimal parameters. We calculate and report the average Gmean for 5-fold CV over 5 runs. Gmean is de ned as follows equation: Gmean = p precision recall (23) Moreover, we have applied statistical tests in order to analyze the results in a better way. To this end, similar to [33], we compute Friedman Rank (FRank)[28], Mean of Gmeans (MGmean) over all the datasets, and Percentage of the Maximum Gmean (PMG). PMG is de ned as follows [33]: PMG = Pno. of datasets i=1  Gmean of classi er for ith dataset Maximum Gmean achieved for ith dataset 100  Number of datasets (24) The same experimental setup is followed for evaluating existing and the proposed one-class classi ers. In our experi- ments, three commonly used kernels are employed which are de ned as follows: 1. Linear kernel (l): KL(xi, xj) = xT i xj 2. Polynomial kernel (p): KP(xi, x j) = (xT i xj + 1)q 3. Gaussian kernel (g): KG(xi, x j) = e (xi x j)T (xi x j) 2 The order of the Polynomial kernel is chosen through the parameter q to be 2 or 3. The value of used in the Gaussian kernel is set to the average of the Square Euclidean distance between all the points in the training data. 1All presented results in this paper are reproducible. Codes with datasets can be found on my GitHub pro le https://github.com/ Chandan-IITI after the acceptance of the paper 11 The Linear kernel has no special parameters. The kernel parameter set that has the highest Gmean on the validation folds is considered to be the best con guration and these parameters are used as input along with the training folds for training the model. The trained model is then evaluated over the test set. Since we use 5-fold CV and repeat the experiment ve times, for each dataset, we have twenty- ve test set results from which we nd and report the average Gmean value with standard deviation and the average percentage of support vectors used. We have also performed z-score normalization on each dataset before training and testing. For comparing our proposed method from the existing kernel-based methods, we have selected 5 popular existing kernel-based one-class classi ers, which are detailed as follows: (i) Support Vector Machine (S VM) based: OCS VM [10], S VDD [9], MKAD [26] (ii) KRR-based:KOC [34] (iii) Principal Component Analysis (PCA) based: KPCA[35]. OCS VM is implemented using LIBSVM2 library [32]. S VDD is implemented by using DD Toolbox3 [36]. 5.2. Results and Discussion In order to illustrate the signi cance of the multiple kernel approach, we have performed extensive experiments on various combinations of kernels for the existing (MKAD) as well as proposed (LMKAD) method. In case of LMKAD, the combination of kernel name (in small letter) with gating function name (in capital letter) is mentioned in a bracket with the method name. For rest of the methods, single kernel namecombination of kernel name is mentioned using small letterletters with the method name in the bracket. Many combinations are possible with linear (l), polynomial (p) and gaussian (g) kernel, however, only those two combinations of kernels (gpl and gpp) are presented in the paper which have exhibited better performance. Overall, 6 variants of LMKAD are generated, namely, LMKAD(S gpl), LMKAD(S gpp), LMKAD(S o gpl), LMKAD(S o gpp), LMKAD(R gpl) and LMKAD(R gpp). 5.2.1. Performance comparison The Gmean of the 5 kernel-based methods over 25 datasets are provided in Tables 2 to 5. Tables 2 and 3 show the results for small-sized datasets and Tables 4 and 5 show the results for medium-sized datasets. Best Gmean per dataset is displayed in boldface in these Tables. Out of 13 small-sized datasets, proposed method performs better for 11 datasets in term of Gmean. For Bupa(2) dataset, LMKAD(R gpp) yields comparable results to MKAD(gpl) and 2https://www.csie.ntu.edu.tw/~cjlin/libsvm/ 3https://www.tudelft.nl/ewi/over-de-faculteit/afdelingen/intelligent-systems/pattern-recognition-bioinformatics/ pattern-recognition-laboratory/data-and-software/dd-tools/ 12 Table 2: Performance in term of average Gmean standard deviation (%) over 5-folds and 5 runs for small-sized datasets One-class Classi er Iris Heart(1) Heart(2) Iono(1) Iono(2) Bupa(1) Bupa(2) KPCA(g)[35] 96.44 0.61 70.42 0.22 63.5 0.78 76.54 0.59 57.1 0.23 62.91 0.4 74.28 0.59 KOC(g)[34] 92.35 0.87 65.03 1.1 66.39 0.53 92.69 0.23 53.4 0.85 57.09 1.48 68.81 0.99 SVDD(g)[9] 84.12 2.86 72.91 0.55 64.9 1.4 93.13 0.64 44.63 0.51 60.64 1.23 69.75 0.21 OCSVM(g)[10] 85.06 2.6 72.91 0.55 64.9 1.4 93.13 0.59 44.63 0.51 60.64 1.3 69.78 0.19 OCSVM(p)[10] 75.18 3.64 43.97 2.07 32.26 1.91 86.46 0.91 21.09 3.2 56.9 4.14 70.63 0.84 OCSVM(l)[10] 48.52 9.93 59.79 6.1 56.76 2.86 62.28 1.59 48.86 3.2 52.71 7.22 60.25 1.91 MKAD(gpl)[26] 57.74 0 73.4 0 67.91 0 83.12 0.11 59.91 0 64.83 0 76.14 0 MKAD(gpp)[26] 57.74 0 73.4 0 67.91 0 86.2 0.27 59.91 0 64.87 0.05 76.14 0 LMKAD(S gpl) 99.81 0.42 73.32 0.23 67.68 0.53 93.99 0.34 60.03 0.35 64.97 0.44 75.67 0.23 LMKAD(S gpp) 99.81 0.42 73.6 0.13 67.78 0.37 95.08 0.22 59.91 0 65.12 0.49 75.68 0.21 LMKAD(So gpl) 100 0 73.3 0.04 67.2 0.51 89.64 0.56 60.1 1.01 64.79 0.47 75.49 0.33 LMKAD(So gpp) 99.19 1.82 73.34 0.06 67.12 0.38 89.49 0.54 59.84 0.17 64.81 0.58 75.67 0.67 LMKAD(R gpl) 100 0 73.36 0.07 67.91 0 88.68 0.25 59.92 0.3 64.97 0.47 75.8 0.41 LMKAD(R gpp) 99.61 0.88 73.36 0.07 67.91 0 89.04 0.45 59.83 0.18 65 0.5 75.87 0.32 Table 3: Performance in term of average Gmean standard deviation (%) over 5-folds and 5 runs for small-sized datasets One-class Classi er Japan(1) Japan(2) Australia(1) Australia(2) Pima(1) Pima(2) KPCA(g)[35] 64.09 0.29 72.29 0.29 63.69 0.29 73.06 0.18 77.98 0.18 57.05 0.4 KOC(g)[34] 67.33 1.24 73.48 1.62 65.07 0.68 74.21 1.12 79.04 0.33 54.78 0.21 SVDD(g)[9] 70.15 0.4 76.58 0.28 65.55 0.47 76.78 0.22 79.21 0.19 56.71 0.62 OCSVM(g)[10] 71.45 0.38 75.78 0.29 66.08 0.6 76.59 0.44 79.18 0.19 56.59 0.47 OCSVM(p)[10] 56.84 2.82 58.71 0.52 56.47 2.65 58.14 1.41 75.12 0.5 51.02 1.25 OCSVM(l)[10] 62.21 2.89 58.54 4.9 56.94 2.16 63.32 4.58 64.9 3.05 44.25 3.11 MKAD(gpl)[26] 66.99 0.08 74.87 0.01 66.57 0.14 75.55 0.1 80.59 0.07 59.07 0.01 MKAD(gpp)[26] 66.95 0.16 75.13 0.05 66.57 0.14 75.8 0.08 80.61 0.11 58.96 0.11 LMKAD(S gpl) 66.96 0.16 75.83 0.15 66.43 0.57 76.48 0.18 80.69 0.25 58.44 0.26 LMKAD(S gpp) 67.31 0.27 76.56 0.24 66.16 0.25 77.13 0.17 80.85 0.2 59.75 0.64 LMKAD(So gpl) 67.07 0.15 76.19 0.2 66.44 0.22 76.67 0.15 80.84 0.2 59.02 0.5 LMKAD(So gpp) 67.02 0 76.23 0.14 66.67 0.08 76.66 0.16 80.69 0.33 59.63 0.62 LMKAD(R gpl) 67.09 0.1 76.22 0.15 66.69 0.19 76.56 0.29 80.88 0.18 58.6 0.31 LMKAD(R gpp) 67.1 0.1 76.22 0.15 66.64 0.09 76.64 0.19 80.89 0.13 58.52 0.2 MKAD(gpp). In case of 12 medium-sized datasets, proposed method yields better Gmean for 6 datasets and compa- rable Gmean for 3 datasets. When we analyze the impact of gating function and kernel combination on LMKAD then LMKAD(S gpp) emerges as the best classi er. Among all LMKAD variants in Tables 2 to 5, gpp kernel combination 13 Table 4: Performance in term of average Gmean standard deviation (%) over 5-folds and 5 runs for medium-sized benchmark datasets One-class Classi er German(1) German(2) Park(1) Park(2) Space(1) Space(2) KPCA(g)[35] 80.77 0.07 49.75 0.28 70.2 0.18 67.79 0.18 67.73 0.07 68.7 0.05 KOC(g)[34] 73.17 0.26 53.41 0.3 96.15 0.17 90.74 0.56 72.92 0.19 70.8 0.25 SVDD(g)[9] 81.1 0.34 52.77 0.82 97.07 0.23 79.77 0.15 71.09 0.75 71.06 0.1 OCSVM(g)[10] 80.34 0.33 52.8 0.86 95.71 0.2 81.82 0.31 72.39 0.1 70.59 0.05 OCSVM(p)[10] 69.64 0.47 24.9 1.25 92.15 0.53 78.95 0.32 70.97 0.23 76.64 0.21 OCSVM(l)[10] 69.46 1.1 48.2 0.89 51.07 2.13 60.82 2.54 54.74 1.73 55.83 2.69 MKAD(gpl)[26] 83.67 0 54.67 0.14 94.27 0.18 70.88 0.12 70.96 0.03 71.18 0.05 MKAD(gpp)[26] 83.64 0.05 54.67 0.14 96.72 0.16 71.53 0.1 71.17 0.05 71.27 0.04 LMKAD(S gpl) 83.13 0.11 54.5 0.14 99.46 0.2 82.51 0.52 72.5 0.11 71.4 0.52 LMKAD(S gpp) 82.76 0.2 54.27 0.05 99.26 0.11 85.27 0.31 72.58 0.13 71.68 0.66 LMKAD(So gpl) 83.21 0.16 55.09 0.22 94.39 0.21 71.74 0.68 71.69 0.2 71.03 0.18 LMKAD(So gpp) 83.24 0.22 54.74 0.16 94.1 0.39 71.67 0.24 71.67 0.17 71.11 0.19 LMKAD(R gpl) 83.37 0.2 54.72 0.33 93.78 0.32 71.28 0.13 71.55 0.08 71.15 0.16 LMKAD(R gpp) 83.25 0.19 54.76 0.17 94.13 0.1 71.32 0.26 71.59 0.1 71.08 0.07 Table 5: Performance in term of average Gmean standard deviation (%) over 5-folds and 5 runs for medium-sized benchmark datasets One-class Classi er Abalone(1) Abalone(2) Spam(1) Spam(2) Wave(1) Wave(2) KPCA(g)[35] 68.72 0.08 68.36 0.06 60.33 0.11 71.95 1.57 47.39 0.16 75.25 0.09 KOC(g)[34] 73.45 0.18 73.39 0.13 79.06 0.36 82.42 0.16 58.31 0.65 69.65 0.35 SVDD(g)[9] 71.05 2.56 72.82 0.46 76.64 1.54 81.34 0.84 64.99 0.07 80.09 0.03 OCSVM(g)[10] 73.06 0.14 73.23 0.09 78.41 0.24 81.66 0.17 65.94 0.1 78.84 0.09 OCSVM(p)[10] 71.91 0.16 72.09 0.16 69.29 0.58 71.04 0.44 36.22 2.11 79.56 0.32 OCSVM(l)[10] 56.73 2.91 54.52 5.24 56.93 2.5 62.67 1.7 45.83 1.95 65.66 0.52 MKAD(gpl)[26] 73.02 0.04 72.97 0.09 76.99 0.2 80.63 0.1 63.37 0.05 80.95 0.1 MKAD(gpp)[26] 72.89 0.08 72.89 0.15 77.2 0.18 81.26 0.06 64.6 0.05 81.06 0.02 LMKAD(S gpl) 72.51 0.11 73.93 0.48 77.44 0.25 80.72 0.12 66.37 0.2 85.09 0.19 LMKAD(S gpp) 74.55 0.49 74.69 0.3 77.82 0.18 81.68 0.1 66.81 0.11 83.66 0.05 LMKAD(So gpl) 73.56 0.59 74.3 0.17 77.57 0.16 80.84 0.08 64.17 0.08 81.77 0.22 LMKAD(So gpp) 73.61 0.34 74.42 0.29 77.64 0.2 80.78 0.09 64.17 0.09 82.71 0.58 LMKAD(R gpl) 73.47 0.2 74.14 0.24 76.21 1.56 80.69 0.51 63.63 0.5 81.38 0.08 LMKAD(R gpp) 73.28 0.15 74.19 0.05 76.73 1.25 80.43 0.41 64.16 0.08 81.34 0 14 Table 6: Number of datasets for which maximum Gmean has been achieved by One-class Classi ers (in sorted order) Position One-class Classi- ers #Datasets with Maximum Gmean 1 LMKAD(S gpp) 10 2 KOC(g) 4 2 LMKAD(R gpl) 4 3 MKAD(gpl) 3 3 LMKAD(So gpl) 3 4 MKAD(gpp) 2 4 LMKAD(S gpl) 2 4 LMKAD(R gpp) 2 5 SVDD(g) 1 5 OCSVM(g) 1 5 OCSVM(p) 1 5 LMKAD(So gpp) 1 6 KPCA(g) 0 6 OCSVM(l) 0 Table 7: Average percentage of Support Vectors over 5-folds and 5 runs by one-class classi ers for 25 datasets One-class Classi er OCSVM (g) OCSVM (p) OCSVM (l) MKAD (gpl) MKAD (gpp) LMKAD (S gpl) LMKAD (S gpp) LMKAD (So gpl) LMKAD (So gpp) LMKAD (R gpl) LMKAD (R gpp) Iris(1) 24.30 46.20 24.60 95.30 79.60 31.10 34.60 13.50 13.60 19.30 16.70 Heart(1) 16.72 77.00 21.56 91.53 83.84 33.09 37.03 9.72 10.16 9.59 9.59 Heart(2) 21.82 78.82 24.49 94.68 86.68 37.24 52.75 15.25 12.65 31.61 29.02 Iono(1) 10.24 19.47 37.24 42.09 45.00 15.16 15.04 8.47 7.31 9.73 10.07 Iono(2) 29.40 75.55 53.49 88.02 82.46 69.30 81.46 34.51 80.33 80.55 71.75 Bupa(1) 14.72 25.45 13.59 73.34 58.55 16.31 21.07 7.86 8.03 8.76 8.17 Bupa(2) 14.53 17.65 11.45 76.95 61.73 14.58 21.45 9.83 10.70 11.63 9.60 Japan(1) 11.97 37.56 14.27 91.97 80.46 18.16 21.82 9.02 16.93 16.27 15.13 Japan(2) 11.05 51.99 12.82 83.66 77.96 15.57 21.29 7.38 7.37 7.25 7.25 Australia(1) 11.19 37.62 14.04 91.86 79.74 25.18 28.25 50.93 47.93 34.68 34.80 Australia(2) 10.57 53.12 12.55 89.16 81.79 15.78 19.24 6.63 6.67 6.44 6.49 Pima(1) 8.71 19.00 7.81 54.89 46.54 8.32 10.05 5.97 6.65 6.85 6.52 Pima(2) 10.22 33.81 10.52 78.62 66.38 11.75 16.90 8.00 6.79 12.23 8.34 German(1) 9.61 33.27 13.21 77.23 75.81 14.91 17.66 9.38 7.80 9.93 9.72 German(2) 12.77 84.02 23.72 90.32 80.25 25.12 32.30 7.27 10.85 25.42 15.53 Park(1) 8.24 15.87 29.86 73.08 37.60 16.15 16.82 8.03 8.56 7.89 7.28 Park(2) 8.22 34.81 25.63 40.80 48.62 15.17 20.51 10.67 7.76 7.98 8.11 Space(1) 5.53 5.76 6.51 41.67 26.28 10.17 12.12 5.48 5.49 5.46 5.43 Space(2) 5.47 5.69 6.52 46.42 36.60 12.39 8.73 5.47 5.42 6.09 6.53 Abalone(1) 5.28 5.73 6.25 13.16 9.94 5.57 9.04 10.41 6.65 5.65 5.48 Abalone(2) 5.40 6.17 5.95 15.16 10.55 5.90 6.28 8.34 5.13 6.45 5.09 Spam(1) 6.30 26.21 10.69 58.29 55.74 6.00 6.77 5.36 5.34 6.62 6.08 Spam(2) 5.88 32.11 9.13 46.39 45.08 5.79 9.54 5.26 5.24 8.01 7.12 Wave(1) 6.38 70.60 9.11 91.13 55.66 9.08 10.88 6.94 6.94 7.70 6.94 Wave(2) 5.70 16.55 7.00 56.01 37.59 19.24 26.33 15.25 16.87 26.01 16.75 15 Table 8: Friedman ranking (FRank) and Mean of Gmean (MGmean) of all one-class classi ers in increasing order of the FRank. One-class Classi- er FRank MGmean(%) LMKAD(S gpp) 2.98 75.59 LMKAD(So gpl) 5.30 74.24 LMKAD(S gpl) 5.38 75.19 LMKAD(So gpp) 5.40 74.25 LMKAD(R gpl) 5.60 74.08 LMKAD(R gpp) 5.70 74.12 MKAD(gpp) 6.48 72.36 MKAD(gpl) 7.32 72.01 OCSVM(g) 7.70 72.86 SVDD(g) 7.98 72.59 KOC(g) 8.20 72.13 KPCA(g) 11.52 68.25 OCSVM(p) 11.84 62.25 OCSVM(l) 13.60 56.87 with Sigmoid gating function (i.e., LMKAD(S gpp)) exhibits better Gmean for 15 datasets and comparable Gmean for rest of the 10 datasets. It is obvious that multiple kernel learning based methods need more percentage of support vectors compared to conventional OCS VM as more than one kernel is employed for classi cation. Impact of localization can be observed from Tables 2 to 5 and Table 7 that LMKAD yields better results compared to MKAD for most of the datasets by using signi cantly lesser percentage of support vectors. Hence, LMKAD leads to sparser solution compared to MKAD. Moreover, LMKAD achieves this performance by combining Gaussian kernel with less complex kernel like polynomial and linear kernel. Further, Gmean-based two other performance criteria viz., MGmean and PMG are computed for all the classi ers to analyze their performances more closely. 5.2.2. A closer look at the above obtained Gmean using MGmean and PMG The performance of each method over the 25 datasets using the MGmean metric is presented in Table 8 and is plotted in a decreasing order in Fig. 1. Based on the obtained results in Fig. 1, it can be clearly stated that all 6 variants of LMKAD have achieved top six positions among all one-class classi ers as per MGmean criterion. Moreover, LMKAD(S gpp) yields best MGmean among existing kernel-based one-class classi ers. It is to be noted from Table 6 that LMKAD(S o gpp) yields maximum Gmean for only one dataset, which is less compared to KOC(g) and rest of the multiple kernel-based methods. However, LMKAD(S o gpp) holds the third position in Fig. 1 as per MGmean criterion, i.e., yields better MGmean compared to most of the methods mentioned in Table 6. Similar can be experienced with some other methods also. Hence, In order to further analyze the performance of the competing 16 LMKAD(S_gpp) LMKAD(S_gpl) LMKAD(So_gpp) LMKAD(So_gpl) LMKAD(R_gpp) LMKAD(R_gpl) OCSVM(g) SVDD(g) MKAD(gpp) KOC(g) MKAD(gpl) KPCA(g) OCSVM(p) OCSVM(l) 0 20 40 60 80 MGmean vs. FRank MGmean FRank Figure 1: All one-class classi ers as per average Gmean (MGmean) in decreasing order and their corresponding Friedman Rank (FRank). 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 Datasets 55 60 65 70 75 80 85 90 95 100 % of Maximum Gmean (PMG) KPCA(g) KOC(g) SVDD(g) OCSVM(g) MKAD(gpp) LMKAD(S_gpp) LMKAD(So_gpp) LMKAD(R_gpp) Figure 2: Percentage of the maximum Gmean (PMG) achieved by various one-class classi ers over 15 datasets (ordered by increasing percentage) one-class classi ers, the Percentage of the Maximum Gmean (PMG) is calculated as per Eq. (24), similar to [33]. PMG metric provides information regarding proximateness of each classi er towards maximum Gmean value. As it can be seen in Table 9, all 6 variants of LMKAD hold the top six positions similar to the ranking based on 17 Table 9: Percentage of the Maximum Gmean (PMG) One-class Classi- ers PMG (%) LMKAD(S gpp) 98.91 LMKAD(S gpl) 98.38 LMKAD(So gpp) 97.36 LMKAD(So gpl) 97.35 LMKAD(R gpp) 97.18 LMKAD(R gpl) 97.15 MKAD(gpp) 95.42 OCSVM(g) 95.28 MKAD(gpl) 95.02 SVDD(g) 94.91 KOC(g) 94.24 KPCA(g) 89.72 OCSVM(p) 80.13 OCSVM(l) 75.41 the MGmean values in Fig. 1. In Fig. 2, PMG values of 3 variants of LMKAD and 5 existing one-class classi ers are plotted in an increasing order for all 25 datasets. LMKAD is a multi-kernel version of the OCS VM(g). The plotted lines for these two classi ers in Fig. 2 clearly indicate the performance improvement of multi-kernel version over single-kernel one. Overall, Fig. 2 illustrates the clear superiority of the localized multi-kernel-based one-class classi ers over the existing methods. Moreover, LMKAD(S gpp) and LMKAD(S gpl) obtain more than 97% PMG value for all datasets except Japan(1), Space(1) and Park(1) datasets. Even for these three datasets, LMKAD(S gpp) and LMKAD(S gpl) achieve more than 90% PMG value. Detailed PMG values for all one-class classi ers over 25 datasets are made available on the web page (https://goo.gl/DuYdJE). Above discussion suggests LMKAD(S gpp) as the best performing classi er in term of Gmean, MGmean, and PMG. Despite this fact, a statistical testing needs to perform for verifying this fact. In the next subsection, Friedman Rank (FRank) testing is performed for statistical testing. 5.3. Statistical comparison For comparing the performance of the proposed and existing kernel-based methods on 25 benchmark datasets, a non-parametric Friedman test is employed. In the Friedman test, the null hypothesis states that the mean of individual experimental treatment is not signi cantly di erent from the aggregate mean across all treatments and the alternate hypothesis states the other way around. Friedman test computes three components viz., F-score, p-value and Friedman Rank (FRank). If the computed F-score is greater than the critical value at the tolerance level = 0.05, then one rejects the equality of mean hypothesis (i.e. null hypothesis). We employ the modi ed Friedman test [28] for the testing, which was proposed by Iman and Davenport [37]. The F-score obtained after employing non-parametric Friedman 18 test is 24.56, which is greater than the critical value at the tolerance level = 0.05 i.e. 24.56 > 1.75. Hence, we can null hypothesis can be rejected with 95% of a con dence level. The computed p-value of the Friedman test is 2.1454e 28 with the tolerance value = 0.05, which is much lower than 0.05. This small value indicates that di erences in the performance of the various methods are statistically signi cant. Afterwards, FRank of each classi er is also calculated to assign a rank to all presented one-class classi ers in this paper. Friedman test assigns a rank to all the methods for each dataset, it assigns rank 1 to the best performing algorithm, the second best rank 2 and so on. If rank ties then average ranks are assigned [28]. The FRank of all one-class classi ers is provided in increasing order in Table 8 and is visualized in Fig. 1 with decreasing order of MGmean. LMKAD(S gpp) still achieves top position, similar to using the MGmean metric. From Table 8 and Fig. 1, it can be observed that FRank of most of the classi ers follows a similar pattern as MGmean, i.e., FRank increases as MGmean decreases. However, some of the one-class classi ers don t follow the same pattern as with MGmean. For example, LMKAD(S gpl) has better MGmean but inferior FRank compared to LMKAD(S o gpl). The above analysis indicates that a one-class classi er with better FRank has better generalization capability compared to the other existing methods. Overall, after the performance analysis of all one-class classi ers, it is observed that none of the existing one-class classi ers perform better than the proposed LMKAD one-class classi er in term of MGmean, PMG, and FRank. 6. Conclusion In this paper, an anomaly detection/OCC method (LMKAD) is proposed as an extension of MKAD. LMKAD provides a localized formulation for multi-kernel learning method by local assignment of weights to each kernel. The derived formulation is also shown to be analogous to conventional OCS VM and is solved in a similar fashion using a LIBSVM solver. In LMKAD, local assignments of weights are achieved by the training of a gating function and a conventional OCS VM with the combined kernel in tandem. The proposed method is empirically tested with 3 types of gating function and over 25 benchmark datasets. Performance of LMKAD is compared against 5 kernel-based one-class classi ers. LMKAD outperforms both conventional OCS VM and MKAD for most of the datasets. For some other datasets, it performs similar to MKAD but uses fewer support vectors and provides a sparser solution. The Friedman test is also performed, which veri es that the experimental outcomes are statistically signi cant. In future work, proposed localized formulation can be extended for other variants of kernel-bases one-class classi er. 19 Acknowledgment This research was supported by Department of Electronics and Information Technology (DeITY, Govt. of India) under Visvesvaraya PhD scheme for electronics & IT. References [1] M. A. Pimentel, D. A. Clifton, L. Clifton, and L. Tarassenko. A review of novelty detection. Signal Processing, 99:215 249, 2014. [2] Y. Chen, K. Wang, and P. Zhong. One-class support tensor machine. Knowledge-Based Systems, 96:14 28, 2016. [3] B. Krawczyk and M. Wo zniak. Dynamic classi er selection for one-class classi cation. Knowledge-Based Systems, 107:43 53, 2016. [4] G. Huang, Z. Yang, X. Chen, and G. Ji. An innovative one-class least squares support vector machine model based on continuous cognition. Knowledge-Based Systems, 123:217 228, 2017. [5] W. Zhu and P. Zhong. A new one-class svm based on hidden information. Knowledge-Based Systems, 60:35 43, 2014. [6] L. V. Utkin and Y. A. Zhuk. An one-class classi cation support vector machine model by interval-valued training data. Knowledge-Based Systems, 120:43 56, 2017. [7] S. Luca, D. A. Clifton, and B. Vanrumste. One-class classi cation of point patterns of extremes. The Journal of Machine Learning Research, 17(1):6581 6601, 2016. [8] N. G ornitz, M. Braun, and M. Kloft. Hidden markov anomaly detection. In International Conference on Machine Learning, pages 1833 1842, 2015. [9] D. M. J. Tax and R. P. W. Duin. Support vector domain description. Pattern recognition letters, 20(11):1191 1199, 1999. [10] B. Sch olkopf, R. C. Williamson, A. J. Smola, J. Shawe-Taylor, and J. C. Platt. Support vector method for novelty detection. In NIPS, volume 12, pages 582 588, 1999. [11] C. Cortes and V. Vapnik. Support-vector networks. Machine learning, 20(3):273 297, 1995. [12] L. M. Manevitz and M. Yousef. One-class svms for document classi cation. Journal of Machine Learning Research, 2(Dec):139 154, 2001. [13] S. Das and Nikunj C. Oza. Sparse solutions for single class svms: A bi-criterion approach. In Proceedings of the 2011 SIAM International Conference on Data Mining, pages 816 827. SIAM, 2011. [14] F. R. Bach, G. R. G. Lanckriet, and M. I. Jordan. Multiple kernel learning, conic duality, and the smo algorithm. In Proceedings of the twenty- rst international conference on Machine learning, page 6. ACM, 2004. [15] M. G onen and E. Alpayd n. Multiple kernel learning algorithms. Journal of machine learning research, 12(Jul):2211 2268, 2011. [16] Q. Fan, D. Gao, and Z. Wang. Multiple empirical kernel learning with locality preserving constraint. Knowledge-Based Systems, 105:107 118, 2016. [17] T. Wang, D. Zhao, and Y. Feng. Two-stage multiple kernel learning with multiclass kernel polarization. Knowledge-Based Systems, 48:10 16, 2013. [18] M. G onen and E. Alpaydin. Localized multiple kernel learning. In Proceedings of the 25th international conference on Machine learning, pages 352 359. ACM, 2008. [19] M. Goene and E. Alpaydin. Localized multiple kernel machines for image recognition. In Neural Information Processing SystemsWorkshop on Understanding Multiple Kernel Learning Method, 2009. [20] M. Gonen and E. Alpaydin. Localized multiple kernel regression. In Pattern Recognition (ICPR), 2010 20th International Conference on, pages 1425 1428. IEEE, 2010. 20 [21] L. Zhang and X. Hu. Locally adaptive multiple kernel clustering. Neurocomputing, 137:192 197, 2014. [22] Y. Lei, A. Binder, U. Dogan, and M. Kloft. Localized multiple kernel learning a convex approach. In Robert J. Durrant and Kee-Eung Kim, editors, Proceedings of The 8th Asian Conference on Machine Learning, volume 63 of Proceedings of Machine Learning Research, pages 81 96. PMLR, 16 18 Nov 2016. [23] Y. Han, K. Yang, Y. Ma, and G. Liu. Localized multiple kernel learning via sample-wise alternating optimization. IEEE transactions on cybernetics, 44(1):137 148, 2014. [24] Y. Han, K. Yang, and G. Liu. l {p} norm localized multiple kernel learning via semi-de nite programming. IEEE Signal Processing Letters, 19(10):688 691, 2012. [25] Y. Han, K. Yang, Y. Yang, and Y. Ma. Localized multiple kernel learning with dynamical clustering and matrix regularization. IEEE Transactions on Neural Networks and Learning Systems, 29(2):486 499, 2018. [26] S. Das, Bryan L. Matthews, Ashok N. Srivastava, and Nikunj C. Oza. Multiple kernel learning for heterogeneous anomaly detection: algorithm and aviation safety case study. In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 47 56. ACM, 2010. [27] R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hinton. Adaptive mixtures of local experts. Neural computation, 3(1):79 87, 1991. [28] J. Dem sar. Statistical comparisons of classi ers over multiple data sets. Journal of Machine learning research, 7(Jan):1 30, 2006. [29] A. Rakotomamonjy, F. Bach, S. Canu, and Y. Grandvalet. More e ciency in multiple kernel learning. In Proceedings of the 24th international conference on Machine learning, pages 775 782. ACM, 2007. [30] S. Amari and S. Wu. Improving support vector machine classi ers by modifying kernel functions. Neural Networks, 12(6):783 789, 1999. [31] M. Lichman. UCI machine learning repository, 2013. [32] C.-C. Chang and C.-J. Lin. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:1 27, 2011. Software available at http://www.csie.ntu.edu.tw/~cjlin/libsvm. [33] M. Fern andez-Delgado, E. Cernadas, S. Barro, and D. Amorim. Do we need hundreds of classi ers to solve real world classi cation problems. J. Mach. Learn. Res, 15(1):3133 3181, 2014. [34] Q. Leng, H. Qi, J. Miao, W. Zhu, and G. Su. One-class classi cation with extreme learning machine. Mathematical Problems in Engineering, pages 1 11, 2014. [35] H. Ho mann. Kernel PCA for novelty detection. Pattern Recognition, 40(3):863 874, 2007. Software available at http://www. heikohoffmann.de/kpca.html. [36] D. M. J. Tax. DDtools, the data description toolbox for MATLAB, version 2.1.2, June 2015. [37] R. L. Iman and J. M. Davenport. Approximations of the critical region of the fbietkan statistic. Communications in Statistics-Theory and Methods, 9(6):571 595, 1980. 21