IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 22, NO. 8, AUGUST 2011 1297 Convergence of Cyclic and Almost-Cyclic Learning with Momentum for Feedforward Neural Networks Jian Wang, Jie Yang, and Wei Wu Abstract Two backpropagation algorithms with momentum for feedforward neural networks with a single hidden layer are considered. It is assumed that the training samples are supplied to the network in a cyclic or an almost-cyclic fashion in the learning procedure, i.e., in each training cycle, each sample of the training set is supplied in a xed or a stochastic order respectively to the network exactly once. A restart strategy for the momentum is adopted such that the momentum coef cient is set to zero at the beginning of each training cycle. Corresponding weak and strong convergence results are then proved, indicating that the gradient of the error function goes to zero and the weight sequence goes to a xed point, respectively. The convergence conditions on the learning rate, the momentum coef cient, and the activation functions are much relaxed compared with those of the existing results. Index Terms Almost-cyclic, backpropagation, convergence, cyclic, feedforward neural networks, momentum. I. INTRODUCTION L EARNING algorithms play an essential role for feed- forward neural networks (NNs). Through learning, the weights of a NN are adapted to meet the requirement of its environment. Backpropagation (BP) method is widely used for training feedforward NNs [1] [3]. This paper considers two BP algorithms with momentum for feedforward NNs with a single hidden layer. There are two popular ways of learning with training samples to implement the BP algorithm, batch mode and incremental mode [4]. Corresponding to the standard gradient method, batch mode learning algorithm is completely deter- ministic but requires additional storage for each weight. On the other hand, incremental mode updates the weights immediately after each sample is fed, and is less demanding on the memory. There are three incremental learning strategies according to the order in which the samples are applied. The rst strategy is online learning (completely stochastic order), i.e., at each learning step, one of the samples is drawn at random from Manuscript received October 14, 2010; revised March 8, 2011; accepted June 11, 2011. Date of publication July 12, 2011; date of current version August 3, 2011. This work was supported in part by the National Natural Science Foundation of China under Grant 10871220 and the China Scholar- ship Council. J. Wang is with the School of Mathematical Sciences, Dalian Univer- sity of Technology, Dalian 116024, China. He is also with the Compu- tational Intelligence Laboratory, Department of Electrical and Computer Engineering, University of Louisville, Louisville, KY 40292 USA (e-mail: wangjiannl@mail.dlut.edu.cn). J. Yang and W. Wu are with the School of Mathematical Sci- ences, Dalian University of Technology, Dalian 116024, China (e-mail: yangjiee@dlut.edu.cn; wuweiw@dlut.edu.cn). Digital Object Identi er 10.1109/TNN.2011.2159992 the training set and presented to the network [4] [7]. The second strategy is almost-cyclic learning (special stochastic order), i.e., the order of sample presentation is continually drawn at random after each training cycle [4], [8] [10]. The third strategy is cyclic learning ( xed order), that is, in each training cycle, each sample in the training set is supplied in a xed order, i.e., a particular order of sample presentation is drawn at random before learning starts and then xed in time [4], [11] [13]. Some researchers have compared the two basic different training schemes (batch mode and incremental mode) for feedforward NNs [4], [5], [8]. Heskes and Wiegerinck [4] reveal several asymptotic properties of the two schemes and conclude that almost-cyclic learning is a better alternative for batch mode learning than cyclic learning. Wilson [5] explains why batch training is almost always slower than online training (often orders of magnitude slower) especially on large training sets. The main reason is the ability of online training to follow curves in the error surface throughout each cycle, which allows it to safely use a larger learning rate and thus converge with fewer iterations through the training data. Nakama [8] theoretically analyzes the convergence properties of the two schemes applied to quadratic loss functions and shows the exact degrees to which the training set size, the variance of the per-instance gradient, and the learning rate affect the rate of convergence for each scheme. However, it is well known that a general drawback of gradient-based BP methods is their slow convergence. Many modi cations of this learning scheme have been proposed to overcome the dif culty [14], [15]. The BP method with mo- mentum is one of the popular variations. Its idea is to update the weights in the direction, which is a linear combination of the present gradient of the error function and the previous weight updating increment, so as to smooth the weight trajec- tory and speed up the convergence of the algorithm [16]. It is also sometimes credited with avoiding local minima in the error surface. A recent method of avoiding local minima by convexifying an error criterion is proposed in [17]. There have been some studies on the momentum algorithm in the literature [18] [24]. Phansalkar and Sastry [18] show that all local minima of the squared error surface are stable points for the BP algorithm with momentum while other equilibrium points are unstable. Hagiwara [19] and Sato [20] show that the momentum coef cient can be derived from a modi ed cost function, in which the squared errors at the output layer are exponentially weighted in time. They demonstrate a qualitative relationship among the momentum term, the learning rate, and the speed of convergence. 1045 9227/$26.00 2011 IEEE 1298 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 22, NO. 8, AUGUST 2011 Qian [21] shows that the momentum parameter is analogous to the mass of Newtonian particles that move through a viscous medium in a conservative force eld. By employing a discrete approximation to the continuous system, Qian also illustrates the conditions for the stability of the algorithm. Torii and Hagan [25] analyze the effect of momentum on steepest descent training for quadratic performance functions. They derive the stability conditions by analyzing the exact mo- mentum equations for the quadratic cost function. In addition, they show a relationship between the momentum coef cient and the speed of convergence of the algorithm. Bhaya [26] points out that the BP with momentum presented in [25] is actually a special case of the more general conjugate gradient method, in which both the learning rate and the momentum coef cient are chosen dynamically in feedback form. We note that the convergence property for feedforward NN learning is an interesting research topic which offers an effective guarantee in real application. For the gradient-based BP methods without momentum, the existing convergence results focus on online, almost-cyclic, and cyclic learning algorithms. The batch mode learning is essentially a standard gradient descent method. It is easy to see that, if the error criterion is monotonically decreasing, then the convergence of the algorithm is obvious and needs no proof. However, due to the arbitrariness in the presentation order of the training samples, online learning is a completely stochastic process. Thus the convergence results for online learning are mostly asymptotic with a probabilistic nature as the size of training samples goes to in nity [6], [7], [27] [31]. Deterministic convergence, on the other hand, lies in almost-cyclic and cyclic learning mainly because every sample of the training set is fed exactly once in each training cycle [10] [13], [32], [33]. It is a bit easier to obtain the convergence for cyclic learning than for almost-cyclic learning. We mention that almost-cyclic learning performs numerically better than cyclic learning since the random property of the training process exists in almost- cyclic learning [4], [9], [10]. Learning rate is an important criterion in the existing conver- gence analysis. For the training method without momentum, we mention that the special condition of learning rate depends on the different learning fashion. A usual requirement is that the learning rates m for online learning satisfy the assumptions  m=0 m = and  m=0 2 m < ( m > 0) [30], [31]. In contrast, to obtain the deterministic convergence for cyclic and almost-cyclic learning, authors usually impose certain extra conditions on the learning rate. An additional condition limm m/ m+1 = 1 is proposed in [11] to guarantee the convergence for cyclic learning. It is actually a big step forward for the convergence analysis of cyclic learning, compared to the conditions in [12], [13], and [33], which are basically m = O(1/m). In the existing result for the almost-cyclic learning [10], the condition m = O(1/m) is still required. The convergence property of the BP methods with mo- mentum has also been considered by researchers. Bhaya [26] and Torii [25] discuss the convergence of the gradient method with momentum under a restriction that the activation function is linear, which unfortunately is not satis ed by usual activation functions. For the batch learning BP algorithm with momentum, a particular criterion to choose the momentum coef cients term is proposed in [34] and [35] for BP NNs with or without hidden layer, respectively, and the corresponding weak convergence (the gradient of the error function goes to zero) and strong convergence (the weight sequence goes to a xed point) are proved. The cyclic learning with momentum is considered for feedforward NNs without a hidden layer in [36] and [37], where some tight conditions are required to guarantee the convergence. The learning rates satisfy 0 < 0 1 and 1/ m+1 = 1/ m + N, where N is a positive constant, and the momentum coef cient m,k for the kth sample at mth learning cycle is chosen as m,k =  2 m p(m,k) wm J+k 1 , if wmJ+k 1 = 0, 0, else (1) where J is the total number of samples, and p(m,k) and wmJ+k 1 denote the gradient of the error function and the previous increment of the weight. To our best knowledge, there is no convergence results for almost-cyclic learning with momentum. In this paper, we present a comprehensive study on the weak and strong convergence results for cyclic and almost- cyclic learning with momentum for feedforward NNs with a hidden layer in a quite general framework. Our convergence results are of global nature in that they are valid for arbitrary initial weights. (As a comparison, the above-mentioned result in [18] can be viewed as a local convergence result.) Unlike the corresponding restrictive conditions in [25], [26], [34], and [35], quite simple and general conditions are required in this paper on the learning rates and the momentum coef cients to guarantee the convergence. And these conditions are satis ed by all typical activation functions. In the following paragraphs, we list and explain in detail the main points of the novel contributions of this paper. 1) The condition on the learning rate for cyclic and almost- cyclic learning with momentum is extended to a more general case:  m=0 m = ,  m=0 2 m < ( m > 0), which is identical to those in [6], [7], and [27] [31] for online learning without momentum. We note that the existing convergence results for cyclic and almost-cyclic learning without momentum [10] [12] are special cases of the momentum methods when the momentum coef cients are set to zero. In a recent convergence result [11] for cyclic learning, an extra condition limm m/ m+1 = 1 is required. And for the almost-cyclic learning without momentum [10], a special condition 1/ m+1 = (1/ m) + l, (l > 0) on the learning rates is required, which basically means m = O(1/m). The convergence results in [36] and [37] for cyclic learning with momentum focus on two-layer feedforward NNs, and require 1/ m+1 = (1/ m) + N (N is a positive constant) and 0 < 0 1. It is obvious to see that the conditions on the learning rate are much relaxed in this paper than those in [10] [12], [36], and [37]. 2) Our condition for the momentum coef cients m to satisfy  m=0 2 m < is more relaxed than those in [36] and [37]. WANG et al.: CONVERGENCE OF CYCLIC AND ALMOST-CYCLIC LEARNING WITH MOMENTUM FOR FEEDFORWARD NNs 1299 We note that the (1) on the momentum coef cients is not only closely related to the learning rate but also dependent on the error and the gradient of the error. It is easy to verify that  m=0 2 m,k < (k = 1, . . ., J) is valid. Thus, the (1) is actually a very special case of our above condition. 3) Our convergence results are valid for both cyclic learning and almost-cyclic learning with momentum. We notice that almost-cyclic learning performs numerically better than cyclic learning due to the stochastic nature of the training process [9], [10]. To our best knowledge, the weak and strong convergence results in this paper are novel for almost- cyclic learning with momentum. 4) We assume that the derivatives g and f of the activation functions are locally Lipschitz continuous. This condition of ours re nes the corresponding conditions in [12], [36], and [37], which demand the boundedness of the second derivative g , and in [11], which needs g to be Lipschitz continuous and uniformly bounded on the real number eld R. The importance of this condition is that it makes our convergence results apply not only to S-S type NNs (both the hidden and output neurons are with sigmoid activation functions), but also to P-P, P-S, and S-P types NNs, where S and P represent sigmoid and polynomial functions, respectively. 5) The restrictive assumption on the stationary point set of the error function for the strong convergence in [11], [32], and [37] is relaxed, in that our only requirement on this set is that it does not contain any interior point. To obtain the strong convergence result, which means that the weight sequence converges to a xed point, an additional assumption is introduced in [32], [36], and [37]: the gradient of the error function has nitely many stationary points. A relaxed condition is presented in [11]: the gradient of the error function has at most countably in nitely many stationary points. These conditions are much improved in our case. The remainder of this paper is organized as follows. In the next section, we formulate mathematically the cyclic and almost-cyclic learning with momentum for feedforward NNs. The main convergence results are presented in Section III, and the rigorous proofs of the main results are provided in Section IV. In Section V, we conclude this paper with some remarks. II. CYCLIC AND ALMOST-CYCLIC LEARNING WITH MOMENTUM We consider a feedforward NN with three layers. The numbers of neurons for the input, hidden, and output layers are p, n, and 1, respectively. Suppose that the training sample set is {x j, O j}J 1 j=0 Rp R, where x j and O j are the input and the corresponding ideal output of the jth sample, respectively. Let V =  vi, j  n p be the weight matrix connecting the input and hidden layers, and we write vi = (vi1, vi2, . . . , vip)T for i = 1, 2, . . . , n. The weight vector connecting the hidden and output layers is denoted by u = (u1, u2, . . . , un)T Rn. To simplify the presentation, we combine the weight matrix V with the weight vector u, and write w =  uT , vT 1 , . . . , vT n T Rn(p+1). Let g, f : R R be given activation functions for the hidden and output layers, respectively. For convenience, we introduce the following vector-valued function: G (z) = (g (z1) , g (z2) , . . . , g (zn))T z Rn. (2) For any given input x Rp, the output of the hidden neurons is G(Vx), and the nal actual output is y = f (u G (Vx)). (3) For any xed weight w, the error of the NNs is de ned as E(w) = 1 2 J 1  j=0 (O j f (u G(Vx j)))2 = J 1  j=0 f j(u G(Vx j)) (4) where f j(t) = 1/2(O j f (t))2, j = 0, 1, . . . , J 1, t R. The gradients of the error function with respect to u and vi are, respectively, given by Eu(w) = J 1  j=0  O j y j f (u G(Vx j))G(Vx j) = J 1  j=0 f j(u G(Vx j))G(Vx j) (5) Evi(w) = J 1  j=0  O j y j f (u G(Vx j))uig (vi x j)x j = J 1  j=0 f j(u G(Vx j))uig (vi x j)x j. (6) Write EV(w) =  Ev1(w)T , Ev2(w)T , . . . , Evn(w)T T , (7) Ew(w) =  Eu(w)T , EV(w)T T . (8) With cyclic learning with momentum, a particular cycle is drawn at random from the set of all the possible cycles and then kept xed at all times [4]. The detailed cyclic learning algorithm with momentum is presented as follows. Starting from an arbitrary initial weight w0 = (u0, V0), the network weights are updated iteratively by umJ+1 = umJ + m 0umJ, j = 0 umJ+ j+1 = umJ+ j + m jumJ+ j + m  umJ+ j umJ+ j 1 , j = 1, . . ., J 1. (9) vmJ+1 i = vmJ i + m 0vmJ i , j = 0 vmJ+ j+1 i = vmJ+ j i + m jvmJ+ j i + m  vmJ+ j i vmJ+ j 1 i  , j = 1, . . . , J 1 (10) 1300 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 22, NO. 8, AUGUST 2011 where kumJ+ j = (Ok ymJ+ j,k) f (umJ+ j GmJ+ j,k)GmJ+ j,k = f k  umJ+ j GmJ+ j, k GmJ+ j,k, (11) kvmJ+ j i =  Ok ymJ+ j,k f (umJ+ j GmJ+ j,k)umJ+ j i g (vmJ+ j i xk)xk = f k  umJ+ j GmJ+ j,k umJ+ j i g  vmJ+ j i xk xk, (12) GmJ+ j,k = G(VmJ+ jxk), (13) ymJ+ j,k = f (umJ+ j GmJ+ j,k), (14) m N; i = 1, 2, . . . , n; j, k = 0, 1, . . . , J 1. Here the parameters m and m are the learning rate and the momentum coef cient, respectively. With almost-cyclic learning with momentum, subsequent training cycles are drawn at random. Almost-cyclic learning is online learning with training cycles instead of training patterns, i.e., the training samples are supplied in a sto- chastic order in each cycle. For the mth training cycle, let {xm,1, xm,2, . . . , xm,J} be a stochastic order of the input vectors {x1, x2, . . . , xJ}. Similar to the above cyclic learning algorithm, starting from an arbitrary initial weight w0 = (u0, V0), the network weights are updated iteratively by umJ+1 = umJ + m m 0 umJ, j = 0, umJ+ j+1 = umJ+ j + m m j umJ+ j + m  umJ+ j umJ+ j 1 , j = 1, . . . , J 1 (15) vmJ+1 i = vmJ i + m m 0 vmJ i , j = 0 vmJ+ j+1 i = vmJ+ j i + m m j vmJ+ j i + m  vmJ+ j i vmJ+ j 1 i  , j = 1, . . ., J 1 (16) where m k umJ+ j = (Ok ymJ+ j,m,k) f (umJ+ j GmJ+ j,m,k)GmJ+ j,m,k = f k  umJ+ j GmJ+ j,m,k GmJ+ j,m,k, (17) m k vmJ+ j i =  Ok ymJ+ j,m,k f (umJ+ j GmJ+ j,m,k)umJ+ j i g (vmJ+ j i xm,k)xm,k = f k  umJ+ j GmJ+ j,m,k umJ+ j i g  vmJ+ j i xm,k xm,k, (18) GmJ+ j,m,k = G(VmJ+ jxm,k), (19) ymJ+ j,m,k = f (umJ+ j GmJ+ j,m,k), (20) m N; i = 1, 2, . . . , n; j, k = 0, 1, . . . , J 1. Remark: A restart strategy for the momentum is adopted here: the momentum coef cient is set to zero at the beginning of each training cycle. A similar restart strategy has been used in [38] for a conjugate gradient method. We note that this restart strategy makes our convergence analysis much easier, while it does not do any harm to the practical convergence of the training procedure. III. MAIN RESULTS Locally Lipschitz Continuous [39]: Function f : Rn R is said to be Lipschitz near x Rn if there exist positive numbers K and such that we obtain | f (x2) f (x1)| K x2 x1 2 for all x1, x2 x + b(0, 1). If f is Lipschitz near every point of its domain, then it is said to be locally Lipschitz continuous. For any vector x = (x1, x2, . . . , xn)T Rn, we write its Euclidean norm as x = n i=1 x2 i . Let 0 = {w  : Ew(w) = 0} be the stationary point set of the error function E(w), where  Rn(p+1) is a bounded region satisfying (A4) below. Let 0,s R be the projection of 0 onto the sth coordinate axis 0,s =  ws R : w = (w1, . . . , ws, . . . , wn(p+1))T 0  (21) for s = 1, 2, . . . , n(p + 1). To analyze the convergence of the algorithm, we need the following assumptions: (A1) g (t) and f (t) are local Lipschitz continuous; (A2) m > 0,  m=0 m = ,  m=0 2 m < ; (A3) m 0,  m=0 2 m < ; (A4) there exists a bounded region  Rn such that {wm} m=0 ; (A5) 0,s does not contain any interior point for every s = 1, 2, . . . , n(p + 1). Theorem 3.1: Assume that (A1) (A4) are valid. Then, start- ing from an arbitrary initial value w0, the weight sequence {wm} de ned by (9) and (10) or by (15) and (16) satis es the following weak convergence: lim m Ew wm = 0. (22) Moreover, if (A5) is also valid, there holds the strong conver- gence: There exists w 0 such that lim m wm = w . (23) Let us make a few remarks on the convergence result. (A1) allows a broad choice for the activation functions. The assumptions on the activation functions in the existing convergence results [11], [12], [36], [37] are special cases of (A1). We note that, typically, S-S type networks (with sigmoid activation functions for both hidden and output neurons) are used for classi cation problems, and S-P type networks (with sigmoid hidden neurons and linear or other polynomial output neurons) are used for approximation problems. In this paper, we give a uniform treatment for all the types (S-S, S-P, P-S, and P-P) of BP NNs. As indicated in Contributions 1) and 2), the conditions on the learning rates and the momentum coef cients in this paper [see (A2) and (A3)] are less restrictive than those in [34] [37] [see (1)]. For the strong convergence, our (A5) on 0 allows it to be a nite set, countably in nite set (such as the set of rational numbers), nowhere dense set (such as Cantor set) or even some uncountable dense set (such as WANG et al.: CONVERGENCE OF CYCLIC AND ALMOST-CYCLIC LEARNING WITH MOMENTUM FOR FEEDFORWARD NNs 1301 the set of irrational numbers). The corresponding assumptions in [11], [32], [36], and [37] that the set 0 contains nitely many points or at most countably in nitely many points, respectively, are simple and special cases of (A5) in this paper. IV. PROOFS For convenience of presentation, we demonstrate in detail the convergence proof for the BP method with cycle learning fashion in Section A. For almost-cycle learning fashion, the proof is similar and introduced in the following Section B. A. Convergence Analysis for Cyclic learning In this section, we present ve useful lemmas for the convergence analysis. Lemma 4.1: Let q(x) be a function de ned on a bounded closed interval [a, b] such that q (x) is Lipschitz continuous with Lipschitz constant K > 0. Then, q (x) is differentiable almost everywhere in [a, b] and q (x)  K, a.e. [a, b]. (24) Moreover, there exists a constant C > 0 such that q(x) q(x0) + q (x0)(x x0) + C(x x0)2, x0, x [a, b]. (25) Proof: Since q (x) is Lipschitz continuous on [a, b], q (x) is absolutely continuous, and the derivative q (x) exists almost everywhere and is integrable on [a, b]. Hence, for almost every x [a, b] q (x)  = lim h 0 q (x + h) q (x) h  = lim h 0  q (x + h) q (x) h  K. (26) Using the integral Taylor expansion, we deduce that q(x) = q(x0) + q (x0)(x x0) + (x x0)2  1 0 (1 t)q (x0 + t(x x0)) dt q(x0) + q (x0)(x x0) + (x x0)2  1 0 K(1 t) dt = q(x0) + q (x0)(x x0) + C(x x0)2 (27) where C = (1/2)K, x0, x [a, b]. Lemma 4.2: Suppose that the learning rate m satis es (A2) and that the sequence {am} (m N) satis es am 0,  m=0 ma m < and |am+1 am| m for some positive constants and . Then we have lim m am = 0. (28) Proof: According to (A2), we know that m 0 as m . We claim that limk infm>k am = 0. Otherwise, if a limk infm>k am (0, ], then by the de nition of inferior limit, there exists an integer M > k such that am (a /2) > 0 for m M, which leads to  m=0 ma m a 2   m M m = . (29) This contradicts  m=0 ma m < and con rms the claim. Next, we claim that limk supm>k am = 0. Otherwise, there exists (0, ] such that limk supm>k am = . Then, for any 0 < < , we can choose two subsequences {aik} and {a jk} of {am} to satisfy (1)aik (0, /4), a jk ( , ); (2) ik + 1 < jk < ik+1; (3) aik+1 [ /4, /2]. (This can be done because limk infm>k am = 0, limk supm>k am = , and |am am+1| m 0 as m .) For any ik < m < jk, we have am [ /4, ]. Thus, we conclude that 2 a jk aik+1  |a jk a jk 1| + + |aik+2 aik+1| jk 1  m=ik+1 m jk  m=ik+1 m. Therefore, we have for all large enough integers k that  m=ik ma m jk  m=ik+1 ma m  4  jk  m=ik+1 m 2  4  +1 . But this contradicts  m=0 ma m < and implies our second claim. Finally, the above two claims together clearly lead to the desired estimate (28). Lemma 4.3: Let {bm} be a bounded sequence satisfying limm (bm+1 bm) = 0. Write 1 = limn infm>n bm, 2 = limn supm>n bm, and S = {a R : There exists a subsequence {bik} of {bm} such that bik a as k }. Then we have S = [ 1, 2]. (30) Proof: It is obvious that 1 2 and S [ 1, 2]. If 1 = 2, then (30) follows simply from limm bm = 1 = 2. Let us consider the case 1 < 2 and proceed to prove that S [ 1, 2]. For any a ( 1, 2), there exists > 0 such that (a , a + ) ( 1, 2). Noting limm (bm+1 bm) = 0, we observe that bm travels to and from between 1 and 2 with very small pace for all large enough m. Hence, there must be in nite number of points of the sequence {bm} falling into (a , a + ). This implies a S and thus ( 1, 2) S. Furthermore, ( 1, 2) S immediately leads to [ 1, 2] S. This completes the proof. Let the sequence {wmJ+ j} (m N, j = 0, 1, . . . , J 1) be generated by (9) and (10). We introduce the following notations: Rm, j = m  jumJ+ j jumJ , (31) rm, j i = m  jvmJ+ j i jvmJ i  , (32) dm, l = umJ+l umJ = m l 1  j=0 jumJ+ j + m l 1  j=1  umJ+ j umJ+ j 1 = m l 1  j=0 jumJ + l 1  j=0 Rm, j 1302 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 22, NO. 8, AUGUST 2011 + m l 1  j=1  umJ+ j umJ+ j 1 , (33) hm, l i = vmJ+l i vmJ i = m l 1  j=0 jvmJ+ j i + m l 1  j=1  vmJ+ j i vmJ+ j 1 i  = m l 1  j=0 jvmJ i + l 1  j=0 rm, j i + m l 1  j=1  vmJ+ j i vmJ+ j 1 i  , (34) m, l, j = GmJ+l, j GmJ, j, (35) m N; j = 0, 1, . . . , J 1; l = 1, 2, . . . , J; i = 1, 2, . . . , n. Let constants C1, C2, and C3 be de ned by [see (A3) and (A4)] max 0 j J 1  x j , |O j|  = C1, sup m N wm = C2, sup m N m = C3. (36) By (A1), f j(t) also satis es Lipschitz condition for j = 0, 1, . . . , J 1. Furthermore, g(t), f (t), and f j(t) are all uniformly continuous on any bounded closed interval. Lemma 4.4: Let (A1), (A3), and (A4) be valid, and let the sequence wmJ+ j be generated by (9) and (10). Then, there are constants C4 C8 such that GmJ+ j, k C4, (37) dm, l C5 m, m, l, j C6 m, (38) Rm, j C7 2 m, rm, j i C8 2 m (39) where m N; j, k = 0, 1, . . . , J 1; l = 1, 2, . . . , J; i = 1, 2, . . . , n. Proof: According to (36), we have vmJ+ j i xk vmJ+ j i  xk C1C2 D1. (40) Thus, there exists a positive constant C4,1 such that max |t| D1 |g(t)| = C4,1, (41) GmJ+ j, k = G  VmJ+ jxk nC4,1 C4. (42) It follows from (36) and (42) that umJ+ j GmJ+ j, k umJ+ j GmJ+ j, k C2C4 D2. (43) Then, there is a positive constant C5,1 such that max |t| D2  f j(t)  C5,1. (44) Furthermore, a combination of (A1), (9), (11), (37), (43), and (44) gives dm, l = umJ+l umJ =  l 1  j=0  umJ+ j+1 umJ+ j  l 1  j=0   umJ+ j+1 umJ+ j l 1  j=0  m j  k=0 j k m f k  umJ+k GmJ+k,k GmJ+k,k  C5 m (45) where C5 = JC4C5,1 J 1 k=0 C J k 1 3 . Employing (40), we nd that there is a positive constant C6,1 such that max |t| D1 g (t)  = C6,1. (46) Moreover, we observe that  m, l, j = GmJ+l, j GmJ, j max 1 i n g (ti)  x j n  i=1 hm, l i  max 1 i n g (ti)  x j n  i=1 l 1  k=0 vmJ+k+1 i vmJ+k i  C6 m (47) where C6 = nJC2 1C2C5,1C2 6,1 J 1 k=0 C J k 1 3 , ti = vmJ i x j + i(vmJ+l i vmJ i ) x j, i (0, 1), and |ti| C1C2, (i = 1, 2, . . . , n). Combining f j(t) s Lipschitz continuity, (36) and (37), we have  f j(umJ+ j GmJ+ j, j) f j(umJ GmJ+ j, j)  L umJ+ j GmJ+ j, j umJ GmJ+ j, j L dm, j GmJ+ j, j LC4 dm, j , (48)  f j(umJ GmJ+ j, j) f j(umJ GmJ, j)  L umJ GmJ+ j, j umJ GmJ, j L umJ  m, j, j LC2  m, j, j (49) where L > 0 is the Lipschitz constant. By the de nition of Rm, j, we see that Rm, j = m  jumJ+ j jumJ = m  f j(umJ+ j GmJ+ j, j)GmJ+ j, j f j(umJ GmJ, j)GmJ, j = m  f j(umJ+ j GmJ+ j, j) m, j, j +  f j(umJ+ j GmJ+ j, j) f j(umJ GmJ+ j, j)  GmJ, j +  f j(umJ GmJ+ j, j) f j(umJ GmJ, j)  GmJ, j . (50) WANG et al.: CONVERGENCE OF CYCLIC AND ALMOST-CYCLIC LEARNING WITH MOMENTUM FOR FEEDFORWARD NNs 1303 Therefore, it follows from (37), (38), (48), and (49) that Rm, j m  LC2 4 dm, j + (C5,1 + LC2C4)  m, j, j  LC2 4C5 +  C5,1 + LC2C4  C6  2 m = C7 2 m (51) where C7 = LC2 4C5 + (C5,1 + LC2C4)C6. Similarly, we can show the existence of a constant C8 > 0 such that rm, j i  C8 2 m. (52) The next lemma reveals an almost monotonicity of the error function during the training process. Lemma 4.5: Let the sequence {wmJ+ j} be generated by (9) and (10). Under (A1), (A3), and (A4), there holds E  w(m+1)J E  wmJ m Ew  wmJ2 + C9  2 m + 2 m  (53) where C9 > 0 is a constant independent of m, m, and m. Proof: By (A1) and Lemma 4.1, we know that g (vmJ i x j +t(hmJ i x j)) is integrable almost everywhere on [0, 1] and f j  umJ GmJ, j umJ m, J, j = f j  umJ GmJ, j n  i=1 umJ i g (vmJ i x j)hm, J i x j + f j  umJ GmJ, j n  i=1 umJ i  hm, J i x j2  1 0 (1 t)g  vmJ i x j + t  hm, J i x j dt. (54) By virtue of Lemma 4.1, (11), (12), and (54), there is a constant C9 > 0 such that f j  u(m+1)J G(m+1)J, j f j  umJ GmJ, j + f j  umJ GmJ, j u(m+1)J G(m+1)J, j umJ GmJ, j + C10  u(m+1)J G(m+1)J, j umJ GmJ, j2 = f j(umJ GmJ, j) + f j(umJ GmJ, j)  dm, J GmJ, j + umJ m, J, j + dm, J m, J, j + C10  u(m+1)J G(m+1)J, j umJ GmJ, j2 = f j(umJ GmJ, j) jumJ dm, J n  i=1  jvmJ i hm, J i  + f j  umJ GmJ, j n  i=1 umJ i  hm, J i x j2  1 0 (1 t)g  vmJ i x j + t  hm, J i x j dt + f j(umJ GmJ, j)dm, J m, J, j + C10  u(m+1)J G(m+1)J, j umJ GmJ, j2 . (55) Summing (55) from j = 0 to j = J 1 and noting (4) (6), (33) and (34), we have E  w(m+1)J E  wmJ m  J 1  j=0 jumJ  2 + n  i=1  J 1  j=0 jvmJ i  2 + m = E  wmJ m Eu  wmJ 2 + n  i=1 Evi  wmJ 2 + m = E  wmJ m Ew  wmJ 2 + m (56) where m = J 1  j=0 jumJ J 1  j=0 Rm, j m J 1  j=0 jumJ J 1  j=1  umJ+ j umJ+ j 1 n  i=1 J 1  j=0 jvmJ i J 1  j=0 rm, j i m n  i=1 J 1  j=0 jvmJ i J 1  j=1  vmJ+ j i vmJ+ j 1 i  + J 1  j=0 n  i=1 umJ i f j  umJ GmJ, j  hm, J i x j2  1 0 (1 t)g  vmJ i x j + t  hm, J i x j dt + J 1  j=0 f j(umJ GmJ, j)dm, J m, J, j + C10 J 1  j=0  u(m+1)J G(m+1)J, j umJ GmJ, j2 . It now follows from (36) and (37) that GmJ, j = G  VmJx j C4, (57) umJ GmJ, j umJ GmJ, , j C2C4 = D2. (58) By (5), (42), (44), and (51), the rst term of m can be estimated as follows:  J 1  j=0 jumJ J 1  j=0 Rm, j  Eu  wmJ J 1  j=0 Rm, j C9,1 2 m (59) where C9,1 = J 2C4C5,1C7. 1304 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 22, NO. 8, AUGUST 2011 Similarly, the second term of m can be obtained as follows:  m J 1  j=0 jumJ J 1  j=1  umJ+ j umJ+ j 1  m Eu  wmJ J 1  j=1 umJ+ j umJ+ j 1 C9,2  2 m + 2 m  (60) where C9,2 = J 2C2 4C2 5,1 J k=0 C J k 3 . The estimates for the other terms of m can be obtained with corresponding constants C9,t > 0 for t = 3, . . . , 7. Finally, the desired estimate (53) is proved by setting C9 = 7 t=1 C9,t. Now, we are ready to prove the convergence theorem. Proof to Theorem 3.1: The proof is divided into two parts, dealing with (22) and (23) respectively. Proof to (22): By (A2), (A3), and Lemma 4.5, we conclude that  m=0 m Ew  wmJ 2 < , (61) lim m m = 0, lim m m = 0. (62) By (A1) and (36), it is easy to see that Eu(w) and Evi (w) (i = 1, 2, . . . , n) all satisfy Lipschitz condition. Hence, Ew(w) also satis es Lipschitz condition with a Lipschitz constant L > 0. By (38), we have  Ew  w(m+1)J Ew  wmJ   Ew  w(m+1)J Ew  wmJ L dm, J + n  i=1 hm, J i  L C5 + nJC1C2C5,1C6,1 J  k=0 C J k 3 m = C11 m (63) where C11 = L (C5 + nJC1C2C5,1C6,1 J k=0 C J k 3 ). Employing (61), (63), and Lemma 4.2, we obtain that lim m Ew  wmJ = 0. (64) Similar to (63), there exists a constant C12 > 0 such that Ew  wmJ+ j Ew  wmJ C12 m, j = 0, 1, . . . , J 1. (65) It is easy to see that Ew  wmJ+ j Ew  wmJ+ j Ew  wmJ + Ew  wmJ C12 m + Ew  wmJ . (66) By (62), (64), and (66), we have lim m Ew  wmJ+ j = 0, j = 1, 2, . . . , J 1. (67) This immediately gives lim m Ew(wm)  = 0. (68) Proof to (23): According to (A4), the sequence {wm} (m N) has a subsequence {wmk} (k N) that is convergent to, say, w 0. It follows from (22) and the continuity of Ew (w) that Ew  w  = lim k Ew  wmk = lim m Ew wm = 0. (69) This implies that w is a stationary point of E(w). Hence, {wm} has at least one accumulation point and every accumu- lation point must be a stationary point. Next, by reduction to absurdity, we prove that {wm} has precisely one accumulation point. Let us assume the contrary that {wm} has at least two accumulation points w = w. We write wm = (wm 1 , wm 2 , . . . , wm n(p+1))T . It is easy to see from (9) (12) that limm wm+1 wm = 0 or equivalently, limm |wm+1 i wm i | = 0 for i = 1, 2, . . . , n(p+1). Without loss of generality, we assume that the rst components of w and w do not equal to each other, that is, w1 = w1. For any real number (0, 1), let w 1 = w1 +(1 ) w1. By Lemma 4.3, there exists a subsequence {w mk1 1 } of {wm 1 } converging to w 1 as k1 . Due to the boundedness of {w mk1 2 }, there is a convergent subsequence {w mk2 2 } {w mk1 2 }. We de ne w 2 = limk2 w mk2 2 . Repeating this procedure, we end up with decreasing subsequences {mk1} {mk2} {mkn(p+1)} with w i = limki w mki i for each i = 1, 2, . . . , n(p + 1). Write w = (w 1, w 2, . . . , w n(p+1))T . Then, we see that w is an accumulation point of {wm} for any (0, 1). But this means that 0,1 has interior points, which contradicts (A5). Thus, w must be a unique accumulation point of {wm} m=0. This completes the proof of the strong convergence. B. Convergence Analysis for Almost-Cyclic Learning Now, let the sequence {wmJ+ j} (m N, j = 0, 1, . . . , J 1) be generated by (15) and (16). We introduce the following notations: Rm, j = m  m j umJ+ j m j umJ , (70) rm, j i = m  m j vmJ+ j i m j vmJ i  , (71) dm,l = umJ+l umJ = m l 1  j=0 m j umJ+ j + m l 1  j=1  umJ+ j umJ+ j 1 = m l 1  j=0 m j umJ + l 1  j=0 Rm, j + m l 1  j=1  umJ+ j umJ+ j 1 , (72) WANG et al.: CONVERGENCE OF CYCLIC AND ALMOST-CYCLIC LEARNING WITH MOMENTUM FOR FEEDFORWARD NNs 1305 hm,l i = vmJ+l i vmJ i = m l 1  j=0 m j vmJ+ j i + m l 1  j=1  vmJ+ j i vmJ+ j 1 i  = m l 1  j=0 m j vmJ i + l 1  j=0 rm, j i + m l 1  j=1  vmJ+ j i vmJ+ j 1 i  , (73) m,l, j = GmJ+l,m, j GmJ,m, j, (74) m N; j = 0, 1, . . . , J 1; l = 1, 2, . . . , J; i = 1, 2, . . . , n. It is obvious that Lemmas 4.1 4.3 are not in uenced by the new de nitions. In place of Lemmas 4.4 and 4.5, we now have the following two lemmas. Lemma 4.6: Let (A1), (A3), and (A4) be valid, and let the sequence  wmJ+ j be generated by (15) and (16). Then, there hold the following estimates with the same constants C4 C8 as in Lemma 4.4: GmJ+ j, m, k C4, (75) dm, l C5 m, m, l, j C6 m, (76) Rm, j C7 2 m, rm, j i C8 2 m (77) where m N; j, k = 1, 2, . . . , J; l = 1, 2, . . . , J; i = 1, 2, . . . , n. Proof: According to (36), we have vmJ+ j i xm, k vmJ+ j i  max 1 k J xk C1C2 D1. (78) Thus, there exists a positive constant C4,1 such that max |t| D1 |g(t)| = C4,1, (79) GmJ+ j, m, k = G  VmJ+ jxm, k nC4,1 C4. (80) Similarly, (76) and (77) can be proved after adjusting the corresponding superscripts in the proof to Lemma 4.4. Lemma 4.7: Let the sequence  wmJ+ j be generated by (15) and (16). Under (A1), (A3), and (A4), there holds E  w(m+1)J E  wmJ m Ew  wmJ 2 +C9  2 m + 2 m  (81) where C9 > 0 is the same constant de ned in Lemma 4.5. Proof: As in the proof to Lemma 4.6, the results are valid as long as the corresponding superscripts be adjusted. The details are left to the interested readers. Proof of Theorem 3.1 for Almost-Cyclic Learning: The weak and strong convergence results for almost-cyclic learning with momentum can be similarly obtained in terms of Lemmas 4.1 4.3 and Lemmas 4.6 4.7 V. CONCLUSION In this paper, the cyclic and almost-cyclic learning algo- rithms with momentum for three-layer BP neural networks were considered, and a comprehensive study on their weak and strong convergence was carried out. Compared with the existing convergence results, the assumptions to guarantee the convergence are much relaxed, and are valid for more extensive classes of feedforward NNs. ACKNOWLEDGMENT The authors would like to thank the reviewers and J. M. Zurada for valuable advice and assistance in the preparation of this paper. REFERENCES [1] S. Haykin, Neural Networks: A Comprehensive Foundation, 2nd ed. Englewood Cliffs, NJ: Prentice-Hall, 2001. [2] D. E. Rumelhart and J. L. McClelland, Parall Distributed Processing- Explorations in the Microstructure of Cognition. Cambridge, MA: MIT Press, 1986. [3] E. A. de Oliveira and R. C. Alamino, Performance of the Bayesian online algorithm for the perceptron, IEEE Trans. Neural Netw., vol. 18, no. 3, pp. 902 905, May 2007. [4] T. Heskes and W. Wiegerinck, A theoretical comparison of batch-mode, on-line, cyclic, and almost-cyclic learning, IEEE Trans. Neural Netw., vol. 7, no. 4, pp. 919 925, Jul. 1996. [5] D. R. Wilson and T. R. Martinez, The general inef ciency of batch training for gradient descent learning, Neural Netw., vol. 16, no. 10, pp. 1429 1451, Dec. 2003. [6] D. S. Terence, Optimal unsupervised learning in a single-layer linear feedforward neural network, Neural Netw., vol. 2, no. 6, pp. 459 473, 1989. [7] W. Finnoff, Diffusion approximations for the constant learning rate backpropagation algorithm and resistance to local minima, Neural Comput., vol. 6, no. 2, pp. 285 295, Mar. 1994. [8] T. Nakama, Theoretical analysis of batch and on-line training for gradient descent learning in neural networks, Neurocomputing, vol. 73, nos. 1 3, pp. 151 159, Dec. 2009. [9] Z. X. Li, W. Wu, and W. Q. Chen, Prediction of stock market by BP neural networks with technical indexes as input, J. Math. Res. Explos., vol. 23, no. 1, pp. 83 97, 2003. [10] Z. X. Li, W. Wu, and Y. L. Tian, Convergence of an online gradient method for feedforward neural networks with stochastic inputs, J. Comput. Appl. Math., vol. 163, no. 1, pp. 165 176, Feb. 2004. [11] Z.-B. Xu, R. Zhang, and W.-F. Jin, When does online BP training converge? IEEE Trans. Neural Netw., vol. 20, no. 10, pp. 1529 1539, Oct. 2009. [12] W. Wu, G. R. Feng, Z. X. Li, and Y. S. Xu, Deterministic convergence of an online gradient method for BP neural networks, IEEE Trans. Neural Netw., vol. 16, no. 3, pp. 533 540, May 2005. [13] W. Wu and Y. S. Xu, Deterministic convergence of an online gradient method for neural networks, J. Comput. Appl. Math., vol. 144, nos. 1 2, pp. 335 347, Jul. 2002. [14] J. Hertz, A. Krogh, and R. G. Palmer, Introduction to the Theory of Neural Computation. Reading, MA: Addison-Wesley, 1991. [15] S. Becker and Y. Le Cun, Improving the convergence of back- propagation learning with second-order methods, in Proc. Connect. Models Summer School, San Mateo, CA, 1989, pp. 29 37. [16] M. T. Hagan, H. B. Demuth, and M. H. Beale, Neural Network Design. Boston, MA: PWS, 1996. [17] L. J. Ting-Ho, Convexi cation for data tting, J. Global Optim., vol. 46, no. 2, pp. 307 315, Feb. 2010. [18] V. V. Phansalkar and P. S. Sastry, Analysis of the back-propagation algorithm with momentum, IEEE Trans. Neural Netw., vol. 5, no. 3, pp. 505 506, May 1994. [19] N. O. Attoh-Okine, Analysis of learning rate and momentum term in backpropagation neural network algorithm trained to predict pavement performance, Adv. Eng. Softw., vol. 30, no. 4, pp. 291 302, Apr. 1999. [20] A. Sato, Analytical study of the momentum term in a backpropagation algorithm, in Proc. Int. Conf. Artif. Neural Netw., 1991, pp. 617 622. 1306 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 22, NO. 8, AUGUST 2011 [21] N. Qian, On the momentum term in gradient descent learning algo- rithms, Neural Netw., vol. 12, no. 1, pp. 145 151, Jan. 1999. [22] X.-H. Yu, G.-A. Chen, and S.-X. Cheng, Dynamic learning rate optimization of the backpropagation algorithm, IEEE Trans. Neural Netw., vol. 6, no. 3, pp. 669 677, May 1995. [23] X.-H. Yu and G.-A. Chen, Ef cient backpropagation learning using optimal learning rate and momentum, Neural Netw., vol. 10, no. 3, pp. 517 527, Apr. 1997. [24] S. V. Kamarthi and S. Pittner, Accelerating neural network training using weight extrapolations, Neural Netw., vol. 12, no. 9, pp. 1285 1299, Nov. 1999. [25] M. Torii and M. T. Hagan, Stability of steepest descent with momentum for quadratic functions, IEEE Trans. Neural Netw., vol. 13, no. 3, pp. 752 756, May 2002. [26] A. Bhaya and E. Kaszkurewicz, Steepest descent with momentum for quadratic functions is a version of the conjugate gradient method, Neural Netw., vol. 17, no. 1, pp. 65 71, Jan. 2004. [27] Y. C. Liang, D. P. Feng, H. P. Lee, S. P. Lim, and K. H. Lee, Successive approximation training algorithm for feedforward neural networks, Neurocomputing, vol. 42, nos. 1 4, pp. 311 322, Jan. 2002. [28] D. Chakraborty and N. R. Pal, A novel training scheme for multi- layered perceptrons to realize proper generalization and incremental learning, IEEE Trans. Neural Netw., vol. 14, no. 1, pp. 1 14, Jan. 2003. [29] T. L. Fine and S. Mukherjee, Parameter convergence and learning curves for neural networks, Neural Comput., vol. 11, no. 3, pp. 747 769, Apr. 1999. [30] D. P. Bertsekas and J. N. Tsitsiklis, Neuro-Dynamic Programming. Belmont, MA: Athena Scienti c, 1996. [31] V. Tadic and S. Stankovic, Learning in neural networks by normalized stochastic gradient algorithm: Local convergence, in Proc. 5th Seminar Neural Netw. Appl. Electron. Eng., Belgrade, Yugoslavia, Sep. 2000, pp. 11 17. [32] W. Wu, H. M. Shao, and D. Qu, Strong convergence of gradient methods for BP networks training, in Proc. Int. Conf. Neural Netw. Brains, Beijing, China, Oct. 2005, pp. 332 334. [33] W. Wu, G. R. Feng, and X. Li, Training multilayer perceptrons via minimization of sum of ridge functions, Adv. Computat. Math., vol. 17, no. 4, pp. 331 347, Nov. 2002. [34] W. Wu, N. M. Zhang, and Z. X. Li, Convergence of gradient method with momentum for back-propagation neural networks, J. Comput. Math., vol. 26, pp. 613 623, Jul. 2008. [35] N. M. Zhang, W. Wu, and G. F. Zheng, Convergence of gradi- ent method with momentum for two-layer feedforward neural net- works, IEEE Trans. Neural Netw., vol. 17, no. 2, pp. 522 525, Mar. 2006. [36] N. M. Zhang, Deterministic convergence of an online gradient method with momentum, in Intelligent Computing, D.-S. Huang, K. Li, and G. Irwin, Eds. Berlin, Germany: Springer-Verlag, 2006, pp. 94 105. [37] N. M. Zhang, An online gradient method with momentum for two-layer feedforward neural networks, Appl. Math. Comput., vol. 212, no. 2, pp. 488 498, Jun. 2009. [38] M. J. D. Powell, Restart procedures for the conjugate gradient method, Math. Program., vol. 12, no. 1, pp. 241 254, Dec. 1977. [39] M. Forti, P. Nistri, and M. Quincampoix, Generalized neural network for nonsmooth nonlinear programming problems, IEEE Trans. Circuits Syst. I, vol. 51, no. 9, pp. 1741 1754, Sep. 2004. Jian Wang received the B.S. degree in computa- tional mathematics from the China University of Petroleum, Dongying, China, in 2002. Since 2006, he has been working toward the Ph.D. degree in computational mathematics at the Dalian University of Technology, Dalian, China. He was an Instructor with the School of Mathe- matics and Computational Science, China University of Petroleum, from 2002 to 2006. Currently, he is sponsored by the China Scholarship Council as a Visiting Scholar in the Department of Electrical and Computer Engineering, University of Louisville, Louisville, KY. His current research interests include numerical optimization and neural networks. Jie Yang received the B.S. degree in computa- tional mathematics from Shanxi University, Taiyuan, China, in 2001, and the Ph.D. degree from the De- partment of Applied Mathematics, Dalian University of Technology, Dalian, China, in 2006. She is currently a Lecturer at the School of Math- ematical Sciences, Dalian University of Technology. Her current research interests include fuzzy sets and systems, fuzzy neural networks, and spiking neural networks. Wei Wu received the Bachelor s and Master s de- grees from Jilin University, Changchun, China, in 1974 and 1981, respectively, and the Ph.D. degree from Oxford University, Oxford, U.K., in 1987. He is currently with the School of Mathematical Sciences, Dalian University of Technology, Dalian, China. He has published four books and 90 research papers. His current research interests include learn- ing methods of neural networks.