Ef cient Sequential Consistency via Con ict Ordering Changhui Lin University of California Riverside linc@cs.ucr.edu Vijay Nagarajan University of Edinburgh vijay.nagarajan@ed.ac.uk Rajiv Gupta University of California Riverside gupta@cs.ucr.edu Bharghava Rajaram University of Edinburgh r.bharghava@ed.ac.uk Abstract Although the sequential consistency (SC) model is the most intu- itive, processor designers often choose to support relaxed memory consistency models for higher performance. This is because SC im- plementations that match the performance of relaxed memory mod- els require post-retirement speculation and its associated hardware costs. In this paper we propose an ef cient approach for enforcing SC without requiring post-retirement speculation. While prior SC implementations guarantee SC by explicitly completing memory operations within a processor in program order, we guarantee SC by completing con icting memory operations, within and across processors, in an order that is consistent with the program order. More speci cally, we identify those con icting memory operations whose ordering is critical for the maintenance of SC and explicitly order them. This allows us to safely (non-speculatively) complete memory operations past pending writes, thus reducing memory or- dering stalls. Our experiments with SPLASH-2 programs show that SC can be achieved ef ciently, with performance comparable to RMO (relaxed memory order). Categories and Subject Descriptors C.1.2 [Processor Architec- tures]: Multiple Data Stream Architectures (Multiprocessors) Parallel processors General Terms Design, Performance, Experimentation Keywords Sequential consistency, Con ict ordering 1. Introduction While parallel architectures are becoming ubiquitous, extracting performance from them is contingent on programmers writing par- allel software. To this end, there has been signi cant research on developing programming models, memory models, and debugging tools for making programmers tasks easier. In particular, one rea- son why programmers nd parallel programming hard is because of the intricacies involved in the underlying memory consistency models. The complexity of the memory models is well illustrated Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. ASPLOS 12, March 3 7, 2012, London, England, UK. Copyright c 2012 ACM 978-1-4503-0759-8/12/03. . . $10.00 in recent work by Sewell et al. [28] in which they describe how the recent Intel and AMD memory model speci cations do not match with the actual behavior observed in real machines. Among the var- ious memory consistency models, the sequential consistency (SC) model in which memory operations appear to take place in the or- der speci ed by the program is most intuitive to programmers. In- deed, most works on semantics and software checking that strive to make concurrent programming easier assume SC [28]; several debugging tools for parallel programs, e.g. RaceFuzzer [27], also assume SC. Finally, SC would simplify the memory models of lan- guages such as Java and C++ by enabling simpler implementations of Java volatile or C++ atomic. (Prior SC implementations). In spite of the advantages of the SC model, processor designers typically choose to support relaxed con- sistency models; none of the Intel, AMD, ARM, Itanium, SPARC, or PowerPC processor families choose to support SC. This is be- cause SC requires reads and writes to be ordered in program or- der, which can cause signi cant performance overhead. Indeed, SC requires the enforcement of all four possible program orderings: r r, r w, w w, w r, where r denotes a read opera- tion and w denotes a write operation. A straightforward SC imple- mentation enforces these orderings by delaying the next memory operation until the previous one completes, introducing a signi - cant number of program ordering stalls. Some of these stalls can be reduced in a dynamically scheduled ILP processor, where in- window speculation support can be used to execute memory oper- ations out-of-order while completing them in program order [13]. Nonetheless, enforcing the w r (and w w) order necessi- tates that the write-buffer is drained before a subsequent memory operation can be completed. Thus, a high latency write can still cause signi cant program ordering stalls which in-window specu- lation is unable to hide. Recent works have utilized post-retirement speculation speculation beyond instruction window to eliminate these program ordering stalls [6 8, 11, 14 16, 24, 31]. While this approach is promising, the signi cant hardware complexity associ- ated with post-retirement speculation could hinder its wide-spread adoption [3]. (Our Approach). SC requires memory operations of all proces- sors to appear to perform in some global memory order, such that, memory operations of each processor appear in this global memory order in the order speci ed by the program [18]. Prior SC imple- mentations ensure this by enforcing program ordering by explic- itly completing memory operations in program order. More specif- ically, if m1 and m2 are two memory operations with m1 pre- ceding m2 in the program order, prior SC implementations ensure m1 m2 by allowing m2 to complete only after m1 completes; in effect, m2 is made to wait until m1 and all of m1 s predeces- sors in the global memory order have completed. In other words, if pred(m1) refers to m1 and its predecessors in the global memory order, m2 is allowed to complete only when all memory operations from pred(m1) have completed. While enforcing program ordering is a suf cient condition for ensuring SC [26], it is not a necessary condition [12, 20, 29]. In contrast with prior SC implementations, we ensure SC by explicitly ordering only con icting memory operations where con icting memory operations are pairs of memory operations that access the same address, with one of them being a write. More speci cally, we observe that m1 appears to be ordered before m2, as long as those memory operations from pred(m1) which con ict with m2, complete before m2. In other words, if predp(m1) refers to memory operations from pred(m1) which are pending completion, m2 can safely complete before m1 as long as m2 does not con ict with any of the memory operations from predp(m1). It is worth noting that the above condition generally evaluates to true, since in well-written parallel programs con icting accesses are relatively rare and staggered in time [15, 20]. Our SC implementation, which allows for memory operations to be safely (and non-speculatively) completed past pending writes, is considered in the context of a scalable CMP with local caches kept coherent using an invalidation based cache coherence proto- col. When a write-miss w is encountered, we conservatively es- timate predp(w) to be the set of write misses that are currently being serviced by the coherence controller. This lets us safely com- plete subsequent memory operations which do not con ict with predp(w), even while w is pending. The main contribution of this paper is an ef cient and lightweight approach for SC using con ict ordering. More speci cally, our SC implementation is: Ef cient. Our experiments with SPLASH-2 programs show that SC can be achieved ef ciently, with performance compa- rable to RMO (relaxed memory order). Lightweight. Ours is the rst SC implementation that performs almost as well as RMO, while requiring no post-retirement speculation. Scalable. Our SC implementation scales well, with perfor- mance that is comparable to RMO even with 32 cores. 2. Background Memory consistency models [4] constrain memory behavior with respect to read and write operations from multiple processors, pro- viding formal speci cations of how the memory system will ap- pear to the programmer. Each of these models offers a trade-off between programming simplicity and high performance. Sequen- tial consistency (SC) is the simplest model that is easy to under- stand but can potentially affect the performance of programs. The strict memory ordering imposed by SC can potentially restrict both compiler and hardware optimizations that are possible in unipro- cessors [4]. For performance reasons, researchers have proposed relaxed memory consistency models at the cost of programming complexity [4, 5]. Such memory models provide fence instructions to enable programmers to explicitly provide memory ordering. For instance, the SPARC RMO model (relaxed memory order) relaxes all four program orderings but provides memory fence instruc- tions that the programmer can use for overriding the relaxations and enforce memory orderings on demand. The complexities of specifying relaxed memory models, however, combined with the additional burden it places on the programmers have rendered re- laxed memory models inadequate [3]. Next, we discuss different approaches for achieving SC. 2.1 SC via program ordering In program ordering based SC implementations the hardware di- rectly ensures all four possible program ordering constraints [6, 13 15, 24]; they are based on the seminal work by Scheurich and Dubois [26], in which they state the suf cient conditions for SC for general systems with caches and interconnection networks. (Naive). A naive way to enforce program ordering between a pair of memory operations is to delay the second until the rst fully completes. However, this can result in a signi cant number of memory ordering stalls; for instance, if the rst access is a miss, then the second access has to wait until the miss is serviced. (In-window speculation). Out-of-order processing capabilities of a modern processor can be leveraged to reduce some of these stalls. This is based on the observation that memory operations can be freely reordered as long as the reordering is not observed by other processors. Instead of waiting for an earlier memory operation to complete, the processor can use hardware prefetching and specu- lation to execute memory operations out of order, while still com- pleting in order [13, 33]. However, such reordering is within the instruction window where the instruction window refers to the set of instructions that are in- ight. If the processor receives an ex- ternal coherence (or replacement) request for a memory operation that has executed out of order, the processor s recovery mechanism is triggered to redo computation starting from that memory opera- tion. Nonetheless, enforcing the w r (and w w) order neces- sitates that the write-buffer is drained before a subsequent memory operation can be completed. Thus, a high latency write can still cause signi cant program ordering stalls, which in-window specu- lation is unable to hide. Our experiments with the SPLASH-2 pro- grams show that programs spend more than 20% of their execution time on average waiting for the write buffer to be drained. For our work, we assume in-window speculation support as part of baseline implementations of SC as well as fences in RMO. (Post-retirement speculation). Since in-window speculation is not suf ciently able to reduce program ordering stalls, researchers have proposed more aggressive speculative memory reordering techniques [6 8, 11, 14 16, 24, 31]. The key idea is to speculatively retire instructions neglecting program ordering constraints while maintaining the state of speculatively-retired instructions sepa- rately. One way to do this is to maintain the state of speculatively- retired instructions at a ne granularity, which enables precise re- covery from misspeculations [14, 15, 24]. This obviates the need for a load to wait until the write buffer is drained and is made to re- tire speculatively. More recently, researchers have proposed chunk based techniques [6 8, 11, 16, 31] which again use aggressive speculation to ef ciently enforce SC at the granularity of coarse- grained chunks of instructions instead of individual instructions. While the above approaches show much promise, hardware com- plexity associated with aggressive speculation, being contrary to the design philosophy of multi-cores consisting of simple energy- ef cient cores [1, 2], can hinder wide-spread adoption. Through our work, we seek to show that a lightweight SC implementation is possible without sacri cing performance. 2.2 Other SC approaches Shasha and Snir, in their seminal work [29], observed that not all pairs of memory operations need to be ordered for SC; only those pairs which con ict with others run the risk of SC violation and consequently need to be ordered. To ensure memory operation pairs are not reordered, memory fences are inserted. They then propose delay set analysis, a compiler based algorithm for minimizing the number of fences inserted for ensuring SC. This work led to other compiler based algorithms that implement various fence insertion and optimization algorithms [10, 19]. Recently, we proposed con- ditional fence [20], a novel fence mechanism that dynamically de- cides if a fence needs to stall; we show that with conditional fences, the number of fences that actually need to stall is reduced signif- icantly. The limitation of compiler-based SC implementations is that they rely on alias analysis for identifying con icts, and so are overly conservative for programs that employ pointers and dynamic data structures [3, 20]. In developing delay set analysis, Shasha and Snir introduced a precise formalism for SC in terms of the program order being con- sistent with the execution order. Their SC formalism re ects the notion that SC does not actually require memory operations to be performed in program order; it merely requires the execution to be consistent with program order. Gharachorloo et al. later built on Shasha and Snir s formalism to derive aggressive SC speci - cations [12] which relaxes program ordering constraints. While our idea of con ict ordering and its formalization is strongly in uenced by Shasha and Snir s SC formalism, we additionally leverage this idea and propose a lightweight and ef cient SC implementation, that performs almost as well as RMO. Mainstream programming languages like C++ and Java use variants of the data-race-free memory model [4, 5] which guarantee SC as long as the program is free of data races. However, if the pro- grams do have data races then these models provide much weaker semantics. To redress this there have been works that employ dy- namic race detection [9] in order to stop execution when semantics become unde ned. Since dynamic data race detection can be slow, recent works [21, 22, 30] propose raising an exception upon en- countering an SC violation as this can be done more ef ciently. Finally, recent work has shown that it is possible to support high performance compiler that preserves SC [23]. Their SC-preserving compiler, however, cannot prevent the hardware from exposing non-SC behaviour. Thus, our work which proposes an ef cient and lightweight hardware SC implementation, is complementary to the above work. 3. SC via con ict ordering In this section, we rst informally describe our approach of enforc- ing SC using con ict ordering with a motivating example. We then formally prove that our approach correctly enforces SC. 3.1 A motivating example SC requires that memory operations appear to complete in program order. To ensure this, prior SC implementations force memory oper- ations to explicitly complete in program order. However, is program ordering necessary for ensuring SC? Let us consider the example in Fig. 1, which shows memory operations a1, a2, b1 and b2 from processors A and B. They are two pairs of con icting accesses. Can a2 complete before a1 in an execution without breaking SC? Prior SC implementations forbid this and force a2 to wait until a1 com- pletes, either explicitly or using speculation. In this work, we ob- serve that a1 appears to complete before a2, as long as a2 is made to complete after b1, with which a2 con icts. In other words, a2 can complete before a1 without breaking SC, as long as we ensure that a2 waits for b1 to complete. Indeed, if b1 and b2 have completed before a1 and a2, the execution order (b1, b2, a2, a1) is equivalent to the original SC order (b1, b2, a1, a2) as the values read in the two executions are identical. We propose con ict ordering, a novel approach to SC, in which SC is enforced by explicitly ordering only con icting memory op- erations. Con ict ordering allows a memory operation to complete, as long as all con icting memory operations prior to it in the global memory order have completed. More speci cally, let m1 and m2 Figure 1. A motivating example: As long as a2 is made to wait until the con icting access b1 completes, a2 can be safely reordered before a1, without breaking SC be consecutive memory operations; furthermore let pred(m1) refer to memory operations that include m1 and those before m1 in the global memory order. Con ict ordering allows m2 to safely com- plete as long as those memory operations from pred(m1), which con ict with m2, have completed. 3.2 Con ict ordering enforces SC In this section, we prove that con ict ordering enforces SC using the formalism of Shasha and Snir [29]. For the following discus- sion, we assume an invalidation based cache coherence protocol for processors with private caches. A read operation is said to com- plete when the returned value is bound and can not be updated by other writes; a write operation is said to complete when the write invalidates all cached copies, and the generated invalidates are ac- knowledged [13]. Furthermore, we assume that the coherence pro- tocol serializes writes to the same location and also ensures that the value of a write not be returned by a read until the write completes in other words, we assume that the coherence protocol ensures write atomicity [4]. De nition 1. The program order P is a local (per-processor) total order which speci es the order in which the memory operations appear in the program. That is, m1Pm2 iff the memory operation m1 occurs before m2 in the program. De nition 2. The con ict relation C is a symmetric relation on M (all memory operations) that relates two memory operations (of which one is a write) which access the same address. De nition 3. An execution E (or execution order or con ict order) is an orientation of C. If m1Cm2, then either m1Em2 or m2Em1 holds. De nition 4. An execution E is said to be atomic, iff E is a proper orientation of C, that is, iff E is acyclic. De nition 5. An execution E is said to be sequentially consistent, iff E is consistent with the program order P, that is, iff P E has no cycles. De nition 6. The global memory order G is the transitive closure of the program order and the execution order, G = (P E)+. Remark. In the scenario shown in Fig. 1, b1 appears before a1 in the global memory order, since b1 appears before b2 in program order, and b2 is ordered before a1 in the execution order. That is, b1Ga1 since b1Pb2 and b2Ea1. De nition 7. The function pred(m) returns a set of memory oper- ations that appear before m in the global memory order G, includ- ing m. That is, pred(m) = {m} {m : m Gm}. De nition 8. An SC implementation constrains the execution by enforcing certain conditions under which a memory operation can be completed. An SC implementation is said to be correct if it is guaranteed to generate an execution that is sequentially consistent. De nition 9. Con ict ordering is our proposed SC implementation in which a memory operation m2 (whose immediate predecessor in program order is m1) is allowed to complete iff those memory operations from pred(m1) which con ict with m2 have completed. That is, m2 is allowed to complete iff {m pred(m1) : mCm2} have already completed. Figure 2. Con ict ordering ensures SC Lemma 1. Any execution E (De nition 3) is atomic. Proof. Let w and r with a subscript be a write operation and a read operation. Write-serialization ensures that w1Ew2 iff w1 completes before w2; furthermore, w1Ew2 and w2Ew3 result in w1Ew3. Write-atomicity also ensures that w1Er2 iff w1 completes before r2. Since reads are atomic, r1Ew2 iff r1 completes before w2. Furthermore, w1Er2 and r2Ew3 result in w1Ew3. Thus, E is acyclic; therefore, from De nition 4, E is atomic. Theorem 1. Con ict ordering is correct. Proof. We need to prove that any execution E generated by con ict ordering is sequentially consistent. That is, to prove that the graph P E is acyclic (from De nition 5). Let us attempt a proof by contradiction, and assume that there is a cycle in P E. Since E is acyclic (from Lemma 1), the assumed cycle should contain at least one program order edge. Without loss of generality, let us assume that the program order edge that contains a cycle is m1Pm2 as shown in Fig. 2. To complete the cycle, there must be a con ict order edge m2Em, where m pred(m1). Con ict ordering, however, ensures that those memory operations from pred(m1), which con ict with m2 would have completed before m2 (from De nition 9). Since m pred(m1), m would have completed before m2. Thus, the con ict order edge m2Em is not possible, which results in a contradiction. Thus, con ict ordering is correct. 4. Hardware Design In this section we describe our hardware design that incorporates con ict ordering to enforce SC ef ciently. We rst describe our basic design in which all memory operations will have to check for con icts before they can complete. We then describe how we can determine phases in program execution where memory operations can complete without checking for con icts. (System Model). For the following discussion we assume a tiled chip multiprocessor, with each tile consisting of a processor, a lo- cal L1 cache and a single bank of the shared L2 cache. We assume that the local L1 caches are kept coherent using a directory based cache coherence protocol, with the directory distributed over the L2 cache. We assume that addresses are distributed across the di- rectory at a page granularity using the rst touch policy. We assume a directory protocol in which the requester noti es the directory on transaction completion, so each coherence transaction can have a maximum of four steps. Thus, each coherence transaction remains active in the directory until the time at which it receives the no- ti cation of transaction completion. Furthermore, we assume that the cache coherence protocol provides write atomicity. We assume each processor core to be a dynamically scheduled ILP processor with a reorder buffer (ROB) which supports in-window specula- tion. All instructions, except memory writes, are made to complete in program order, as and when they retire from the ROB. Writes on the other hand, are made to retire into the write-buffer, so that the processor need not wait for them to complete. Finally, it is worth noting that our proposal is also applicable to bus based coherence protocols; indeed, we report experimental results for both bus based and directory based protocols. 4.1 Basic con ict ordering We describe our con ict ordering implementation which allows memory operations (both reads and writes) to complete past pend- ing writes, while ensuring SC; it is worth noting, however, that our system model does not allow memory operations to complete past pending reads. Recall that con ict ordering allows a memory operation m2, whose immediate predecessor is m1, to complete as long as m2 does not con ict with those memory operations from pred(m1) which are pending completion let us call such pending operations as predp(m1). Thus, for m2 to safely com- plete, we need to be sure that m2 does not con ict with any of the memory operations from predp(m1). Alternatively, if addrp(m1) refers to the set of addresses accessed by memory operations from predp(m1), we can safely allow m2 to complete if its address is not contained in addrp(m1). The challenge is to determine addrp(m1) as quickly as possibly and in particular without waiting for m1 to complete. Next, we show how we compute addrp(m1) for a write-miss, cache-hit, and a read-miss respec- tively. (Write-misses). Our key idea for determining addrp(m1) for a write-miss m1 is to simply get this information from the directory. We show that those memory operations from predp(m1), if any, would be present in the directory (as pending memory operations that are currently being serviced in the directory), provided write- misses such as m1 are issued to the directory, before subsequent memory operations are allowed to complete. In other words, if addr-list refers to the set of addresses of the cache misses being serviced in the directory, addrp(m1) would surely be contained in addr-list. In addition to this, since our system model does not allow memory operations to complete past pending reads, we are able to show that addrp(m1) would surely be contained in write- list where write-list refers to the set of memory addresses of the write-misses being serviced in the directory. Consequently, when the write-miss m1 is issued to the directory, the directory replies back with the write-list, containing the addresses of the write-misses which are currently being serviced by the directory (to minimize network traf c we safely approximate the write-list by using a bloom lter). A subsequent memory operation m2 is allowed to complete, only if m2 does not con ict with any of the writes from the write-list. If m2 does con ict, then the local cache block m2 maps to is invalidated, and the memory operation m2 and its following instructions are replayed when necessary. During replay, m2 would turn out to be a cache miss and hence the miss request would be sent to the directory. This would ensure that m2 would be ordered after its con icting write that was pending in the directory. It is worth noting that memory operations that follow a pending write, will now need to wait only until the write is issued to the directory and get a reply back from the directory, as opposed to waiting for the write-miss to complete. While the former takes as much time as the round trip latency to the directory, the latter can take a signi cantly longer time since it may involve the time taken to invalidate all shared copies, and also access memory if it is a miss. Figure 3. predp(b1) = {b1} predp(b0) predp(a1). (Cache-hits). Fig. 3 shows how predp(b1) relates to predp(b0), where b0 is the immediate predecessor of b1 in the program order, and a1 is the immediate predecessor of b1 in the con ict order. As we can see, predp(b1) = {b1} predp(b0) predp(a1); this is because the global memory order is the union of the program order and the con ict order. If, however, b1 is a cache hit, then its immedi- ate predecessor in the con ict order must have completed and hence cannot be pending; if it were pending, b1 would become a cache- miss. Thus predp(b1) = {b1} predp(b0). Since memory opera- tions that follow a cache-hit are allowed to complete only after the cache hit completes, predp(b1) remains the same as predp(b0). Consequently, addrp(b1) remains the same as addrp(b0) and thus, the write-list for a cache-hit need not be computed afresh. Figure 4. Basic Con ict Ordering: Example (Read-misses). One important consequence of completing mem- ory operations past pending writes is that, when a read-miss com- pletes, there might be writes before it in the global memory order that are still pending. As shown in Fig. 4(a), if b2 is allowed to com- plete before b1, b1 might still be pending when a1 completes. Now, con ict ordering mandates that memory operations that follow a1 can complete, only when they do not con ict with predp(a1). To enable this check, read-misses are also made to consult the direc- tory and determine the write-list. Accordingly, when read-miss a1 consults the directory, it fetches the write-list and replies to the pro- cessor. Having obtained the write-list, a2 which does not con ict with memory operations from the write-list, is allowed to complete. Memory operation a3, since it con icts, is replayed obtaining its value from b1 via the directory. 4.1.1 Handling distributed directories To avoid a single point of contention, directories are typically distributed across the tiles, with the directory placement policy deciding the mapping from the address to the corresponding home directory; we assume the rst touch directory placement policy. With a distributed directory, a write-miss (or a read-miss) m1 is issued to its home directory and can only obtain the list of pending writes from that directory. This means that a memory operation m2 that follows m1 can use the write-list to check for con icts only if m2 maps to the same home directory as m1. If m2 does not map to the same home node as m1, then m2 will have to be conservatively assumed to be con icting. Consequently m2 will have to go to its own home directory to ensure that it does not con ict. If it does not con ict, then m2 can simply commit like a cache hit; otherwise, it is treated like a miss and replayed. When m2 goes to its home directory to check for con icts, we take this opportunity to fetch the write-list from m2 s home directory, so that future accesses to the same node can check for con icts locally. To accommodate this, the local processor has multiple write-list registers which are tagged by the tile id. The scenario shown in Fig. 4(b) illustrates this. Let us assume that the variable X maps to Node 1, while variables Z, W and Y map to Node 2. Note that the processor also has two write-list registers. When a1 is issued to its directory, it returns the pending write-misses from Node 1. Consequently, this is put into one of the write-list register which is tagged with the id of Node 1. The contents of the other write-registers are invalidated as they might contain out-of-date data. When a2 tries to commit, the tags of the write-list registers are checked to see if any of the write-list registers is tagged with the id of Node 2, which is the home node for a2. Since none of the write-list registers have the contents of Node 2, a2 is sent to its home directory (Node 2) to check for con icts. After ensuring that a2 does not con ict, the pending stores of the home directory (from Node 2) are returned and inserted into the write-list register. This allows us to commit a3 which maps to the same node. Likewise, when a4 tries to commit, it is found to be con icting and hence replayed. Although the distributed directory could potentially reduce the number of memory operations that can complete past pending writes, our experiments show that, due to locality, most of the nearby accesses tend to map to the same home node, which allows us to complete most memory operations past pending writes. 4.1.2 Correctness The correctness of our con ict ordering implementation hinges on the fact that when a cache-miss m1 is issued to the directory, the write-list returned by the directory is correct; that is, the write-list should include addrp(m1), the addresses of all pending memory operations that occur before m1 in the global memory order. We prove this formally, next. Lemma 2. When a cache-miss m1 is issued to the directory, all pending memory operations prior to it in the global memory order must be present in the directory . That is, when m1 is issued to the directory, {m : mGm1} must be present in the directory. Figure 5. Correctness of con ict ordering implementation: m2 cannot be a read in each of the 3 scenarios Proof. (1) m1 is a write-miss. When m1 is issued to the directory, con ict ordering ensures that all pending memory operations prior to m1 in the program order must have been issued to the directory. (2) m1 is a read-miss. m1 is issued to the directory to fetch the write-list only when it is retired, and all memory operations prior to it have in turn been issued. Thus, whether m1 is a write-miss or a read-miss, when m1 is issued to the directory, {m : mPm1} must be present in the directory. Likewise, when m1 is issued to the directory all memory operations prior to m1 in the con ict order, {m : mEm1} must have been issued to the directory. Thus, when m1 is issued to the directory, {m : mGm1} must be present in the directory. Lemma 3. When a read-miss m1 is issued to the directory (to obtain the write-list), none of the pending memory operations prior to it in the global memory order are read-misses. Proof. Let us attempt a proof by contradiction and assume that such a pending read-miss exists. Let m2 be the assumed read-miss such that m2 predp(m1). Now the read-miss m2 cannot occur before m1 in the program order, as all such read-misses would have retired and hence cannot be pending. m2 cannot occur before m1 in the con ict order, as two reads do not con ict with each other. Thus, the only possibility is as shown in Fig. 5(a), where there is a write-miss m3 such that m2Pm3 and m3Em1 (without loss of generality). This again, however, is impossible since the write m3 will be issued only after all reads before it (including m2) complete. Lemma 4. When a write-miss m1 is issued to the directory, all pending read-misses prior to it in the global memory order, con ict with m1. Proof. As shown in Fig. 5(b), it is possible that there exists m3, a read-miss, such that m3Gm1 since m3Em1. There cannot be, however, any read-miss m2 such that m2Gm1 with m2 not con- icting with m1. Proof is by contradiction, along similar lines to Lemma 3 (m2 in Fig. 5 (b) and Fig. 5 (c) cannot be reads). Theorem 2. When a cache-miss m1 is issued to the directory, the write-list returned will contain addrp(m1), the addresses of all pending memory operations that occur before m1 in the global memory order. Proof. When a cache-miss m1 is issued to the directory, the ad- dresses of the memory operations serviced in the directory (addr- list) will contain addrp(m1) (from Lemma 2). All pending read- misses which occur before m1 in the global memory order, if any, would con ict with m1 and hence access the same address as m1. (from Lemma 3 and Lemma 4). Thus, the write-list is guaranteed to contain addrp(m1). 4.2 Enhanced con ict ordering In this section, we describe our enhanced con ict ordering design, in which we identify phases in program execution where memory operations can complete without checking for con icts. But rst, we discuss the limitations of basic con ict ordering, to motivate our approach. 4.2.1 Limitations of basic con ict ordering We illustrate the limitations of basic con ict ordering with the example shown in Fig. 6(a) and (b). As we can see, in Fig. 6(a), the write-miss a1 estimates predp(a1) by consulting the directory and obtaining the write-list. All memory operations that follow the pending write-miss a1 need to be checked with the write-list for a con ict, before they can complete. It is important to note that even after a1 completes, memory operations following a1 will still have to be checked for con icts. This is because b1, which precedes a1 in the global memory order, could still be pending when a1 completes. In other words, the completion of the store a1 is no longer an indicator of all memory operations belonging to pred(a1) completing. Thus, in basic con ict ordering, all memory operations that follow a write miss (or a read miss) need to be continually checked for con icts before they can safely complete. Another limitation stems from the fact that a read-miss (or a write-miss) conservatively estimates its predecessors in the global memory order by accessing the directory. Let us consider the sce- nario shown in Fig. 6(b), in which the read-miss a1 estimates predp(a1) from the directory. However, if b2 and a1 are suf ciently staggered in time, which is typically common [20], then all mem- ory operations belonging to pred(b2) (including b1) might have already completed by the time a1 is issued. In such a case, there is no need for a1 to compute its write list; indeed, memory operations such as a2 can be safely completed past a1. 4.2.2 Our approach (HW support and operation). Our approach is to keep track of the (local) memory operations that have retired from the ROB, but whose predecessors in the global memory order are still pending. We call such memory operations as active and we track such mem- ory operations in a per-processor augmented write buffer (AWB). The AWB is like a normal write-buffer, in that, it buffers write- misses; unlike a conventional write-buffer, however, it also buffers the addresses of other memory operations (including read-misses, read-hits and write-hits) that are active. Therefore, an empty AWB indicates an executing phase in which all preceding memory oper- ations in the global memory order have completed; this allows us to complete succeeding memory operations without checking for con icts. To reduce the space used by AWB, the consecutive cache hits to the same block are merged. We now explain how we keep track of active memory operations in the AWB. A write-miss that retires from the ROB is marked active by inserting it into the AWB, as usual. The write-miss, however, is not necessarily removed from the AWB (i.e. marked inactive) when it completes; we will shortly explain the conditions under which a write-miss is removed from the AWB. A memory operation which retires from the ROB while the AWB is non-empty is active by de nition. Consequently, a cache hit which retires while the AWB is non-empty is marked active by inserting its cache block address into the AWB; it is subsequently removed when the memory operation becomes inactive i.e., when all prior entries in the AWB have been removed. A cache miss which retires while the AWB is non-empty, like a cache hit, is marked active by inserting its cache block address into the AWB. However, unlike a cache hit, its block address is not necessarily removed when all prior entries in the AWB have been Figure 6. (a) and (b): Limitations of basic con ict ordering; (c) and (d): Our approach removed. Indeed, a cache miss remains active, and hence its cache block address remains buffered in the AWB, until its predecessors in the con ict order become inactive. Accordingly, even if a write- miss completes, it remains buffered in the AWB until all the cache blocks that it invalidates turn inactive. Likewise, a read-miss that completes is inserted into the AWB, if it gets its value from a cache block that is marked active. Here, a cache block is said to be active if the corresponding cache block address is buffered in the AWB, and inactive otherwise. Thus, to precisely keep track of cache misses that are active, we need to somehow infer whether its predecessors in the con ict order are active. We implement this by tagging coherence messages as active or inactive. When a processor receives a coherence request for a cache block, we check the AWB to see if it is marked active. If it is marked active, the processor responds to this coherence request with a coherence response that is tagged active. This would enable the recipient (cache miss) to infer that the coherence response is from an active memory operation. At the same time, when a processor receives a coherence request to an active cache block, it is buffered; when the cache block eventually becomes inactive, we again respond to the buffered coherence request with a coherence response that is tagged inactive. This would enable the recipient (cache miss) to infer that the response is from a block that has transitioned to inactive. A cache miss, when it receives a coherence response, is allowed to complete irrespective of whether it receives an active or an inactive coherence response; however, it remains active (and hence buffered in the AWB) until it gets an inactive response. Finally, by not allowing an active cache block to be replaced, we ensure that if a cache miss has predecessors in the con ict order, it will surely be exposed via the tagged coherence messages. (Write-Miss: Example). We now explain the operation with the scenario shown in Fig. 6(c) which addresses the limitation shown in Fig. 6(a). First, the write-miss b1 is issued from the ROB of pro- cessor B and is inserted into the AWB (step 1), after which the read-hit b2 is made to complete past it. Since, the AWB is non- empty when b2 completes, b2 is marked active by inserting the cache block address X into the AWB (step 2). Then, write-miss a1 to same address X is issued in processor A, inserted into the AWB, and issued to the directory (step 3). The directory then services this write-miss request, sending an invalidation request for cache block address X to processor B. Since cache block address X is active in processor B (cache block address X is buffered in processor B s AWB), processor B sends an active invalidation acknowledgement back to processor A (step 4); at the same time, processor B buffers the invalidation request so that it would be able to respond again when cache block address X eventually becomes inactive. When processor A receives the invalidation acknowledgement, a1 com- pletes (step 5), and so it sends a completion acknowledgement to the directory. It is worth noting that a1 has completed at this point, but is still active and hence is still buffered in processor A s AWB. Let us assume that b1 completes at step 6; furthermore let us assume that at this point all the predecessors of b1 in the global memory or- der have also completed in other words, b1 has becomes inactive. Since b1 has become inactive, cache block address Y is removed from the AWB. This, in turn, causes b2 (cache block address X) to be removed from the AWB, because all those entries that pre- ceded b2 (including b1) have been removed from the AWB. Since cache block address X has become inactive, processor B again re- sponds to the buffered invalidation request by sending an inactive invalidation acknowledgement to processor A. When processor A receives this inactive acknowledgement, the recipient a1 is made inactive and hence removed from the AWB (step 7). This will allow succeeding memory operations like a2 to safely complete without checking for con icts (step 8). (Read-Miss: Example). Fig. 6(d) addresses the limitation shown in Fig. 6(b). First, the write-miss b1 is issued and made active by inserting it into the AWB (step 1). Then, the write-hit b2 which completes past it is made active by inserting address X into the AWB (step 2). Let us assume that b1 then completes, and also becomes inactive (step 3). This causes b1 to be removed from the AWB, which in turn causes b2 (cache block address X) to be removed. When the read-miss a1 is issued to the directory (step 4), it sends a data value request for address X to processor B. Since the cached block address X is inactive, processor B responds with an inactive data value reply to processor A (step 5). Upon receiving this value reply, a1 completes. Furthermore, since the reply is inactive, it indicates that all the predecessors of a1 in the con ict order have completed. Thus, the write-list is not computed and a1 is not inserted into the AWB. Indeed when a2 is issued (step 8), assuming the AWB is empty, it can safely be completed. (Misses to uncached blocks). When a cache-miss is issued to the directory and it is found to be uncached in each of the other pro- cessors, this indicates that the particular cache block is inactive in each of the other processors. This is because an active block is not allowed to be replaced from the local cache. This allows us to han- dle a miss to an uncached block similar to a cache hit, in that, we do not need to compute a new write-list for such misses. Indeed, if there are no other pending entries in the AWB, then memory op- erations that come after a miss to an uncached block can be com- pleted without checking for con icts. It is worth noting that misses to local variables, which account for a signi cant percentage of misses, would be uncached in other processors. This optimization would allow us to freely complete memory operations past such local misses. (Avoiding AWB snoops). In the above design, all coherence re- quests and replacement requests must snoop the AWB rst to see if the corresponding cache block is marked active. To avoid this, we associate an active bit with every cache block; the active bit is set whenever the corresponding cache block address is inserted into the AWB and reset, whenever the cache block address is not contained in the AWB. (Deadlock-freedom). Deadlock-freedom is guaranteed because memory operations that have been marked active will eventually become inactive. It is worth recalling that a memory operation becomes inactive when all its predecessors in the global memory order in turn become inactive. In theory, since con ict ordering guarantees SC, there cannot be any cycles in the global memory order, which ensures deadlock-freedom. The fact that an active cache block is not allowed to be replaced, however, can potentially cause the following subtle deadlock scenario. In this scenario, an earlier write-miss is not allowed to bring its cache block into its local cache, since all the cache blocks in its set have been marked active by later memory operations which have completed. In such a scenario, the later memory operations that have completed are marked active, and are waiting for the earlier write-miss to turn inactive; the earlier write-miss, however, cannot complete (and be- come inactive), since it is waiting for the later memory operations to turn inactive. We avoid such a scenario by forcing a write-miss to invalidate the cache block chosen for replacement and set its active bit, before the write-miss is issued to the directory. This will ensure that later memory operations which complete before the write-miss will not be able to use the same cache block used by the write-miss, as this cache block has been marked active by the write-miss. Thus, the deadlock is prevented. Figure 7. Hardware Support: Con ict Ordering 4.3 Summary (Hardware support). Fig. 7 summarizes the hardware support re- quired by con ict ordering. First, each processor core is associated with a set of write-list registers. Second, each processor core is as- sociated with an augmented write buffer (AWB), which replaces a conventional write-buffer. Like a conventional write-buffer, each entry of the AWB is tagged by the cache block address; unlike a conventional write buffer, however, it also buffers read-misses, read-hits and write-hits. To distinguish write-misses from other memory operations, each entry of the AWB is associated with a write-miss bit. If the current entry corresponds to a write-miss, i.e. the write-miss bit is set, the entry additionally points to the data that is written by the current write-miss. Each entry is also associ- ated with a con ict-active bit, which is set if any of its predeces- sors which con ict with it are active. Third, an active bit is added to each local cache block, which indicates whether that particular cache block is active. Lastly, con ict ordering requires minor exten- sions to the cache coherence subsystem. Each processor is associ- ated with a req-buffer which is used to buffer coherence requests to active cache blocks. Each coherence response message (data value reply and invalidation acknowledgement) is tagged with an active bit, to indicate whether it is an active response or an inactive re- sponse. Also, con ict ordering involves the exchange of additional coherence message. While additional coherence messages are ex- changed as part of con ict ordering, it is worth noting that the co- herence protocol and its associated transactions for maintaining co- herence are left unchanged. We summarize the additional transac- tions next. (Write-miss actions). The actions performed when the write-miss retires from the ROB are as follows: Insert into the AWB. The write-miss is rst inserted into the AWB as follows: the inserted entry is tagged with the cache block address of the write-miss; since the entry corresponds to a write-miss, the write-miss bit is set to 1 and the entry points to the data written by the write-miss; the con ict-active bit is set to 1 since its predecessor in the con ict order may be active. Invalidate replacement victim. Once the write-miss is inserted into the AWB, the cache block chosen for replacement is invali- dated and its active bit is set to 1 it is worth recalling that this is done to prevent the deadlock scenario discussed earlier. Issue request to home-directory. Now, the write-miss is issued to its home directory. If the corresponding cache block is found to be uncached in any of the other processors, the directory replies with an empty write-list. If the cache block is indeed cached in some other processor, the directory replies with the list of pending write-misses being serviced in the directory. To minimize network traf c, before sending the write-list, a bloom lter is used to compress the list of addresses. Process response. Once the processor receives the write-list, if it indeed receives a non-empty write-list, it updates the local write-list register. When the write-miss receives all of its inval- idation acknowledgements, it completes. When the write-miss completes, it sends a completion message to the directory, so that the directory knows about its completion. When a write- miss receives all of its acknowledgements and each of the ac- knowledgements are tagged inactive, its corresponding con ict- active bit (in the AWB) is reset to 0. When the con ict-active is reset, the write-miss checks the AWB to see if there are any entries prior to it. If there are none, it implies that the write-miss has become inactive and can be removed from the AWB. Remove write-miss and other inactive entries from the AWB. Before removing the write-miss entry, we rst identify those memory operations which follow the original write-miss that have also become inactive. To identify such inactive memory operations, the AWB is scanned sequentially starting from the original entry, selecting all those entries whose con ict-active bit is reset to 0; the scanning is stopped when an entry whose con ict-active bit is set to 1 is encountered. All such selected entries are removed from the AWB, and the active bits of their respective cache blocks are reset to 0. (Cache-hit actions). The actions performed when a cache-hit reaches the head of the ROB are as follows: AWB empty. The AWB is rst checked to see if it is empty. If the AWB is empty, the cache-hit completes without checking for con icts. Check for con icts. If the AWB is not empty, the write-list registers are checked to see if the cache-hit con icts with it. If the cache-hit con icts (or if write-list register does not cache the directory entries of the cache-hit s home node), the cache-hit is treated like a miss and is issued to its home directory. No con icts. If the cache-hit does not con ict, it is allowed to safely complete. Since the AWB is non-empty, the cache-hit is marked active by inserting it into the AWB with the write-miss bit set to 0, the con ict-active bit set to 0 (since it is a cache-hit, its predecessors in the con ict order must have completed), and the active bit of the cache block is set to 1. (Read-miss actions). The actions performed when a read-miss reaches the head of the ROB are as follows: AWB empty. The AWB is rst checked to see if it is empty. If the AWB is empty, the read-miss completes without check- ing for con icts. Then, the data value reply that the read-miss received as a coherence response is examined. If it is tagged ac- tive, it is inserted into the AWB with the write-miss bit set to 0, the con ict-active bit set to 1 (since it obtained an active re- sponse), and the active bit of the cache block is set to 1. Later, when the read-miss receives its inactive response, the read-miss entry is removed along with subsequent inactive entries, as dis- cussed earlier. Check for con icts. If the AWB is not empty, the write-list registers are checked to see if the read-miss con icts with it. If the read-miss con icts (or if write-list register does not cache the directory entries of the read-miss home node), the cache-miss is re-issued to its home directory. No con icts. If the read-miss does not con ict, the read-miss is allowed to safely complete. Since the AWB is non-empty, the read-miss is marked active by inserting into the AWB with the write-miss bit set to 0. The con ict-active bit is set to 0 or 1 depending on whether the read-miss received an inactive or an active data value response, respectively. If it received an active response later, when the read-miss receives the inactive response, the read-miss entry is removed along with subsequent inactive entries, as discussed earlier. (Coherence and replacement requests). When a coherence re- quest (invalidate or data value request) is received for a cache block that is marked active/inactive, the coherence response is in turn tagged active/inactive. Additionally, if the coherence request is for an active cache block, the coherence request is buffered in the req- buffer. When the cache block is eventually reset to inactive, the cor- responding entry is removed from the req-buffer and the processor again responds to the buffered request but now with an inactive response. When a coherence request is received for an active cache block and the req-buffer is full, the coherence response is delayed until a space in the req-buffer frees up. As discussed earlier, this cannot cause a deadlock, since active cache blocks will eventually turn inactive. Finally, a cache block that is marked active is not al- lowed to be replaced. When a replacement request is received, and all cache blocks in the set are marked active, the response is delayed until once of the blocks in the set turns inactive again, without the risk of a deadlock. 5. Experimental Evaluation We performed our experiments with several goals in mind. First and foremost, we want to evaluate the bene t of ensuring SC via con ict ordering in comparison with the baseline SC and RMO im- plementations. We then study the effect of varying the values of the parameters in our HW implementation, on the performance. Since the performance of our approach is dependent on how fast requests can get back replies from directories, we study the sensitivity to- wards the network latency. We also vary the size of write-lists to evaluate their effects on performance. We then study the character- ization of con ict ordering to see how it reduces the overhead of using in-window speculation technique. Furthermore, we measure the additional network bandwidth that is used up and nally, we also measure the on-chip hardware resources that con ict ordering utilizes. However, before we present the results of our evaluation, we brie y describe our implementation. 5.1 Implementation Processor 8, 16 and 32 core CMP, out of order ROB size 176 L1 Cache private 32 KB 4 way 2 cycle latency L2 Cache shared 8 MB 8 way 9 cycle latency Memory 300 cycle latency Coherence directory based invalidate # of AWB entries 50 per core # of req-buffer entries 8 per core # of write-list registers 6 per core write-list size 160 bits 2D torus (2 4 for 8-core, Interconnect 4 4 for 16-core, 4 8 for 32-core) 5 cycle hop latency Table 1. Architectural Parameters. Benchmark Inputs barnes 16K particles fmm 16K particles ocean 258 258 radiosity batch raytrace car water-ns 512 molecules water-sp 512 molecules cholesky tk15.O fft 64K points lu 512 512 radix 1M integers Table 2. Benchmarks. We implemented con ict ordering using SESC [25] simulator, targeting the MIPS architecture. The simulator is a cycle-accurate, execution-driven multi-core simulator with detailed models for the processor and the memory systems. To implement con ict order- ing, we added the associated control logic to the simulator. We considered con ict ordering in the context of a CMP with local caches kept coherent using a distributed directory based protocol. The architectural parameters for our implementation are presented in Table 1. The default architectural parameters were used in all experiments unless explicitly stated otherwise. We measured per- formance with 8, 16 and 32 processors in Section 5.2, and for other studies, we assumed 32 processors. We used the SPLASH-2 [32], a standard multithreaded suite of benchmarks for our evaluation. We could not get the program volrend to compile using the compiler infrastructure that targets the simulator and hence we omitted it. We used the input data sets described in Table 2 and ran the bench- marks to completion. (Our baseline SC implementation). Our SC baseline, referred to as conventional SC in the experiments below, uses in-window spec- ulation support. It is an aggressive implementation that uses hard- ware prefetching and support for speculation in modern processors to speculatively reorder memory operations while guaranteeing SC ordering using replay. However, such speculation is within the in- struction window where the instruction window refers to the set of instructions that are in- ight. The implementation we use is sim- ilar to ones used as SC baselines in proposals such as [6, 15, 31]. (Our baseline RMO implementation). Our baseline RMO im- plementation allows memory operations to be freely reordered, enforcing memory ordering only when fences are encountered. Even when fences are encountered, in-window speculation support (as used in the SC baseline) is used to mitigate the delays. The RMO implementation is similar to ones used in recent works such as [6, 15, 31]. 5.2 Execution time overhead 1x 1.1x 1.2x 1.3x 1.4x 1.5x barnes fmm ocean radiosity raytrace water_ns water_sp cholesky fft lu radix average Normalized Execution Time Overhead 8 processors 16 processors 32 processors Figure 8. Conventional SC normalized to RMO 1x 1.02x 1.04x 1.06x 1.08x 1.1x barnes fmm ocean radiosity raytrace water_ns water_sp cholesky fft lu radix average Normalized Execution Time Overhead 8 processors 16 processors 32 processors Figure 9. Con ict ordering normalized to RMO We measure the execution time overhead of ensuring SC via our approach con ict ordering and compare it with the corresponding overhead for conventional SC. We conducted this experiment for 8, 16 and 32 processors using the default hardware implementation presented in Table 1. Fig. 8 and Fig. 9 show the execution time overheads for conventional SC and con ict ordering, respectively. The execution time overheads are normalized to the performance achieved using RMO. As we can see, most benchmark programs experience signi cant slowdown for conventional SC (more than 20% overhead on average for all numbers of processors). In par- ticular, radix has the highest overhead. This is because radix has a relatively high store-miss rate which forces the following loads to wait longer before they can be retired. As we can see, with con ict ordering the overhead of ensuring SC is signi cantly reduced. On average, the overhead is only 2.0% for 8 processors, 2.2% for 16 processors and 2.3% for 32 processors, and thus the performance of our approach is comparable to RMO. With con ict ordering loads and stores do not need to wait until outstanding stores complete; they can mostly retire as soon as the pending stores get replies back from directories. Since the time to get replies from directories is signi cantly less than the time for outstanding stores to complete, loads do not end up causing stalls and stores can also be performed out of order. This explains why the performance with con ict order- ing is signi cantly better than conventional SC. Furthermore, it is worth noting that, with different numbers of processors, the perfor- mance does not vary signi cantly. Therefore, our approach appears scalable. 1x 1.02x 1.04x 1.06x 1.08x 1.1x barnes fmm ocean radiosity raytrace water_ns water_sp cholesky fft lu radix average Normalized Execution Time Overhead 4 processors 8 processors Figure 10. Performance for bus-based implementation. (Bus-based implementation). In addition to the distributed directory- based implementation, we also evaluated a bus-based implementa- tion to nd out if con ict ordering is applicable to current multi- core processors which are predominantly bus based. For our bus based implementation, we just implement basic con ict ordering; we maintain a centralized on-chip structure called write-list-buffer (WLB), which records the addresses of the write-misses which are currently pending. When a write-miss retires into a local write- buffer it is sent to the WLB; upon receiving the write-miss, the WLB inserts its address into the WLB and replies back with the write-list containing all the addresses that are currently present in the WLB except the addresses from the source processor. Sim- ilar to the directory-based implementation, a bloom lter is used to compress the addresses. Memory operations which attempt to complete past the write-miss, check the received write-list to de- cide whether they can be completed safely, without violating SC. We conducted experiments for bus-based implementation with 4 and 8 processors, and set the round-trip latency for accessing WLB as 5 cycles. Fig. 10 shows the execution time normalized to RMO. As we can see, the execution time of our technique is close to RMO for both 4 and 8 processors, with the overhead less than 2% on av- erage. This shows that con ict ordering is also applicable to small scale multi-core processors that use a bus for coherence. 5.3 Sensitivity studies 1x 1.01x 1.02x 1.03x 1.04x 1.05x 1.06x 1.07x 1.08x 1.09x barnes fmm ocean radiosity raytrace water_ns water_sp cholesky fft lu radix average Normalized Execution Time Overhead 3 cycles 5 cycles 8 cycles Figure 11. Varying the latency of network. (Sensitivity towards network latency). The performance of con- ict ordering hinges on the time for a miss to get replies from the directory. It is desirable that con ict ordering is reasonably toler- ant to the network latency. In our experiments, we used 2D torus network and varied the latency of each hop with values of 3 cycles, 5 cycles, and 8 cycles, as shown in Fig. 11. The performance is measured with 32 processors. As we can see, the overhead does not vary signi cantly when the latency is increased from 3 cycles to 5 cycles. Even when the latency is increased to 8 cycles, the average performance drops only slightly, to 3.5%. This shows con ict or- dering is reasonably tolerant to the network latency. In our default design, we choose 5 cycles as each hop latency, assuming 2 cycle wire delay between routers and 3 cycle delay per pipelined router. 0 1 2 3 4 5 6 7 8 9 10 a b c -15% -10% -5% 0% 5% 10% 1.00x 1.02x 1.04x 1.06x 1.08x 1.10x False Positive Rate Normalized Execution Time Overhead 128 160 192 128 160 192 Figure 12. Varying number of bits of write-list. (Sensitivity towards size of write-list). Recall that, to minimize network bandwidth, the addresses within replies to cache miss re- quests are compressed to form a write-list using a bloom lter. While a smaller size is bene cial as far as saving network band- width is concerned, it could also result in false positives. A false positive can lead us to falsely conclude that a load or store con- icts with a pending store, causing the load to re-execute or the store to stall. In this experiment, we evaluated the minimum size of the write-list that does not result in performance loss. We varied the number of bits of write-list with the value 128, 160, and 192 to evaluate the performance and corresponding false positive rates with 32 processors. In our implementation, we used a bloom lter with 4 hash functions. Fig. 12 shows the results, where lines rep- resent false positive rates and bars represent execution time over- heads. As the number of bits increases from 128 to 192, the false positive rate decreases (on average, 8.01%, 2.42%, and 0.69% re- spectively), and the execution overhead also decreases (on average, 2.97%, 2.33% and 2.29% respectively). A size of 160 bits performs slightly better than 128 bits and very close to 192 bits. Therefore, we choose 160 bits (20 bytes) as the size of the write-list in our implementation. 5.4 Characterization of con ict ordering In this experiment, we wanted to examine how con ict ordering reduces the overhead of using conventional SC. Recall that, in the conventional SC implementation, loads cannot be retired if there are pending stores and stores cannot be performed out of order, which leads to the gap between the performance of SC and RMO. On the other hand, using con ict ordering, we can safely retire loads and complete stores past pending stores by checking the write-list. Hence, performance hinges on how often loads and stores can be reordered with their prior pending stores. 0% 20% 40% 60% 80% 100% barnes fmm ocean radiosity raytrace water_ns water_sp cholesky fft lu radix Breakdown with conflict without conflict empty w.l. Figure 13. Breakdown of checks against write-lists. 0% 5% 10% 15% 20% 25% barnes fmm ocean radiosity raytrace water_ns water_sp cholesky fft lu radix average Percentages Figure 14. Reduced accesses to directories. Fig. 13 shows the breakdown for all checks against write-lists. We categorize these checks into three types: checks against empty write-lists, checks against non-empty write-lists without nding any con ict, and checks against non-empty write-lists with nding a con ict. When a request to the directory nds that the targeted block is not cached, it indicates that no active memory operation has accessed the block and the requesting processor can get a reply with an empty write-list. As we can see, most checks are against empty write-lists (around 90% on average). For the checks against non-empty write-lists, there is almost no con ict. Hence, our approach allows almost all memory operation to be reordered, achieving performance comparable to RMO. Fig. 14 shows the reduced directory accesses using enhanced con ict ordering, compared to basic con ict ordering. The perfor- mance of our approach also hinges on the frequency of directory ac- cesses. Hence, it is important to have fewer directory accesses. As we can see, using enhanced con ict ordering, on average we only have about 6% directory accesses of basic con ict ordering. This is because, for most benchmark programs, AWB is empty most of the time. For some benchmarks, such as ocean, the access frequency does not reduce as much as other benchmarks. However, the access frequency for these benchmarks in basic con ict ordering is already low. Therefore, their overhead is still low. 5.5 Bandwidth increase Benchmark Bandwidth increase (%) barnes 2.49 fmm 0.98 ocean 2.64 radiosity 8.36 raytrace 1.21 water-ns 2.08 water-sp 2.21 cholesky 0.24 fft 1.16 lu 1.96 radix 1.71 Table 3. Bandwidth Increase. In this experiment, we measure the bandwidth increase due to write-lists that need to be transferred and extra traf c introduced by con ict ordering for 32 processors. Table 3 shows the bandwidth in- crease compared to RMO. As we can see, for most benchmarks, the bandwidth increase is less than 3% (2.27% on average). radiosity has relatively higher bandwidth increase. This is because its write miss rate is relatively higher and requests to directories usually get non-empty write-lists, resulting in write-lists taking up a relatively high proportion of bandwidth. 5.6 HW resources utilized Recall that the additional HW resources utilized by con ict order- ing are AWB, req-buffers, write-list registers, and the active bits added to each cache block. Each AWB entry contains the cache block address, the con ict-active and the write-miss bits; we do not count the storage required for data written by the write-miss, since it is already present in conventional write buffers. Since we use 50 AWB entries per core, each of size 5 bytes, the total size per processor core amounts to 250 bytes for the AWB. Since we use 8 entries for the req-buffer, each of size 6 bytes (for storing the cache block address and the processor id), the total size per proces- sor core amounts to 48 bytes. With 6 write-list registers per core, each of size 20 bytes, the total size per processor core amounts to 120 bytes. In addition, we also require active bits to be added to each cache block in the L1 cache. This amounts to an additional 64 bytes of on-chip storage per processor core. Thus the total ad- ditional on-chip storage space amounts to 482 bytes per processor core. In addition to this we require the hardware resources needed for in-window speculation which is also required by our SC and RMO baselines. Thus, the additional hardware resources utilized for con ict ordering is nominal. 6. Conclusion Should hardware enforce SC? Researchers have examined this question for over 30 years, with no de nite answers, yet. While there has been no clear consensus on whether hardware should support SC [3, 17], it is important to note, however, that the bene- ts of supporting SC are widely acknowledged [3]. Indeed, critics of hardware enforced SC, question it based on whether the costs of supporting SC justify its bene ts all prior SC implementa- tions needing to employ aggressive speculation and its associated complexity for supporting SC. In this paper we demonstrate that the bene ts of SC can indeed be realized using nominal hardware resources. While prior SC im- plementations guarantee SC by explicitly completing memory op- erations within a processor in program order, we guarantee SC by completing con icting memory operations, within and across pro- cessors, in an order that is consistent with the program order. More speci cally, we identify those shared memory dependencies whose ordering is critical for the maintenance of SC and intentionally or- der them. This allows us to non-speculatively complete memory operations past pending writes and thus reduce the number of stalls due to memory ordering. Our experiments with SPLASH-2 suite showed that SC can be achieved ef ciently, incurring only 2.3% additional overhead compared to RMO. Acknowledgments We would like to thank the reviewers and our shepherd for their helpful comments and advice for improving this paper. This work is supported by NSF grants CCF-0963996 and CCF-0905509 to the University of California, Riverside, and by the Centre for Numer- ical Algorithms and Intelligent Software, funded by EPSRC grant EP/G036136/1 and the Scottish Funding Council to the University of Edinburgh. References [1] Intel Single-Chip Cloud Computer. http://techresearch. intel.com/articles/Tera-Scale/1826.htm. [2] Telera Tile-Gx processor. http://www.tilera.com/products/ processors. [3] S. V. Adve and H.-J. Boehm. Memory models: a case for rethink- ing parallel languages and hardware. Commun. ACM, 53(8):90 101, 2010. [4] S. V. Adve and K. Gharachorloo. Shared memory consistency models: A tutorial. IEEE Computer, 29:66 76, 1995. [5] S. V. Adve and M. D. Hill. Weak ordering - a new de nition. In Proceedings of the 17th annual international symposium on Computer Architecture, ISCA 90, pages 2 14, 1990. [6] C. Blundell, M. M. Martin, and T. F. Wenisch. InvisiFence: performance-transparent memory ordering in conventional multipro- cessors. In Proceedings of the 36th annual international symposium on Computer architecture, ISCA 09, pages 233 244, 2009. [7] L. Ceze, J. Tuck, P. Montesinos, and J. Torrellas. BulkSC: bulk enforcement of sequential consistency. In Proceedings of the 34th annual international symposium on Computer architecture, ISCA 07, pages 278 289, 2007. [8] H. Cha , J. Casper, B. D. Carlstrom, A. McDonald, C. C. Minh, W. Baek, C. Kozyrakis, and K. Olukotun. A scalable, non-blocking approach to transactional memory. In Proceedings of the 2007 IEEE 13th International Symposium on High Performance Computer Archi- tecture, HPCA 07, pages 97 108, 2007. [9] T. Elmas, S. Qadeer, and S. Tasiran. Goldilocks: a race and transaction-aware java runtime. In Proceedings of the 2007 ACM SIG- PLAN conference on Programming language design and implementa- tion, PLDI 07, pages 245 255, 2007. [10] X. Fang, J. Lee, and S. P. Midkiff. Automatic fence insertion for shared memory multiprocessing. In Proceedings of the 17th annual international conference on Supercomputing, ICS 03, pages 285 294, 2003. [11] M. Galluzzi, E. Vallejo, A. Cristal, F. Vallejo, R. Beivide, P. Stenstr om, J. E. Smith, and M. Valero. Implicit transactional memory in kilo- instruction multiprocessors. In Asia-Paci c Computer Systems Archi- tecture Conference, pages 339 353, 2007. [12] K. Gharachorloo, S. V. Adve, A. Gupta, J. L. Hennessy, and M. D. Hill. Specifying system requirements for memory consistency models. Technical Report CSL-TR-93-594, Stanford University, 1993. [13] K. Gharachorloo, A. Gupta, and J. Hennessy. Two techniques to en- hance the performance of memory consistency models. In Proceed- ings of the 1991 International Conference on Parallel Processing, ISCA 91, pages 355 364, 1991. [14] C. Gniady and B. Falsa . Speculative sequential consistency with little custom storage. In Proceedings of the 2002 International Conference on Parallel Architectures and Compilation Techniques, PACT 02, pages 179 188, 2002. [15] C. Gniady, B. Falsa , and T. N. Vijaykumar. Is SC + ILP = RC? In Proceedings of the Twenty Sixth Annual International Symposium on Computer Architecture, ISCA 99, pages 162 171, 1999. [16] L. Hammond, V. Wong, M. Chen, B. D. Carlstrom, J. D. Davis, B. Hertzberg, M. K. Prabhu, H. Wijaya, C. Kozyrakis, and K. Oluko- tun. Transactional memory coherence and consistency. In Proceedings of the 31st annual international symposium on Computer architecture, ISCA 04, page 102, 2004. [17] M. D. Hill. Multiprocessors should support simple memory- consistency models. Computer, 31(8):28 34, 1998. [18] L. Lamport. How to make a multiprocessor computer that correctly executes multiprocess program. IEEE Trans. Comput., 28(9):690 691, 1979. [19] J. Lee and D. A. Padua. Hiding relaxed memory consistency with a compiler. IEEE Trans. Comput., 50(8):824 833, 2001. [20] C. Lin, V. Nagarajan, and R. Gupta. Ef cient sequential consistency using conditional fences. In Proceedings of the 19th international con- ference on Parallel architectures and compilation techniques, PACT 10, pages 295 306, 2010. [21] B. Lucia, L. Ceze, K. Strauss, S. Qadeer, and H.-J. Boehm. Con ict exceptions: simplifying concurrent language semantics with precise hardware exceptions for data-races. In Proceedings of the 37th annual international symposium on Computer architecture, ISCA 10, pages 210 221, 2010. [22] D. Marino, A. Singh, T. Millstein, M. Musuvathi, and S. Narayanasamy. DRFx: a simple and ef cient memory model for concurrent programming languages. In Proceedings of the 2010 ACM SIGPLAN conference on Programming language design and implementation, PLDI 10, pages 351 362, 2010. [23] D. Marino, A. Singh, T. Millstein, M. Musuvathi, and S. Narayanasamy. A case for an SC-preserving compiler. In Proceedings of the 32nd ACM SIGPLAN conference on Programming language design and implementation, PLDI 11, pages 199 210, 2011. [24] P. Ranganathan, V. S. Pai, and S. V. Adve. Using speculative retire- ment and larger instruction windows to narrow the performance gap between memory consistency models. In Proceedings of the ninth an- nual ACM symposium on Parallel algorithms and architectures, SPAA 97, pages 199 210, 1997. [25] J. Renau, B. Fraguela, J. Tuck, W. Liu, M. Prvulovic, L. Ceze, S. Sarangi, P. Sack, K. Strauss, and P. Montesinos. SESC simulator, January 2005. http://sesc.sourceforge.net. [26] C. Scheurich and M. Dubois. Correct memory operation of cache- based multiprocessors. In Proceedings of the 14th annual interna- tional symposium on Computer architecture, ISCA 87, pages 234 243, 1987. [27] K. Sen. Race directed random testing of concurrent programs. In Proceedings of the 2008 ACM SIGPLAN conference on Programming language design and implementation, PLDI 08, pages 11 21, 2008. [28] P. Sewell, S. Sarkar, S. Owens, F. Z. Nardelli, and M. O. Myreen. x86-TSO: a rigorous and usable programmer s model for x86 multi- processors. Commun. ACM, 53(7):89 97, 2010. [29] D. Shasha and M. Snir. Ef cient and correct execution of parallel programs that share memory. ACM Trans. Program. Lang. Syst., 10(2):282 312, 1988. [30] A. Singh, D. Marino, S. Narayanasamy, T. Millstein, and M. Musu- vathi. Ef cient processor support for DRFx, a memory model with exceptions. In Proceedings of the sixteenth international conference on Architectural support for programming languages and operating systems, ASPLOS 11, pages 53 66, 2011. [31] T. F. Wenisch, A. Ailamaki, B. Falsa , and A. Moshovos. Mechanisms for store-wait-free multiprocessors. In Proceedings of the 34th annual international symposium on Computer architecture, ISCA 07, pages 266 277, 2007. [32] S. C. Woo, M. Ohara, E. Torrie, J. P. Singh, and A. Gupta. The SPLASH-2 programs: characterization and methodological consider- ations. In Proceedings of the 22nd annual international symposium on Computer architecture, ISCA 95, pages 24 36, 1995. [33] K. C. Yeager. The MIPS R10000 superscalar microprocessor. IEEE Micro, 16(2):28 40, 1996.