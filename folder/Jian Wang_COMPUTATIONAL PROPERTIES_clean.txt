COMPUTATIONAL PROPERTIES AND CONVERGENCE ANALYSIS OF BPNN FOR CYCLIC AND ALMOST CYCLIC LEARNING WITH PENALTY$ Jian Wanga,b, Wei Wua, Jacek M. Zuradab, aSchool of Mathematical Sciences, Dalian University of Technology, Dalian, 116024, P. R. China bDepartment of Electrical and Computer Engineering, University of Louisville, Louisville, KY 40292, U.S. Abstract Weight decay method as one of classical complexity regularizations is simple and appears to work well in some applications for backpropagation neural networks (BPNN). This paper shows results for the weak and strong conver- gence for cyclic and almost cyclic learning BPNN with penalty term (CBP-P and ACBP-P). The convergence is guaranteed under certain relaxed condi- tions for activation functions, learning rate and under the assumption for the stationary set of error function. Furthermore, the boundedness of the weights in the training procedure is obtained in a simple and clear way. Nu- merical simulations are implemented to support our theoretical results and demonstrate that ACBP-P has better performance than CBP-P on both con- vergence speed and generalization ability. Keywords: Weight decay, Backpropagation, Cyclic, Almost cyclic, Convergence $Project supported by the National Natural Science Foundation of China (No.11171367) and China Scholarship Council (CSC). Corresponding author. Email address: jmzura02@louisville.edu (Jacek M. Zurada ) Preprint submitted to Neural Networks April 23, 2012 1. Introduction Multilayer perceptron network trained with a highly popular algorithm known as the error backpropagation (BP) has been successfully applied to solve some di cult and diverse problems [1, 2]. This algorithm, based on the error-correction learning rule can be viewed as a generalization of the least- mean-square (LMS) algorithm. There are two main modes to implement it: batch learning, in which optimization is carried out with respect to all training samples simultaneously, and incremental learning, where it follows the presentation of each training sample [3]. There are three di erent incremental BP learning strategies: on-line learning, cyclic learning, and almost cyclic learning [4]. Incremental learning strategies require less storage capacity than batch mode learning. Due to the random presentation order of the training samples, incremental learning im- plementing the instant gradient of the error function is a stochastic process, whereas batch mode learning corresponds to the standard gradient descent method and is deterministic [4, 5, 6]. It is well known that the general drawbacks of gradient-based BPNN training methods are their more likely divergence and weak generalization. In real-world problems, the BP method is usually prone to require the use of highly structured networks of a rather large size [2]. Thus, it is requisite to reach an appropriate tradeo between reliability of the training and the goodness of the model. Knowing that the network design is statistical in nature, the tradeo can be achieved by minimizing the overall risk with regularization theory [7]. A general setting is to add an extra regularization term which is called penalty term for BPNN [2]. There are three classical di erent penalty terms for BPNN: weight decay [8], weight elimination [9] and approximate smoother [10]. In the weight de- cay procedure, the penalty term is stated as the squared norm of the weights in the BPNN [8, 11]. All the weights in the networks are treated equal- ly. Some of the weights are forced to take values close to zero, while other weights maintain reasonably large values, and consequently improve the gen- eralization of BPNN [2]. In the weight elimination procedure, the complexity penalty represents the complexity of the network as function of the weight magnitudes relative to a pre-assigned parameter [12]. The approximate s- moother approach is proposed in [10] for BPNN with a hidden layer and a single output neuron. This method appears to be more accurate than weight decay or weight elimination for the complexity regularization of BPNN. How- 2 ever, it is much more computationally complex than its counterparts [2]. Below we discuss the convergence of BPNN with penalty term from a mathematical point of view. Insofar as the satisfying performance in weight decay method, there are quantitative studies of the convergence property with di erent BP learning strategies [13, 14, 15, 16, 17, 18, 19]. For batch mode learning, the weak convergence and monotonicity are proved as a special case for the typical gradient descent method of opti- mization theory. A highlight in [13] is that the boundedness of the weights between input and hidden layers are guaranteed. As an extension, the bound- edness of the total weights in the BP feedforward neural networks based on batch learning has been proved in [14]. For online learning, [15] focuses on the linear output of BPNN, while an extension that the activation function satis es twice continuously di erentiable is proposed in [16]. The main con- tribution of these two papers is to theoretically prove the boundedness of the weights and an almost sure convergence of the approach to the zero set of the gradient of the error function. Assuming the training samples are supplied in random order in each cycle (almost cyclic), the monotonicity and weak convergence of the almost cyclic learning for BPNN with penalty term (ACBP-P) are guaranteed based on restricted conditions for activation functions and learning rates [17]. Addi- tionally, the results in [17] are valid for BPNN without hidden layer. On the basis of cyclic learning BPNN with penalty term (CBP-P), the convergence results are proved in [18, 19]. A momentum term to speed up the training procedure is considered as well in [19]. Within the framework of BPNN with cyclic and almost-cyclic learning, the latest convergence results concentrate on the regular BPNN [21] and on BPNN with momentum term [22] under much relaxed conditions such as activation functions and learning rates. The training method of BPNN based on the common gradient descent without any additional term is considered in [21]. Furthermore, the strong convergence result was rst proved which allows the stationary points of error function to be uncountable somehow. In [22], the weak and strong convergence results have been obtained for BPNN with momentum term which performs much better than regular BPNN. None of the earlier studies focused on convergence results for similar learning modes with penalty term based on relaxed conditions. This paper attempts to ll this gap. The aim of this paper is to present a comprehensive study for CBP- P and ACBP-P of weak and strong convergence with the identical relaxed 3 training conditions [21, 22], indicating that the gradient of the error function goes to zero and the weight sequence goes to a xed point, respectively. In comparison to the convergence results which consider the CBP-P and ACBP- P [17, 18, 19], quite simple and general conditions are formulated below for the learning rate and the activation functions to guarantee the convergence. The main points and novel contributions of this paper are as follows: 1) The derivatives g , f of the activation functions g, f are Lipschitz con- tinuous on R. This improves the corresponding conditions in [17, 18, 19], which requires the boundedness of the second derivatives g , f . From a mathematical point of view, we mention that di erent analytical tools are employed in [13, 17, 18, 19] and this study for the convergence analysis. The di erential Taylor expansion in [13, 17, 18, 19], which requires the boundedness of the second derivative of the activation function g, is considered, while in this paper, we discuss the integral Taylor expansion and hence require the Lipschitz continuity of g , f on R [20]. 2) The condition on the learning rate in this paper is extended to a more general case: m=0 m = ; m=0 2 m < , ( m > 0), which is identical to those in [21] for cyclic learning without penalty. Learning rate is an important criterion in the convergence analysis of BPNN. The convergence results in [19] for cyclic learning with penalty and momentum term focus on no hidden layer feedforward neural networks, and require 1 k+1 = 1 k + , (k N, > 0), where k is the learning rate of the k-th training cycle. Basically, this condition is equivalent to k = O ( 1 k ) . It is easy to see that the conditions on the learning rate are more relaxed in this paper than those in [17, 18, 19]. 3) The restrictive assumptions for the strong convergence in [13, 17, 19] are relaxed such that the stationary points set of the error function is only required not to contain any interior point. To obtain the strong convergence result, which means that the weight sequence converges to a xed point, an extra condition is considered in [13, 17, 18, 19]: the gradient of the error function has nitely many stationary points. Thus, this additional assumption is a special case in this paper (cf. (A3)). 4 4) The deterministic convergence results are valid for ACBP-P as well. We mention that CBP-P is typically a deterministic iteration procedure in that the updating fashion is deterministic for xed order of samples. Due to the random order of samples in each training cycle, the experiment shows that ACBP-P behaves numerically better than CBP-P [17]. In this paper, our convergence results are generalizations of both the results of [18], which considers CBP-P, and of the results of [17, 19], which considers ACBP-P. Remark: Considering the batch learning BPNN with penalty term we note that this method corresponds to the standard gradient descent algo- rithm. The convergence results are valid as well once the di erential Taylor expansion in [13] is replaced by the integral Taylor expansion in this paper. In addition, a simple and clear proof for the boundedness of the weights is presented. 5) Illustrated experiments have been done to verify the theoretical results of this paper, such as boundedness of the weights, convergence property of BPFNN with penalty term. Comparing to [23], three di erent simulations have been performed to demonstrate clearly the important properties of BPFNN with penalty ter- m. Furthermore, one of the classi cation simulations shows that ACBP-P performs generally much better than CBP-P. The rest of this paper is organized as follows: Section 2 introduces the two weights updating algorithms: CBP-P and ACBP-P. The main convergence results are presented in Section 3. The performance of the presented two algorithms are reported and discussed in Section 4. The detailed proofs of the main results are stated as Appendix for interested readers. 2. Algorithm Description Denote the numbers of neurons of the input, hidden and output layers of BPNN are p, n and 1, respectively. Suppose that the training sample set is {xj, Oj} J 1 j=0 Rp R, where xj and Oj are the input and the corre- sponding target output of the j-th sample, respectively. Let V = (vi,j)n p be the weight matrix connecting the input and the hidden layer, and write vi = (vi1, vi2, , vip)T for i = 1, 2, , n. The weight vector connecting the hidden and the output layers is denoted by u = (u1, u2, , un)T Rn. To 5 simplify the presentation, we combine the weight matrix V with the weight vector u, and write w = ( uT, vT 1 , , vT n )T Rn(p+1). Let g, f : R R be the activation functions for the hidden and output layers, respectively. For convenience, we introduce the following function G (z) = (g (z1) , g (z2) , , g (zn))T , z Rn. (1) For any given input x Rp, the output of the hidden neurons is G(Vx), and the actual output is y = f (u G (Vx)) . (2) For xed weights w, the output error is de ned as E(w) = 1 2 J 1 j=0 (Oj f(u G(Vxj)))2 + w 2 = J 1 j=0 fj(u G(Vxj)) + w 2, (3) where fj(t) = 1 2(Oj f(t))2, j = 0, 1, , J 1, t R and > 0 is the penalty coe cient. The gradients of the error function with respect to u and vi are given by respectively Eu(w) = J 1 j=0 ( Oj yj) f (u G(Vxj))G(Vxj) + 2 u = J 1 j=0 f j(u G(Vxj))G(Vxj) + 2 u, (4) Evi(w) = J 1 j=0 ( Oj yj) f (u G(Vxj))uig (vi xj)xj + 2 vi = J 1 j=0 f j(u G(Vxj))uig (vi xj)xj + 2 vi, (5) where yj = f(u G(Vxj)), i = 1, , n and j = 0, 1, , J 1. Write EV(w) = ( Ev1(w)T, Ev2(w)T, , Evn(w)T)T , (6) Ew(w) = ( Eu(w)T, EV(w)T)T . (7) 6 2.1. Cyclic Learning of BP with Penalty (CBP-P) Given an initial weight w0 Rn(p+1), the cyclic learning of BPNN with penalty term (CBP-P) updates the weights iteratively by umJ+j+1 = umJ+j m jumJ+j, (8) vmJ+j+1 i = vmJ+j i m jvmJ+j i . (9) where m > 0 is the learning rate for m-th cycle, kumJ+j = f j ( umJ+j GmJ+j, k) GmJ+j, k + 2 umJ+j, (10) kvmJ+j i = f j ( umJ+j GmJ+j, k) umJ+j i g ( vmJ+j i xk) xk + 2 vmJ+j i , (11) { GmJ+j, k = G(VmJ+jxk), ymJ+j, k = f(umJ+j GmJ+j, k), (12) m N; i = 1, 2, , n; j, k = 0, 1, , J 1. For brevity, the above weights updating indicates wmJ+j+1 = wmJ+j m jwmJ+j, (13) where wmJ+j = ( ( umJ+j)T , ( vmJ+j 1 )T , , ( vmJ+j n )T )T , (14) jwmJ+j = ( ( jumJ+j)T , ( jvmJ+j 1 )T , , ( jvmJ+j n )T )T . (15) 2.2. Almost Cyclic Learning of BP with Penalty (ACBP-P) The order of the training samples for CBP-P is xed in the whole training procedure. For almost cyclic learning of BP with penalty term (ACBP-P), each sample is chosen with a stochastic order and is fed exactly once in each training cycle. Let {xm(0), xm(1), , xm(J 1)} be a stochastic permutation of the samples set {x0, x1, , xJ 1}. The learning rate of the training pro- cedure is xed as m > 0 in the m-th cycle. The weights updating follows as: 7 wmJ+j+1 = wmJ+j m m(j)wmJ+j, (16) That is, umJ+j+1 = umJ+j m m(j)umJ+j, (17) vmJ+j+1 i = vmJ+j i m m(j)vmJ+j i . (18) where m(k)umJ+j = f j ( umJ+j GmJ+j,m(k)) GmJ+j,m(k) + 2 umJ+j, (19) m(k)vmJ+j i = f j ( umJ+j GmJ+j,m(k)) umJ+j i g ( vmJ+j i xm(k)) xm(k) + 2 vmJ+j i , (20) { GmJ+j,m(k) = G(VmJ+jxm(k)), ymJ+j,m(k) = f(umJ+j GmJ+j,m(k)), (21) m N; i = 1, 2, , n; j, m(k) = 0, 1, , J 1. 3. Summary of Main Results For any vector x = (x1, x2, , xn)T Rn, we write its Euclidean norm as x = n i=1 x2 i . Let 0 = {w : Ew(w) = 0} be the stationary point set of the error function E(w). Let 0,s R be the projection of 0 onto the s-th coordinate axis, that is, 0,s = { ws R : w = (w1, , ws, , wn(p+1))T 0 } (22) for s = 1, 2, , n(p + 1). To analyze the convergence of the algorithm, following assumptions are needed: (A1) g (t) and f (t) are Lipschitz continuous on R; (A2) m > 0, m=0 m = , m=0 2 m < ; (A3) 0,s does not contain any interior point for every s = 1, 2, , n(p+1). 8 Theorem 3.1. Assume the Conditions (A1) and (A2) are valid. Then, s- tarting from an arbitrary initial weight w0, the learning sequence {wm} gen- erated by (8) and (9) or by (17) and (18) is uniformly bounded, that is, there exists a positive constant C > 0 such that wm < C, (23) and satis es the following weak convergence lim m Ew (wm) = 0; (24) Moreover, if the assumption (A3) is also valid, there holds the strong conver- gence: There exists an unique w 0 such that lim m wm = w . (25) 4. Simulations In this section, three di erent simulations are presented to verify the con- vergence property of CBP-P and ACBP-P. In addition, the performance of CBP-P and ACBP-P with and without penalty are compared for: 4-Parity problem, regression and benchmark classi cations. The network architec- tures for each of the above problems are demonstrated below, respectively. The logistic function tansig( ) is employed as the activation function of hidden layer for all of the preceding networks, while the output activation function is di erent and depends on the network output in terms of the following d- i erent applications. To illustrate the convergence results in this paper, we have performed di erent trials: one trial for the rst two simulations, while twenty trials for the third classi cation problems. We note that the perfor- mance of CBP-P and ACBP-P is very similar with slight di erences such as e ectiveness and stochastic property. Thus, we verify the theoretical results of this paper based on CBP-P in the rst two examples and compare the performance of CBP-P and ACBP-P in the last example. 4.1. Example 1: 4-Parity Problem. In this example, the 4-Parity problem is considered for ve inputs (in- cluding bias), nine hidden units (including bias) and one output. All transfer functions are tansig( ). This experiment has been conducted by selecting the learning rate and penalty factor with di erent values from 0.1 to 0.5, 9 D E Figure 1: Performance behavior of CBP-P for Example 1, a) Error, b) Norm of gradient.  = 0.1 = 0.0003  = 0.1 = 0.0 Figure 2: Comparison between CBP-P and CBP of norm of weights for Example 1. and 0.001 to 0.0001, respectively. The initial weights are randomly chosen in [ 1, 1]. The training procedure is stopped after 10, 000 iterations or when the error is less than 1e 6. We note that the performance behavior of the above tests is consistent with the convergence results which proved in The- orem 3.1. We select one parity of the parameters to show and compare the performance with and without the penalty factor. The performance results of CBP-P are shown in Fig. 1 for = 0.1, = 0.0003 for 4-Parity problem. It can be seen that the error function decreases monotonically in Fig. 1(a), and the norm of the gradient of error func- tion approaches zero in Fig. 1(b), as depicted by the convergence results in (24). Fig. 2 demonstrates the e ectiveness of the algorithm in controlling the magnitude of weights. The norm of weights increases during the train- ing procedure without the penalty term, while the norm of weights initially 10 increases and then remains bounded with penalty term as indicated in the theoretical results (23). 4.2. Example 2: An approximation problem. In this subsection, we consider the following function demonstrated in [24] to show the function approximation capability of BPNN with penalty term. F(x) = 0.5x sin(x), x [ 4, 4]. (26) D E Figure 3: Approximation performance of CBP-P for Example 2, a) Target function and training samples, b) Approximation. The training pairs are generated as follows: 100 inputs (xi, i = 1, , 100) are randomly chosen from the interval [ 4, 4] with the corresponding outputs F(xi) + ei, where ei N(0, 0.1) is noise and N(0, 0.1) stands for the normal distribution with expectation and variance being 0 and 0.1, separately. The desired function and the training pairs ( ) are shown in Fig. 3(a). We construct one CBP-P network with 2 input neurons (including bias), 8 hidden neurons and 1 output neuron to implement this approximation problem. The activation function purelin( ) is employed for the output layer in terms of the special approximation problem (26). The initial weights are chosen stochastically in [ 1, 1]. The training parameters take the following settings: = 0.02 and = 0.0005, respectively. The stop criteria are set to be: 10, 000 training cycles or the desired error below 1e 6. Fig. 3(b) shows that CBP-P approximates the presented nonlinear func- tion (26) very well, which demonstrates that BPNN with penalty term can 11  = 0.02 = 0.0005  = 0.02 = 0.0 ---- ___ Figure 4: Comparison between CBP-P and CBP of norm of weights for Example 2. be successfully used for approximation problems. It can be seen that CBP-P in Fig. 4 can e ectively control the magnitude of the weights in the training procedure, which shows that the norm of weights for CBP-P tends to be steady. 4.3. Example 3: Benchmark classi cation problems. The CBP-P and ACBP-P methods have also been compared using 10 benchmark classi cation datasets from the UCI Machine Learning Repository [25] as shown in Fig. 5. In this example, 5-fold cross-validation has been performed, i.e. each dataset is randomly split 5 subsets with one set of the ve used as testing set while the four remaining subsets as training sets. Data Set Data Size Input Features Classes 5 fold Cross Validation 1. Breast Caner 286 9 2 2. Ecoli 336 7 8 3. Iris 150 4 3 4. Glass Identification 214 9 7 5. Liver Disorders 345 6 2 6. Monk s Problems 432 6 2 7. Diabetes 768 8 2 8. Splice-junction 3,190 61 3 9. Waveform Version 2 5,000 40 3 10. Mushroom 8124 22 2 Figure 5: Benchmark classi cation datasets for Example 3. 12 To compare the computational performance, all training parameters are identically chosen except for the order of the training, as indicated in Section 2. The original learning rate and penalty factor are set to be 0.1 and 0.0001, separately. The termination criteria are: 30, 000 training cycles or maximum error of 1e 5 for the rst 7 small size datasets and 400, 000 training cycles or maximum error of 1e 5 for the last 3 datasets. Data Sets Algorithm CPU time(s) Iterations Training Accuracy Testing Accuracy 1. Breast Caner CBP-P 5.2086 9.4458e+003 0.8834 0.8089 ACBP-P 4.8836 8.4998e+003 0.8698 0.8079 2. Ecoli CBP-P 0.0851 134.1 0.6990 0.6837 ACBP-P 0.0726 118.6 0.7025 0.6988 3. Iris CBP-P 10.8253 1.7771e+004 0.9790 0.9556 ACBP-P 11.3256 1.7822e+004 0.9800 0.9556 4. Glass Identification CBP-P 11.9971 2.0541e+004 0.7460 0.5378 ACBP-P 11.5307 2.0216e+004 0.7600 0.5625 5. Liver Disorders CBP-P 16.7298 2.6065e+004 0.6913 0.6327 ACBP-P 16.7099 2.6065e+004 0.6946 0.6462 6. Monk s Problems CBP-P 0.0846 115.6 0.6735 0.6254 ACBP-P 0.0636 99.6 0.6740 0.6362 7. Diabetes CBP-P 4.8798 7.8294e+003 0.7116 0.6774 ACBP-P 2.0673 3.5036e+003 0.7116 0.7048 8. Splice-junction CBP-P 43.7541 9.3581e+004 0.8746 0.8107 ACBP-P 29.0815 6.9210e+004 0.9031 0.8430 9. Waveform Version 2 CBP-P 91.6936 2.1409e+005 0.7718 0.7516 ACBP-P 72.7902 1.7937e+005 0.7905 0.7601 10. Mushroom CBP-P 71.9903 1.8061e+005 0.8639 0.7836 ACBP-P 54.0482 1.3983e+005 0.8821 0.8015 Figure 6: Comparison between CBP-P and ACBP-P for Example 3. Four performance metrics have been listed in Fig. 6. The CPU time measures the time when perform the training procedure. It can be seen that the training times for ACBP-P are less than CBP-P except for the Iris set. The main reason is that the stochastic nature survives in ACBP-P in which the order of training samples are randomly chosen. This shows that ACBP-P training runs much faster than CBP-P in terms of the stochastic property. Training and testing accuracies play a crucial role in measuring the performance of feedforward neural networks. Training accuracy presents the classi cation capability of BPNN in training procedure, while testing accuracy shows the generalization of BPNN. It can also be seen from Fig. 6 that ACBP-P does much better than CBP-P on the selected benchmark classi cation problems except for the rst Breast Cancer set. This demon- strates that ACBP-P has better generalization performance in terms of the stochastic property. 13 5. Conclusions Cyclic and almost cyclic learning of BPNN with penalty term (weight decay) are considered in this paper. The weak convergence which indicates that the gradient of the error function goes to zero as the iteration goes to in nity is proved under relaxed conditions of the activation functions and the learning rate. In comparison to existing convergence results, the assumption for the strong convergence in this study is a big-step extension as well. Il- lustrative experiments are implemented to illustrate theoretical results, and the comparison between CBP-P and ACBP-P shows that stochastic nature plays an important role in improving the performance of ACBP-P. Appendix The convergence proof for CBP-P is presented in the following Subsection A. Then, in Subsection B, we brie y point out how to extend the results to ACBP-P. The following three lemmas are very useful in convergence analysis for CBP-P and ACBP-P methods, and the speci c proofs are presented in [21]. Lemma 1: Let q(x) be a function de ned on a bounded closed interval [a, b] such that q (x) is Lipschitz continuous with Lipschitz constant K > 0. Then, q (x) is di erentiable almost everywhere in [a, b] and |q (x)| K, a.e. [a, b]. (27) Moreover, there exists a constant T > 0 such that q(x) q(x0) + q (x0)(x x0) + T(x x0)2, (28) where x0, x [a, b]. Lemma 2: Suppose that the learning rate m satis es (A2) and that the sequence {am} (m N) satis es am 0, m=0 ma m < and |am+1 am| m for some positive constants and . Then we have lim m am = 0. (29) Lemma 3: Let {bm} be a bounded sequence satisfying limm (bm+1 bm) = 0. Write 1 = limn infm>n bm, 2 = limn supm>n bm and S = 14 {a R : There exists a subsequence {bik} of {bm} such that bik a as k }. Then we have S = [ 1, 2]. (30) Lemma 4: Let Yt, Wt and Zt be three sequences such that Wt is nonneg- ative and Yt is bounded for all t. If Yt+1 Yt Wt + Zt, t = 0, 1, . (31) and the series t=0Zt is convergent, then Yt converges to a nite value and t=0Wt < . Proof: This Lemma follows directly from [20]. The following lemma is crucial for the strong convergence analysis, and it basically follows the same proof as in (21) of Theorem 3.1 in [21]. Its proof is thus omitted. Lemma 5: Let F : Rp R, (p 1) be continuous for a bounded closed region ( ), and 0 = {z : F (z) = 0}. If the projection of 0 on each coordinate axis does t contain any interior point. Let the sequence {zn} satisfy: (i) limn F(zn) = 0; (ii) limn zn+1 zn = 0. Then, there exists an unique z 0 such that lim n zn = z . A. Convergence Analysis for CBP-P For brevity, we introduce the following notations: Rm, j = m ( jumJ+j jumJ) , (32) rm, j i = m ( jvmJ+j i jvmJ i ) , (33) dm, l = umJ+l umJ = m l 1 k=0 kumJ + l 1 k=0 Rm, k, (34) hm, j i = vmJ+j i vmJ i = m j 1 k=0 kvmJ i + j 1 k=0 rm, k i , (35) 15 m, l, j = GmJ+l, j GmJ, j, (36) m, J, j = u(m+1)J G(m+1)J, j umJ GmJ, j. (37) where m N, j = 0, 1, , J 1, i = 1, , n and l = 1, 2, , J. The boundedness of the weight sequence is an important property for CBP-P. We rstly give the proof of the boundedness of the weight sequence. Proof to (23): By the assumption (A2), it is easy to know that limm m = 0. There exists a positive constant M1 N such that 1 2 m > 0, (m > M1) (38) Let A1 = max { umJ+j , m M1, j = 0, , J 1 } . Applying the assump- tion (A1), there exists a constant A2 > 0 such that A2 = sup { 1 2 g j ( umJ+j GmJ+j, k) GmJ+j, k } , where m N, j = 0, , J 1. Let A = max {A1, A2}. By the updating formulas (8) and (10), we have umJ+j+1 (1 2 m) umJ+j + m g j ( umJ+j GmJ+j, k) GmJ+j, k (1 2 m)A + 2 mA = A. (39) Using mathematical induction, it is easy to conclude that umJ+j A, j = 0, 1, , J 1, m N. Similarly, we can get that vmJ+j i (m N, i = 1, 2, , n, j = 0, 1, , J 1) is also bounded. Immediately, we obtain the uniform boundedness of the weight sequence {wm} wmJ+j C, j = 0, 1, , J 1, m N, (40) where C > 0 is a suitable constant. This proof is complete. Lemma 6: Assume condition (A1) is valid, and let the weight sequence wmJ+j be generated by (8)-(11). Then there are some positive constants C1-C6 such that GmJ+j, k C1, (41) dm, l C2 m, (42) m, l, j C3 m, (43) m, J, j C4 m, (44) Rm, j C5 2 m, (45) rm, j i C6 2 m, (46) 16 where m N; j, k = 0, 1 , J 1; l = 1, 2, , J and i = 1, 2, , n. The following lemma demonstrates an almost monotonicity of the error function during the updating procedure. Lemma 7: Let the weight sequence { wmJ+j} be generated by (8)-(11). Under condition (A1), there holds E ( w(m+1)J) E ( wmJ) m Ew ( wmJ) 2 + C7 2 m, (47) where m N and C7 > 0 is a constant independent of m and m. Proof: According to assumption (A1) and Lemma 1, we observe that g ( vmJ i xj + t ( hmJ i xj)) is integrable almost everywhere on t [0, 1]. Thus, f j ( umJ GmJ, j) umJ m, J, j = f j ( umJ GmJ, j) n i=1 umJ i g (vmJ i xj)hmJ i xj + f j ( umJ GmJ, j) n i=1 umJ i ( hmJ i xj)2 1 0 (1 t)g ( vmJ i xj + t ( hmJ i xj)) dt. (48) By virtue of (34) and (35), we have w(m+1)J 2 = u(m+1)J 2 + n i=1 v(m+1)J i 2 , (49) u(m+1)J 2 = umJ 2 + 2dm,J umJ + dm,J 2 , (50) v(m+1)J i 2 = vmJ i 2 + 2hm,J i vmJ i + hm,J i 2 . (51) Under assumption (A1), it is easy to see that f j is Lipschitz continuous. By (10), (11), (34), (35), (48), (49)-(51) and Lemma 1, we obtain that fj ( u(m+1)J G(m+1)J,j) =fj(umJ GmJ,j) + f j(umJ GmJ,j) ( dm,J GmJ,j + umJ m,J,j + dm, J m, J, j) + ( m,J,j)2 1 0 (1 t)f j ( umJ GmJ,j + t m,J,j) dt. (52) 17 Furthermore, we have ( m J 1 j=0 jumJ ) dm,J = J 1 j=0 jumJ 2 m J 1 j=0 jumJ J 1 j=0 Rm, j, (53) ( m J 1 j=0 jumJ ) hm,J i = m J 1 j=0 jvmJ i 2 m J 1 j=0 jvmJ i J 1 j=0 rm, j i . (54) On the basis of the above results, we can get that E ( w(m+1)J) = J 1 j=0 fj ( u(m+1)J G(m+1)J, j) + w(m+1)J 2 = E ( wmJ) m Ew ( wmJ) 2 + m, (55) where m = J 1 j=0 jumJ J 1 j=0 Rm, j + n i=1 (J 1 j=0 jvmJ i J 1 j=0 rm, j i ) + n i=1 J 1 j=0 f j ( umJ GmJ, j) umJ i ( hmJ i xj)2 1 0 (1 t)g ( vmJ i xj + t ( hmJ i xj)) dt + dm, J dm, J + n i=1 hm, J i hm, J i + J 1 j=0 f j ( umJ GmJ, j) dm, J m, J, j + J 1 j=0 ( m, J, j)2 1 0 (1 t)f j ( umJ GmJ, j + t m, J, j) dt. Using assumption (A1), (23) and Lemma 6, we can evaluate the rst term of m as follows: J 1 j=0 jumJ J 1 j=0 Rm, j J 1 j=0 jumJ J 1 j=0 Rm, j C7,1 2 m. (56) where C7,1 > 0 is a suitable constant. 18 Similarly, the evaluations for the other terms of m can be accessed with corresponding constants C7,t > 0 for t = 2, , 7. Thus, the desired evalua- tion (47) is obtained by setting C7 = 7 t=1 C7,t. Proof of (24): By the assumption (A2), Lemma 4 and Lemma 7, we conclude that m=0 m Ew ( wmJ) 2 = m=0 m ( Eu ( wmJ) 2 + EV ( wmJ) 2) < . (57) Naturally, it holds m=0 m Eu ( wmJ) 2 < . (58) Combining (4), (10), there exists a suitable constant C8 > 0 such that Eu ( w(m+1)J) Eu ( wmJ) J 1 j=0 ju(m+1)J jumJ C8 m. (59) A combination of (58), (59) and Lemma 2 immediately gives lim m Eu ( wmJ) = 0. (60) Considering the assumption (A2), it is to see that limm m = 0. By (41), we conclude that Eu ( wmJ+j) 1 m J 1 j=0 Rm, j + Eu ( wmJ) JC5 m + Eu ( wmJ) . (61) A combination of the above two Eqs. (60) and (61) gives that lim m Eu ( wmJ+j) = 0 for j = 0, 1, , J 1. Similarly, it holds that limm Evi ( wmJ+j) = 0 Thus, we have 19 lim m Ew (wm) = 0. (62) This completes the proof of weak convergence for CBP-P. Proof of (25): By the assumptions (A1), it indicates that Ew (w) is continuous. Combining (8), (9), (13) and (A2), we obtain that lim m w(m+1)J wmJ = 0. (63) According to the assumption (A3), (24), (63) and Lemma 5, there exists an unique w 0 such that lim m wmJ = w . (64) By the assumption (A2), (13) and the boundedness of jwmJ+j for m N, j = 0, 1, , J 1, it holds that lim m wmJ+j wmJ = 0. (65) Thus, we conclude that lim m wmJ+j = w , j = 0, 1, , J 1. (66) This immediately indicates the strong convergence of CBP-P. B. Convergence Analysis for ACBP-P Let the weight sequence { wmJ+j} (m N; j = 0, 1, , J 1) be updated by (17) and (18). The following notations for ACBP-P are introduced as: Rm,j = m ( m(j)umJ+j m(j)umJ) , (67) rm,j i = m ( m(j)vmJ+j i m(j)vmJ i ) , (68) dm,l = umJ+l umJ = m l 1 k=0 m(k)umJ + l 1 k=0 Rm,k, (69) 20 hm,j i = vmJ+j i vmJ i = m j 1 k=0 m(k)vmJ i + j 1 k=0 rm,k i , (70) m,l,m(j) = GmJ+l,m(j) GmJ,m(j), (71) m,J,m(j) = u(m+1)J G(m+1)J,m(j) umJ GmJ,m(j). (72) where m N, j, m(j) = 0, 1, , J 1, i = 1, , n and l = 1, 2, , J. We mention that the only di erence between CBP-P and ACBP-P is the order of training samples. Basically, the related Lemmas can be proved by adjusting the corresponding indexes of the formulas. In contrast to Lemma 6 and Lemma 7, we have the following Lemmas: Lemma 8: Assume condition (A1) is valid, and let the weight sequence wmJ+j be generated by (17)-(20). Then there exist some positive constants C1-C6 such that GmJ+j,m(k) C1, (73) dm,l C2 m, (74) m,l,m(j) C3 m, (75) m,J,m(j) C4 m, (76) Rm,j C5 2 m, (77) rm,j i C6 2 m, (78) where m N; j, m(k), m(j) = 0, 1 , J 1; l = 1, 2, , J and i = 1, 2, , n. Proof : According to the assumption (A1), it is obvious that the activation function g(t) is uniformly bounded on R. Thus, we obtain the same inequality result as (41): GmJ+j,m(k) = G ( VmJ+jxm(k)) n sup t R g(t) = C1. (79) Similarly, the remaining inequalities (74)-(78) can be estimated by ad- justing the corresponding superscripts. Lemma 9:Let the weight sequence { wmJ+j} be generated by (17)-(20). Under condition (A1), it holds E ( w(m+1)J) E ( wmJ) m Ew ( wmJ) 2 + C7 2 m, (80) 21 where m N and C7 > 0 is the same constant as in Lemma 7. Proof: It is easy to see that the proof can be completed by replacing the corresponding superscripts in Lemma 7. The details are left to interest readers and thus omitted. Proof of (24) and (25): For ACBP-P, the weak and strong convergence results can be similarly obtained in terms of Lemmas 1-5 and Lemmas 8-9. References [1] D.E. Rumelhart, et al., Parallel distributed processing: explorations in the microstructure of cognition. Cambridge, Mass.: MIT Press, 1986. [2] S.S. Haykin, Neural networks: a comprehensive foundation. Upper Sad- dle River, N.J.: Prentice Hall, 1999. [3] D. Saad, On-line learning in neural networks. Cambridge [England]; New York: Cambridge University Press, 1998. [4] T. Heskes and W. Wiegerinck, A theoretical comparison of batch-mode, on-line, cyclic, and almost-cyclic learning , IEEE Transactions on Neu- ral Networks, vol. 7, no. 4, pp. 919-925, 1996. [5] D.R. Wilson and T.R. Martinez, The general ine ciency of batch train- ing for gradient descent learning , Neural Networks, vol. 16, no. 10, pp. 1429-1451, 2003. [6] T. Nakama, Theoretical analysis of batch and on-line training for gra- dient descent learning in neural networks , Neurocomputing, vol. 73, no. 1-3, pp. 151-159, 2009. [7] A.N. Tikhonov, On solving incorrectly posed problems and method of regularization , Doklady Akademii Nauk USSR, vol. 151, pp. 501-504, 1963. [8] G.E. Hinton, Connectionist Learning Procedures , Arti cial Intelli- gence, vol. 40, no. 1-3, pp. 185-234, 1989. [9] A.S. Weigend, et al., Generalization by weight-elimination applied to currency exchange rate prediction , Proc. of Intern. Joint Conference on Neural Networks, Seattle, WA, vol. 1, pp. 837-841, 1991. 22 [10] J.E. Moody and T.S. Rognvaldsson, Smoothing Regularizers for Pro- jective Basis Function Networks , Advances in Neural Information Pro- cessing Systems, 1997. [11] K. Saito and R. Nakano, Second-order learning algorithm with squared penalty term , Neural Computation, vol. 12, no. 3, pp. 709-729, 2000. [12] R. Reed, Pruning Algorithms - a Survey , IEEE Transactions on Neu- ral Networks, vol. 4, no. 5, pp. 740-747, 1993. [13] W. Wu, et al., Convergence of Batch BP Algorithm with Penalty for FNN Training , in Neural Information Processing. vol. 4232, I. King, et al., Eds., ed: Springer Berlin / Heidelberg, pp. 562-569, 2006. [14] H.S. Zhang, et al., Boundedness of a Batch Gradient Method with Penalty for Feedforward Neural Networks , 12th WSEAS Int. Conf. on APPLIED MATHEMATICS, Cairo., pp. 175-178, 2007. [15] H. Zhang and W. Wu, Boundedness and Convergence of Online Gra- dient Method with Penalty for Linear Output Feedforward Neural Net- works , Neural Processing Letters, vol. 29, no. 3, pp. 205-212, 2009. [16] H.S. Zhang, et al., Boundedness and Convergence of Online Gradient Method With Penalty for Feedforward Neural Networks , IEEE Trans- actions on Neural Networks, vol. 20, no. 6, pp. 1050-1054, 2009. [17] H.M. Shao, et al., Convergence of online gradient method with a penal- ty term for feedforward neural networks with stochastic inputs , Numer- ical Mathematics: A Journal of Chinese Universities, vol. 14, no. 1, p. 10, 2005. [18] H.M. Shao, et al., Convergence and monotonicity of an online gradient method with penalty for neural networks , WSEAS Trans. Math., vol. 6, pp. 469-476, 2007. [19] H. Shao and G. Zheng, Boundedness and convergence of online gradient method with penalty and momentum , Neurocomputing, vol. 74, no. 5, pp. 765-770, 2011. [20] Z.B. Xu, et al., When Does Online BP Training Converge? , IEEE Transactions on Neural Networks, vol. 20, no. 10, pp. 1529-1539, 2009. 23 [21] W. Wu, et al., Convergence analysis of online gradient method for BP neural networks, Neural Networks, vol. 24, no. 1, pp. 91-98, 2011. [22] J. Wang, J. Yang and W. Wu., Convergence of Cyclic and Almost- Cyclic Learning With Momentum for Feedforward Neural Networks , IEEE Transactions on Neural Networks, vol. 22, pp. 1297-1306, 2011. [23] J. Wang, W. Wu and J. M. Zurada, Boundedness and Convergence of MPN for Cyclic and Almost Cyclic Learning with Penalty , Proc. of Intern. Joint Conference on Neural Networks, San Jose, California, pp.125-132, 2011, . [24] Y.J. Ren, Numerical analysis and the implementations based on Mat- lab, Beijing, Higer Education Press, 2007. [25] http://archive.ics.uci.edu/ml. 24