Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. 2375-4699/2023/1-ART1 $15.00 http://dx.doi.org/10.1145/3571584 ACM Trans. Asian Low-Resour. Lang. Inf. Process. Personality Detection using Kernel-based Ensemble Model for leveraging Social Psychology in Online Networks AKSHI KUMAR Dept. of Computing & Mathematics, Manchester Metropolitan University, Manchester, United Kingdom ROHIT BENIWAL Dept. of Computer Science & Engineering, Delhi Technological University, New Delhi, India DIPIKA JAIN* Dept. of Computer Science & Engineering, Delhi Technological University, New Delhi, India The Asian social networking market dominates the world landscape with the highest consumer penetration rate. Businesses and investors often look for winning strategies to attract consumers to increase revenues from sales, advertisements, and other services offered on social media platforms. Social media engagement and online relational cohesion have often been defined within the frameworks of social psychology and personality identification is a possible way in which social psychology can inform, engage, and learn from social media. Personality profiling has many real- world applications, including preference-based recommendation systems, relationship building, and career counseling. This research puts forward a novel kernel-based soft-voting ensemble model for personality detection from natural language, KBSVE-P. The KBSVE-P model is built by firstly evaluating the performance of various Support Vector Machine (SVM) kernels, namely radial basis function (RBF), linear, sigmoidal, and polynomial, to find the best-suited kernel for automatic personality detection in natural language text. Next, an ensemble of SVM kernels is implemented with a variety of voting techniques, such as soft voting, hard voting, and weighted hard voting. The model is evaluated on the publicly available Kaggle_MBTI dataset and a novel South Asian, Indian, low-resource Hindi language _MBT) (pronounced as vishesh charitr, meaning personality in Hindi) dataset for detecting a user's personality across four personality traits, namely introvert/extrovert (IE), thinking/feeling (TF), sensing/intuitive (SI), and judging/perceiving (JP). The proposed kernel-based ensemble with soft voting, KBSVE-P, outperforms the existing models on English Kaggle- MBTI dataset with an average F-score of 85.677 and achieves an accuracy of 66.89 for the (indi _MBT) dataset. CCS CONCEPTS Information systems Social networking sites; Computing methodologies Natural language processing; Machine learning Additional Keywords and Phrases: Social Networks, Personality Psychology, Natural language, kernel-based methods ACM Trans. Asian Low-Resour. Lang. Inf. Process. Table.1. Taxonomy Abbreviation Definition I/E Introvert-Extrovert N/S iNtution Sensing J/P Judging Perceiving T/F Thinking Feeling MBTI Myers Briggs Type Indicator MLP Multi-Layer Perceptron MCC Matthews Correlation Coefficient NLP Natural Language Processing NLTK Natural Language Tool Kit RBF Radial Basis Function ROC-AUC Area Under Receiver Operating Characteristic Curve SVM Support Vector Machine TN True Negative TP True Positive FN False Negative FP False Positive TF-IDF Term Frequency- Inverse Document Frequency 1 INTRODUCTION Personality is defined as the summation of the habits in which a person reacts and interacts with others [1, 2]. Certain determinants assist in the analysis of the personality of a person's heredity, environment, and situation. It can be characterized as psychological factors that affect the actions, thoughts, and feeling patterns of a person that distinguish characters from each other. No two individuals have the same personalities. Each of them has a unique character. The individuals might possess some identical characteristics, but not all the features can be similar. Personality profiling helps people become both self-aware and aware of how others around them think, feel, and act. Personality is usually measured through questionnaire-based psychometric testing, in which a series of questions are asked about the respondents behavior, thoughts, and emotions. This approach has been criticized for two reasons. First, it requires respondents to have good introspection and an accurate picture of who they are. Second, if these questionnaires are used in selection contexts, for example, in job hiring, some of the candidates could try to portray a desirable image of themselves to be hired. Recently, personality psychology can become an important tool for designing new applications, predicting the success of a new product, introducing new features to existing systems, or developing user adaptation approaches. The development of personality computing methods can be of high advantage to personality and social psychology. Computational technologies allow the processing of a larger volume of behavioral data that might be hard to analyze with techniques traditionally applied in psychology (observational methods, surveys, etc.). In this respect, personality computing might help to establish patterns of thought, emotion, and behavior from machine measurable cues, including affective computing [3,[4] (the domain dealing with emotions), social signal processing (the area related to non-verbal communication in public interactions) and sociolinguistics (the domain that analyses traces of social phenomena in language), etc. Social networks [5,6] define a complex global system that comprises several mathematical models and philosophies to study its behavior and to explain and predict its dynamics. Sometimes called "relationship networks," these help people and organizations connect online to share information and ideas. The benefits of using social networking [5] include market research, brand awareness, lead generation, relationship building, and customer service. Undoubtedly, social networks are a vast and growing body of research that has influenced the social, cultural [7], economic, and psychological aspects of people across the globe. In Asia, the online social networking landscape is even more dynamic. Asians are among the heaviest social media consumers in the world with the highest social media penetration rates across the globe. The most popular social network apps in the region include Facebook [8-12], WhatsApp, Messenger, Instagram, Youtube, Wechat, Line, Reddit [13], Twitter [14], and Snapchat. Fig. 1 shows the social media statistics in Asia from August 2021 to ACM Trans. Asian Low-Resour. Lang. Inf. Process. August 20221. Quite clearly, the Asian Social networks are a dominant and ongoing real-world phenomenon which have been top of mind for every investor, entrepreneur, business executive, and consumer. Fig. 1. Asian Social Network statistics Social networks [15] have unequivocally brought the largest upheaval to the social context, changing the mode in which people interact and engage with each other. Understanding this context is an important aspect of social psychology which is the scientific study of how individuals think, feel and behave in social conditions and situations. Social media [16] engagement and online relational cohesion have often been defined within the frameworks of social psychology [17] and personality identification is a possible way in which social psychology can inform, engage, and learn from social media. Further, multimodal [18] affective combinations of speaking style (prosody, intonation, etc.) and body movements can serve as cue accounting as features for personality traits. Personality theorists claim that a user's personality traits are a good starting point for predicting a user's social behavior, which can have a substantial influence on his/her preferences and decisions. Recognizing personality types that can aid in understanding people's preferences and associated cognitive processes is a well-studied problem of computational psychology. Real-time analytics of personality attributes has appeared as a portion of vibrant market research and branding approaches that can facilitate the personalization of web, recommendation services, and intelligent empathetic conversational agents. Various personality trait theories have been developed to categorize, interpret, and understand the human personality. These include Cattell's Sixteen Personality Factor [11], (ans Eysenck s psychoticism, extraversion, and neuroticism, Myers Briggs Type Indicator, and Big-Five [27]. Further, according to the input data type, automatic personality trait recognition is categorized into unimodal and multimodal. Specifically, unimodal contains the audio only, text only, visual (static images and dynamic videos) or only physiological signals for personality trait recognition, and multimodal personality trait recognition, integrating multiple modalities of behavioral data, such as audio, visual, and text information. In recent years, several datasets with different data modalities for personality detection have been created, such as Big Five Personality Tests, Myers Briggs Type Indicator (Kaggle_MBTI), Standard Essays, First Impression- 1 Social Media Stats Asia | Statcounter Global Stats ACM Trans. Asian Low-Resour. Lang. Inf. Process. Chalearn dataset, multimodal databaASe for impliCit pERsonaliTy and Affect recognitIoN (ASCERTAIN) [27], and the Personality and Demographics of Reddit Authors (PANDORA) dataset [13]. Fig.2 depicts the key modality features reported in the literature on automatic personality detection. Fig. 2. Modality features for automatic personality detection Text-based personality prediction from multiple social media data sources has gained much attention among researchers in recent years. Kaggle_MBTI is a popular text-based personality dataset that was collected using the PersonalityCafe forum. It consists of 8600 rows elaborating 8 distinct types of personality explaining the perspective of cognitive functions of Jungian [19] theory/ topology. The dataset has a four-letter code for each of the sixteen personality types distributed over four axis as shown in fig.3. These dimensions and their corresponding possibilities are Introversion (I) Extroversion (E), Intuition (N) Sensing (S), Thinking (T) Feeling (F), and Judging (J) Perceiving (P) and indicates that each person will have a personality abbreviated such as ENFJ, INFJ, ENFP, INFP, ENTJ, INTJ, ENTP, INTP, ESFJ, ISFJ, ESFP, ISFP, ESTJ, ISTJ, ESTP, and ISTP from the combination of all 4 axis. ACM Trans. Asian Low-Resour. Lang. Inf. Process. Fig. 3. MBTI personality types Pertinent literature reports the use of deep learning techniques for text based Kaggle_MBTI personality trait recognition tasks. In real-world applications, the most common data type is tabular data, comprising samples (rows) with the same set of features (columns) but recent studies report the challenges with deep learning models when applied to tabular data such as Kaggle_MBTI. These include missing values, multimodality, and lack of locality. Moreover, the real- world datasets Kaggle_MBTI are probably never linearly separable, especially that focus on multi-class classification, and therefore higher accuracy by hyperplanes is challenging. Most of the learning models consider all the Kaggle_MBTI classes independent of each other and fail to capture the in-built relatedness of some types to other types, for example, INFP is much more like INTJ as compared to ESTJ. Moreover, with the growth of multilingual social media, the online use of local and regional languages is also rapidly expanding. However, most of the research on MBTI personality traits on social media has been restricted to the English language only. Hindi is ranked among the 3rd most spoken languages in the world, with 615 million active speakers [20] and various studies report the increased use of Hindi in social media interactions [21-25]. It will therefore be noteworthy to study and evaluate automatic personality detection in a low- resource Indian language, Hindi. This research sets forward a kernel-based soft voting ensemble for personality detection in natural language textual data. The Support Vector Machine (SVM) [12] with soft voting ensembled kernels is used as a classifier to detect different traits of the MBTI personality type in an English and a Hindi dataset. SVM s Kernel Trick addresses non-linearly separable cases as it utilizes existing features, applies some transformations, and creates new features and these new features are the key for SVM to find the non-linear decision boundary. Simultaneously, the soft voting ensemble involves summing the predicted probabilities for class labels and predicting the class label with the largest sum probability. Thus, the primary contribution of this work is: To develop an efficient kernel-based ensemble model for text-based personality prediction. To evaluate the performance of different SVM kernels and detect the best-suited kernel for developing models for automatic personality detection in textual data, English and Low-resource South Asian language, Hindi To build an MBTI trait-based Hindi personality dataset, _MBTI (pronounced as vishesh charitr, meaning personality in Hindi) ACM Trans. Asian Low-Resour. Lang. Inf. Process. To evaluate the performance of ensembles of different SVM kernels aggregated with soft voting, hard voting, and weighted hard voting. The organization of the paper is as follows: The next section briefs about the related work within the domain of personality detection followed by the discussion on the methodology used and the proposed model in section 3. Section 4 layout the details of model performance and the conclusion is given in section 5. 2 RELATED WORK Overall, personality is relevant to any computing area relating to understanding, prediction, or synthesis of human behavior. Still, while being different and diverse in terms of data, technologies, and methodologies, all computing domains concerned with personality consider the following three main problems: identification or detection of the true personality of an individual (Automatic Personality Detection) estimation of the personality traits of a given individual (Automatic Personality Perception), fabrication of artificial personalities through embodied agents (Automatic Personality Synthesis). This research primarily focuses on the problem of automatic personality detection. Some of the work conducted by researchers for identifying the personality of an individual is on unimodal data (text-based or video-based), whereas some are on multimodal data [6], which includes bimodal (audio-visual) and trimodal (audio-visual-textual) data. Based on the modalities, various datasets with a wide spectrum of distal cues, including written texts, nonverbal behavior, and, more recently data collected via mobile or wearable devices and online games have been available. The popular datasets include the Myers-Briggs Type Indicator (Kaggle_MBTI), Essays, myPersonality, and Chalearn. Big Five Personality Test is a textual dataset based on the answers for the questionnaire asked via the online platform open psychometric. It has a collection of 1,015,342 tuples concentrated over the five-factor model or the OCEAN model for personality traits. Kaggle_MBTI is another popular text-based personality dataset based on Carl Jung s [19] theory collected using the PersonalityCafe forum platform consisting of 8600 rows elaborating 8 distinct types of personality explaining the perspective of cognitive functions of Jungian theory/ topology. The dataset has 4 letter code for each of the 16- personality types distributed over 4 axes as explained above in figure-2. Essays dataset is also named as stream-of-consciousness essays or gold-standard essays is also a unimodal dataset consisting of texts. It has 2400 instances compiled by Pennebaker and King (1999) [26] from the essays written by students for the study of different linguistic styles of different individuals. First Impression V2 (CVPR 17) is a multimodal dataset consisting of visual and audio modalities collected from about 3000 youtube videos and converted into 10000 short clips of 15 seconds each. ASCERTIAN is a Big-5 personality trait dataset abbreviated for a multimodal databaASe for impliCit pERsonaliTy and Affect recognitIoN which focused on the physiological signals such as EEG, ECG, GSR, and facial activity. This shows an interrelation between emotions and personality traits for 58 users. In 2012, Lee et al. [27] proposed a novel algorithm, namely MBTI-based-Evolutionary Algorithm for a Genetic Robot s Personality (MBTI-EAGRP), which identified the characteristics and behavior by measuring the user preference via the assessment method. This novel algorithm worked in two phases namely, Self Organizing Map-MBTI (SOMMBTI) and Evolutionary Algorithm for Genetic Robot Personality (EAGRP). The first phase mapped a user feature using Neural Networks with self-organizing maps. Thereafter, the second phase of the model precisely matched the genome of the robot. This application was designed to be used over mobile phones. In 2017, Cui and Qi [7] used Na ve Bayes and SVM to analyze the dataset and it was found that the Na ve Bayes classifier provided 26% accuracy with overfitting data while SVM conceded 38% accuracy. Moreover, they also used the Deep Learning techniques where multi-layer LSTM was used and it yielded an accuracy of 36%. In the end, their results concluded that the performance of Deep Learning was better as compared to SVM and Na ve Bayes. Hernandez and Scott in 2017 [28] used different RNN models such as SimpleRNN, GRU, LSTM, and Bi-directional LSTM for MBTI personality traits. They concluded that the performance of LSTM was found best among all four used models. In 2019, Keh and Cheng [29] used the personality caf forums dataset to deploy MBTI personality traits. They built a model after fine-tuning the BERT model to perform the classification task. At last, the model provided 49% accuracy. ACM Trans. Asian Low-Resour. Lang. Inf. Process. In 2021, Ren et al. [30] pointed out the ignorance of contextual information and polysemous words while examining different personalities and proposed a model for multi-label personality detection, which combined the pre-trained BERT model with a Neural Network to understand the text and semantics of social media. Additionally, they proposed a method to combine emotions with semantic features for detecting various personality traits. Cerkez et al. [31] also worked upon Kaggle_MBTI personality traits and proposed a technique as well as a loss function for multi-class text classification to detect personalities. They reported that the results of LSTM and CNN models were better as compared to Multi-Layer Perceptron, Na ve Bayes, and Logistic Regression. Basto C. [32] also worked upon Kaggle_MBTI and introduced data centric approach using three primary features such as sentimental, grammatical and aspects. Shafi, H. [33] worked on the MBTI framework using course tree, fine KNN, weighted KNN, ensembled bagged tree and ensembled boosted tree algorithms. A testing accuracy of 70.75% and 75.51% was respected achieved for the bagged and boosted ensemble models. In 2022, Bokishev[34] worked on Kaggle_MBTI using three models namely, multimodal logistic regression, linear support vector classifier, and XGBoost Classifier. Amongst the three models, XGBoost had the best model with a training accuracy of 92%.The existing works based on MBTI personality trait theory are restricted to the English language only. Few attempts on personality detection in other languages exists but are based on other trait theories such as Big 5 [35][36]. In 2020, Gjurkovic et al. [13] developed the first dataset, Pandora, based on the personality and demography of around 10 thousand Reddit users. This dataset supports MBTI, Big-5, and Enneagram personality models. Apart from the MBTI personality label, Pandora also had other labels such as age, gender, location, and (sociolinguistic) language. Furthermore, with Pandora, they achieved an accuracy of 45% for the MBTI personality traits. It is evident from the literature that so far, personality detection techniques address traits separately because these are supposed to be, by construction, uncorrelated and independent. However, it is imperative to capture the in-built relatedness of some types. At the same time, current studies on personality detection are based on English language sociolinguistic cues and therefore studies in local, low-resource languages such as Hindi needs further exploration. Motivated by these gaps, this research proffers a novel model for automatic personality detection 3 METHODOLOGY The novel kernel-based soft-voting ensemble model for personality detection from natural language, KBSVE-P is built by firstly evaluating the performance of various Support Vector Machine (SVM) kernels, namely radial basis function (RBF), linear, sigmoidal, and polynomial, to find the best-suited kernel for automatic personality detection in natural language text. Next, an ensemble of SVM kernels is implemented with a variety of voting techniques such as soft voting, hard voting, and weighted hard voting. The model is evaluated on the publicly available Kaggle MBTI dataset for detecting a user's personality using various performance metrics. Simultaneously, a MBTI based Hindi personality dataset, _MBT) (pronounced as vishesh charitr, meaning personality in Hindi) is built and the KBSVE-P model is evaluated on this dataset too. The following sub-sections expound on the details of the methodology used for personality detection in textual data. 3.1 Dataset In this research, two textual datasets have been used, Kaggle-MBTI [38][7] and _MBT). The concept of Kaggle_MBTI is that every persona consists of four magnitudes, and each magnitude has 2 possibilities. These dimensions or the magnitudes and their corresponding possibilities are Introversion (I) Extroversion (E), Intuition (N) Sensing (S), Thinking (T) Feeling (F), and Judging (J) Perceiving (P). In total, this dataset has 8675 rows along with 16 combinations or personality types, which are denoted using a four-letter abbreviation. It means that each person will have a personality abbreviated such as ENFJ, INFJ, ENFP, INFP, ENTJ, INTJ, ENTP, INTP, ESFJ, ISFJ, ESFP, ISFP, ESTJ, ISTJ, ESTP, and ISTP from the permutation of all the four axis. For example, someone who is an extrovert, relies more on sensing, thinking, and perceiving rather than judging and will be labeled as an ESTP. As far as the data in the Kaggle_MBTI is concerned, in a single row, it consists of around 50 social media posts of a specific user separated by "|||". The dataset has no null or missing values neither in the type of attribute nor in the post attribute. The following fig. 4 shows the personality traits distribution in the Kaggle_MBTI dataset. ACM Trans. Asian Low-Resour. Lang. Inf. Process. Fig. 4. Personalities distribution in the MBTI dataset The above fig. 3 shows that the Introversion-Intuition-Feeling-Perception (INFP) trait has the maximum number of posts, while the Extroversion-Sensing-Thinker-Judgemental (ESTJ) has the least number of posts in the dataset. Moreover, the percentage share of each personality trait is clearly visible from the above figure. We also developed a South Asian, Indian low-resource Hindi language dataset, _MBT) by annotating the utterances (quotes) of different people on Google using Google API. The dataset has 1824 rows annotated with 16 labels out of which a four-letter abbreviation is used for each personality trait with the help of a psychology expert. Fig.5 shows the annotated example of the first five rows of the _MBT) dataset. Fig. 5. Sample of Hindi dataset _MBT) ACM Trans. Asian Low-Resour. Lang. Inf. Process. 3.2 Data Preprocessing Data pre-processing is done to clean the data, remove unnecessary and error-prone data, and tokenize textual data to ultimately transform it from unstructured to structured data. For data cleaning, a regular expression is utilized to remove URLs, punctuations, unnecessary string marks, and numerical values. NLTK library is used to remove English stop words, which add very little meaning and are predominantly present in textual data and can cause errors in feature extraction and reduce overall model accuracy. Data cleaning also includes the lemmatization of data and converting MBTI 4 letter code to binary. Lemmatization converts all words into their original form. For example, 'playing' will be converted to 'play', and similarly, 'running' will be converted to 'run'. Converting the MBTI letter code to binary means assigning value 0 or 1 to each character so that it is easier for the model to distinguish between two characteristics. Here E, S, F, and P are all assigned with 0, whereas I, N, T, and J are all assigned with 1. Therefore, personality type INFJ will have [1,1,0,1] value. Thereafter, the data will be split randomly into train and test sets in the ratio of 75:25 (3:1), respectively as it was seen as efficient after regressive tuning of trainable dataset ratios. 3.3 Feature Extraction The CountVectorizer transformation is used to convert tokenized textual data into a token count matrix. Term Frequency- Inverse Document Frequency (TF-IDF) [15][38] transformation is used to convert the matrix into a normalized TF-IDF representation. It is used to analyze how much a word is relevant in the collection of sentences. Here, TF is an estimate of the frequency of a word that appears in a document, while IDF represents how important a word is in a set of documents. It is calculated based on how rarely a word appears in a set of documents. The formulas used for the calculation of TF-IDF are as follows: TF-IDF(word,document) = TF(word,document)*IDF(word) (1) IDF(word) = log[n/(DF(word)+1)], if smooth_idf=False (2) or IDF(word) = log[n+1/(DF(word)+1)]+1, if smooth_idf=True 3.4 Proposed Kernel-Based Ensemble Model The architectural flow of the proposed a kernel-based ensemble model for personality detection, KBSVE-P is shoen in fig. 6. The sub-sections discuss the details of the architecture. 3.4.1 Support Vector Machine (SVM) The purpose of an SVM is to classify the given data into two classes by finding an appropriate hyperplane that can separate the classes effectively. SVM uses different linear and non-linear kernels to find the best-suited hyperplane. Support Vector machines can be used in case of non-regularly distributed data and data with unknown distribution; hence it is suitable for textual data. It efficiently finds hyperplanes to separate two classes present in any distribution due to the availability of different kernels that define equations for hyperplanes. The SVM kernel used for the classification of textual data includes the Linear Kernel, Radial Basis function kernel (RBF), Sigmoid kernel, and Polynomial Kernel. In SVM, kernel implies internal feature transformation. It is a way to compute the dot product of two vectors in a higher dimensional feature space. That is, the kernel method helps in calculating the dot product in a different space without even visiting the original space. The time taken to compute the standard dot product is O(n2), whereas the kernel takes only O(n) time to calculate the dot product. RBF is used in SVMs to help SVM to become non-linear rather than linear. A linear kernel is mainly used when we have a linearly separable set of data points. A polynomial kernel is a non-stationary kernel, which is well suited for problems where all the training data is normalized. To find the best suited SVM kernel for classification from textual data, we train four SVM models, one model for each kernel. The Kernels mentioned above are the same kernels used for analyzing the performance of an ensemble of SVM kernels aggregated with the three voting techniques, namely soft voting, hard voting, and weighted hard voting. ACM Trans. Asian Low-Resour. Lang. Inf. Process. Fig. 6. Kernel-based Ensemble model (KBSVE-P) 3.4.2 Voting Voting classifiers are ensemble methods for decision-making and are further divided into 2 categories: soft and hard voting. Hard voting entails picking the prediction with the highest number of votes, whereas soft voting entails combining the probabilities of each prediction in each model and picking the prediction with the highest total probability. This voting [38] classifier built by combining different classification models turns out to be a stronger meta-classifier that balances out the individual classifier's weakness on a particular dataset. It takes majority voting based on weights applied to the class or class probabilities and assigns a class label to a record based on the majority vote. A soft voting classifier is one in which the output class is predicted based on the average probability given to that class by different classifiers. The class with the highest average probability will be the final output of the ensemble. In soft voting, every individual classifier provides a probability value that a specific data point belongs to a particular target class. The predictions are weighted by the classifier's importance and summed up. Then the target label with the greatest sum of weighted probabilities wins the vote. Assume that the prediction probability of class A by 3 different classifiers is given as [0.30, 0.47, 0.53] and the prediction probability of class B is [0.20, 0.32, 0.40]. Therefore, the average prediction probability of class A is 0.43 and B is 0.306. Hence the output of the ensemble aggregated with soft voting will be class A. In hard voting classification, the output class is predicted based on the number of votes given to the concerned class. The class with the majority of votes is the output class. In hard voting, the predictions of each algorithm are considered with the ensemble selecting the class with the highest number of votes. For example, if three algorithms predict the color of a particular wine as white, white, and red, the ensemble will predict red. It is also called majority voting. Suppose that the predicted output of 3 different classifiers is class A, A, and B; it results in the majority predicted class 'A' as an output. Hence, the output of the ensemble aggregated with hard voting will be class A. Weighted hard voting is an extension of hard voting where different classifiers may have different weights. The class with the majority votes will be predicted as the output class, but every classifier may have different weights associated with them, which means the output of a particular model may be more significant than others. Fig.7 depicts diagrammatically representing these voting classifiers. ACM Trans. Asian Low-Resour. Lang. Inf. Process. Fig.7. Voting Classifiers (Soft Voting, Hard Voting, and Weighted-Hard Voting) 4 RESULTS & DISCUSSION The model implementation is done using Google Colab and Python libraries (TensorFlow and Keras along with the basic NumPy, Pandas, Scikit -Learn, and Matplotlib libraries). After several tests, it was found that the best ratio for weights is as follows: Linear:Polynomial:Radial basis:Sigmoid::3:2:3:2 The following metrics are used to analyze the performance of individual kernels, voting techniques, and the proposed model: Accuracy: It is defined as the ratio of correct predictions (TP and TN) to the total number of samples (all entries of the confusion matrix summed up). Equation (4) defines the accuracy, which is as follows: Accuracy = (4) Precision: It measures how many of the samples predicted as positive are actually positive. Thus, it is defined as the ratio of true positive (TP) values to the summation of all predicted positive values (TP and FP). Equation (5) specifies the precision, which is as follows: Precision = (5) Recall: It measures how many of the positive samples are captured by the positive predictions. Therefore, it is defined as the ratio of true positive (TP) values to the summation of all actual positive values (TP and FN). Equation (6) characterizes the recall, which is as follows: Recall = (6) F1-Score: It is also termed as F-score and is an important metric for measuring a model s performance. It is defined as the harmonic mean of precision and recall. Equation (7) defines the F1-Score, which is as follows: F1- Score = (7) ROC-AUC: ROC-AUC is the Area Under Receiver Operating Characteristic Curve that summarizes the performance of a classifier over all possible thresholds. The ROC curve plots the true positive rate also known as recall on the y-axis against the false positive rate on the x-axis. MCC: MCC stands for Matthews Correlation Coefficient, which measures the correlation coefficient between the predicted values and actual values. Its values range between -1 and +1. A coefficient of +1 represents a perfect prediction, 0 an average random prediction, and -1 in an inverse prediction. Equation (8) specifies the MCC, which is as follows: MCC = (8) where; TP: True Positive, TN: True Negative, FP: False Positive, and FN: False Negative Now, we divide this section into the following four subsections. In the first and second subsections, we discuss the performance of individual kernel models and voting techniques using the six performance matrices as mentioned above. Afterward, in the third subsection, we compare the results of the KBSVE-P model with the state-of-the-art. At last, in the ACM Trans. Asian Low-Resour. Lang. Inf. Process. fourth subsection, we evaluate and observe the results of the KBSVE-P model on the South Asia low-resource Hindi Language dataset. 4.1 Performance of Kernel models: Kaggle_MBTI This subsection discusses the results obtained after training and testing four different kernels such as RBF, linear, sigmoidal, and polynomial. All the personality traits have been studied over the four kernels. The performance of all the SVM kernels is evaluated using the six metrics discussed above in the preceding subsection. The results are then tabulated in Table 2. F1-Score is evaluated above accuracy because the accuracy metric is more delicate toward the distribution of target values; thus, it is important to capture a classifier's specificity, performance, and sensitivity on an imbalanced dataset. Table 2. Performance of different SVM kernels: Kaggle_MBTI Kernels Characteristic Accuracy (%) Precision (%) Recall (%) F1- Score (%) ROC-AUC (%) MCC (%) Training Time (in sec) Confusion Matrix RBF I/E 85.62 86.20 96.97 91.27 71.74 54.3 132.5 [227, 261], [51, 1630] N/S 88.24 88.81 98.82 93.55 60.32 36.2 111.3 [65, 233], [ 22,1849] T/F 85.52 83.69 84.54 84.11 85.44 70.8 144.1 [1024,162], [152, 831] J/P 79.48 80.90 63.10 70.90 76.66 56.4 168.1 [1182,128], [317, 542] LINEAR I/E 86.17 87.71 95.54 91.46 74.72 56.9 82.9 [263, 225], [75, 1606] N/S 89.67 90.63 98.18 94.25 67.21 47.9 60.5 [108, 190], [34, 1837] T/F 84.42 82.87 82.71 82.79 84.27 68.5 88.1 [1018, 68], [170, 813] J/P 79.35 78.42 66.01 71.68 77.05 56.1 109.1 [1154,156], [292,567] SIGMOID I/E 86.03 87.49 95.66 91.39 74.26 56.3 81.8 [258, 230], [73, 1608] N/S 89.40 90.20 98.40 94.12 65.64 45.7 58.6 [98, 200], [30, 1841] T/F 84.56 82.93 83.01 82.97 84.42 68.8 86.4 [1018,168], [167,816] J/P 79.16 78.38 65.42 71.32 76.80 55.7 107.5 [1155,155], [297,562] POLYNOMIAL I/E 83.31 83.36 98.04 90.10 65.31 45.1 201.4 [159, 329], [33, 1648] N/S 87.09 87.47 99.25 92.99 54.99 23.8 190.8 [32, 266], [14, 1857] T/F 84.65 81.99 84.74 83.34 84.66 69.1 207.1 [1003,183], [150,833] J/P 78.61 85.08 55.76 67.37 74.68 55.1 209.3 [1226, 84], [380, 479] I/E: Introvert-Extrovert; N/S: iNtution-Sensing; T/F: Thinking-Feeling; and J/P: Judgemental-Perceiving. From Table 2, the following observations are made. First, the accuracy of the linear kernel is found to be highest in the case of I/E and N/S personality traits. Whereas in the case of T/F and J/P personality traits, the accuracy of RBF is highest. ACM Trans. Asian Low-Resour. Lang. Inf. Process. Second, the F1 score of the linear kernel turned out to be the highest in the case of I/E, N/S, and J/P personality traits. On the other hand, RBF shows the highest FI score in the case of the T/F personality trait. Third, the ROC-AUC of the linear kernel is found to be highest in the case of I/E, N/S, and J/P personality traits. Whereas in the case of T/F personality trait, the ROC -AUC of RBF is highest. Fourth, the MCC of the linear kernel turned out to be the highest in the case of I/E and N/S personality traits. On the other hand, RBF shows the highest MCC in the case of T/F and J/P personality traits. At last, from the above four observations, it can be said that the linear kernel performs better in most of the performance matrices when compared to other kernels. The corresponding visualization of accuracies and F1-scores are shown in the following fig. 8 and 9, respectively. Fig. 8. Accuracies of different SVM kernels: Kaggle_MBTI Fig.8 shows that the accuracy of the linear kernel is highest in the case of I/E (86.17) and N/S (89.67) personality traits. On the other hand, the accuracy of RBF is highest in the case of J/P (79.48) and T/F (85.52). Fig. 9. F1-Score of different SVM kernels: Kaggle_MBTI [VALUE] [VALUE] [VALUE] [VALUE] [VALUE] [VALUE] [VALUE] [VALUE] [VALUE] [VALUE] [VALUE] [VALUE] [VALUE] [VALUE] [VALUE] 72 74 76 78 80 82 84 86 88 90 92 I/E J/P N/S T/F Accuracy (%) Personality Traits Accuracy of Different SVM Kernels Linear Sigmoid RBF Polynomial 84.65 [VALUE] [VALUE] [VALUE] [VALUE] [VALUE] [VALUE] [VALUE] [VALUE] [VALUE] [VALUE] [VALUE] [VALUE] [VALUE] [VALUE] [VALUE] [VALUE] 0 10 20 30 40 50 60 70 80 90 100 I/E N/S T/F J/P F1-score (%) Personality Traits F1- Score of Different Kernels Linear Sigmoid RBF Polynomial ACM Trans. Asian Low-Resour. Lang. Inf. Process. Further from fig. 9, we can infer that the F1 score of the linear kernel is highest in the case of I/E (91.46), N/S (94.25), and J/P (71.68) personality traits, whereas in the case of T/F personality trait, RBF shows the highest F1 score. Hence, we conclude that the linear kernel performs better in 3 out of 4 cases as compared to the other three kernels. The results show that the linear kernel performs best among all kernels with average accuracy and F1 score of 84.7 and 84.9, respectively for the personality traits. 4.2 Performance of voting techniques: Kaggle_MBTI The results obtained after training and testing SVM kernels over the MBTI personality traits aggregated with the three different voting techniques such as soft voting, hard voting, and weighted hard voting are depicted in Table 3. Table 3. Performance of ensemble of SVM kernels aggregated with different voting techniques Voting Characteri stic Accuracy (%) Precision (%) Recall (%) F1-Score (%) ROC-AUC (%) MCC (%) Training Time (In Second) Confusion Matrix SOFT VOTING IE 86.35 88.24 95.06 91.52 75.71 57.8 2264.6 [275, 213], [ 83, 1598] NS 90.27 91.58 97.70 94.54 70.66 52.5 1884.4 [130, 168], [43, 1828] TF 85.38 83.70 84.13 83.92 85.28 70.5 2380.1 [1025,161], [156,827] JP 79.94 78.80 67.52 72.73 77.81 57.4 2695.3 [1154,156], [279,580] HARD VOTING IE 86.17 87.59 95.72 91.47 74.50 56.8 507.1 [260, 228], [72, 1609] NS 89.44 90.25 98.40 94.14 65.81 46.1 408.8 [99, 199], [ 30, 1841] TF 84.88 84.01 82.30 83.14 84.66 69.4 512.7 [1032,154], [174,809] JP 79.07 80.82 61.82 70.05 76.10 55.5 597.3 [1184,126], [328, 531] WEIGH TED HARD VOTING IE 86.17 87.59 95.72 91.47 74.50 56.8 476.2 [260, 228], [ 72, 1609] NS 89.44 90.25 98.40 94.14 65.81 46.1 395.4 [99, 199], [ 30, 1841] TF 84.88 84.01 82.30 83.14 84.66 69.4 501.8 [1032,154], [174, 809] JP 79.11 80.85 61.93 70.14 76.16 55.6 561.8 [1184,126], [327, 532] I/E: Introvert-Extrovert; N/S: iNtution-Sensing; T/F: Thinking-Feeling and J/P: Judgemental-Perceiving. From Table 3, it can be inferred that soft voting outperforms the other voting techniques, such as hard voting and weighted hard voting, while considering most of the performance matrices, such as accuracy, F1 score, ROC-AUC, and MCC. Here, another vital point to note is that hard voting and weighted hard voting perform almost identical across all performance matrices. The corresponding visualizations of accuracy and F1-score are shown below in fig. 10 and fig. 11, respectively. ACM Trans. Asian Low-Resour. Lang. Inf. Process. Fig. 10. Accuracies of voting techniques: Kaggle_MBTI From fig. 10, we can conclude that the average accuracy of the soft voting classifier is 85.48, which is higher than the other voting classifiers, such as the hard voting and the weighted hard voting classifier. Here average accuracy of hard voting and weighted hard voting classifier is 84.89 and 84.9, respectively. Moreover, another critical point to note is that the accuracy of the soft voting classifier is better than other voting classifiers in the case of all personality traits. Fig. 11. F1-Score of different Voting techniques: Kaggle_MBTI Further from fig. 11, we can infer that the F1 score of the soft voting classifier is the highest for all personality traits when compared to that for hard voting and weighted hard voting classifiers. Here average F1 score for the soft voting classifier is 85.67, while it is 84.7 and 84.72 for hard voting and weighted hard voting classifiers. [VALUE] [VALUE] [VALUE] [VALUE] [VALUE] [VALUE] [VALUE] [VALUE] [VALUE] [VALUE] [VALUE] [VALUE] 72 74 76 78 80 82 84 86 88 90 92 I/E N/S T/F J/P Accuracy (%) Personality Traits Accuracy of Different Voting Techniques SOFT VOTING HARD VOTING WEIGHTED-HARD VOTING [VALUE] [VALUE] [VALUE] [VALUE] [VALUE] [VALUE] [VALUE] [VALUE] [VALUE] [VALUE] [VALUE] [VALUE] 0 10 20 30 40 50 60 70 80 90 100 I/E N/S T/F J/P F1-Score (%) Personality Traits F1-Score of Different Voting Techniques SOFT VOTING HARD VOTING WEIGHTED HARD VOTING ACM Trans. Asian Low-Resour. Lang. Inf. Process. 4.3 Comparison with existing works: Kaggle_MBTI The results provided by the proposed kernel-based ensemble model, KBSVE-P, are compared with the state-of-the-art on Kaggle-MBTI. The proposed model outperformed the existing works. Table 4 results show an improved performance of 9.4%, 4.6%, 12.19%, and 18.95% for I/E, N/S, T/F, and J/P MBTI personality traits, respectively. Table 4. Comparison with the State-of-Art (Accuracy): Kaggle_MBTI Model used I/E N/S T/F J/P BERT + MLP [17] 78.8 86.3 76.1 67.2 BERT + SVM [17] 77.0 86.2 73.7 60.5 Proposed KBSVE-P 86.35 90.27 85.38 79.94 Fig. 12. Comparison with the existing work in Kaggle_MBTI Fig. 12 shows the accuracy comparison of models, and the proposed KBSVE-P model outperforms the existing models with an average accuracy of 85.48 for the personality traits. 4.4 Evaluation of KBSVE-P on _MBTI dataset The proposed KBSVE-P model is also evaluated on the South Asian, Indian low-resource Hindi language dataset, __MBTI dataset (vishesh charitr_MBTI). Table 5 shows the performance of the model. Table 5. KBSVE-P on _MBT) dataset Dataset Accuracy (%) Precision (%) Recall (%) F1-Score (%) ROC-AUC (%) MCC (%) _MBTI (vishesh charitr) 66.89 67.40 70.17 67.96 64 30.09 [VALUE] [VALUE] [VALUE] 68 70 72 74 76 78 80 82 84 86 88 BERT +MLP BERT + SVM KBSVE-P Accuracy (%) Models ACM Trans. Asian Low-Resour. Lang. Inf. Process. With our observations we conclude that the soft voting classifier is performing better than the hard voting and weighted hard voting classifiers. It is also observed that polynomial kernel and the radial basis function kernel performed better than linear kernel and the sigmoid kernel. 5 CONCLUSION AND FUTURE SCOPE Personality profiling has many real-world applications, such as recruitment, team development, preference-based recommendation systems, relationship building, coaching, and development, etc. Therefore, personality detection is one of the trending research areas. However, the existing state-of-the-art lacks in efficiently determining the individual personality trait when examined using various performance matrices, especially in the low-resource textual content on social media. In this research, we proposed a novel kernel-based soft-voting ensemble model for personality detection, KBSVE-P. This model was built by firstly evaluating the performance of various Support Vector Machine (SVM) kernels such as radial basis function (RBF), linear, sigmoidal, and polynomial found the best-suited kernel for automatic personality detection in textual data. Next, an ensemble of SVM kernels was implemented with a variety of voting techniques such as soft voting, hard voting, and weighted hard voting. The model was evaluated on the publicly available Kaggle MBTI dataset for detecting a user's personality across four personality traits, namely introvert/extrovert (I/E), thinking/feeling (T/F), sensing/intuitive (S/N), and judging/perceiving (J/P). The results were evaluated on the publicly available Kaggle_MBTI dataset and a novel Hindi language personality dataset. Evidently, the linear kernel performed the best amongst kernels and the proposed kernel-based ensemble with soft voting outperformed the existing models on Kaggle-MBTI with an average F1 score of 85.677 and average accuracy of 85.48 for the personality traits. An accuracy of 66.89 is achieved by using the proposed KBSVE-P model on the novel Hindi _MBT) dataset. Multimodal and multilingual personality detection remain some open research gaps within the domain of automatic personality detection and as a future work we intend to explore interpretable deep learning models for the same. REFERENCES [1] Aulia, M. R., Djamal, E. C., & Bon, A. T. (2020, August). Personality Identification Based on Handwritten Signature Using Convolutional Neural Networks. In Proceedings of the 5 th NA International Conference on Industrial Engineering and Operations Management Detroit. [2] Ariyanto, A., Djamal, E. C., & Ilyas, R. (2018, August). Personality Identification of Palmprint Using Convolutional Neural Networks. In 2018 International Symposium on Advanced Intelligent Informatics (SAIN) (pp. 90-95). IEEE. [3] Mehta, Y., Fatehi, S., Kazameini, A., Stachl, C., Cambria, E., & Eetemadi, S. (2020, November). Bottom-up and top-down: Predicting personality with psycholinguistic and language model features. In 2020 IEEE International Conference on Data Mining (ICDM) (pp. 1184-1189). IEEE. [4] Mehta, Y., Majumder, N., Gelbukh, A., & Cambria, E. (2020). Recent trends in deep learning based personality detection. Artificial Intelligence Review, 53(4), 2313-2339. [5] Bharadwaj, S., Sridhar, S., Choudhary, R., & Srinath, R. (2018, September). Persona traits identification based on Myers-Briggs Type Indicator (MBTI)-a text classification approach. In 2018 international conference on advances in computing, communications and informatics (ICACCI) (pp. 1076-1082). IEEE. [6] Bhowmick, R. S., Ganguli, I., Paul, J., & Sil, J. (2021). A multimodal deep framework for derogatory social media post identification of a recognized person. Transactions on Asian and Low-Resource Language Information Processing, 21(1), 1-19. [7] Cui, B., & Qi, C. (2017). Survey Analysis of Machine Learning Methods for Natural Language Processing for MBTI Personality Type Prediction. [8] Furnham, A., & Stringfield, P. (1993). Personality and occupational behavior: Myers-Briggs type indicator correlates of managerial practices in two cultures. Human Relations, 46(7), 827-848. [9] Gosling, S. D., Augustine, A. A., Vazire, S., Holtzman, N., & Gaddis, S. (2011). Manifestations of personality in online social networks: Self-reported Facebook-related behaviors and observable profile information. Cyberpsychology, Behavior, and Social Networking, 14(9), 483-488. ACM Trans. Asian Low-Resour. Lang. Inf. Process. [10] Kunte, A. V., & Panicker, S. (2019, November). Using textual data for personality prediction: a machine learning approach. In 2019 4th international conference on information systems and computer networks (ISCON) (pp. 529-533). IEEE. [11] Ross, C., Orr, E. S., Sisic, M., Arseneault, J. M., Simmering, M. G., & Orr, R. R. (2009). Personality and motivations associated with Facebook use. Computers in human behavior, 25(2), 578-586. [12] Utami, N. A., Maharani, W., & Atastina, I. (2021). Personality classification of facebook users according to big five personality using SVM (support vector machine) method. Procedia Computer Science, 179, 177-184. [13] Gjurkovi , M., Karan, M., Vukojevi , )., Bo njak, M., & najder, J. . Pandora talks: Personality and demographics on reddit. arXiv preprint arXiv:2004.04460. [14] Pratama, B. Y., & Sarno, R. (2015, November). Personality classification based on Twitter text using Naive Bayes, KNN and SVM. In 2015 International Conference on Data and Software Engineering (ICoDSE) (pp. 170-174). IEEE. [15] Najadat, H., Alzubaidi, M. A., & Qarqaz, I. (2021). Detecting Arabic spam reviews in social networks based on classification algorithms. Transactions on Asian and Low-Resource Language Information Processing, 21(1), 1-13. [16] Sangwan, Saurabh Raj, and M. P. S. Bhatia. "Denigration bullying resolution using wolf search optimized online reputation rumour detection." Procedia Computer Science 173 (2020): 305-314. [17] Robins, G., & Kashima, Y. (2008). Social psychology and social networks: Individuals and social systems. Asian Journal of Social Psychology, 11(1), 1-12. [18] Kumar, Akshi, Sukriti Verma, and Himanshu Mangla. "A survey of deep learning techniques in speech recognition." 2018 International Conference on Advances in Computing, Communication Control and Networking (ICACCCN). IEEE, 2018. [19] Subramanian, R., Wache, J., Abadi, M. K., Vieriu, R. L., Winkler, S., & Sebe, N. (2016). ASCERTAIN: Emotion and personality recognition using commercial sensors. IEEE Transactions on Affective Computing, 9(2), 147-160. [20] Barbuto Jr, J. E. (1997). A critique of the Myers-Briggs Type Indicator and its operationalization of Carl Jung's psychological types. Psychological Reports, 80(2), 611-625. [21] Anna Klappenbach. 2022. The 12 most spoken languages in the world. Retrieved Jan 07, 2022 from https://blog.busuu. com/most-spoken-languages-in-the-world/ [22] Jain, D., Kumar, A., & Garg, G. (2020). Sarcasm detection in mash-up language using soft-attention based bi-directional LSTM and feature-rich CNN. Applied Soft Computing, 91, 106198. [23] Kumar, A., Sangwan, S. R., Singh, A. K., & Wadhwa, G. (2022). Hybrid deep learning model for sarcasm detection in Indian indigenous language using word-emoji embeddings. Transactions on Asian and Low- Resource Language Information Processing. [24] Jain, D. K., Kumar, A., & Sangwan, S. R. (2022). TANA: The Amalgam Neural Architecture for Sarcasm Detection in Indian Indigenous Language combining LSTM and SVM with Word-Emoji Embeddings. Pattern Recognition Letters. [25] Kumar, A., & Albuquerque, V. H. C. (2021). Sentiment Analysis Using XLM-R Transformer and Zero-shot Transfer Learning on Resource-poor Indian Language. Transactions on Asian and Low-Resource Language Information Processing, 20(5), 1-13. [26] Sangwan, S. R., & Bhatia, M. P. S. (2021). Denigrate comment detection in low-resource Hindi language using attention-based residual networks. Transactions on Asian and Low-Resource Language Information Processing, 21(1), 1-14. [27] Pennebaker, J. W., Boyd, R. L., Jordan, K., & Blackburn, K. (2015). The development and psychometric properties of LIWC2015. [28] Lee, K. (., Choi, Y., & Stonier, D. J. . Evolutionary algorithm for a genetic robot s personality based on the Myers Briggs Type Indicator. Robotics and Autonomous Systems, 60(7), 941-961. [29] Hernandez, R. K., & Scott, I. (2017). Predicting Myers-Briggs type indicator with text. In 31st Conference on Neural Information Processing Systems (NIPS 2017). [30] Keh, S. S., & Cheng, I. (2019). Myers-Briggs personality classification and personality-specific language generation using pre-trained language models. arXiv preprint arXiv:1907.06333. [31] Ren, Z., Shen, Q., Diao, X., & Xu, H. (2021). A sentiment-aware deep learning approach for personality detection from text. Information Processing & Management, 58(3), 102532. [32] Cerkez, N., Vrdoljak, B., & Skansi, S. (2021). A Method for MBTI Classification Based on Impact of Class Components. IEEE Access, 9, 146550-146567. ACM Trans. Asian Low-Resour. Lang. Inf. Process. [33] Basto, C. (2021). Extending the Abstraction of Personality Types based on MBTI with Machine Learning and Natural Language Processing. arXiv preprint arXiv:2105.11798. [34] Shafi, H., Sikander, A., Jamal, I. M., Ahmad, J., & Aboamer, M. A. (2021). A Machine Learning Approach for Personality Type Identification using MBTI Framework. Journal of Independent Studies and Research Computing, 19(2), 6-10. [35] Adi, G. Y. N., Tandio, M. H., Ong, V., & Suhartono, D. (2018). Optimization for automatic personality recognition on Twitter in Bahasa Indonesia. Procedia Computer Science, 135, 473-480. [36] Ong V, Rahmanto ADS, Williem, Suhartono D, Nugroho AE, Andangsari EW, et al. Personality Prediction Based on Twitter Information in Bahasa Indonesia. In: Proceedings of the 2017 Federated Conference on Computer Science and Information Systems. Prague, Czech Republic; 2017. p. 367 372 [37] Choong, E. J., & Varathan, K. D. (2021). Predicting judging-perceiving of Myers-Briggs Type Indicator (MBTI) in online social forum. PeerJ, 9, e11382. [38] Yong Zhang, Hongrui Zhang, Jing Cai, and Binbin Yang, "A Weighted Voting Classifier Based on Differential Evolution".