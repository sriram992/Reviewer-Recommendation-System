This is an Open Access document downloaded from ORCA, Cardiff University's institutional repository: https://orca.cardiff.ac.uk/id/eprint/152361/ This is the author s version of a work that was submitted to / accepted for publication. Citation for final published version: Dwivedi, Rudresh, Dave, Devam, N aik, H et, Singhal, S miti, Rana, Omer ORCID: https://orcid.org/0000-0003-3597-2646, Patel, Pankesh, Qian, Bin, Wen, Zhenyu, Shah, Tejal, Morgan, Graha m and Ranjan, Rajiv 2023. Explainable AI (XAI): core ideas, techniques and solutions. ACM Computing Surveys 55 (9) , 835. 10.1145/3561048 file Publishers page: http://dx.doi.org/10.1145/3561048 < h ttp://dx.doi.org/10.1145/3561048 > Please note: Changes m ade as a result of publishing processes such as copy-editing, formatting and page nu mbers m ay not be reflected in this version. For the definitive version of this publication, please refer to the published source. You are advised to consult the publisher s version if you wish to cite this paper. This version is being m ade available in accordance with publisher policies. See http://orca.cf.ac.uk/policies.html for usage policies. Copyright and moral rights for publications m ade available in ORCA are retained by the copyright holders. Explainable AI (XAI): Core Ideas, Techniques and Solutions RUDRESH DWIVEDI, Netaji Subhas University of Technology (formerly NSIT), India, India DEVAM DAVE, HET NAIK, SMITI SINGHAL, Pandit Deendayal Petroleum University, India, India OMER RANA, School of Computer Science and Informatics, Cardiff University, UK, UK PANKESH PATEL, AI Institute, University of South Carolina, USA, USA BIN QIAN, ZHENYU WEN, TEJAL SHAH, GRAHAM MORGAN, RAJIV RANJAN, School of Computing, Newcastle University, UK, UK As our dependence on intelligent machines continues to grow, so does the demand for more transparent and inter- pretable models. In addition, the ability to explain the model generally is now the gold standard for building trust and deployment of Arti cial Intelligence (AI) systems in critical domains. Explainable Arti cial Intelligence (XAI) aims to provide a suite of machine learning (ML) techniques that enable human users to understand, appropriately trust, and produce more explainable models. Selecting an appropriate approach for building an XAI-enabled appli- cation requires a clear understanding of the core ideas within XAI and the associated programming frameworks. We survey state-of-the-art programming techniques for XAI and present the different phases of XAI in a typical ML development process. We classify the various XAI approaches and using this taxonomy, discuss the key differences among the existing XAI techniques. Furthermore, concrete examples are used to describe these techniques that are mapped to programming frameworks and software toolkits. It is the intention that this survey will help stakeholders in selecting the appropriate approaches, programming frameworks, and software toolkits by comparing them through the lens of the presented taxonomy. CCS Concepts: Computing methodologies Knowledge representation and reasoning. Additional Key Words and Phrases: Explainable Arti cial Intelligence, Interpretable AI, Programming framework, Software toolkits 1 INTRODUCTION As a society, our dependence on intelligent machines is on a continuous upswing. From driverless cars, exible email- lters to forward-looking and preemptive law maintenance models, machine learning (ML) based systems are increasingly being deployed across several domains. With the frequent utilization of complex Deep Learning (DL) architectures, it requires urgent attention to understand the inner workings and to get insights into the outcomes. This is a core motivation of Explainable AI (XAI) [15]. The prime reason for rapid growth in XAI is the increased robustness of AI systems in business, enterprise computing and critical industries [14, 15]. For companies such as Google, a false prediction can led Authors addresses: Rudresh Dwivedi, Netaji Subhas University of Technology (formerly NSIT), India, Dwarka, Delhi, India, rudresh.dwivedi@nsut.ac.in; Devam Dave, Het Naik, Smiti Singhal, Pandit Deendayal Petroleum University, India, Raysan, Gandhinagar, India, devam.dce18@sot.pdpu.ac.in,het.nce18@sot.pdpu.ac.in,smiti.sce18@sot.pdpu.ac.in; Omer Rana, School of Computer Science and Informatics, Cardiff University, UK, Cardiff, UK, RanaOF@cardiff.ac.uk; Pankesh Patel, AI Institute, University of South Carolina, USA, South Carolina, USA, PPANKESH@mailbox.sc.edu; Bin Qian, Zhenyu Wen, Tejal Shah, Graham Morgan, Rajiv Ranjan, School of Computing, Newcastle University, UK, UK, B.Qian3@newcastle.ac.uk. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. 2022 Association for Computing Machinery. 0360-0300/2022/9-ART $15.00 https://doi.org/10.1145/3561048 ACM Comput. Surv. 2 R. Dwivedi et al. to the application user being shown a wrong recommendation [62]. However, in critical sectors such as healthcare, nance, and military, inaccurate predictions can have serious consequences on human life. Hence it is crucial to understand how these systems make their decisions. As AI permeates into these critical areas, the human limitation to understand complex AI models is a major roadblock. The main reason for this is that the data insights and the tasks solved by machines remain out of sight in increasingly complex models. The user would need access to tons of numbers to explain a Deep Neural Network (DNN), and moreover there is no concrete way to understand the model completely. In addition to the black box nature of a model, bias can creep in when dealing with data [60]. Model performance metrics (e.g., model accuracy) do not always exhibit true prediction decisions [9]. Just having a highly accurate model is not suf cient to trust and deploy the model in real-world applications. XAI has been receiving much attention across multiple application domains [21]. Consequently, an increasing number of XAI tools and techniques are being proposed both in industry and academia. The current XAI systems exhibit a diverse set of dimensions and functionalities for simple exploratory data analysis to understanding complex AI models. Therefore, selecting the correct method(s) for given requirements necessitates a clear understanding of the methods and basic differences among the different XAI approaches. However, the state-of-the-art analysis with respect to existing approaches for building XAI-enabled applications has been investigated to a limited extent. In Do ilovi c et al. [13], general interpretability and explainability have been discussed but limited to supervised ML models. On the other hand, Bhatt el al. [7] discuss how explanation schemes are adapted for ML engineers, end users or other stakeholders. They provide recommendations for organizations to achieve real time explanation and to improve performance. Arrieta et al. [4] present concepts, taxonomies, opportunities and challenges towards responsible AI. The focus of their survey is on schemes for fairness and discrimination in ML models. Our survey focuses on model and data explainability and on model performance metrics. Additionally, case studies are used to explain the signi cance of feature importance. Furthermore, design considerations for XAI frameworks along with software systems have been discussed to support fairness and accountability. We believe that a comparative analysis is needed to counsel various stakeholders involved in XAI- enabled application development. The focus of this survey is on approaches to develop XAI applications, covering tools and technologies for XAI and related concepts to aid implementation in AI-based systems. Outline. The rest of this paper 1 is structured as follows: the different stages of building a typical ML model and an XAI-based model are compared in Section 2. The need for XAI in AI-based applications and in the explanation phases of the ML process is then discussed in Section 3 followed by the taxonomy of XAI techniques in Section 4 discussed in the subsequent sections. Sections 5 and 6 cover the major pillars of XAI system, i.e., Data Explainability and Model Explainability respectively. The focus in Section 5 is on basic data visualization and dimensionality techniques along with the associated software libraries, and Section 6 on model explainability techniques and the associated software libraries. Using examples, Feature-based and Example-based XAI techniques are discussed in Sections 7 and 8 respectively. Open source and commercially available toolkits for building XAI models are covered in Section 9 with an analysis on requirements during implementation of XAI presented in Section 10. The paper is concluded in Section 11. 2 DEVELOPING XAI-BASED APPLICATIONS While an ML pipeline can provide accurate predictions, it lacks two important phases: understanding and explaining. The Understanding phase involves the training and quality assurance of an AI model 1Some of the content is derived from our unpublished technical report: https://arxiv.org/abs/2011.03195 ACM Comput. Surv. Explainable AI (XAI): Core Ideas, Techniques and Solutions 3 Stakeholders Description Developers They build the AI applications. Their primary motive for seeking explainability/interpretability is quality assurance, i.e. to aid system testing, debugging and evaluation, and to improve the robustness of their applications. Theorists They understand and advance AI theory. The motive to better understand fundamental properties of deep neural networks has led to interdisciplinary research being labelled arti cial neuroscience". Membership of this group overlaps with the developer community. For example, in the case of an industry researcher who carries out theoretical work on deep neural network technology (theorist) while also applying the technology to build systems (developer). Data Scientists The data scientist should be aware of and understand every aspect of the AI system, from the data used for training, the model implemented and why the predictions developed by the model. A data scientist should also be address errors faced by the AI system. Table 1. Stakeholders during the understanding phase whereas the Explaining phase is important when an ML model is deployed and used in real-world applications. Figure 1 illustrates a revised ML life-cycle with the additional steps [58]. Training & Dev Sets Model Validation Test Set Learned Model Historical Data Development Test Training + Cross Validation New Data Deployed Model Production Prediction 1 2 Learning Testing Deploying 4 5 Explaining 3 Understanding Explanation Interpretation Interpretation Evaluation Fig. 1. A pipeline for building ML models with explanation. 2.1 Understanding phase The objective of this phase is to improve the model during the training phase prior to deploying it. The stakeholders, as described in Table 1, cross-check to make sure that the nal model is as precise as it can be and works as intended in the real-world. The activities involved in this phase are interpreting the important features and how they interact with each other, interpreting what patterns have been learned by the trained model, analysing biases in data and ensuring that they are not propagated into the trained model. In the following, we present use cases of XAI at the understanding phase of a ML process: Debugging and enhancing AI models. An AI model goes through iterations before it is fully devel- oped [33], e.g. to improve performance incrementally. During this process sources of model error are found and removed after meticulously checking and testing. Explanations can shorten this process by helping with the recognition of model error sources. ACM Comput. Surv. 4 R. Dwivedi et al. Stakeholders Description Users People who use AI systems. Members of this group need explanations to help them decide whether/ how to act given the outputs of the system, and/ or to help justify those actions. An example is an insurance company that uses an AI tool to help decide whether and at what cost to sell policies to clients. The end-users of the tool, the director of the company, and the clients are all members of the user community. Consumers Consumers are recipients of products and services. Explanations need to be simple and clear, enabling users with limited understanding to make use of the information without assistance. This provides trustworthiness as well as increases the transparency of AI Systems. Businesses A business stakeholder is someone who wants to deploy an AI system within their product. A business stakeholder can include: policemen, judges, bank associates, government of cials, doctors etc. Business stakeholders should understand how the model makes a decision. This ensures fairness and also protects users from false decisions taken by the model. Regulators An expert or a regulator consistently monitors and audits the AI System. In case of a false output/ decision made by the model, this group follows the decision trail. Another job of regulators is to ensure that the model is up to date, by training it with new data as and when required. Table 2. Stakeholders at explaining phase. Detecting bias. A decision-making process can be fully or partially automated with AI models [33]. However, if these models are trained on biased historical data, then such bias can permeate the entire system negatively impacting the decision outcomes. XAI is a valuable tool that can help to identify biases in AI models. For example, feature importance (discussed in Section 6.1), a common type of explanation, shows the comparative signi cance of input features for any speci c model prediction(s). Scienti c understanding. The Automated Statistician project [56] explains its predictions by breaking down complex datasets into manageable interpretable sections and communicating its results to a user. This enables researchers to enhance their understanding of data features [33]. Building a robust model. Models less likely to be impacted by small changes in inputs are referred to as robust" and models that are explainable also tend to be more robust [33, 35]. In a way, this is somewhat intuitive: having a coherent explanation for predictions implies that the reasoning is logical, and logical reasoning is less likely to be affected by noise. AutoML. AutoML has made explainability more important, as the entire data pipeline is convered into a black box (as opposed to just the machine learning model) [27]. When using AutoML, a user does not have the ability to engineer or select features, or has visibility into a model s decision-making process. 2.2 Explaining phase This is the phase where an ML model is deployed and used in real-life applications. The purpose of this phase is to interpret how predictions on real-world data are made by the model. This would expose human-readable explanations to end-users, describing how the prediction results are derived especially important in mission critical applications. Table 2 describes stakeholders in this phase and why explainability is important for them. Below we present use cases of XAI at the Explaining phase of ML process. Better decision making. The ability to understand why AI systems make certain predictions or generate outcomes can help organisations make appropriate decisions. To illustrate, in churn prediction an AI model can accurately predict customers likely to leave in the future, and the business is alerted however ACM Comput. Surv. Explainable AI (XAI): Core Ideas, Techniques and Solutions 5 Use cases of Explainable AI Stages Explaining Phase (After Model Deployment) Understanding Phase (Before Model Deployment) Detecting Bias Scientific Understanding Building a robust model Hypothesizing about New Knowledge AutoML Better Decision Making Descrimination Justifiability Debugging and Enhancing AI Models Fig. 2. Use cases of XAI at diferent phases no solution is presented. XAI can shed more light on this decision-making process and answer the why". Armed with this knowledge, businesses can formulate an appropriate goal-directed plan. Discrimination:. Datasets often contain inherent discrimination. For example, in 2015, searching for CEO" on Google Images returned women personalities only 11% of the time. This did not match with the real-world representation of 27%. Unlike black box models, XAI models can present the reasoning behind the result, which can help identify the source for the discrimination and address it accordingly. Justi ability:. As legal questions may not addressable with black-box models, modern AI systems will have no choice except to add explainability. For example, In the area of individual rights, regulators will demand more explainability from AI models. People who are adversely impacted by an AI system s decision (e.g. those rejected for a loan) would be interested to know why the system chose not to approve the loan. 3 TAXONOMY OF XAI TECHNIQUES This section explores different approaches for interpreting machine learning models and lays the foun- dations of interpretability techniques. Table 3 presents our comparative analysis and a taxonomy of different XAI techniques covered in subsequent sections (Section 4, Section 5, Section 6, Section 7). Figure 3 provides an overview of different XAI techniques. 3.1 White-Box vs Black-Box Model Techniques Black-box models are non-transparent in nature while white-box models are transparent and compara- tively easy to understand.The black-box model is also termed as intrinsic as it is achieved by limiting the complexity of an AI model, while a white-box model is also termed as post-hoc as it is applied on the model after training. White-Box Model Techniques. Some AI models are simple and self-explanatory. For example, the predicted outcome y can be mathematically expressed as a weighted sum of all of its features x. It is visualized as a a straight line graph, with a as the slope of the line and b as the intercept on the y-axis. A linear model is a white-box model because its mechanism is transparent and simple (as opposed to a black-box whose mechanism is not readily understood). Though simple, these are less capable of representing a larger dataset featuring complex interactions. Therefore, for higher accuracy, we require more complex and expressive models. Black-Box model Techniques. Black box models such as neural networks or complex ensembles of much lower complexity (like a gradient boosting model based on decision-trees). The architecture of these ACM Comput. Surv. 6 R. Dwivedi et al. models is hard to decipher, as it is not clear how important a role any given feature plays in the prediction model or how it interacts with other features. For example, in a fully connected neural network, tracing the output features rendered by a model against a speci c causative input feature remains a challenge. 3.2 Model-specific Techniques vs Model-agnostic Techniques There is another dimension to understanding the interpretability of models. It depends on the model being examined. Model-speci c techniques deals with inner working of a model to interpret its results. XAI 4. Data Explainability 4.1 Exploratory Analysis and Visualization 4.2 Dimensionality Reduction Techniques 4.3 Software Libraries for Data Explainability 5. Model Explainability 5.1 White Box Models 5.1.1 Linear Models 5.1.2 Decision Tree 5.1.3 Generalized Additive Models 5.2 Black Box Models 5.2.1 Tree Ensembles 5.2.2 Support Vector Machines 5.2.3 Explainable Neural Networks 5.3 Neural-Symbolic Approaches 5.4 Model Performance Evaluation Metrics 5.5 Software Libraries for Model Explainability 5.6 Distributed Learning 6. Feature-based Techniques 6.1 Feature Importance 6.2 Partial Dependence Plots 6.3 Individual Conditional Expectation 6.4 Accumulated Local Effects 6.5 Global Surrogate 6.6 LIME 6.7 Shapley 7. Example-based Techniques 7.1 Anchors 7.2 Counterfactuals 7.3 Contrastive Explanations Method 7.4 Kernel and Tree SHAP 7.5 Integrated Gradients Fig. 3. A Taxonomy of XAI Techniques applies to data and model ACM Comput. Surv. Explainable AI (XAI): Core Ideas, Techniques and Solutions 7 Classi cation XAI Techniques Global Local Model- speci c Model- Agnostic White- box Black- box Data Explainability Commonly used Data Visualization Plots N.A. N.A. Dimensionality Reduction Techniques N.A. N.A. White-Box Models Linear Model (Section 5.1) Decision Tree (Section 5.1) Generalized Additive Mod- els (GAMs) (Section 5.1) Tree Ensembles (Section 5.1) Arti cial Neural Networks Neural Networks (Section 5.2) Neural-Symbolic (Section 5.3) Evaluation Metrics Model Evaluation Metrics (Section 5.4) Feature Based XAI Techniques Feature Importance (Section 6.1) Partial Dependence Plots (Section 6.2) Individual Conditional Expecta- tion (Section 6.3) Accumulated Local Effects (ALE) (Sec- tion 6.4) Global Surrogate (Section 6.5) LIME (Section 6.6) Shapley Value (Section 6.7) Example Based XAI Techniques Counterfactuals (Section 7.2) Anchors (Section 7.1) Contrastive Explanation Method (Sec- tion 7.3) Prototype Counterfactuals (Section 7.2) Integrated Gradients (Section 7.5) Kernel Shap (Section 7.4) Tree Shap (Section 7.4) Table 3. XAI Techniques versus Taxonomy of XAI Techniques. Here, N.A. represents not applicable It involves examining the structure of an algorithm, including its intermediate representations. Model- agnostic techniques deals with analyzing features, their relationship with outputs and the data distribution. In the following, we brie y present these two techniques [44]: Model-speci c techniques. Model-speci c interpretation tools are designed purely to interpret models with speci c features and capabilities. They can be used only for a single algorithm class. We present various model-speci c techniques in Section 5. Model-agnostic techniques. The interpretation techniques classi ed as model-agnostic can be used on any machine learning model. The widely used LIME technique is model-agnostic and can be used to analyze and interpret any set of machine learning inputs and corresponding predictions (outputs). 3.3 Global Interpretation vs Local Interpretation This classi cation is based on the scope of the interpretation [44]. The global interpretation analyzes the decision-making process at a broader level and is goal-oriented. The local interpretation gives detailed explanations for every decision made. ACM Comput. Surv. 8 R. Dwivedi et al. Global Interpretation. Global interpretation methods involve an overall analysis of a model and its general behavior. The process of de ning variables, their dependency, and interactions goes alongside with the process of assigning importance to these components. Two global interpretation techniques: feature importance and partial dependence plots, are described in Section 6. Local Interpretation. Involves an analysis of individual predictions and decisions made by the model, to clarify why the model suggested a particular course of action. When a data point prediction/ decision is analyzed, the focus is on the sub-region around that data point. It enables us to understand the contextual importance of the data point output in that space. LIME and Shapley value are two such techniques used to understand individual predictions Section 6. 4 DATA EXPLAINABILITY The rst step of explainability is data Visualization which provides an idea and insight to the dataset. This is where all the model validation and explanation kick off. This section presents commonly used data explainability techniques a rst step for validating, explaining and trusting a model. Section 4.3 presents programming frameworks, which can be used to implement these data explainability techniques. 4.1 Exploratory Analysis and Visualization Visual analysis is crucial for interpretable ML as knowing its contents is vital for setting a baseline expectation for how models behave and what they create. For long, exploratory data analysis and visualization has been a major tool for gleaning meaningful information from data. 4.2 Dimensionality Reduction Techniques Visualizing is crucial for interpretable ML, however data sets are sometimes hard to visualise due to too many variables and sizes. Although multiple dimensions can be plotted, their interpretation can be complex and error prone. Dimensionality reduction techniques such as PCA, ICA, Isometric Mapping (Isomap), t-sne, LDA, UMAP, LDS, LLE, and Autoencoders can be used to improve visualisation and interpretation. PCA converts observations of correlated features into a set of linearly uncorrelated features through various orthogonal transformations while ICA extracts independent components equivalent to the number of dimensions or features present in the original data set. Isomap is used to preserve the geodesic distance in the lower dimension whereas t-SNE produces slightly different results each time on the same data set, preserving the structure of neighbouring points. LDA provides the highest possible discrimination between multiple classes. On the other hand, Uniform Manifold Approximation and Projection for Dimension Reduction (UMAP) and Locally Linear Embedding (LLE) are manifold learning methods based on Riemannian geometry and algebraic topology. Multidimensional Scaling (MDS) represents measures of similarity/ dissimilarity among pair of objects by computing distances between points in a low-dimensional space. Autoencoders contain an abstract representation of data, unlike other non-linear dimension reduction algorithms such as LLE or MDS. 4.3 Sotware Libraries for Data Explainability Table 4 describes frameworks to implement data explainability techniques. denotes af rmative and unrealisability of the technique. The methodologies provide exploration as well as explanation of the observation data. It gives insights into a data set by expressing the features visually enabling trends and anomalies to be identi ed. Although Data Visualization Plots give a good data explainability, plots such as Histogram are much less exible than KDEs. The latter give estimates of an unknown density function wherein we can not only differ the bandwidth, but also implement kernels with various shapes ACM Comput. Surv. Explainable AI (XAI): Core Ideas, Techniques and Solutions 9 and sizes. On the other hand PCA and t-SNE are purely dimensionality reduction techniques, e.g. to 2D or 3D representations. Software ibraries such as Sklearn, NetworkX can be used for building ML models, network models and provide algorithms for classi cation and regression. Sklearn provide ease in interpretation for deep learning models. Along with many supervised and unsupervised learning algorithms, Sklearn also consists of cross validation methods to assess model performance/ prediction on unseen data. Using NetworkX, different kinds of networks such as random, weighted, symmetric and asymmetric networks can be created. WordCloud supports visualisation of frequent words in a given text identifying the most important words in the text. 5 MODEL EXPLAINABILITY In this section, we outline commonly used model explainability techniques to understand AI models. laying the foundation for techniques discussed in Section 6 and Section 7. In Section 5.1, we rst describe techniques used for white box model, whose internal mechanisms lend themselves to be interpreted in a direct manner. Section 5.2 outlines techniques used in black-box models while Section 5.3 discusses the application of knowledge representation techniques for explainability. Section 5.4 describes model performance evaluation metrics. Section 5.5 presents programming frameworks, which can be used to implement the presented model explainability techniques. 5.1 White-Box Models In this section we present white-box models and programming methods used for interpretation. 5.1.1 Linear models. Explainability of linear models involves a linear combination of feature values, adjusted by the coef cients of the model. For example, in y = mx + c, m is coef cient of feature x1 and c is coef cient of x0 so polynomial of degree 1 is a linear polynomial. Similarly, Logistic Regression is one of the most interpretable linear ML models for certain class of events. seaborn, matplotlib, sklearn and ALE libraries can be used to unfold and visualise a Logistic Regression Model. 5.1.2 Decision Tree. A decision tree predicts the value of a target variable against multiple input variables. The terminal node, also called the leaf node, depicts the value of the target variable based on the input variable. A key bene t of decision trees lies in establishing the input and target variable relationship XAI Techniques numpy, pandas matplotlib seaborn sklearn wordcloud networkX Data Visualization Plots Kernel Density Estimation (KDE) Box and Whisker Plot Correlation Matrix Word Cloud Network Diagram Principal Component Analysis (PCA) HeatMaps t-distributed Stochastic Neighbor Em- bedding (t-SNE) Table 4. Programming Frameworks for Data Explainability. ACM Comput. Surv. 10 R. Dwivedi et al. with a logic similar to Boolean. Scikit-learn library includes methods that can be used for interpretation of trees, e.g. sklearn.tree.export_text, sklearn.tree.plot_tree, sklearn.tree.export_graphviz, and dtreeviz and graphviz package. Sklearn also provides a way to evaluate feature importance the total decrease entropy due to splits over a given feature. 5.1.3 Generalized Additive Models (GAMs). Generalized Additive Models (GAMs) are an extension of Generalized Linear Models (GLMs) with a smoothing function. GAMs offer a trade-off between simple, interpretable models such as logistic regression and more complex, sophisticated models such as neural networks, which (usually) offer better accuracy and predictive power as compared to simple models. Over tting is unlikely in GAMs due to the regularization of prediction functions. 5.2 Black Box Models Black Box models cannot be understood or interpreted by themselves. Black Box models include Tree Ensembles, Support Vector Machines (SVM) as well as a variety of neural networks. Inclusion of several layers in a neural network makes it dif cult for designers to explain how the algorithm has reached a particular prediction outcome [1]. 5.2.1 Tree Ensembles. Tree ensembles method is a learning technique that focuses on integrating several decision trees to create an output. It determines which features to chose at each split. Since a single decision tree may not be enough to yield optimal performance, several decision trees can be combined together to get an optimal performance model. However, the model s complexity increases due to multiple decision trees. As a result, it becomes a lot more dif cult to understand model behaviour. The following methods are developed for interpreting complex tree ensembles: Simpli ed Tree Ensemble Learner (STEL). The InTrees (Interpretable Trees) package coverts a complex tree ensemble into a rule based learner known as STEL. The ensemble method averages over the variance of multiple models, which in turn deprive the interpretations of individual models. Tree interpreter. Tree interpreter provides interpretations of decision trees and random forests. It decomposes every prediction result to a sum of feature contributions and bias. This enables a better interpretation of how a feature led to a particular prediction. 5.2.2 Support Vector Machines. SVM is a supervised ML algorithm used for classi cation and regression. A Hyperplane is calculated based on the data points which are plotted in an N-dimensional space. This Hyperplane is oriented in such a way that it differentiates between classes by maximizing the distance between the data points of different classes. It works well in high dimensional spaces and scenarios where number of dimensions are greater than the number of samples. 5.2.3 Explainable Neural Networks: Inclusion of several layers in a large and complex neural nets makes it dif cult to explain how a speci c prediction or conclusion [1] was reached. This black box explainability refers to assessing predictions made by a model for any given input without having the knowledge of its inherent working. The design of an explainable Neural Network provides insights into the model. These post-hoc explanation schemes are being used for both single/ multilayer Neural Networks, Convolutional Neural Networks and Recurrent Neural Networks. 5.3 Neural-Symbolic Approaches Subsymbolic approaches, such as those based on arti cial neural networks, are growing in popularity due to their robustness against noisy data and ability to perform complex tasks not otherwise manually ACM Comput. Surv. Explainable AI (XAI): Core Ideas, Techniques and Solutions 11 possible. However, their black-box nature is a major hindrance to explainability. On the other hand, symbolic AI has long known to be explainable natively [3] since it represents knowledge using meaning- ful symbols such as language that is understandable by humans as well as interpretable by machines. However, the lack of scaleability of symbolic systems and their inability to handle noisy data has limited their use. More recently, a combination of symbolic and subsymbolic approaches based on arti cial neural networks, variously known as neuro-symbolic or neural-symbolic (henceforth NS) systems, has shown signi cant promise in exploiting the complementary strengths of individual approaches [20, 28]. Consequently, such systems often perform better; for example, Mao et al. [41] present a Neuro-Symbolic Concept Learner, which incorporates symbolic reasoning for better interpretation of visual concepts, and can be trained on less data (10% of the original dataset) while being explainable. The symbolic element of NS systems commonly consists of Knowledge Representation and Reasoning techniques, such as ontologies and knowledge graphs [28, 47]. These represent domain/expert knowl- edge as concepts and relationships between them, which together provide semantically rich background context for the domain. Knowledge graphs represent facts, usually in the W3C standard, Resource Description Framework [5] or using graph databases and can be linked to ontologies that represent background or contextual knowledge commonly in the Web Ontology Language [29]. Ontologies can thus perform deductive reasoning to derive new, deeper knowledge from existing knowledge in a traceable manner. Doran et al. [14] postulate that for a system to be comprehensible, i.e. not only be interpretable by experts but also understandable by non-experts, it must emit symbols" to enable the user to understand how a conclusion is reached. Symbols are closer to how humans understand, compared to vectors or numerical encodings as seen in neural networks. However, to achieve this explainability, the authors argue the critical need for reasoning to understand the why and how behind a particular outcome [14]. Ontologies enable reasoning not only to deduce new knowledge and facts but also for causal inference to understand the cause and effect behind the outcomes, such as through transitivity [23]. For example, in the ontology, SNOMED CT [32], the following concepts are represented in a transitive relationship: Acute Rheumatic Arthritis due to Streptococcus pyogenes In f ection causative agent Streptcoccus pyogenes Based on transitivity, x, y, z(R(x, y) R(y, z) = R(x, z)), it can be inferred that Acute Rheumatic Arthritis (effect) is associated with Streptococcus pyogenes (cause). Using further knowledge from the ontology, such as type relationships (classi cation), a natural language explanation can be generated such as, Acute Rheumatic Arthritis, an Autoimmune Disorder, is caused due to an Infectious Disease, Streptococcus pyogenes Infection. This Disease has causative agent a Bacterium, Streptococcus pyogenes. Hence, Streptococcus pyogenes is a causative agent for Acute Rheumatic Arthritis as well". Causal inference has other practical applications such as in personalised recommendations, for instance, drug contraindications [38]. It therefore follows that outputs from subsymbolic approaches when linked to background and contextual knowledge can generate user-speci c and understandable explanations [30]. Sarker et al. [47] provide a useful pipeline to demonstrate how background knowledge made available through ontologies can be used to explain the classi cation behaviours of arti cial neural networks. They use DL-Learner [8], a system for supervised machine learning based on inductive learning, to automatically create class expressions from a knowledge base, which can be used towards explaining the classi cation behaviour. Furthermore, researchers have exploited advances in Natural Language Processing, speci cally Natural Language Generation approaches, for further linguistic re nement of explanations [16, 49]. ACM Comput. Surv. 12 R. Dwivedi et al. Figure 4 shows an overview of the interaction between neural and symbolic approaches within an NS system [3]. We explain the process with the help of our previous example. Consider a neural network model trained for clinical prediction of Streptococcus pyogenes infection in patients. The symbolic knowledge consisting of a knowledge base (KB), represented for example, as a formal ontology, is used to determine the subtypes of the infection to include those in the prediction. The neural model is trained on patient data from several sources such as clinical notes, electronic records, laboratory results, etc. The output from the model is used to re ne the KB by incorporating new associations for causal analysis to encode the rationale behind the neural network s decisions. This and the other encoded knowledge in the KB is now used to construct human-understandable explanations as exempli ed earlier. Moreover, the system can be questioned for further analysis such as, What other conditions do these patients risk developing? or What other conditions are caused by S. pyogenes?". In practice, however, only partial elements of such NS integration are utilised [3] and many NS systems are still based on non-logical or at" approaches prompting Sarker et al. [48] to argue for the need to focus more on the logical aspects of NS systems in order to realise improved explainability. What we can infer from contemporary works is that the combination of neural and symbolic ap- proaches brings together ...two most fundamental aspects of intelligent cognitive behaviour: the ability to learn from experience and the ability to reason from what was learned [59]. Thus, where neural systems excel at learning, symbolic approaches can be employed to explain what has been learnt. In fact, deep understanding of cognitive science will most certainly bene t the eld of NS-AI particularly for the integration of neuro-symbolic approaches [6] to develop a mature and robust integrated model and consequently, for the design of explainable systems that are closer to how humans understand, think, and communicate. 5.4 Model Performance Evaluation Metrics In any data science life-cycle, model performance evaluation is an important phase before the optimal model is chosen. Evaluation can lead to adjusting model hyper-parameters. Evaluation metrics can be both problem speci c and agnostic. We present model performance metrics that are incorporated to interpret a model. Fig. 4. High-level view of a neural-symbolic system [3] ACM Comput. Surv. Explainable AI (XAI): Core Ideas, Techniques and Solutions 13 5.5 Sotware Libraries for Model Explainability In the following, we present some of the software libraries, which can be used for Model Explainability. Table 5 describes frameworks, which can be used to implement data explainability techniques. It explains if the particular explainable ML algorithm on the left can be carried out with the help of the framework speci ed at the top. denotes the af rmative case. Similarly, signi es that the stated framework cannot be used to implement the technique. The mentioned algorithms for generation of models such as linear models, decision trees, GAMs, neural networks and tree ensembles belong to a family of models that are accounted as explainable. However, some techniques viz. linear model coef cients do no ensure explainability in practice due to the reason that implementing them on a high dimensional input may not be explainable. It could only be possible if the number of input features are limited using regularization. Furthermore, the linear model coef cients could be unstable in case of multi-collinearlity (multiple correlated features). Decision tree, unline Linear Model coef cients, can also be applied to non - linear models. Decision Tree visualizations are intuituve and reveal all decisions during model training, thus supporting interpretability and explainability. A generalized additive model (GAM) is much more exible than Regression. Although GAMs are interpretable, it comes at an cost of not tting every type of data. GAMs are explainable in the sense that the distributions of values for any given feature function can be plotted in 2D, which a domain expert can easily interpret.The most reliable of all, neural networks are the most exible for model explainability. Tee ensembles can be understood as a bunch of decision trees whose results are combined thus providing the support for explainability same as decision trees but with the bene t of stringer learning during training. XAI Techniques Basic Li- braries* Tensorflow Keras Pytorch PyGAM Linear model coef cients (Section 5.1) Decision Tree (Section 5.1) Generalized Additive Models (GAMs) (Section 5.1) Neural networks (Section 5.2) Tree ensembles (Section 5.2) Model Performance Evaluation Metrics (Section 5.4) Table 5. Programming frameworks for Model Explainability. *Basic libraries: numpy, pandas, matplotlib, seaborn, sklearn Ktrain is an interface to Keras to build and train an explainable model. It works for text and image classi cation. Considering an example image, it focuses on the area of the image over which the classi er can be used for prediction. The visualizations are supported by the eli5 library and based on Grad-CAM technique. The text dataset classi cation is carried out with LIME where the prediction is made using the relative importance of the words. PyTorch uses Captum for model interpretability, using smoothGrad, integrated gradients, Deeplift, GradientShap and other PyTorch models. It is used for understanding the important neurons and layers of the PyTorch model. It also provides a Web Interface Insights for data visualization. Arize AI. Arize is an ML observability framework for model monitoring and assessment, used to diagnose the root cause of a model output and to detect pre- and post-validation checks. In addition, it unveils and explains how models arrive at speci c outcomes for any set of predictions. ACM Comput. Surv. 14 R. Dwivedi et al. AIF360. Arti cial Intelligence Fairness 360, also referred to as AIF360, is a Python toolkit for the detection and reduction of bias in a Machine Learning Model in order to increase the trust in AI, which is the primary vision of XAI. The AIF360 toolkit includes an extensive set of metrics for datasets and models in order to test for biases along with the explanation of these metrics. These metrics are an integral part of the explanation of biases of a machine learning model. It also has algorithms to mitigate bias in datasets or/and in models. AIX360. AI Xplainability 360 is an open source Python toolkit developed by IBM similar to AIF360. This Python toolkit includes several algorithms whose primary motive is the explanation and interpretation of ML models and data sets. Due diligence should be performed before choosing the most appropriate algorithm for explanation of a particular machine learning model. A decision tree for choosing the appropriate algorithm for a speci c condition is given below: Fig. 5. Decision tree for AIX360 InterpretML. an open-source Python toolkit by Microsoft for training intelligible models and explaining the black-box systems. InterpretML is based on human interpretation of the global and local explanations of the model. It also helps in debugging the model to be able to understand the predictions. InterpretML has the feature of comparing different XAI methods using the same function class. The different XAI methods include Decision Tree, Decision Rule List, Linear Regression, Logistic Regression, SHAP Kernel Explainer, SHAP Tree Explainer, LIME, Morris Sensitivity Analysis and Partial Dependence. It includes Explainable Boosting Machine (EBM), which is an algorithm for explaining models with higher accuracy. Amazon SageMaker. Similar to Microsoft Azure, Amazon SageMaker is a service offered by Amazon Web Services (AWS) to prepare, build, train and deploy a ML model. A subcomponent of Amazon ACM Comput. Surv. Explainable AI (XAI): Core Ideas, Techniques and Solutions 15 SageMaker, called Amazon SageMaker Clarify is used to interpret the ML model by explaining the predictions made by the model. The approach can also be used to detect bias at various stages of the model i.e. during data cleaning, model training and deployment. Fairlearn. an open-source project to make ML models more transparent. Various kinds of biases and other factors pertaining to unfairness exist in a ML model. Fairlearn believes that fairness cannot be introduced by technical toolkits alone. The objective of Fairlearn is to detect and reduce this unfairness, often in the form of biases. 5.6 Distributed learning Small ML models lack the representation capability for expressing complex data patterns. To train for large models and data volumes, the limitation of the computing power and storage of a single machine calls for distributing the machine learning workload across multiple machines and aggregating the results for a coherent model. We describe various technical issues to consider during implementation of distributed models, and report their impact on QoS metrics e.g. scalability, latency and convergence speed. Data Parallel, Model Parallel and Pipeline Parallel:. Data parallel partitions and places data onto worker nodes, with each one applying the same model for the training process. The worker nodes and central server communicate periodically to ensure the model is synchronized across all nodes. In the model parallel approach [42, 43] the same model is split and placed on different worker nodes, while processing the same copy of data set. The result is then aggregated from all sub-models. Model parallel depends heavily on the model structure as not all can be split up easily. For example during training of the sequential models, only one GPU is utilized at a time as it has to wait for results from the previous/ subsequent GPU. To alleviate this problem, pipeline parallel [31] splits the data into smaller batches and pipelines them for better utilization of the available resources. Topology:. One important design consideration of distributed learning is the organization and synchro- nization of worker nodes within a system. There are several considerations before actual implementation: 1) The system has to be scaleable for large number of worker nodes so that the dispatching and ag- gregating of worker information are ef cient and constant. 2) The system communication should be ef cient and easy to setup. Parameter sever (PS) [37] is most prominent for centralized aggregation of data parallel training. In PS, worker nodes periodically upload their model parameter updates to the central server. In a decentralized setting where there is no central server, the worker nodes communicate via an AllReduce approach for exchanging model updates. Overall, the communication topology has a very signi cant impact on the training performance. The ring-allreduce [10] usually achieves better scaleability due to ef cient use of network bandwidth. PS can suffer from network congestion at the server side due to the model aggregation approach. Synchronization and asynchronization SGD:. In single machine training, SGD is used for updating the parameters of a single model. In distributed learning, a global model is updated with the aggregate of all worker gradients computed with SGD. This updated model is then sent to all worker nodes. Parameter aggregation between worker nodes impacts several system metrics: loss in model training quality, convergence speed. Overall, there are three key approaches: Synchronous SGD [51], Stale asynchronous SGD [61], and Asynchronous SGD [11]. In Asynchronous SGD, the delayed gradients create noise in the global model and delay the convergence speed. However, it provides better model generalization capability and is fast in training. Synchronous training usually takes more time as it requires waiting for idle nodes. But compared to an asynchronous approach, it often generates models that produces better performance. ACM Comput. Surv. 16 R. Dwivedi et al. Distributed Learning System. Apart from the commonly used computation frameworks that are used for distributed computation, e.g. MapReduce, Apache Spark, there are many frameworks that are dedicated for deep learning. These frameworks include generic deep learning frameworks such as TensorFlow, Pytorch, MXNet that incorporate different distributed learning approaches. On the other hand, some frameworks are dedicated for distributed learning, as summarized in Table 6. For example, frameworks such as PaddlePaddle includes support for both sync and async training, PS and AllReduce topology. Theano is an optimizing compiler instead of a development framework. We aim to list the trending frameworks that the users/ developers could choose from, thus Theano is not included in the table. Synchronizaiton Architecture Sync Async Stale Parameter Server RingAllReduce Tensor ow Pytorch Mxnet Paddle Paddle Caffe2 Baidu AllReduce Horovod CNTK Distbelief petuum DMTK Table 6. Distributed Learning frameworks 6 FEATURE-BASED TECHNIQUES This section presents feature-based model explainability techniques, which describes howinput fea- tures contribute to model output. There are many feature-based methods available: permutation Fea- ture Importance, Partial Dependence Plots (PDPs), Individual Conditional Expectation (ICE) plots, Accumulated Local Effects (ALE) Plot, Global surrogate models, Local Interpretable Model-agnostic Explanations (LIME) and Shapley Additive Explanations (SHAP). 6.1 Feature Importance Feature importance [17, 19] refers to a class of techniques that are used to assign scores to input features. It shows each feature s relative importance when a prediction is made. They also de ne the basis for dimensionality reduction and feature selection to improve the ef ciency of a predictive model. Permutation Importance is a widely used feature importance technique that measures importance by looking at how reshuf ing of each predictor randomly impacts the performance of the model. It is considered as a computationally expensive technique. Table 6 presents a feature importance example of a cancer dataset. The output of the feature importance is a matrix, listing weights and features of the data sets. The topmost values indicate the features which are most important and the numbers in the bottom represent the least important features. The randomness in our permutation importance evaluation is measured by repetition with several shuf es. ACM Comput. Surv. Explainable AI (XAI): Core Ideas, Techniques and Solutions 17 Fig. 6. An example output of Feature Importance Fig. 7. An example of Partial Dependence Plots. Fig. 8. An Example of ICE Plot. 6.2 Partial Dependence Plots (PDP) Partial Dependence Plots (PDP) depict scenarios in which a feature affects predictions. For example, the following questions can be answered with PDP [18, 25]: What would be the impact of longitude and latitude on prices of houses?, how would houses of similar size be priced in different geographic areas?. Figure 7 illustrates a simple PDP example. The x-axis plots the value of feature f0 (i.e. worst concave points), and the y-axis plots the predicted value. The solid line drawn in the shaded area represents the variation of average prediction when the value of f0 changes. The y axis is represented as a change in the prediction from what it would be predicted as a baseline point (or the leftmost value). The blue shaded area denotes the level of con dence. In our example, Worst Concave points" feature has initially a increasing negative in uence, and thereafter it remains neutral. Hence we can say that for less values, we can initially predict that there are less chances of malignant condition. Bene ts. First, there is an ease in implementation [44]. Second, The computation part is quite intuitive when it comes to partial dependence plots. The partial dependence function at the point of a speci c feature value would represent the average prediction. It is easy for a laymen to understand the logic of PDPs. Limitations. The concern associated with PDP is the assumption of their independence [44]. The feature(s) for which the partial dependence is computed are assumed to be not correlated with other features. Second, the maximum number of features in a partial dependence function is realistically just two. The underlying reason lies with their 2-dimensional representation (paper or screen), or lack of ability to visualize more than 3 dimensions. Third, Heterogeneous effects can be uncovered by analysing the individual conditional expectation (outlined in Section 6.3) curves, ignoring the aggregated line. 6.3 Individual Conditional Expectation (ICE) Instead of average plotting in PDP, ICE [24] shows one line per instance. ICE scores outperforms PDP on intuitiveness as each line stands for the predictions for one instance. As with partial dependence, ACM Comput. Surv. 18 R. Dwivedi et al. ICE explains what happens to model predictions when a speci c feature varies. Figure 8 presents an ICE plot for feature worst concave points" of a cancer dataset. The worst concave points" plot is decreasing in nature (i.e., Lower value(< 0.10)) of concave points. It is a factor for higher value of target variable (malignant cases). Between 0.10 and 0.20 the graph decreases and after 0.21 it is constant (has lower y value benign cases). Bene ts. They are far more intuitive to understand when compared to PDP [44]. A single line plots the predictions for one instance if we change the feature of interest. Distinct from PDP, ICE curves can unveil heterogeneous relationships. Limitations. First, too many ICE curves could lead to an overcrowded plot without the ability to assess anything. Second, ICE curves and PDPs share the same concern: If the interest feature correlates with the other features, then a few points in lines could be invalid data points. Third, it is dif cult to nd the average in ICE plots. One solution is to group together the individual conditional expectation curves with the partial dependence plot. PDP vs ICE vs Feature Importance. Table 7 presents a comparative analysis among PDP, ICE, and Feature Importance. PDP demonstrates global effects, concealing the heterogeneous effects. ICE unravels the heterogeneous effects but makes it dif cult to nd the average. In addition, all three methods consider the features as independent entity. Hence, if features are correlated, it will result in the creation of unlikely data points. Approaches Advantages Disadvantages PDP Intuitive Easy to implement Shows global effects Assumption of independence Heterogeneous effects maybe hidden ICE Intuitive Easy to implement Can uncover heterogeneous relationships Can only display one feature meaningfully Assumption of independence Not easy to see the average Feature Im- portance Provide a highly compressed global insight Comparable across problems Automatically takes into account all interactions Not additive Shuf ing the feature added randomness Need access to true data Assumption of independence Table 7. PDP vs ICE vs Feature Importance [53]. 6.4 Accumulated Local Efects (ALE) Plot ALE Plot [2, 44] handles the inherent limitations of PDP. In few cases, PD Plots produce erroneous results when the features of the dataset are highly correlated. This is where ALE Plots come into picture. ALE Plot works on a model agnostic algorithm that provides global explanations for classi cation and regression models on tabular data. They are preferred over PD Plots, as they produce optimal results in spite of correlation between features and are less computationally expensive. ALE Plots visualise the effect on the predictions of the model for each features isolated from all other features. Example. Figure 9 shows implemented ALE on the Iris classi cation dataset from sklearn that has 4 features (sepal length, sepal width, petal length, petal width) and 3 target values (setosa, versicolor, virginica). The ALE Plots visualise feature effects linearly in Logit Space. Looking at the plot for Petal Length (bottom right plot), it can be observed that the three lines overlap at the 3.8 cm mark. From that, it can be concluded that the effects of the Petal Length on each class will be one and the same at 3.8 cm. It can also be observed that the more the petal length is, the more is the chance of the ower being from ACM Comput. Surv. Explainable AI (XAI): Core Ideas, Techniques and Solutions 19 the Virginica species and the less the petal length is, the more is the chance of it being from the Setosa species. Fig. 9. ALE Plots on Iris classification datasets 6.5 Global Surrogate A global surrogate is an interpretable model, developed for approximating black box model predictions. Figure 10 explains a simple three steps process. In step 1, the data has to be fed into the black box model for making a prediction. In step 2, model type needs to be determined whether it can be trained as a surrogate model, for instance, linear regression or decision trees. In step 3, the surrogate model is trained. The surrogate training is performed with the use of independent variables from the input data and dependent variables from the black box prediction. It is noteworthy that the surrogate model can be any interpretable model such as linear model, decision tree, or human de ned rules. At the end, the prediction error of the surrogate model can be evaluated and compared with the black box predictions. A smaller error means that the explanation of the black box outperfomed the surrogate model. Fig. 10. A Process of Creating a Surrogate Model [53]. Bene ts: The bene ts of surrogate model [44, 53] lie in exibility to interchange interpretable model and the underlying black model. Two surrogate models (linear model and decision tree) can be trained for the original black box model. This aids to provide two types of explanations. Limitations: By creating a surrogate model [44, 53], we derive conclusions about an ML model; not about the data. It is possible that an interpretable model is close for one subset but broadly disparate for a different subset of a dataset. In this scenario, how the simple model is interpreted, cannot be replicated across all data points. Moreover, this method would not work well to understand how a single prediction was made for a given observation. ACM Comput. Surv. 20 R. Dwivedi et al. 6.6 Local Interpretable Model-agnostic Explanations (LIME) LIME [46] is different from global surrogate in the sense that LIME does not try to explain the whole model. By perturbing the input of data samples and comprehending how the predictions change, LIME tries to understand the model. LIME enables local model interpretability. A single data sample is modi ed adjusting some feature values and the resultant output impact is observed. This is often linked to what human interests are when the output of a model is observed. A computer vision example. Figure 11 illustrates how LIME is used for image classi cation. For example, a classi er has to be explained which predicts the likelihood of the image containing an Egyptian cat. The image (part (b) of Figure 11) is acquired and split into easily interpretable components. As seen in the part (c) of Figure 11, a dataset of perturbed instances is generated by turning off" (turning them grey here) some of the interpretable components. For every instance of perturbation, we compute the probability of an Egyptian cat being in the image as per the model. We then try to understand a locally weighted simple (linear) model on the dataset. The emphasis is more on erring in perturbed instances which best match the original image. Ultimately, we provide the super-pixels showing the highest positive weights as our explanation (part (d) of Figure 11). We distinguish it by turning everything else grey. Fig. 11. Explaining a prediction with LIME: (a) Original image of Egyptian cat to be read by LIME, (b) Generated super-pixels in the image using mark boundaries, (c) Perturbed image, (d) Area of the image that produced the prediction of Egyptian cat. An example: explanations by LIME. Figure 12 illustrates the explanations generated by the LIME technique, using the ML-based price prediction model. It generates price recommendations with explana- tions, describing how the recommended price is derived at with explanations, instead of just predicting the price. The output of Figure 12 is a list of explanations, considering the contribution of each feature to a predicted price. The left part shows the range of a maximum (1487.18) and minimum (240.07) value, which is predicted by the ML-based price prediction module. The middle part shows the features (i.e., WT and PPK), which contribute the most in the predicted price of an animal. It can be observed that when the weight is in a range between 308.00 < WT <= 327.00 it is contributing in a negative direction of the prediction. It can also be assessed that when PPK is in a range 210.50 < PPK <= 214.10 it is contributing to the positive side of the total price. The right part shows the actual value of a particular feature (i.e., Weight = 327.00, PPK = 214.10). Potential pitfalls. Trust is important for the ef cient interaction between humans and ML systems. The explanation of individual predictions is the ideal way to assess trust. Even though LIME seems superior in terms of ease in implementation and computation cost, there are a couple of potential limitations such as: First and foremost, only linear models are used to approximate local behaviour in the current implementation. This assumption is correct to some extent when only a small region around the data sample is considered. However, when this region is expanded, there is a possibility that the linear model ACM Comput. Surv. Explainable AI (XAI): Core Ideas, Techniques and Solutions 21 Fig. 12. Model explanations generated by LIME. might be impotent to explain the original model s behaviour. There would be non-linearity at local regions for datasets requiring complicated, non-interpretable models. The inability to deploy LIME in such settings is a major pitfall. 6.7 Shapley Value SHAP (SHapley Additive exPlanations) is a game theoretic approach to explain the output of any machine learning model. The SHAP value [40] is a great tool similar to LIME where interpretations measures the impact of having a certain value for a given feature in comparison to the prediction. This section discusses Shapley value and how the SHAP (SHapley Additive exPlanations) value arises from the concept of Shapley. It also demonstrates how SHAP values increase the transparency of the model. An example. Figure 13 shows an output SHAP plot of an instance of cancer dataset2. The output value is the prediction for that observation. The base value is the value that would be predicted if the features are unknown for the current output i.e. mean prediction. Features (red/blue) that push the prediction higher (to the right) are shown in red, and those pushing the prediction lower are in blue. Fig. 13. Example: output of SHAP plot. The explanation in Figure 13 displays features that contribute to push the model output from the base value to the model output. The basevalue is the model output average over the passed training dataset. Features that push the prediction higher are color-coded in red. Their size shows the extent of the effect of the feature visually. Features that push the prediction lower are depicted in blue color. We predicted 3.89, whereas the basevalue is 1.837. The biggest impact comes from worst area i.e. 989.5. Though the mean concave points value has high effect (0.04079) increasing the prediction. If we subtract the length of the blue bars from the length of the pink bars, it equals the distance from the base value to the output (Here, 3.89 + 3.837 5.89 = 1.837). Bene ts. An important advantage of SHAP value lies in its transparency and interpretability locally, which implies that each observation can have its own SHAP value set. It is possible to explain why a instance gets its prediction and the contributions of its predictors. Example scenarios are: When a model denies a loan to an individual and the bank is legally bound to explain why the loan was rejected. A physician wants to determine the factors responsible for each patient s disease risk so that he can directly look at those risk factors with directed health interventions. The traditional variable 2Breast Cancer Dataset: https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic) ACM Comput. Surv. 22 R. Dwivedi et al. importance algorithms provide results for the entire population ignoring individual scenarios. The local interpretability facilitates to locate and compare the effect of these factors. Shapley Value vs. LIME. Shapley Value provides local and global interpretation including explanation with theoretical inferences. However, it is computationally expensive to calculate the Shapley Value in comparison to LIME. This problem may be resolved with the recently developed kernel SHAP method which applies a fast kernel approximation. However, crunching large background data still incurs high computational costs. LIME is an optimal alternative for few models such as knn model in terms of computation cost. However, it fails to work out-of-the-box on all models. For example, LIME cannot handle the requirement of XGBoost to use xgb.DMatrix() on the input data. Contrary, SHAP provides fast, reliable evaluations and incorporates TreeExplainer, which optimally estimates Shapley Values through XGBoost. 6.8 Sotware Libraries for Feature-based XAI Techniques In the following, we present few software libraries, which can be used to implement feature-based XAI techniques. Table 8 describes frameworks, which can be used to implement the following techniques. Feature-Based XAI Techniques Basic Li- braries* Keras, Tensorflow, PyTorch Lime Shap Skater eli5 pdpbox XAI Feature Importance (Section 6.1) Partial Dependence Plots (Section 6.2) Individual Conditional Expecta- tion (Section 6.3) Global Surrogate (Section 6.5) LIME (Section 6.6) Shapely Value (Section 6.7) Accumulated Local Effects Plot (Section 6.4) Table 8. Programming frameworks for Advanced XAI Techniques. ELI53. an abbreviated form of Explain Like I m 5 a popular Python package which provides explanation and a visualisation of the predictions of ML model, and aids in debugging an ML classi er. There are two primary ways to assess how an ML model works using ELI5: (1) Global ELI5 provides the method show_weights() to explain how parameters are acting with respect to the entire model; (2) Local ELI5 provides show_prediction() to look at a speci c instance of a prediction, and an explanation for the model prediction for that instance. SHAP4. a popular libraries used for model explainability, based on Shapley values which measure the in uence of various features, i.e. the contribution of every feature of the dataset towards the prediction. It is capable of visualizing both local and global interpretations. Local explanation consists of why prediction on an individual instance was made and Global explanation provides a summary of feature importance over the entire dataset. SHAP offers a method to visualise the overall data patterns and understand the model in a global context. SHAP has a number of explainers: deep (based on the DeepLIFT algorithm [40, 52]), gradient, kernel (to estimate SHAP for regression and classi cation models), linear (to compute the SHAP values for a linear model with independent features), tree (to 3ELI5 documentation: https://eli5.readthedocs.io/ 4Shap Documentation: https://github.com/slundberg/shap ACM Comput. Surv. Explainable AI (XAI): Core Ideas, Techniques and Solutions 23 calculate SHAP values for Decision Tree models), sampling (computes SHAP values by using random permutation of features). SHAP is a robust method that provides the integration of several methods such as feature importance, feature dependence, interactions, clustering and summary plots, all included in a single library. SHAP is computationally expensive on certain models such as KNN but runs fast on trees, such as gradient boosted trees from XGBoost. XAI 5. It is an explainability toolbox for ML which is speci cally designed for data analysis and model evaluation as follows: (1) Data Analysis: It facilitates the user to balance the class by up-sampling or down-sampling and thereby splitting a testing and training dataset. It has the ability of visualizing the correlation matrix thereby explaining the model s behavior. (2) Model Evaluation: The interaction between the predictions and the input features can be analyzed by building a deep learning model. Pdpbox 6. Partial Dependence Plot toolbox, abbreviated as PDPbox is similar to ICEbox. It is used to compute and visualize the impact or effect of features on the prediction of target variable for any scikit-learn algorithm, thereby explaining the prediction of the model. This library is one step ahead of random forest, as Pdpbox provides the direction in which the feature is in uencing the prediction. Skater 7. It is a Python library used for interpreting and identifying relationships between data or features that act as input to the model and the nal prediction the model makes. It is used to reveal the interpretations of black box models globally as well as locally. With Skater, one can perform global interpretations using partial dependence plots and feature importance techniques. For example, how a loan prediction model uses the customer s credit history, account status, income to approve or deny his loan. Skater can also measure how a model s performance alters over time after its deployment in a production environment. Skater has the ability to interpret allowing the practitioner to measure how feature interactions varies across different versions of models. Tf-explain8. a library used for Keras API (TensorFlow v2.0 work ow), primary features include tensor- board integration and callbacks. Supports visualization of heatmaps and gradients for hyper-parameter tuning or confusion matrix generation, for explanation and visualization of the prediction. The methods include GradCAM [50] and SmoothGrad [54]. 7 EXAMPLE-BASED TECHNIQUES Section 7.1-7.5 describe commonly used example-based XAI techniques. Section 7.6 presents program- ming frameworks to implement these techniques and their comparative analysis. 7.1 Anchors Anchors capture limit/ suf cient conditions of features at which the model gives a high precision prediction. Anchors support model-agnostic approaches for classi cation models of text, image or tabular data. Anchor takes into account the global dataset and then give the anchor feature-values, using If-Then rules for nding features associated with input instances responsible for prediction. Anchors are similar to LIME as they both provide local explanations linearly. However, LIME only covers a local region and may not be generalisable. If the same perturbation space is allotted to both LIME and Anchors, the latter will build a more valid region of instances which better describe the model s behaviour. 5An eXplainability Toolbox for Machine Learning: https://github.com/EthicalML/xai 6Python partial dependence plot toolbox: https://pdpbox.readthedocs.io/ 7Python Library for Model Interpretation/Explanations: https://oracle.github.io/Skater/ 8Interpretability methods for Tensor ow 2.0: https://tf-explain.readthedocs.io/ ACM Comput. Surv. 24 R. Dwivedi et al. Example. Consider an example of Heart Disease dataset, with a feature instance suggesting heart disease for an individual. The value of thalach of this person is 131 and ca is 3, therefore AnchorTabular method reaches the following conclusions (See Figure 14). Further, We apply the AnchorTabular method to assess which features contribute signi cantly for such type of prediction. Fig. 14. Example: An Output of Anchor. The person s maximum heart rate is 131 (which is less than 138). The blood vessels coloured by fluoroscopy are 3 (greater than 1). As the maximum heart rate of a person should be high and the blood vessels coloured by fluoroscopy should be low, the above features act as Anchors for the patient, and deduce that the person has a heart disease. 7.2 Counterfactual Explanations Determining what should be the change in features in order to switch prediction" when an ML-based model is applied to real world data (along with the rationale behind its outcome) is important. Counterfactual explanations is a model-agnostic XAI technique that provides changes that can be made tofeature values to change the current output to a prede ned output. Counterfactual explainer method work on black box models and best suited for binary datasets. They can also be applied on classi cation with more than 3 target values, but the performance degrades as compared to binary classi cation. Heart Disease dataset example:. a value of condition eld 0 or 1 signi es the presence or absence of heart disease. From this dataset, we have considered a speci c instance where the patient has a heart disease. Figure 15 shows this instance. Using input in Figure 15, we generate 4 different counterfactuals as shown in Figure 16. All of these exhibit the minimum changes to the feature values in order to change the condition of a patient. Fig. 15. An input to a Counterfactual A specific instance where the patient has a heart disease. Fig. 16. Diferent counterfactuals outputs of Counterfactual Explainer. The following are the observations regarding the output: (i) The sex, age or the type of chest-pain ( eld cp) of a person suffering from a heart disease cannot be changed. Therefore, these features have to be xed in each of the counterfactuals. (ii) the 4 different counterfactuals are all different schemes to change ACM Comput. Surv. Explainable AI (XAI): Core Ideas, Techniques and Solutions 25 the target value from 1 to 0. For example, the second counterfactual on the list indicates that reduction of cholesterol leads to a decrease in the intensity of heart disease. It also shows that there should be normal results and no defects upon performing the Thallium test on the heart. (iii) a recurring theme in all counterfactuals is the reduction of ca from 2 to 0. ca signi es the number of blocked vessels of the heart ca is the most important feature contributing to having a heart disease. Hence, the most important factor in changing the condition is to reduce the number of blocked vessels by using methods like angioplasty. Counterfactual guided by Prototypes. Refers to explanations described on the basis of a prototype i.e. using a representive sample of instances belonging to a class. Counterfactuals guided by prototypes is more advanced and accurate, and works on black-box models. This method is a model agnostic approach to interpret results using the prototypes of classes of target variable. It is much faster than counterfactual, as prototype speed up the search process signi cantly by directing counterfactual to the prototype of a particular class. 7.3 Contrastive Explanations Method (CEM) CEM is a XAI Method for generating local explanations on a black box model. CEM de nes explanations for classi cation models by providing insight on preferable features along with unwanted features i.e. Pertinent Positives (PP) and Pertinent Negatives (PN). It is the rst method that provides the explanations of both what should be minimally present and what should be necessarily absent from the instance to be explained in order to maintain the original prediction class. Further, it also identi es a minimal set of features that is adequate to differentiate it from the nearest different class. Using CEM, the accuracy of the ML model can be enhanced by looking at cases of mis-classi ed instances and using the explanations provided by CEM. The two categories of explanations includes Pertinent Positives and Pertinent Negatives [12]. The Pertinent Positives explanation nds the features that are necessary for the model to predict the same output class as the predicted class. For example, this includes the important pixels of an image, the features having a high feature weight, etc. PP works similarly to Anchors. The Pertinent Negatives explanation nds the features that should be minimally and suf ciently be removed from an instance whilst maintaining the original output class. PN works similarly to Counterfactuals. An example using heart disease dataset. Figure 17 generates counter explanations in terms of Pertinent Negative. The original prediction was 0 which is altered to 1 after applying CEM with pertinent negative. Pertinent Negative explanations work similarly to the counterfactual explanations, as describe above. The CEM values in the array which are different from the original one change the prediction class. Some of them are cp, ca, thal. Hence, changes in these features should necessarily be eliminated to retain the original prediction as 0 as they are responsible for ipping the prediction class. Fig. 17. An output by CEM using heart disease dataset. ACM Comput. Surv. 26 R. Dwivedi et al. 7.4 Kernel and Tree Shapley Additive Explanations The goal of SHAP is to calculate the impact of every feature on the prediction [22, 40]. Compared to Shapley values, Kernel SHAP provides computational ef ciency and accurate approximation in higher dimensions. In Kernel SHAP, the full model has been utilized that is already trained instead of retraining models with different feature permutations. Here, the missing features" are replaced with samples from the data". This means that the "absent feature values" are equated with "feature value replaced by random feature values selected from data". Now, this modi ed feature space is tted to the linear model and the coef cients of this model act as Shapley values. Local explanation. An example of local explanation using heart disease dataset is illustrated in Figure 18. The base value is the average of all output values of the model on the training data(here : -0.3148). Pink values drag/push the prediction towards 1 (pushes the prediction higher i.e. towards having heart disease) and the blue towards 0 (pushes the prediction lower i.e. towards no disease). The magnitude of in uence is determined by the length of the features on the horizontal line. The value shown correspond- ing to the feature are the values of feature at the particular index (eg. 2.583 for ca). Here, the highest in uence is of ca for increasing the prediction value and of sex for decreasing the value. Fig. 18. An example of local explanation using heart disease dataset. Global explanation. Figure 19 plots visualizes the impact of features on the prediction class 1. The features are arranged such that the highest in uence is of topmost feature. Thus, ca is the feature that in uences prediction the most; followed by thal and so on. The colour shades depicts the direction in which the feature impacts the prediction. For example, higher shap values of ca are shown in red colour which means high feature value. The higher the value of ca, the higher is the SHAP value i.e. more towards 1. High value of ca indicates more chances of Heart Disease. However, it is the opposite for some features: High thalach will indicate no heart disease. Tree Shapley Additive Explanations. It is an algorithm to compute exact SHAP values for decision trees based models. The algorithm provides interpretable explanations suitable for regression and classi cation of models with tree structure applied to tabular data [22, 39]. It attributes the change of a model output with respect to a baseline (e.g., average over a reference set or inferred from node data) to each of the input features. Similar to Kernel SHAP, the Shapley value of each feature is computed by averaging the difference of the model output observed when the feature is part of a group of present" features. 7.5 Integrated Gradients Integrated Gradients, also known as Path-Integrated Gradients is an XAI technique that assign an importance value to each feature of the input using the gradients of the model output [57]. It is a local method that helps explain each individual prediction. This method provides the speci c attributions of the feature inputs that are positive attributions and negative attributions. Positive attributions are attributions that contribute or in uence a model to make the decision whereas negative attributions are attributions that contribute or in uence a model against a decision made. ACM Comput. Surv. Explainable AI (XAI): Core Ideas, Techniques and Solutions 27 Fig. 19. An example of global explanation using heart disease dataset. An example. Let us understand it more using an example using MNIST dataset, as presented in positive attributions and negative attributions. Consider an example of Fashion MNIST dataset, which consists of 70,000 grayscale 28x28 images of different clothing items. The label consists of 10 classes, which denotes different kinds of clothing items such as shirt, hoodies, shoes and jeans. The rst example (top part of Figure 20) is is an image of a shoe. The attributions section shows a melange of positive and negative attributions together. It can be observed from the bar on the right side that green signi es positive attributions and purple signi es negative attributions. The shoe is unique compared to other clothing items, and hence, it has a lot more positive attributes than negatives. The lining, collar and back part of the shoe are the main pixels that in uence the decision of the model. On the other hand, the negative attributions are negligible for this particular instance. The second example (bottom part of Figure 20) is an image of a shirt where there is an equal number of positive and negative attributions. The pixels around the collar and the sleeves are the biggest positive attributions. However, the middle portion of the shirt can be mistaken to be a part of a pair of jeans or trousers. Therefore, due to this ambiguity, they are the negative attributions for the prediction. All in all, we can af rm that when the positive attributions outweigh the negative attributions, the model makes the correct prediction. 7.6 Sotware Libraries for Example-based XAI Techniques In the following, we present some of the software libraries, which can be used to implement example- based XAI techniques. Table 9 describes frameworks, which can be used to implement these techniques. Diverse Counterfactual Explanations (DiCE) for ML9. DiCE [45] is a package that gives the what-if" explanations for the model prediction. The ML models perform prediction based on the values/features present in the data. However, in practical situations, this might not be enough. It is important to know the answer to the question What should be the modi cations in features so that the prediction ips?". DiCE not only provides the most in uential feature for the prediction but also recommends the feature modi cations needed for the result. It implements counterfactual explanations that include perturbed features which in turn will lead to the required result.Additionally, if some features are dif cult to modify 9DiCE for ML: https://github.com/interpretml/DiCE ACM Comput. Surv. 28 R. Dwivedi et al. Example-Based XAI Techniques Basic Li- braries* Keras, Tensorflow, PyTorch DiCE Alibi Tf-explain Anchors (Section 7.1) Counterfactuals (Section 7.2) Prototype Counterfactuals (Section 7.2) Contrastive Explanation Method (Section 7.3) Kernel Shap (Section 7.4) Tree Shap (Section 7.4) Integrated Gradients (Section 7.5) Table 9. Programming frameworks for Example-based XAI Techniques. (e.g., nancial status is dif cult to change than working hours per week), DiCE allows input of relative dif culty by specifying feature weights . A higher feature weight means that the feature is dif cult to modify than that of others. ALIBI. Alibi [34] is an open source Python library which aims at ML-based model inspection and interpretation. The library consists of a wide range of algorithms, most of which focus on black box models and local interpretation i.e. interpretation for the prediction to be explained at a particular instance. This library comprises different types of explainers depending on data we are dealing with. 8 SOFTWARE TOOLKITS This section presents toolkit available for building XAI applications. Unlike the previous sections (Section 4, Section 5, Section 6, Section 7), we consider the candidates for a toolkit, which include an extensive set of tools and techniques (instead of focusing one single aspect of XAI), in order to cover different aspects of XAI techniques such as visualization tools, tools to debug and evaluate ML models and so on. In the following, we present some of the toolkits available both commercially and open source. Fig. 20. An example of Integrated Gradients using Fashion MNIST dataset. ACM Comput. Surv. Explainable AI (XAI): Core Ideas, Techniques and Solutions 29 8.1 What-If Tool What-If Tool (WIT)10 is an attribute of Google s open-source Tensorboard web application. It is a user- friendly tool that has the ability to debug and evaluate ML models effectively. This tool aid to understand a model in a simple and intuitive way through a visual interface. It works on both classi cation and regression models. The prominent feature of WIT is that it allows everyone from ML researchers and developers to non-technical stakeholders for extensive use as it is free of complex coding. It also provides answers to different what-if scenarios and aid to visualize and explore the counterfactual examples. For example, in a classi cation model, it returns the instance with the most similar features but different prediction. Thus it enables and simpli es the task to assess the modi cations in model in real time. One of the other features is its visualization of the dataset, which inculcates how diverse is the data, impact on model s results on altering different feature values, how the hyperparameters should be tuned and other observations. Also, it can be bene cial in comparing the results of two different models on the same input data. It is also capable of post training evaluation. The what-if tool has 3 tabs with different features: Data-point Editor, Performance & Fairness and Feature: The datapoint editor shows the prediction for every datapoint passed. Using Data-point editor, we are also able to inspect individual input points and create custom visualisations by changing the feature values. The Performance and Fairness provides the overall performance using evaluation metrics such as confusion matrix, ROC curve etc. It provides a way to slice data of different features and then applies different strategies for accuracy and fairness enhancement. The Features tab shows the balance in the dataset for every feature by assigning the range of every feature prior to the training. 8.2 TensorBoard TensorBoard is a visualization tool that helps in inspecting the inner workings of the model as training a deep neural network can be complex and dif cult to comprehend. It is used for measuring the evaluation metrics like loss and accuracy needed during the machine learning work ow and projecting embeddings to a lower dimensional space, etc. TensorBoard is also capable of debugging and optimizing TensorFlow programs. All TensorFlow programs have two basic components: Operations and Tensors. Tensors are values of a multidimensional array i.e. data is stored in tensors and operations manipulate the stored data. The data is fed into the model which consists of a set of operations, and tensors ow between the operations to get the output tensor. The current implementation of TensorBoard allows ve visualizations: scalars, images, audio, histograms and graphs: TensorBoard s Scalar Dashboard visualizes scalar valued tensors that vary with time similar to loss or learning rate. The Image Dashboard can display saved images. It can also be used to build an image classi er on arbitrary image data as it is able to display input images of a network, generated output images of an autoencoder or a GAN. Audio enables saving audio which can be embedded and played in the Audio Dashboard using audio widgets. The Histogram Dashboard visualizes the distribution of a non-scalar Tensor over different periods of time. It can be used to visualize weight or bias matrices of a neural network. The Graph Explorer can visualize a TensorBoard graph, allowing understanding of the TensorFlow model and its operations. Tensorboard also provides an Embedding Projector, which aids in visualizing high-dimensional data and examining embedding layers. For example, in sentiment analysis it allows searching for speci c terms, and highlights words that are nearby in the embedding space. It requires a checkpoint through which it reads the input data as well as metadata such as vocabulary les or additional information about the layer to visualize. Another component of Tensorboard is summary, a special operation that is 10https://pair-code.github.io/what-if-tool/ ACM Comput. Surv. 30 R. Dwivedi et al. required to visualize the model parameters such as weights and biases of a neural network, evaluation metrics, and images such as input images to a network. It feeds in a regular tensor and outputs the summarized data to the disk. 8.3 InterpretML InterpretML is an open-source Python toolkit by Microsoft for training intelligible models and explaining black-box systems. It is based on human interpretation of the global and local explanations of the model. It also simpli es debugging the model to be able to understand the predictions. InterpretML has the ability of comparing different XAI methods using the same feature as well as it optimizes real-life datasets. The different XAI methods include Decision Tree, Decision Rule List, Linear Regression, Logistic Regression, SHAP Kernel Explainer, SHAP Tree Explainer, LIME, Morris Sensitivity Analysis, and Partial Dependence. It includes Explainable Boosting Machine (EBM), which is an algorithm for explaining glass-box models with higher accuracy.There are currently 3 features available: (i) tabular data interpretability: what-if based interactive visualization is available in the toolkit, where a user can assess the modi cations for features of a particular data point in the model s predictions; (ii) Interpretability for text data A text-speci c visualization dashboard is also available for a text classi cation and sentimental analysis; (iii) Counterfactual example analysis using DiCE The demo analysis is also available where it recommends the required modi cations to the input features so that the model yield the desired output. Other features of the dashboard include ltering the data and creating cohorts. A Model performance tab visualizes model performance metrics, as well as the distribution of rejection probability. Overall model explanations show techniques like feature importance and individual feature importance factors. 9 DESIGN CONSIDERATIONS FOR IMPLEMENTING XAI Enhancing explainability into AI systems can bring in many positives. However, the implementation of XAI is not the easiest of tasks. In the following, we discuss the considerations while implementing XAI models: Trade-off: XAI vs Model Performance. While applying XAI concepts to models, we may have to choose between model interpretability and model performance [26]. For example, few simple linear or tree-based models can easily explain the model decisions that lead to a speci c insight or prediction. Conversely, complex ensemble and deep learning models often produce superlative performance, but they are considered as black-box models as it is very dif cult to assess model s decisions. Different users need different forms of explanation in different situations. A decision or recommenda- tion may have to be explained in many ways, depending on audience requirements and the factors in different scenarios [55]. For understanding the system functioning, users may have questions such as, what kind of data was used by the system, the origin of such data and the reason why that data was chosen; how does the model work and what factors impact the decision-making process; why a speci c result was achieved. To gure out what kind of explanations are required, it is necessary to engage with all stakeholders and build a robust system design. System design often needs to balance competing requirements. XAI technologies utilize various schemes and approaches, each with its own pros and cons. These methods when used in different applications, the interpretability, accuracy, and privacy varies. For instance, in healthcare and nance applications, AI systems are privy to con dential data for making decisions and offering recommenda- tions. Explainability in such cases differs as businesses must keep in mind to which extent they need to be transparent. This puts a question mark on the suitability of such systems for applications where ACM Comput. Surv. Explainable AI (XAI): Core Ideas, Techniques and Solutions 31 the knowledge of the decision-making process is vital for purposes of general acceptance and overall accountability. The quality of data is part of the XAI. Modern AI methods rely on huge volumes of data for making predictions and decisions [55]. Being aware of data quantity and its provenance in AI systems ensures system explainability. For instance, image data might be biased against minorities, social media data might be restricted to a particular demographic, or city sensor data might only represent a particular neighborhood. Explainability may not always be the priority while designing an AI system. Explainability needs must be looked at within the overall objectives of the system. Figure 21 shows various AI applications based on human participation and cost of poor decisions. The extent of requisite explainability differs from one experiment to another. In the example of shopping recommendations, the customer may not want an explanation for the items recommended. On the other hand, in situations where ML model is utilized for making crucial decisions, explainability is paramount. The bottom left quadrant represents the most successful AI use cases, where both potential cost and human participation are low. Decisions listed in the top right quadrant, e.g. for credit risk pro ling, medical diagnosis etc, represent exponentially increasing cost. The top left quadrant features decisions where any errors can result in disastrous consequences. If AI cannot explain itself in these domains, then its risk of making a wrong decision may override its accuracy and decision-making ef cacy. Fig. 21. XAI: Cost of poor decisions vs Human participation [36]. 10 CONCLUSION In this survey, we have covered a variety of XAI techniques currently in use, ranging from white box models, such as linear models, decision trees and generalized additive models which are ML-based models. The techniques that are intrinsically more explainable and interpretable as compared to their black box counterparts to feature-based techniques such as LIME, SHAP and global surrogate models in addition to plots such as PDP, ICE and ALE have been presented along with their role in the progress and development of AI. In essence, this survey seeks to facilitate structured and acute information for the coupled researchers. We have explored our discussions beyond what has been gone so far in the XAI domain toward the idea of distributed AI, a paradigm that imposes storage of data on multiple nodes when implementing AI models in practice, including privacy, transparency, and fairness. We have also investigated the implications of espousing XAI techniques in the context of different applications such as livestock mart and healthcare, unveiling the potential of XAI to compromise the privacy of a user. ACM Comput. Surv. 32 R. Dwivedi et al. Our cogitations towards the future of XAI, articulated in the contrasting discussions carried out throughout this work, capitulate on the cogent need for apt understanding of the capability and limita- tions emerged up by XAI techniques. It is our prevision that model interpretability must be considered in conjunction with constraints and requirements associated with data explainability, model explainability, fairness and accountability. A progenitive implementation and deployment of AI schemes in institutions and organizations worldwide will be only assured upon disquisition of all AI axioms jointly. REFERENCES [1] Plamen Angelov and Eduardo Soares. 2020. Towards explainable deep neural networks (xDNN). Neural Networks 130 (2020), 185 194. https://doi.org/10.1016/j.neunet.2020.07.010 [2] Daniel W Apley and Jingyu Zhu. 2016. Visualizing the effects of predictor variables in black box supervised learning models. arXiv preprint arXiv:1612.08468 (2016). [3] Sebastian Bader and Pascal Hitzler. 2005. Dimensions of Neural-symbolic Integration - A Structured Survey. In We Will Show Them! Essays in Honour of Dov Gabbay, Volume One, Sergei N. Art mov, Howard Barringer, Artur S. d Avila Garcez, Lu s C. Lamb, and John Woods (Eds.). College Publications, 167 194. [4] Alejandro Barredo Arrieta, Natalia D az-Rodr guez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera. 2020. Explainable Arti cial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion 58 (2020), 82 115. https://doi.org/10.1016/j.inffus.2019.12.012 [5] David Beckett, Tim Berners-Lee, Eric Prud hommeaux, and Gavin Carothers. 2014. RDF 1.1 Turtle. World Wide Web Consortium (2014), 18 31. [6] Tarek R. Besold, Artur S. d Avila Garcez, Sebastian Bader, Howard Bowman, Pedro M. Domingos, Pascal Hitzler, Kai-Uwe K hnberger, Lu s C. Lamb, Daniel Lowd, Priscila Machado Vieira Lima, Leo de Penning, Gadi Pinkas, Hoifung Poon, and Gerson Zaverucha. 2017. Neural-Symbolic Learning and Reasoning: A Survey and Interpretation. CoRR abs/1711.03902 (2017). arXiv:1711.03902 http://arxiv.org/abs/1711.03902 [7] Umang Bhatt, Alice Xiang, Shubham Sharma, Adrian Weller, Ankur Taly, Yunhan Jia, Joydeep Ghosh, Ruchir Puri, Jos M. F. Moura, and Peter Eckersley. 2020. Explainable Machine Learning in Deployment. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (Barcelona, Spain). ACM, New York, NY, USA, 648 657. https://doi.org/10.1145/ 3351095.3375624 [8] Lorenz B hmann, Jens Lehmann, and Patrick Westphal. 2016. DL-Learner A framework for inductive learning on the Semantic Web. Journal of Web Semantics 39 (2016), 15 24. [9] Diogo V Carvalho, Eduardo M Pereira, and Jaime S Cardoso. 2019. Machine learning interpretability: A survey on methods and metrics. Electronics 8, 8 (2019), 832. [10] NVIDIA Corporation. 2015. NVIDIA Collective Communications Library (NCCL). Retrieved July 15, 2021 from https: //developer.nvidia.com/nccl [11] Jeffrey Dean, Greg S Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Quoc V Le, Mark Z Mao, Marc Aurelio Ranzato, Andrew Senior, Paul Tucker, et al. 2012. Large scale distributed deep networks. (2012). [12] Amit Dhurandhar, Pin-Yu Chen, Ronny Luss, Chun-Chen Tu, Paishun Ting, Karthikeyan Shanmugam, and Payel Das. 2018. Explanations based on the missing: Towards contrastive explanations with pertinent negatives. In Advances in Neural Information Processing Systems. 592 603. [13] Filip Karlo Do ilovi c, Mario Br ci c, and Nikica Hlupi c. 2018. Explainable arti cial intelligence: A survey. In 2018 41st International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO). 0210 0215. https://doi.org/10.23919/MIPRO.2018.8400040 [14] Derek Doran, Sarah Schulz, and Tarek R. Besold. 2017. What Does Explainable AI Really Mean? A New Conceptualization of Perspectives. In Proceedings of the First International Workshop on Comprehensibility and Explanation in AI and ML 2017 co-located with 16th International Conference of the Italian Association for Arti cial Intelligence (AI*IA 2017), Bari, Italy, November 16th and 17th, 2017 (CEUR Workshop Proceedings, Vol. 2071), Tarek R. Besold and Oliver Kutz (Eds.). CEUR-WS.org. http://ceur-ws.org/Vol- 2071/CExAIIA_2017_paper_2.pdf [15] Finale Doshi-Velez and Been Kim. 2017. Towards A Rigorous Science of Interpretable Machine Learning. arXiv:1702.08608 [stat.ML] [16] Basil Ell, Andreas Harth, and Elena Simperl. 2014. SPARQL query verbalization for explaining semantic search engine queries. In European Semantic Web Conference. Springer, 426 441. [17] Aaron Fisher, Cynthia Rudin, and Francesca Dominici. 2018. Model class reliance: Variable importance measures for any machine learning model class, from the Rashomon perspective. arXiv preprint arXiv:1801.01489 68 (2018). ACM Comput. Surv. Explainable AI (XAI): Core Ideas, Techniques and Solutions 33 [18] Jerome H Friedman. 2001. Greedy function approximation: a gradient boosting machine. Annals of statistics (2001), 1189 1232. [19] Jerome H Friedman, Bogdan E Popescu, et al. 2008. Predictive learning via rule ensembles. The Annals of Applied Statistics 2, 3 (2008), 916 954. [20] Giuseppe Futia and Antonio Vetr . 2020. On the integration of knowledge graphs into deep learning models for a more comprehensible AI Three challenges for future research. Information 11, 2 (2020), 122. [21] Krishna Gade, Sahin Cem Geyik, Krishnaram Kenthapadi, Varun Mithal, and Ankur Taly. 2019. Explainable AI in Industry. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (Anchorage, AK, USA) (KDD 19). Association for Computing Machinery, New York, NY, USA, 3203 3204. https://doi.org/10.1145/3292500.3332281 [22] Mar a Vega Garc a and Jos L Aznarte. 2020. Shapley additive explanations for NO2 forecasting. Ecological Informatics 56 (2020), 101039. [23] Christine Golbreich, Evan K Wallace, and Peter F Patel-Schneider. 2009. OWL 2 Web Ontology Language new features and rationale. W3C working draft, W3C (June 2009) http://www. w3. org/TR/2009/WD-owl2-new-features-20090611 (2009). [24] Alex Goldstein, Adam Kapelner, Justin Bleich, and Emil Pitkin. 2015. Peeking inside the black box: Visualizing statistical learning with plots of individual conditional expectation. Journal of Computational and Graphical Statistics 24, 1 (2015), 44 65. [25] Brandon M Greenwell. 2017. pdp: An R Package for Constructing Partial Dependence Plots. R J. 9, 1 (2017), 421. [26] David Gunning and David W Aha. 2019. DARPA s explainable arti cial intelligence program. AI Magazine 40, 2 (2019), 44 58. [27] Xin He, Kaiyong Zhao, and Xiaowen Chu. 2019. AutoML: A Survey of the State-of-the-Art. arXiv:1908.00709 [cs.LG] [28] Pascal Hitzler, Federico Bianchi, Monireh Ebrahimi, and Md Kamruzzaman Sarker. 2020. Neural-symbolic integration and the semantic web. Semantic Web 11, 1 (2020), 3 11. [29] Pascal Hitzler, Markus Kr tzsch, Bijan Parsia, Peter F Patel-Schneider, and Sebastian Rudolph. 2009. OWL 2 web ontology language primer. W3C recommendation 27, 1 (2009), 123. [30] Andreas Holzinger, Chris Biemann, Constantinos S Pattichis, and Douglas B Kell. 2017. What do we need to build explainable AI systems for the medical domain? arXiv preprint arXiv:1712.09923 (2017). https://doi.org/10.48550/ARXIV.1712.09923 [31] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, et al. 2019. Gpipe: Ef cient training of giant neural networks using pipeline parallelism. Advances in neural information processing systems 32 (2019), 103 112. [32] SNOMED International. 2022. Data Analytics with SNOMED CT. Retrieved June 29, 2022 from https://con uence.ihtsdotools. org/display/DOCANLYT/Data+Analytics+with+SNOMED+CT [33] Bahador Khaleghi. 2019. The Why of Explainable AI. Blog Article https://www.elementai.com/news/2019/the-why-of- explainable-ai. [34] Janis Klaise, Arnaud Van Looveren, Giovanni Vacanti, and Alexandru Coca. 2019. Alibi: Algorithms for monitoring and explaining machine learning models. https://github.com/SeldonIO/alibi [35] Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. 2016. Adversarial Machine Learning at Scale. CoRR abs/1611.01236 (2016). arXiv:1611.01236 http://arxiv.org/abs/1611.01236 [36] Accenture Labs. 2020. Understanding Machines: Explainable AI. Technical Report https://www.accenture.com/_acnmedia/ pdf-85/accenture-understanding-machines-explainable-ai.pdf. [37] Mu Li, David G Andersen, Jun Woo Park, Alexander J Smola, Amr Ahmed, Vanja Josifovski, James Long, Eugene J Shekita, and Bor-Yiing Su. 2014. Scaling distributed machine learning with the parameter server. In 11th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 14). 583 598. [38] Joanne S Luciano, Bosse Andersson, Colin Batchelor, Olivier Bodenreider, Tim Clark, Christine K Denney, Christopher Domarew, Thomas Gambet, Lee Harland, Anja Jentzsch, et al. 2011. The Translational Medicine Ontology and Knowledge Base: driving personalized medicine by bridging the gap between bench and bedside. In Journal of biomedical semantics, Vol. 2. BioMed Central, 1 21. [39] Scott M. Lundberg, Gabriel Erion, Hugh Chen, Alex DeGrave, Jordan M. Prutkin, Bala Nair, Ronit Katz, Jonathan Himmelfarb, Nisha Bansal, and Su-In Lee. 2020. From local explanations to global understanding with explainable AI for trees. Nature Machine Intelligence 2, 1 (2020), 2522 5839. [40] Scott M Lundberg and Su-In Lee. 2017. A uni ed approach to interpreting model predictions. In Advances in neural information processing systems. 4765 4774. [41] Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B Tenenbaum, and Jiajun Wu. 2019. The neuro-symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision. arXiv preprint arXiv:1904.12584 (2019). https: //doi.org/10.48550/ARXIV.1904.12584 [42] Azalia Mirhoseini, Anna Goldie, Hieu Pham, Benoit Steiner, Quoc V Le, and Jeff Dean. 2018. A hierarchical model for device placement. In International Conference on Learning Representations. [43] Azalia Mirhoseini, Hieu Pham, Quoc V Le, Benoit Steiner, Rasmus Larsen, Yuefeng Zhou, Naveen Kumar, Mohammad Norouzi, Samy Bengio, and Jeff Dean. 2017. Device placement optimization with reinforcement learning. In International ACM Comput. Surv. 34 R. Dwivedi et al. Conference on Machine Learning. PMLR, 2430 2439. [44] Christoph Molnar. 2019. Interpretable Machine Learning. https://christophm.github.io/interpretable-ml-book/. [45] Ramaravind Kommiya Mothilal, Amit Sharma, and Chenhao Tan. 2019. Explaining Machine Learning Classi ers through Diverse Counterfactual Explanations. CoRR abs/1905.07697 (2019). arXiv:1905.07697 http://arxiv.org/abs/1905.07697 [46] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2018. Anchors: High-Precision Model-Agnostic Explanations.. In AAAI, Vol. 18. 1527 1535. [47] Md. Kamruzzaman Sarker, Ning Xie, Derek Doran, Michael L. Raymer, and Pascal Hitzler. 2017. Explaining Trained Neural Networks with Semantic Web Technologies: First Steps. In Proceedings of the Twelfth International Workshop on Neural-Symbolic Learning and Reasoning, NeSy 2017, London, UK, July 17-18, 2017 (CEUR Workshop Proceedings, Vol. 2003), Tarek R. Besold, Artur S. d Avila Garcez, and Isaac Noble (Eds.). CEUR-WS.org. http://ceur-ws.org/Vol-2003/NeSy17_paper4.pdf [48] Md Kamruzzaman Sarker, Lu Zhou, Aaron Eberhart, and Pascal Hitzler. 2021. Neuro-symbolic arti cial intelligence: Current trends. arXiv preprint arXiv:2105.05330 (2021). https://doi.org/10.48550/ARXIV.2105.05330 [49] Arne Seeliger, Matthias Pfaff, and Helmut Krcmar. 2019. Semantic web technologies for explainable machine learning models: A literature review. PROFILES/SEMEX@ ISWC 2465 (2019), 1 16. [50] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. 2017. Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE international conference on computer vision. 618 626. [51] Alexander Sergeev and Mike Del Balso. 2018. Horovod: fast and easy distributed deep learning in TensorFlow. arXiv preprint arXiv:1802.05799 (2018). [52] Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. 2017. Learning important features through propagating activation differences. arXiv preprint arXiv:1704.02685 (2017). [53] Two Sigma. 2019. Interpretability Methods in Machine Learning: A Brief Survey. Blog Article https://www.twosigma.com/ insights/article/interpretability-methods-in-machine-learning-a-brief-survey/. [54] Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Vi gas, and Martin Wattenberg. 2017. Smoothgrad: removing noise by adding noise. arXiv preprint arXiv:1706.03825 (2017). [55] The Royal Society. 2019. Explainable AI: the basics, Policy Brie ng, The Royal Society. Policy Projects https://royalsociety. org/topics-policy/projects/explainable-ai/. [56] Christian Steinruecken, Emma Smith, David Janz, James Lloyd, and Zoubin Ghahramani. 2019. The Automatic Statistician. In Automated Machine Learning, Frank Hutter, Lars Kotthoff, and Joaquin Vanschoren (Eds.). Springer. https://doi.org/10.1007/ 978-3-030-05318-5_9 [57] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017. Axiomatic attribution for deep networks. arXiv preprint arXiv:1703.01365 (2017). [58] Ajay Thampi. 2020. Interpretable AI, Building explainable machine learning systems. Manning Publications, USA. [59] Leslie G Valiant. 2003. Three problems in computer science. Journal of the ACM (JACM) 50, 1 (2003), 96 99. [60] Danding Wang, Qian Yang, Ashraf Abdul, and Brian Y Lim. 2019. Designing theory-driven user-centric explainable AI. In Proceedings of the 2019 CHI conference on human factors in computing systems. 1 15. [61] Eric P Xing, Qirong Ho, Wei Dai, Jin Kyu Kim, Jinliang Wei, Seunghak Lee, Xun Zheng, Pengtao Xie, Abhimanu Kumar, and Yaoliang Yu. 2015. Petuum: A new platform for distributed machine learning on big data. IEEE Transactions on Big Data 1, 2 (2015), 49 67. [62] Yongfeng Zhang and Xu Chen. 2018. Explainable Recommendation: A Survey and New Perspectives. CoRR abs/1804.11192 (2018). arXiv:1804.11192 http://arxiv.org/abs/1804.11192 ACM Comput. Surv.