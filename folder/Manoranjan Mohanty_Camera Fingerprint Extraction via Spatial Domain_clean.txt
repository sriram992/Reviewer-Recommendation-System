IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY 1 Camera Fingerprint Extraction via Spatial Domain Averaged Frames Samet Taspinar, Manoranjan Mohanty, and Nasir Memon Abstract Photo Response Non-Uniformity (PRNU) based camera attribution is an effective method to determine the source camera of visual media (an image or a video). To apply this method, images or videos need to be obtained from a camera to create a camera ngerprint which then can be compared against the PRNU of the query media whose origin is under question. The ngerprint extraction process can be time consuming when a large number of video frames or images have to be denoised. This may need to be done when the individual images have been subjected to high compression or other geometric processing such as video stabilization. This paper investigates a simple, yet effective and ef cient technique to create a camera ngerprint when so many still images need to be denoised. The technique utilizes Spatial Domain Averaged (SDA) frames. An SDA-frame is the arithmetic mean of multiple still images. When it is used for ngerprint extraction, the number of denoising operations can be signi cantly decreased with little or no performance loss. Experimental results show that the proposed method can work more than 50 times faster than conventional methods while providing similar matching results. Index Terms PRNU, video forensics, camera ngerprint extraction, image forensics. I. INTRODUCTION PRNU-based source camera attribution is a well-studied and successful method in media forensics for nding the source camera of an anonymous image or video [1]. The method is based on the unique Photo Response Non Uni- formity (PRNU) noise of a camera sensor array stemming from manufacturing imperfections. This PRNU noise can act as a camera ngerprint. The PRNU approach is often used in two scenarios: camera veri cation and camera identi cation. Camera veri cation aims to establish if a given query image or a video is taken by a suspect camera. This is done by correlating the noise estimated from the query image or video with the ngerprint of the camera usually is computed by taking pictures from the camera under controlled conditions. In camera identi cation, the potential source camera of the query image or video is determined from a large database of camera ngerprints. One can view camera identi cation as essentially the same as performing n camera veri cation tasks where n is the number of camera ngerprints in the database. However, when performing identi cation, it is assumed that the camera ngerprints are pre-computed. In both veri cation and identi cation, it is often the case that there is no camera available to create ngerprints under con- Samet Taspinar (email: st89@nyu.edu) and Manoranjan Mohanty email: manoranjan.mohanty@nyu.edu) are with Center for Cyber Security, New York University Abu Dhabi, UAE. Nasir Memon (email: nm1214@nyu.edu) is with Department of Computer Science and Engineering, New York University, New York, USA. trolled conditions. Rather, camera ngerprints are estimated from a set of publicly available media assumed to be from the same camera. Such media can have a very diverse range of quality and content and often lacks metadata. For ef cient ngerprint matching in large databases, various approaches have been proposed. Fridrich et al. [2] proposed the use of ngerprint digests in which a subset of ngerprint elements having the highest sensitivity are used instead of the entire ngerprint. Bayram et al. [3] introduced binarization where each ngerprint element is represented by a single bit. Valsesia et al. [4] proposed the idea of applying random projections to reduce ngerprint dimension. Bayram et. al. [5] introduced group testing via composite ngerprint that focuses on decreasing the number of correlations rather than decreas- ing the size (storage) of a ngerprint. Recently, Taspinar et al. [6] proposed a hybrid approach that utilizes both decreasing the size of a ngerprint and the number of correlations. All these methods were designed and tested for images, however they can also be used for videos. Although the image-centric PRNU-based method can be extended to video [7] [9], source camera attribution with video presents a number of new challenges. First, a video frame is much more compressed than a typical image. Therefore, the PRNU signal extracted from a video frame is of signi cantly lower quality than one obtained from an image. As a result, a larger number of video frames are required to compute the ngerprint. In fact, Chuang et. al. [7] found that it is best to use all the frames instead of using only the I- or P-frames to compute a ngerprint. Using a large number of frames can introduce signi cant computation overhead. For example, computing a ngerprint from 60 I-frames of a one-minute HD video requires one to two minutes, whereas 30 to 40 minutes is required if all frames are used. In the case of camera identi cation, the amount of compu- tation can be prohibitive in practical scenarios. For example, for computing ngerprints from a thousand one-minute Full HD videos (using all 1800 frames) using a PC may take more than 20 days. Clearly, with billions of media objects uploaded every day on the Internet, large scale camera source identi cation becomes quickly infeasible. Although camera ngerprints stored in a database may have to be computed just once by a system, computing a ngerprint estimate at run-time from a query video can be prohibitive when faced with a reasonable number of query videos presented to the camera identi cation system in a day. Besides source camera identi cation, digital stabilization operations performed within modern cameras also present a signi cant challenge for PRNU-based source camera veri - arXiv:1909.04573v1 [cs.MM] 10 Sep 2019 IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY 2 cation for video [8], [10], [11]. Video stabilization results in sensor-pixel mis-alignments between individual frames of the video as the geometric transformations performed to compen- sate for camera motion and spatially align each frame are different. An accurate camera ngerprint cannot be obtained using mis-aligned frames as is done with non-stabilized video even if video quality is very high. Although there are some preliminary methods that address source camera veri cation for stabilized video, [8], [10], these methods are either limited in scope or have low performance (low true positive rate) and high computation overhead. An alternate approach to address the stabilization issue for a fairly long video (at least a couple of minutes) [12] is to use a large number of frames for computing the ngerprint. The idea being that with a large number of frames, there will be suf cient number of aligned pixels at each spatial location that can result in the computation of an accurate ngerprint. As discussed above, this approach however, can again introduce high computation overhead unsuitable for practical use. As a third example, modern devices such as smartphones capture different types of media with different resolutions. For example, most cameras don t use the full sensor resolution when capturing a video and downsize the sensor output to a lower resolution by proprietary and often unknown in-camera processing techniques. For such a challenging task PRNU based source camera matching may often fail if only I-frames are used. This paper proposes a computationally ef cient way to compute a camera ngerprint from a large number of media objects, such as individual frames of a video or a large number highly compressed images taken from a social media platform. In contrast to the two-step conventional ngerprint computation method (which rst estimates PRNU noise from each frame using a denoising lter and then averages several estimated individual PRNU noise estimates to get a reliable ngerprint estimate), the proposed method uses a three step ap- proach: frame averaging, denoising, and noise averaging. The frame averaging step gets the arithmetic mean of the frames in spatial domain, resulting in a Spatial Domain Averaged frame (SDA-frame) (Figure 2). Then, in the second step each SDA- frame is denoised, and an averaging of the estimated PRNU noise is done to arrive at the nal ngerprint estimate. The goal here is to minimize the number of denoising operations (as denoising is most expensive step), and also get rid of scene dependent noise by averaging multiple frames. Experiments with VISION dataset [13] and NYUAD-MMD [14] show that the proposed method provides signi cant speed up in comput- ing accurate ngerprints. It achieves signi cantly higher true positive rate than a ngerprint computed by I-frames only and much lower computation cost than a ngerprint obtained from all available frames while yielding similar performance. The rest of the paper has been organized as follows. Section II summarizes the PRNU-based method and provides an overview of how digital video stabilization works. Sec- tion III explains the proposed ngerprint extraction method using SDA-frames as well as an analysis comparing it with the conventional approach. The insights obtained from the analysis are experimentally validated in Section IV. Section V examines applications for which SDA-frames based technique can be used and reports the improvement that can be achieved using an SDA-based method for those cases. Section VI section provides a discussion on future work and concludes the paper. II. BACKGROUND AND RELATED WORK In this section, we provide a brief review of PRNU-based source camera attribution and video stabilization. A. PRNU-based Source Camera Attribution PRNU-based camera attribution is established on the fact that the output of the camera sensor, I, can be modeled as I = I(0) + I(0)K + (1) where I(0) is the noise-free still image, K is the PRNU noise, and is the combination of additional noise, such as readout noise, dark current, shot noise, content-related noise, and quantization noise. The multiplicative PRNU noise pattern, K, is unique for each camera and can be used as a camera ngerprint which enables the attribution of visual media to its source camera. Using a denoising lter F (such as a Wavelet lter) on a set of images (or video frames) of a camera, we can estimate the camera ngerprint by rst getting the noise residual, Wk, (i.e., the estimated PRNU) of the kth image as Wk = Ik I(0) k , I(0) k = F(Ik), and then averaging the noise residuals of all the images. For determining if a speci c camera has taken a given query image, we rst obtain the noise residual of the query image using F and then correlate the noise residual with the camera ngerprint estimate. For images, the PRNU-based method has been well studied. Following the seminal work in [1], much research has been done to improve the scheme [15] [19], and also make camera identi cation effective in practical situations [2], [3], [5], [6], [20]. Researchers have also studied the effectiveness of the PRNU-based method by proposing various counter forensics and anti-counter-forensics methods [21], [22] It has also shown that the PRNU method can withstand a multitude of image processing operations, such as cropping, scaling [23], compression [24], [25], blurring [24], and even printing and scanning [26]. In contrast, there has been lesser work dedicated to PRNU- based camera attribution from a video [27]. Mo Chen et al. [28] rst extended PRNU-based approach to camcorder videos. They used Normalized Cross-Correlation (NCC) to correlate ngerprints calculated from two videos, as the videos may be subject to translation shift, e.g., due to letter-boxing. To compensate for the blockiness artifacts introduced by heavy compression (such as MPEG-x and H26-x compression), they discard the boundary pixels of a block (e.g., a JPEG block). In [29], McCloskey proposed a con dence weighting scheme that can improve PRNU estimation from a video by mini- mizing the contribution from regions of the scene that are likely to distort PRNU noise (e.g., excluding high-frequency content). Chuang et al. [7] studied PRNU-based source camera identi cation problem with a focus on smart-phone cameras. Since smart-phones are subject to high compression, they IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY 3 considered only I-frames for ngerprint calculation and corre- lation. Chen et al. [9] proposed a method to nd PRNU noise from wireless streaming videos, which are subject to blocking and blurring. In their approach, they divided a video frame into multiple blocks and did not consider the blocks having signi cant blocking or blurring artifacts. Chaung et al. [7] showed that the best possible ngerprint could be computed when all the frames are considered (instead of using only the I- or P-frames). However, to the best of our knowledge, ef cient computation of ngerprint from a given video is a relatively unexplored area. B. Af ne Transformation in Video Stabilization Fig. 1: Video Stabilization Pipeline. This gure is a modi ed version of a gure that appeared in [30]. An out-of-camera digital video stabilization process con- tains three major stages: camera motion estimation, motion smoothing, and motion correction (Figure 1) [31] [30]. In the motion estimation step, the global inter-frame motion between adjacent frames of a non-stabilized video is modeled from the optical ow vectors of the frames using an af ne transformation. In the motion smoothing step, unintentional translations, rotations, shearing, are ltered out from the global motion vectors using a low pass lter. Finally, in the motion correction step, stabilized video is created by shifting, rotating, shearing, or zooming frames according to the parameters in the ltered motion vector. Since each video frames can use different parameters, pixels can be misaligned with the sensor array. For example, one frame can be rotated with an angle -1 degree while another by 0.5 degrees. Digital video stabilization presents a big challenge for PRNU-based camera attribution. The frame speci c af ne transformations described above make the PRNU method ineffective as there is misalignment between frames. The brute-force methods [10], [22] proposed to address the sta- bilization issue have had limited success and resulted in low performance. These brute-force methods try to overcome the desynchronization issue by rst nding the stabilization parameters through an exhaustive search and then performing the corresponding inverse af ne transformation. Such methods, therefore, have very high computation overhead. Recently, Mandelli et al. [11] improved over brute-force approaches by using a best- t reference frame in the parameter searching process rather than using the rst frame of the given video. The best- t reference frame is obtained by looking for a frame that matches with the largest number of frames. Their approach also has high computation overhead. III. SPATIAL DOMAIN AVERAGING As mentioned in the introduction, this paper proposes spatial domain averaging for computing camera ngerprints, which reduces the number of denoising operations when many visual objects are available. In the proposed method, ef cient com- putation of a ngerprint is achieved by rst creating averaged frames from a large collection, and using these averaged frames for computing the ngerprint. For example, given a video with m frames, g non-intersecting equal-sized subgroups are formed each with d = m g frames. A Spatial Domain Averaged frame (SDA-frame) is created from each subgroup by getting the mean of the d frames in the subgroup. Then, in the second step, each SDA-frame is denoised, and an averaging of the estimated PRNU noise patterns is done to arrive at the nal camera ngerprint estimate. In this manner, the number of frames that are denoised gets reduced by a factor of d. An SDA-frame obtained from three different images is shown in Figure 2. (a) 1st (b) 2nd (c) 3rd (d) SDA-frame Fig. 2: SDA-frame is the average of 1st, 2nd, and 3rd frames. The proposed method is inspired by the fact that although the denoising lter is designed to remove random noise from an image originating from the camera sensors (e.g., readout noise, shot noise, dark current noise etc.), as well as noise caused by processing (e.g., quantization and compression), it is not able to do a perfect job. Therefore, some scene content leaks into the extracted noise pattern. Averaging in the spatial domain acts as a preliminary lter that smoothens the image and potentially reduces the content noise that leaks into the extracted noise pattern. Of course, the effectiveness of the approach then depends on the nature of the two noise signals. Below we analyzed this fact and characterized the relationship between the noise signal arrived at by using the conventional approach and the SDA-approach. Further, when using the proposed approach, many questions arise. First, does frame-averaging lead to a drop in the accuracy of the ngerprint computed as compared to the conventional method, assuming the same number of images are used for both? If so, what is the trade-off between the decrease in computation and the loss in the accuracy? Can accuracy be increased by utilizing more images in the SDA method? If so, what is the optimal combination of averaging and denoising that leads to the least computation while yielding the best performance? Then, we investigated these questions, both the- oretically and experimentally. We rst provide a mathematical IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY 4 analysis using a simple framework in the two subsections below. We then validate our study in the next section by providing experimental results. The results show that spatial domain averaging strategy can indeed result in signi cant savings in computation while maintaining performance and in some cases, improving it. The rest of this section provides an analysis of spatial domain averaging. To this end, we rst provide an analysis of the conventional method and then analyze the SDA method. A. Conventional method As discussed in Section II, in the conventional method, the camera ngerprint is estimated from n images from a known camera. Each image I can be modeled as I = I(0)+I(0)K+ , where is the random noise accumulated from a variety of sources (as in (1)) and K is the PRNU noise. To estimate K, a denoising lter, F, such as [32], BM3D [33], is used to estimate the noise free signal I(0). Using such a lter, we denote the noise residual as W = I(0)K + + , where is the content noise. This noise is essentially due to sub-optimal denoising lter that is unable to completely eliminate the content from PRNU noise. Then, from the n known image, the camera ngerprint estimate, K, can be obtained using Maximum Likelihood Estimation (MLE) as K = Pn i=1 Wi.Ii Pn i=1 I2 i (2) where Wi is noise pattern extracted from Ii. Note that in the estimated camera ngerprint, K, and are the unwanted noise. The quality of K can be assessed from its variance V ar( K) [34]. The lower the variance is (i.e., images with smooth content), the higher the quality becomes. Assuming that and are independent White Gaussian Noise with variances 2 1 and 2 2 respectively, V ar( K) can be found as (using Cramer-Rao Lower Bound as shown by Fridrich et al. [34]) V ar( K) 2 1 + 2 2 Pn i=1 I2 i . (3) Thus a better PRNU is obtained from lower 2 1 and 2 2 (i.e., high luminance and and low textured image [34]). B. Proposed SDA method In this subsection, we derive the variance of the estimated camera ngerprint obtained using frame averaging. We then compare this variance with that obtained by the conventional approach (in (3)). Suppose I1, I2, . . . , Im are m images used to compute the camera ngerprint using SDA method. With frame averaging, these m images are divided into g = m d disjoint sets of equal size with d pictures in each set. From each set, an SDA- frame is computed. Thereafter, the process is similar to the conventional approach. Each SDA-frame is denoised, and the camera ngerprint is computed from g noise residuals using MLE. Suppose, ISDA i is the SDA-frame obtained from the ith image set. Then ISDA i = Pid j=(i 1)d+1 Ij d = Pid j=(i 1)d+1(I(0) j + I(0) j K + j) d We can write the above equation as ISDA i = I(0),SDA i + I(0),SDA i K + SDA i , (4) where I(0),SDA i is the noise free image, and SDA i is the random noise (from pre- ltering sources) in the SDA-frame. This noise can be written as SDA i = Pid j=(i 1)d+1 j d . Suppose 2 1 is the variance of s (which is assumed to be White Gaussian Noise). Then, the variance of SDA i turns out to be 2 1 d . Suppose W SDA is the noise residual of each SDA-frame, ISDA. Then, W SDA = ISDA F(ISDA) = I(0),SDAK + SDA + , where F is the denoising lter, and = I(0),SDA F(ISDA) is the content noise due to the sub-optimal nature of the denoising lter. Note that is assumed to be independent of PRNU signal I(0),SDAK (although contains content layover I(0),SDA F(ISDA) as is negligible compared to ISDA 0 K [34]. We know that is dependent on the smoothness of the SDA-frames. If the frames contain textured content, is high. Assuming that SDA-frames have similar smoothness to the input frames from which they are created, we consider that and have the same variance 2 2. Using MLE, the camera ngerprint can now be estimated from g SDA-frames ISDA 1 , ISDA 2 , . . . , ISDA g as KSDA = Pg i=1 W SDA i .ISDA i Pg i=1  ISDA i 2 . Using Cramer-Rao Lower Bound, the variance of the esti- mated ngerprint KSDA becomes V ar( KSDA) 2 1 d + 2 2 Pg i=1  ISDA i 2 . (5) In an ideal case, we want that the averaging operation does not degrade the quality of the estimated PRNU from the SDA-frames. In other words, we want that V ar( KSDA) is approximately equal to the variance from the conventional method V ar( K). That is, in other words, using the results from (3) and (5), it is desired that 2 1 d + 2 2 Pg i=1  ISDA i 2 2 1 + 2 2 Pn i=1 I2 i . IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY 5 By simplifying the above equation, we get 2 1 d + 2 2 2 1 + 2 2 Pg i=1  ISDA i 2 Pn i=1 I2 i . Suppose Pg i=1(ISDA i )2 Pn i=1 I2 i = g n k where k = (Pg i=1(ISDA i )2)/g (Pn i=1 I2 i )/n . Note that the value of k is a temporary variable that is less than or equal to 1 as the numerator Pg i=1(ISDA i )2)/g is less than equal to the denominator Pn i=1 I2 i )/n. Putting these values in the above equation, we get g n k 2 1 d + 2 2 2 1 + 2 2 . Putting g = m d in the above equation, we get m k d n 2 1 + d 2 2 d ( 2 1 + 2 2). or, m n k 2 1 + d 2 2 2 1 + 2 2 (6) We then discard the temporary variable, k, from the equa- tion. Since 0 < k 1, the nal equation becomes m n  2 1 + d 2 2 2 1 + 2 2  (7) From (7), we can derive the following concluding remarks: Since d 1, the right hand side of the equation is at least 1. Therefore, the number of images required in the proposed SDA method (i.e., m) will be more than or equal to the number of images required in the conventional method (i.e., n). For smooth images 2 2 is close to zero. So, the impact of SDA-depth, d, will be negligible for such images. Therefore, SDA and conventional approaches will have similar performance. However the SDA technique will be d times faster in the best case. For textured images, when the number of for both tech- niques is equal (i.e., m = n), because 2 is greater than zero, conventional approach is expected to outperform SDA approach. Since 2 2 is greater than zero for textured images, the ratio of images for SDA- divided by conventional approach, m n , will increase as the SDA-depth, d, increases. There- fore, SDA approach will require more images to achieve same performance for textured images. Notice that it is hard to characterize the relationship of 1 and 2, also 1 depends on various factors such as shot noise, exposure time, temperature, illumination, image content and so on. Therefore, we are not focusing on their relationship in this research. In the following section, we experimentally validate the observations listed above. IV. VALIDATION OF ANALYSIS In this section, we experimentally verify the main conclu- sions arrived at by the analysis performed in the previous section. In our experiments we use both at eld and textured images from the VISION dataset [13]. The implementations were done using Matlab 2016a on Windows 7 PC with 32 GB memory and Intel Xeon(R) E5-2687W v2 @3.40GHz CPU. The wavelet denoising algorithm [32] was used to obtain ngerprint and PRNU noise. PCE and NCC methods were used for comparison. A preset threshold of 60 [35] was used for PCE values. Values higher than this threshold were taken to conclude that the two media objects originated from the same camera. A. Studying the effect of smoothness To verify the observations of the analysis related to smooth- ness of the images used to compute a camera ngerprint, we randomly selected 50 at eld images and 50 textured images from each camera in the dataset. For each of these types, ve experiments were conducted by using a random set of 5, 10, 20, 30, and 50 images for computing the ngerprint. So for example, when we chose 30 at eld images, we created one ngerprint using the conventional approach by denoising each of the 30 images and then averaging the PRNU noise patterns to arrive at the ngerprint estimate. Then a ngerprint estimate using the SDA approach was computed by averaging the same 30 images in the spatial domain rst and then denoising this SDA-frame of depth 30 to directly arrive at another ngerprint estimate. Therefore, a total of 20 ngerprints were obtained for each camera (2 types of images; 2 ngerprint extraction techniques; 5 different cardinalities of image sets used for ngerprint computation). Each of these two ngerprints was correlated with the PRNU noise obtained from the rest of the images in the dataset taken with the same camera. This set consisted of both textured and at- eld images. To create an abundance of test cases, we divided each full resolution ngerprint into 500 500 disjoint blocks and correlated them with the corresponding blocks in the test images to match the PRNU noise. As a result, a total of 244, 127 comparisons were made. 0 200 400 600 800 1000 1200 5 10 20 30 50 0 200 400 600 800 Fig. 3: The effect of texture in terms of PCE Fig. 3 shows how image content affects the PCE for nger- prints obtained from 5, 10, 20, 30 or 50 at eld and textured images. The gure shows that with at eld images, despite the signi cantly lower number of denoising operations performed IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY 6 by the SDA approach, the results obtained are similar to the conventional approach. This observation holds regardless of the number of images averaged for ngerprint extraction. The performance of the SDA approach drops for textured images. However, this difference can be overcome by increasing the number of images used for SDA technique but still keeping the number of denoising operations lower than the conventional approach. We investigate this issue in the next subsection. If we consider the above results in terms of TPR, the SDA approach starts doing better as the PCE is thresholded around a set value (60 in our case) to arrive at the attribution result. So a drop in PCE does not necessarily result in a wrong decision. This improvement can be observed in Fig. 4 which shows TPR for the same experiments when the threshold is set to 60 as proposed in [35]. The other implications of these gures are already well-known in the eld (i.e., at eld images are better than textured and as the number of images increase quality of ngerprint also increases which results in a higher PCE and TPR.) 0 0.2 0.4 0.6 0.8 1 5 10 20 30 50 0 0.2 0.4 0.6 0.8 1 Fig. 4: The effect of texture in terms of TPR Table I shows the average time it takes to extract a ngerprint estimate by the two methods in the above experi- ment. Notice that in both cases the same number of images, m, are read from the disk but for the SDA technique only one denoising operation is needed whereas for conventional way, m denoising operations are done. This implies that as the training images increase, the speedup also increases. A speedup of 13.5 times can be achieved by averaging 50 images before denoising. TABLE I: Average time to extract ngerprints with proposed and conventional methods (in sec) 5 10 20 30 50 SDA 4.97 5.99 8.22 10.35 14.49 Conventional 21.57 40.81 79.96 118.79 196.59 Speedup 4.34 6.81 9.73 11.48 13.57 B. Fingerprint equivalence for textured images For textured images, our analysis indicated that more images are needed by the SDA method and hence a corresponding reduction in the speedup obtained would occur. In this ex- periment, our goal is to investigate the relationship between the number of images required by SDA compared to the number needed by the conventional approach to yield similar performance for textured images while still retaining a speed- up in ngerprint computation. This experiment was again performed using images from the VISION dataset [13]. We created a training set from 50 textured images for each camera in the VISION dataset. 19 ngerprints were created using 2, 3, . . . 20 images using the conventional approach. We also created 49 ngerprints using SDA method using 2, 3, . . . 50 images. As done in the previous experiment, each ngerprint was partitioned into disjoint 500 500 blocks and correlations were computed with the corresponding blocks of the test PRNU noise pattern. 2 4 6 8 10 12 14 16 18 20 0 5 10 15 20 25 30 35 40 45 50 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5 Fig. 5: Fingerprint equivalence for SDA and conventional ap- proaches. x-axis indicates number of images for conventional. The left of y-axis (red) is the number of images required for SDA and the right one (blue) is the speedup gained in this case. Figure 5 shows the number of images required by the SDA approach to achieve at least the same TPR as the conventional approach. Moreover, it shows the speedup gained in these cases. For example, when ngerprint is created from 20 textured images using conventional way, the same TPR can be achieved using 48 images in SDA approach. In this way, the ngerprint extraction is approx. 3.85 times faster for SDA approach. The gure shows that using 2 3 times more images for SDA method, up to 4 times speedup can be achieved with no loss in TPR when the images are textured. C. Effect of SDA-depth on image ngerprint In Section III, we have shown that as the SDA-depth in- creases, when the number of images for ngerprint extraction is constant, the TPR is expected to drop. To verify this remark, we used 50 textured images for ngerprint extraction. We didn t include any at eld image in this set as at eld images results in a negligible difference in performance between SDA and conventional ngerprints. We then created ngerprints using 50 textured images from each camera in the VISION dataset. We set SDA-depth to 1, 2, 5, 10, 25 and 50. Therefore, we created 50, 25, 10, 5, 2, and 1 SDA-frames, respectively. The SDA-frames were de- noised and then averaged to arrive at the nal ngerprint estimate. For each ngerprint estimate computed, the rest of the images were used as test images. We correlated each IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY 7 ngerprint with the PRNU noise extracted from the test images in a block-wise manner as done in previous experiments. Notice that SDA 1 is the same as conventional approach. SDA-1 SDA-2 SDA-5 SDA-10 SDA-25 SDA-50 PCE 652.8 514.6 390.0 332.2 285.0 252.4 TPR 0.80 0.78 0.75 0.72 0.69 0.67 TABLE II: SDA-depth vs TPR and PCE, change with gure Table II shows that as the SDA-depth increases, the average PCE decreases. For textured images, the more images we combine to create an SDA-frame, the lower the PCE and TPR values that will result. This supports the third observation of the analysis in Section 3. This section has provided a validation of Section III by experimentally supporting all three observations derived from the analysis. Namely, when images are not textured, hence resulting in low post- ltering noise, both the SDA and con- ventional ngerprints from the same images perform similarly which can lead to 13.5 times speedup. On the other hand, textures images and larger SDA-depth result in requiring higher number of images to achieve the same performance as conventional approach. Yet, a speedup by a factor of 4 can still be achieved in most cases. In the next section, we apply the proposed approach to prac- tical problems, and show that SDA ngerprints can perform with a signi cantly higher accuracy or result in signi cant speedup compared to state-of-the-art ngerprint extraction techniques. V. APPLICATION TO COMPUTING VIDEO FINGERPRINTS In this section, we investigate a more practical use case of the proposed SDA technique which is its usage for extracting FE from videos. As Section II explains, two of the most common ways to extract a ngerprint from a video are using only I-frames or using all frames (or the rst n frames). While the former results in low performance, the latter can be impractical in many real life applications due to very high computational needs. For example, ngerprints from 50 1 minute videos (i.e., approximately 1800 frame per video) using a single-thread may take up to a day to compute. In this section, we provide experimental results that demonstrate how using the SDA approach can provide signi cant improvements in the time needed for computing ngerprint estimates from video, while retaining the same performance obtained using a signi cantly larger number of denoising operations using conventional approaches. In each experiment below, three different types of nger- prints (i.e., I-frames only, SDA-frames and ALL-frames) were obtained from each video. For the sake of simplicity, we refer to them as I-FE (i.e., Fingerprint Estimate), SDA-FE, and ALL-FE, respectively. Moreover, in some cases, we add an indication of the SDA-depth when we need to highlight it. For example, SDA-50-FE indicates that the video frames were divided into groups of 50 and each group averaged to create an SDA-frame. In the rst experiment, we examine source matching for videos. That is given two videos, can we determine if they are from the same camera. Next we investigate a more dif cult case that involves mixed media. In this subsection, we also analyze an important question related to mixed media: What is a good balance of SDA-depth which optimizes speed and performance? . In the next two subsections, we examine the performance achieved with video and images obtained from social media such as Facebook and YouTube. Finally, we show how the proposed technique can be used for source attribution with moderate length stabilized videos (i.e., up to 4 minutes) from which obtaining a reliable FE might take couple of hours each using all frames. Two datasets were used in all the experiments, the NYUAD- MMD, and VISION datasets. The NYUAD-MMD dataset contains images and videos of different resolutions and aspect ratios from 78 cameras from different models and brands. This makes it a challenging dataset for mixed media attribution. Moreover, it contains stabilized videos longer than 4 minutes from 5 cameras. Hence, we used this dataset for experiments using mixed media and stabilized video. The videos in the dataset are typically around 40 seconds( i.e., each video is approximately 1200 frames) and images are pristine (i.e., no out-camera operations). The VISION dataset contains differ- ent high quality videos and images from social media such as Facebook and YouTube. Hence, we used this dataset in experiments involving social media. A. Matching Two Non-Stabilized Videos In the rst experiment, we examine source matching for videos using FE computed from the three different approaches that have been presented. Our goal was to estimate the length of videos and the resulting computation time needed to achieve greater than 99% TPR for I-FEs, SDA-FEs and ALL-FEs. This way, a clear comparison of the the three approaches could be made. FE from the non-stabilized videos of the same resolution from the VISION dataset were rst created. FE were extracted from the rst 5, 10, . . . 40 seconds of each video using the two techniques mentioned in Section II and the proposed method. On average, each video had approximately one I-frame per second. We selected an SDA-depth of 30 resulting in an SDA- frame from each second of video. 5 10 15 20 25 30 35 40 0.4 0.5 0.6 0.7 0.8 0.9 1 Fig. 6: TPR for different lengths of video using I-FEs, SDA- FEs, and ALL-FEs IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY 8 Figure 6 shows TPR using I-FE, SDA-FE, and ALL-FE as the length of the videos increases. As seen, SDA-FEs outperforms ALL-FEs in this setting for all video lengths. The difference varies between 0.5 (for 5 sec videos) and 1.7%(for 15 sec videos). Both FE achieve signi cantly higher TPR than I-FEs. For example, for 10 seconds video, SDA-FEs and ALL- FEs result in 94.1% and 95.6% TPR, respectively, whereas I-FEs can only reach 62.2% TPR. The highest TPR achieved using I-FEs was 83.7% (i.e., for 40 second videos) which is still lower than the TPR of SDA- FEs and ALL-FEs when they were computed from only 5- second videos (i.e., more than 87%). This is because SDA-FE and ALL-FEs use all the 150 frames in a 5 second video (i.e., I-, B- or P-frames) whereas the I-FEs use only 40 I-frames on average and waste the rest of the frames. Hence, for this setting, I-FEs fail to reach to a comparable accuracy as the other two methods. TABLE III: Time for video ngerprint extraction in second type averaging I/O + denoising total I-FE 0 50 50 SDA-FE 12 50 62 ALL-FE 0 1407 1407 We then estimated the time required for extraction of each FE from a 40 second Full HD video captured @30 FPS. Table III compares the average times for them. It takes 50, 62, and 1407 seconds for an I-FE, SDA-FE and ALL-FE, re- spectively. However, these times are when each one is obtained from 40 second videos. When we evaluate the required time to achieve 83% TPR, we need less than 5 seconds of video for SDA-FEs and ALL-FEs whereas I-FEs require 40 seconds of video. This suggest that the required time for SDA-FEs and ALL-FEs are less than 8 and 176 seconds, respectively. Hence, SDA technique is at least 6 times faster than I-FEs and requires 8 times shorter videos, yet still achieves a higher TPR. Moreover, it performs up to 1.7% higher than ALL-FEs in terms of TPR and speeds up approximately 22.5 times in this setting. Moreover, while SDA-FEs can achieve 99% TPR with 20 seconds videos, the same can be achieved with 30 seconds for ALL-FEs. Therefore, close to 34 times speedup can be achieve in this case when SDA-depth is set to 30. Notice that these results involve videos that did not undergo any processing such as scaling, compression in social media and so on. Also, all videos were taken with high luminance in the VISION dataset. Therefore, it is possible to have lower performance with more dif cult datasets such as when videos are dark or processed. However, our intention here was to demonstrate the effectiveness of SDA approach rst for the simplest of cases. We examine more challenging situations in further experiments below. B. Mixed Media Attribution As we have seen in the previous subsection, using I-FEs causes a signi cant drop in TPR whereas 20 30 seconds of video is enough to achieve more than 99% TPR for both SDA-FEs or ALL-FEs. In this subsection, we investigate a more challenging scenario where a video FE needs to be matched with a single query image. In [14], source attribution with mixed-media was investigated using the NYUAD-MMD dataset which is a very challenging dataset containing images and videos of various resolutions from 78 of cameras. Here, we performed Train on videos and test on images experiment for I-FEs, SDA-FEs, and ALL-FEs. That is a camera FE was computed from the video and the query image was cropped and resized and its PRNU matched with the FE. The resizing and cropping parameters to perform the matching were obtained from the Train on images, test on videos experiment done in [14]). The videos in this dataset were typically around 40 seconds long; each having approximately 1200 frames. The dataset contains a total of 301 non-stabilized videos and 6892 images from those cameras. Each video FE was correlated with the PRNU noise of all the test images from the same camera to estimate true cases which ended up with 23571 correlations. Then, each video FE from ith camera was compared with the PRNU noise of images from (i+1)th camera for resizing and cropping parameters that maximizes the PCE for the image FE (i.e., the FE obtained from all images of the camera using conventional approach). This way, we estimated the false cases resulted in 17755 correlations. In the previous experiment we had used a xed SDA- depth, d, of 30. In this experiment we used different SDA- depths to investigate its impact on performance and speed. Given a video of m frames (in our case approximately 1200 frames), we divided the frames into groups of d = 1, 5, 10, 30, 50, 200, 1200. Therefore, the number of SDA- frames, g, became 1200, 240, 120, 40, 24, 6, 1 respectively. When g = 1, the technique becomes the same as using all frames whereas when p = 1200, only a single SDA-frame is created by averaging all 1200 frames. After obtaining the PCE of the true and false cases, we created an ROC curve for each video FE type/depth. Figure 7 shows the ROC curves for each of the SDA-FEs of different depths, as well as I-FE and ALL-FE. The results show that ALL- FE results in the highest performance, whereas I-FE perform signi cantly poorer compared to others. The proposed SDA method performs close to ALL-FE method for all depths. Fig. 7: The ROC curves for varying SDA-depths Table IV shows more detailed results. |PCE| stands for the average of the PCE ratios with respect to I-FEs. For example, when an ALL-FE from ith video is correlated with the noise of jth image, its PCE is on average 3.2% times higher compared to the I FE obtained from the same video. The reason IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY 9 we used such a normalization instead of average PCE is that outliers have a big impact on average PCE. Moreover, the table shows the TPR for the PCE threshold of 60, average time to extract a FE, and the speedup compared to ALL-FEs. As seen, the results indicate that the TPR of SDA method are very close to ALL-FE. However, a speedup of up to 52 times can be achieved using the SDA method. TABLE IV: Detailed information for mixed media attribution I- ALL- 5 10 30 50 200 1200 |PCE| 1.0 3.2 3.1 2.9 2.6 2.6 2.5 2.4 TPR(%) 64.0 83.1 82.3 81.3 80.0 79.8 80.1 79.8 time(s) 50 1407 276 142 62 48 32 27 speedup 28.1 1.0 5.1 9.9 22.7 29.3 44.0 52.1 Similar to the previous experiment using I-FEs have signif- icantly lower accuracy (at least 16% lower TPR). Moreover, when SDA-depth 30, SDA-FEs are faster to extract as com- pared to I-FEs. Notice that when ALL-FEs are used, it takes approximately ve days to extract all the FEs from the 301 videos in the NYUAD-MMD dataset using a single-threaded implementation. This type of performance will clearly imprac- tical for many applications. C. Train and test on YouTube videos This experiment explores the performance achieved when two video FEs from YouTube are correlated. Although this experiment is essentially the same as the Section V-A, it is relevant in practice as high compression is involved. Note that a key motivation of the SDA approach is that when high compression is used, a large number of frames are needed for computing a reliable FE. We created FE from all non-stabilized YouTube videos in VISION dataset (i.e., the ones labeled atYT, indoorYT, and outdoorYT) using only I-frames, SDA- 50, SDA-100, SDA-200, and ALL-frames. Here, we used the rst 10, 20, . . . 60 seconds of the YouTube videos to extract FEs. Each 60 second video had approximately 1800 frames that were used for SDA- or ALL-FEs, whereas they contained 31.3 I-frames on average. After ngerprint extraction, we correlated each video FE with others of the same type and same length taken by the same camera. For example, an I-FE from 20 seconds of video is correlated with all I-FEs obtained from the rest of the 20 seconds videos from the same camera. The same was done for SDA- and ALL-FEs. This way, a total of 3124 correlations were done for each type. Figure 8 shows the TPR for varying lengths of video for each FE type. The gure shows that I-FEs perform very poorly for all cases and any FE type created from video of more than 20 seconds outperforms I-FEs. While ALL-FEs perform better than SDA-FEs for the same-length videos, this difference can be overcome by increasing the video length but still using much fewer denoising operations. For example, SDA 50 obtained from 50 second videos or SDA 100 from 60 seconds videos, perform approximately the same as ALL- FEs obtained from 30 seconds (within + 1% TPR range). Hence, instead of using 900 frames for ALL-FEs, using 1800 frames for SDA 100 can result in signi cant speedup with no loss in TPR. While an ALL-FE from 900 frame of a Full HD 10 20 30 40 50 60 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Fig. 8: The effect of FE type and video length on TPR for YouTube videos video takes 1045 seconds to compute, and SDA 100 FE from 1800 frames, which only does 18 denoising instead of 900, takes 56 seconds to compute. Therefore, a speedup of close to 19 times can be achieved with SDA 100 with 1% increase in TPR. Notice that, because most videos are around 60 seconds in the VISION dataset, it limits the maximum length we could use in our experiments. D. Train on Facebook images, test on YouTube videos From the previous experiments, we know that the SDA method can help achieve a signi cant speedup for both videos and images with a small loss in performance which can be overcome by increasing the number of still images used for ngerprint extraction if available. In this experiment, our goal was to show that the proposed method can be successfully applied to other social media. Speci cally, in this subsection, we extract FEs from Facebook images and match them with the FE of YouTube videos. We call this the Train on Facebook images, test on YouTube videos experiment. The importance of this experiment is both media sharing services contain billions of visual media and computing ALL-FEs from these collections can have very high time complexity. Therefore, faster ngerprint extraction methods (along with search tech- niques) that speeds up attribution are badly neededl In this experiment, for the cameras in the VISION dataset that had non-stabilized videos, we created a FE from 100 Facebook images (i.e., the ones labeled FBH) using con- ventional ngerprint computation method. We then used the FEs from non-stabilized YouTube videos (those created in the previous experiment). We again used I-frames, SDA-50, SDA- 200, SDA-600, and ALL-frames that were computed from the rst 60 seconds of YouTube videos. We then correlated the image FE of a camera with the FE of each video of each type using the ef cient search proposed in [14] and a total of 343 pairs were compared for each FE type. Table V shows the TPR of these correlations. Similar to Train on videos, test on images experiment, these results show that for FEs obtained from Facebook images matches with 81.34% TPR with the YouTube videos for SDA-50 which is higher than both ALL- FEs and I-FEs. On the other hand, FEs from I-frames yield approximately 30% lower TPR. These results show that SDA approach is a good replacement over using I-FEs or ALL-FEs IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY 10 for this scenario. TABLE V: TPR of different FE types when a FE from Face- book images and another from YouTube videos are correlated I-FE SDA-50 SDA-200 SDA-600 ALL-FE TPR 51.60 81.4 79.88 78.13 79.59 E. Matching two stabilized videos A recent work [12] has shown that a FE obtained from a long stabilized video can successfully be matched with other videos from the same camera. However, thousands of frames must be denoised. This may not be practical in many circumstances. A potential alternative for this problem is the use of SDA method which may lead to a signi cant speed up. To evaluate this, we captured stabilized videos from 5 cameras. A total of 37 videos were captured which added up to 260 minutes. We extracted FEs from the frames of 20, 40, . . . 240 second video lengths using conventional (I-frame and ALL-Frame) method as well as SDA method for SDA-depths of 30, 50, and 200. These depths were deemed to be reasonable choices from previous experiments. As shown in [8], [10], [11], the rst frame of the videos are typically not geometrically transformed. Since we divide video into pieces, some video pieces do not have an untransformed frame. So, we discarded the rst frame of each video to avoid inconsistencies. We correlated each FE with the other FEs of different videos from the same camera that are created using the same number of frames. For example, SDA 30 FEs of 20 second videos are correlated with the same type FEs from the same camera. Figure 9 shows the TPR for three cameras (i.e., Huawei Honor, Samsung S8, and iPhone 6plus) and the total average of all the ve cameras. 0.2 0.4 0.6 0.8 1 20 40 60 80 100 120 140 160 180 200 220 240 0 0.2 0.4 0.6 0.8 1 20 40 60 80 100 120 140 160 180 200 220 240 Fig. 9: TPR for stabilized videos for varying SDA-depths The results show that as videos get longer, ALL-FEs and SDA-FEs achieve higher TPR. Moreover, the effect of increased SDA-depth is more signi cant for this case in comparison to non-stabilized videos. While for some cameras ALL-FEs and SDA-FEs perform similarly (e.g., Huawei and Samsung cameras), for others (e.g., iPhone cameras) there is a signi cant difference between the two. For example, for Samsung S8 SDA 200-FE from 120 seconds video, perform similarly as 180 seconds ALL-FE. Therefore, for this particular case, SDA 200 can speedup 66 times  i.e. 180 120 1407 32  (see Table IV for times). On the other hand for iPhone 6 plus, ALL-FEs from 60 seconds video and 160 seconds SDA 50 have similar TPR. Therefore, 11 times  i.e., 60 160 1407 48  speedup can be achieved in this case. Hence, a speedup between these numbers (i.e. 11 and 60) can be achieved without any loss in TPR if a long video is available. Overall, this section shows that the proposed SDA-FEs outperforms the commonly used I-frame-only technique in all the cases for videos. These include mixed media, stabilized videos, and social media. On the other hand, the SDA-FEs achieves comparable results as ALL-FEs with up to 52 times speedup in these experiments. We also show the impact of SDA-depth on the performance that can be achieved in various cases. VI. CONCLUSION AND FUTURE WORK This paper has investigated camera ngerprint extraction using Spatial Domain Averaged frames, which are the arith- metic mean of multiple still images. By adding one extra step of averaging before denoising, a signi cant speedup can be achieved for ngerprint extraction. We show that this technique can successfully be used for images, non-stabilized videos as well as stabilized video to speedup ngerprint extraction process. The proposed method is especially useful when the number of denoising operations needed can be very high. For example, when dealing with non-stabilized or highly compressed stabilized videos or images from social media. It is often considered that for video source attribution, using only I-frames for ngerprint extraction (I-FEs) is enough to achieve high performance. However, in this research, we have shown that I-FEs performs poorly compared to ALL-FEs in all cases. On the other hand, using ALL-FEs is impractical due to the large computation time needed for practical scenarios where thousands of videos can be available. The proposed SDA approach comes into play here to resolve the problem of I-FEs (i.e., accuracy) and ALL-FEs (i.e., speed). Both SDA- and ALL-FEs perform similarly in most cases. When the SDA method performs worse, this can be overcome by using more of the available frames if any. The proposed technique can be used for other source attribution related problems where many denoising operations are needed. For instance, this method can be applied when many partially misaligned still images and a suspect camera are available. For example, a seam carved video contains many partially misaligned frames with its source camera. In such a scenario, instead of denoising all frames of the video, the SDA technique can be used as a way to speed up this process. Moreover, determining whether a video is stabilized or not is another issue which requires a number of denoising operations. As an alternative to using only I-frames, the proposed SDA technique could successfully work with only 2 denoising operations. Another avenue for future research is to create an SDA- FE in a weighted manner such that performance achieve with SDA method can be increased. Two of the potential ways to IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY 11 achieve this are weighting I-, P- and B- frames differently, and weighting the frames in a block-by-block manner. For example, it has been shown that at eld images perform better with SDA method compared to textured ones. Using this idea, one may weight textured regions differently from smooth regions. REFERENCES [1] J. Lukas, J. Fridrich, and M. Goljan, Digital camera identi cation from sensor pattern noise, IEEE Transactions on Information Forensics and Security, vol. 1, no. 2, pp. 205 214, 2006. [2] M. Goljan, J. Fridrich, and T. Filler, Managing a large database of camera ngerprints, in Media Forensics and Security II, vol. 7541. International Society for Optics and Photonics, 2010, p. 754108. [3] S. Bayram, H. T. Sencar, and N. Memon, Ef cient sensor ngerprint matching through ngerprint binarization, IEEE Transactions on Infor- mation Forensics and Security, vol. 7, no. 4, pp. 1404 1413, 2012. [4] D. Valsesia, G. Coluccia, T. Bianchi, and E. Magli, Compressed ngerprint matching and camera identi cation via random projections, IEEE Transactions of Information Forensics and Security, vol. 10, no. 7, pp. 1472 1485, July 2015. [5] S. Bayram, H. T. Sencar, and N. Memon, Sensor ngerprint identi ca- tion through composite ngerprints and group testing, IEEE Transac- tions of Information Forensics and Security, vol. 10, no. 3, pp. 597 612, March 2015. [6] S. Taspinar, H. T. Sencar, S. Bayram, and N. Memon, Fast camera ngerprint matching in very large databases, in Image Processing (ICIP), 2017 IEEE International Conference on. IEEE, 2017, pp. 4088 4092. [7] W.-H. Chuang, H. Su, and M. Wu, Exploring compression effects for improved source camera identi cation using strongly compressed video, in Image Processing (ICIP), 2011 18th IEEE International Conference on. IEEE, 2011, pp. 1953 1956. [8] S. Taspinar, M. Mohanty, and N. Memon, Source camera attribution using stabilized video, in Information Forensics and Security (WIFS), 2016 IEEE International Workshop on. IEEE, 2016, pp. 1 6. [9] S. Chen, A. Pande, K. Zeng, and P. Mohapatra, Video source iden- ti cation in lossy wireless networks, in IEEE INFOCOM, 2013, pp. 215 219. [10] M. Iuliani, M. Fontani, D. Shullani, and A. Piva, A hybrid approach to video source identi cation, arXiv preprint arXiv:1705.01854, 2017. [11] S. Mandelli, P. Bestagini, L. Verdoliva, and S. Tubaro, Facing device attribution problem for stabilized video sequences, IEEE Transactions on Information Forensics and Security, 2019. [12] J. Lubin, M. Isnardi, C. Spence, I. Sur, and A. Chaudhry, Joint sensor ngerprinting and processing history recovery for visual media forensics, Private conversation, 2018. [13] D. Shullani, M. Fontani, M. Iuliani, O. Al Shaya, and A. Piva, Vision: a video and image dataset for source identi cation, EURASIP Journal on Information Security, vol. 2017, no. 1, p. 15, 2017. [14] S. Taspinar, M. Mohanty, and N. Memon, Source camera attribution of multi-format devices. [15] J. Luk a s, J. Fridrich, and M. Goljan, Digital camera identi cation from sensor pattern noise, IEEE Transactions Information Forensics and Security, vol. 1, no. 2, pp. 205 214, 2006. [16] Y. Sutcu, S. Bayram, H. T. Sencar, and N. Memon, Improvements on sensor noise based source camera identi cation, in IEEE International Conference on Multimedia and Expo, 2007, pp. 24 27. [17] C. T. Li and Y. Li, Color-decoupled photo response non-uniformity for digital image forensics, IEEE Transactions on Circuits and Systems for Video Technology, vol. 22, no. 2, pp. 260 271, 2012. [18] G. Chierchia, S. Parrilli, G. Poggi, C. Sansone, and L. Verdoliva, On the in uence of denoising in PRNU based forgery detection, in ACM Multimedia in Forensics, Security and Intelligence, 2010, pp. 117 122. [19] C. T. Li, Source camera identi cation using enhanced sensor pattern noise, IEEE Transactions on Information Forensics and Security, vol. 5, no. 2, pp. 280 287, 2010. [20] W. Yaqub, M. Mohanty, and N. Memon, Towards camera identi cation from cropped query images, in 25th ICIP. IEEE, 2018, pp. 3798 3802. [21] S. Bayram, H. T. Sencar, and N. Memon, Seam-carving based anonymization against image & video source attribution, in IEEE Workshop on Multimedia Signal Processing, 2013, pp. 272 277. [22] S. Taspinar, M. Mohanty, and N. Memon, Prnu based source attribution with a collection of seam-carved images, in Image Processing (ICIP), 2016 IEEE International Conference on. IEEE, 2016, pp. 156 160. [23] M. Goljan and J. Fridrich, Camera identi cation from scaled and cropped images, Proc. SPIE, Electronic Imaging, Forensics, Security, Steganography, and Watermarking of Multimedia Contents X, vol. 6819, pp. 68 190E 68 190E 13, 2008. [24] E. J. Alles, Z. J. Geradts, and C. J. Veenman, Source camera identi ca- tion for low resolution heavily compressed images, in Computational Sciences and Its Applications, 2008. ICCSA 08. International Confer- ence on. IEEE, 2008, pp. 557 567. [25] K. Rosenfeld and H. T. Sencar, A study of the robustness of prnu- based camera identi cation, in Media Forensics and Security, ser. SPIE Proceedings, E. J. Delp, J. Dittmann, N. D. Memon, and P. W. Wong, Eds., vol. 7254. SPIE, 2009, p. 72540. [26] M. Goljan, J. Fridrich, and J. Luk a s, Camera identi cation from printed images, Proceedings of SPIE, vol. 6819, p. 68190I, 2008. [Online]. Available: http://www.ws.binghamton.edu/fridrich/Research/Printed.pdf [27] S. Milani, M. Fontani, and P. B. et. al., An overview on video forensics, Signal Processing Systems, vol. 1, pp. 1 18, June 2012. [28] M. Chen, J. Fridrich, M. Goljan, and J. Lukas, Source digital camcorder identi cation using sensor photo response non-uniformity, in SPIE Electronic Imaging, 2007, pp. 1G 1H. [29] S. McCloskey, Con dence weighting for sensor ngerprinting, in IEEE CVPR Workshops, 2008, pp. 1 6. [30] N. Ejaz, W. Kim, S. I. Kwon, and S. W. Baik, Video stabilization by detecting intentional and unintentional camera motions, in IEEE Inter- national Conference on Intelligent Systems, Modelling and Simulation, 2012, pp. 312 316. [31] Y. Matsushita, E. Ofek, W. Ge, X. Tang, and H.-Y. Shum, Full- frame video stabilization with motion inpainting, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 28, pp. 1150 1163, July 2006. [32] M. K. Mihcak, I. Kozintsev, K. Ramchandran, and P. Moulin, Low- complexity image denoising based on statistical modeling of wavelet coef cients, IEEE Signal Processing Letters, vol. 6, no. 12, pp. 300 303, 1999. [33] K. Dabov, A. Foi, V. Katkovnik, and K. Egiazarian, Bm3d im- age denoising with shape-adaptive principal component analysis, in SPARS 09-Signal Processing with Adaptive Sparse Structured Repre- sentations, 2009. [34] J. Fridrich, Sensor defects in digital image forensic, Digital Image Forensics, pp. 1 43, 2013. [35] M. Goljan, J. Fridrich, and T. Filler, Large scale test of sensor ngerprint camera identi cation, in Media forensics and security, vol. 7254. International Society for Optics and Photonics, 2009, p. 72540I.