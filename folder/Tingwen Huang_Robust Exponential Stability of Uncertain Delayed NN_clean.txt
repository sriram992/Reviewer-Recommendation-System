See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/254061469 Robust Exponential Stability of Uncertain Delayed Neural Networks With Stochastic Perturbation and Impulse Effects Article in IEEE Transactions on Neural Networks and Learning Systems June 2012 DOI: 10.1109/TNNLS.2012.2192135 CITATIONS 306 READS 285 4 authors: Some of the authors of this publication are also working on these related projects: Special Issue on Efficient Network Design for Convergence of Deep Learning and Edge Computing View project Electronic Nose View project Tingwen Huang Texas A&M University at Qatar 663 PUBLICATIONS 22,195 CITATIONS SEE PROFILE Chuandong Li Southwest University in Chongqing 381 PUBLICATIONS 9,907 CITATIONS SEE PROFILE Shukai Duan Southwest University in Chongqing 345 PUBLICATIONS 3,906 CITATIONS SEE PROFILE Janusz Starzyk Ohio University 71 PUBLICATIONS 1,029 CITATIONS SEE PROFILE All content following this page was uploaded by Janusz Starzyk on 09 September 2014. The user has requested enhancement of the downloaded file. 866 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 23, NO. 6, JUNE 2012 Robust Exponential Stability of Uncertain Delayed Neural Networks With Stochastic Perturbation and Impulse Effects Tingwen Huang, Chuandong Li, Senior Member, IEEE, Shukai Duan, Member, IEEE, and Janusz A. Starzyk, Senior Member, IEEE Abstract This paper focuses on the hybrid effects of parameter uncertainty, stochastic perturbation, and impulses on global stability of delayed neural networks. By using the Ito formula, Lyapunov function, and Halanay inequality, we estab- lished several mean-square stability criteria from which we can estimate the feasible bounds of impulses, provided that parameter uncertainty and stochastic perturbations are well-constrained. Moreover, the present method can also be applied to general differential systems with stochastic perturbation and impulses. Index Terms Delayed neural networks (DNN), exponential stability, impulse, mean-square stability, parameter uncertainty, stochastic perturbation. I. INTRODUCTION I N THE past two decades, since the initiation of Hop- eld neural network model [1], several types of recurrent neural networks have been extensively studied and success- fully applied in many areas, such as combinatorial optimiza- tion, signal processing, pattern recognition, and nonlinear control [1] [5]. In these applications, ensuring the global exponential/asymptotic stability of designed neural networks is very important. On the other hand, axonal signal trans- mission delays often occur in various neural networks, and may cause undesirable dynamic network behaviors, such as oscillation and instability. Therefore, there has been a growing research interest on analyzing stability problems for delayed neural networks, and a large number of studies have been available [6] [17]. Manuscript received September 25, 2011; accepted March 21, 2012. Date of publication April 9, 2012; date of current version May 10, 2012. This work was supported in part by the Section III was made possible by NPRP Grant 4-1162-1-181 from the Qatar National Research Fund (a member of the Qatar Foundation). The authors acknowledge support from the National Natural Science Foundation of China under Grant 60974020, Grant 60972155, and Grant 10971240, and the Fundamental Research Funds for the Central Universities of China under Project CDJZR10 18 55 01 for other sections. The statements made herein are solely the responsibility of the authors. T. Huang is with Texas A&M University at Qatar, Doha 23874, Qatar (e-mail: tingwen.huang@qatar.tamu.edu). C. Li is with the College of Computer Science, Chongqing University, Chongqing 400044, China (e-mail: licd@cqu.edu.cn). S. Duan is with the School of Electronics and Information Engineering, Southwest University, Chongqing 400715, China (e-mail: oh8849@yahoo.com). J. A. Starzyk is with the School of Electrical Engineering and Computer Science, Ohio University, Athens, OH 45701 USA, and also with the Univer- sity of Information Technology and Management, Rzeszow 35-064, Poland (e-mail: starzykj@gmail.com). Color versions of one or more of the gures in this paper are available online at http://ieeexplore.ieee.org. Digital Object Identi er 10.1109/TNNLS.2012.2192135 As is well known, a real system is usually affected by external disturbances. Generally speaking, there are two kinds of disturbances to be considered when one models neural networks: parameter uncertainty and stochastic perturbations, with the latter being unavailable in practice. Usually, there are two kinds of parameter uncertainty: time-varying, norm- bounded structures and interval structure. Both structures have been widely exploited in the problems of robust control and ltering of uncertain systems, including neural networks ([18] for the rst kind and [20], [21] for the second). In real nervous systems, the synaptic transmission is a noisy process brought on by random uctuations from the release of neurotransmit- ters and other probabilistic causes. It has also been known that a neural network could be stabilized or destabilized by certain stochastic inputs [18]. Hence, the stability analysis problem for stochastic neural networks becomes increasingly signi cant, and some results related to this problem have recently been published [19] [37]. Besides delay effects, impulse effects also exist in neural networks. For instance, in the implementation of electronic networks, the state of the network is subject to instanta- neous perturbations and experiences abrupt change at certain instances, which may be caused by switching phenomenon, frequency change, or sudden noise. Moreover, many evolu- tionary processes, particularly some biological systems such as biological neural networks and bursting rhythm models in pathology, might exhibit impulse effects. Other examples include optimal control models in economics, frequency- modulated signal processing systems, and ying object motions. All these real-world systems and natural processes behave in a discrete or continuous style interlaced with instan- taneous and abrupt changes. The mathematical foundations of impulsive systems are impulsive differential or difference equations, which are found in almost every eld of applied science. Impulse effects have also been introduced to several neural network models, and the dynamics of these models with impulse effects have been thoroughly studied [38] [45]. Recently, impulsive and stochastic effects have been taken into account when modeling neural networks. The dynamic analysis of delayed neural networks with impulsive and stochastic effects has been an attractive topic, and several stability criteria have been reported [46], [47]. For impul- sive and stochastic Cohen-Grossberg neural networks with delays, Song and Wang [46] investigated the existence, unique- ness, and exponential p-stability of the equilibrium point by 2162 237X/$31.00 2012 IEEE HUANG et al.: ROBUST EXPONENTIAL STABILITY OF UNCERTAIN DNNs 867 employing M-matrix theory and stochastic analysis technique. Wang et al. [47] generalized a few previously known results and removed some restrictions by using L-operator differential inequality, M-cone theory, and stochastic analysis technique. However, the results in [46] require that impulses must have the stabilizing effects. This condition is restrictive and might lead to conservatism. Meanwhile, although the authors in [47] overcome this weakness, the proposed results are dif cult to verify. Moreover, neither [46] nor [47] addressed the parameter uncertainty problem. Motivated by the shortcomings of the aforementioned research in this area, in this paper we study the robust exponential stability of delayed neural networks with para- meter uncertainties, stochastic perturbation, and impulses, and establish several easily veri ed stability criteria to make the readers understand the complex effects of delays, uncer- tainties, stochastic perturbation, and impulses on the sys- tem s stability. In addition, from the main results in this paper, we can estimate the feasible upper bound of impulse when parameter uncertainties are norm-bounded. Speci cally, this paper has three contributions. First, we formulate the generalized neural network model with time delays, parameter uncertainties, stochastic perturbations, and impulses. Second, we establish several easily veri ed stability criteria with the characterization of complex effects of time delays, parameter uncertainties, stochastic perturbations, and impulses on the stability of the reference networks. And third, we estimate the feasible upper bound of impulses. These contributions generalize and/or improve the existing results, including those in [46] and [47]. The rest of this paper is organized as follows. In the next section, we describe the problem to be considered and introduce the needed preliminaries. We then state the main results in Section III. In Section IV, we present two examples to verify our theoretical results. Finally, the conclusions are drawn in Section V. II. PROBLEM DESCRIPTIONS AND PRELIMINARIES The model we consider in this paper is composed of two subsystems given by du(t) =  (C + C)u(t) + (A + A) f (u(t)) + (B + B) f (u(t )) +  dt + [W0u(t) + W1u(t )]dW(t), t = tk (1a) u(tk) = Jk(u(tk )), t = tk, k = 1, 2, . . . (1b) where impulse-free subsystem or continuous component (1a) is the continuous part of (1), which describes the continuous evolution processes of the neural networks, in which u(t) = [u1(t), . . . , un(t)]T Rn denotes the state vector of neurons, C = diag(c1, . . . , cn) > 0, A = (aij )n n, B = (bij)n n, f (u) = [ f1(u1), . . . , , fn(un)]T Rn, and = [ 1, . . . , n]n Rn denote the external inputs, and > 0 denotes system delay. C, A, B, W0 and W1 are time-varying matrices on Rn n that denote the parameter uncertainties. W(t) = [w1(t), . . . , , wn(t)]T is an n-dimensional Brownian motion de ned on a complete probability space (, F, {Ft}t 0, P) with a ltration {Ft}t 0 satisfying the usual conditions (i.e., it is right continuous and F0 contains all P-null sets). The second part, (1b), is the discrete part of model (1), which describes the abrupt change of state at the moments of time tk (called impulsive moments), in which {tk} satisfy 0 t0 < t1 < t2 < and lim k tk = , u(tk) = u(t+ k ) u(tk) is the impulse at moment tk, and Jk(u) C[Rn, Rn] is the jump operator. Throughout this paper, we assume that for any PCb F0 ([ , t0], Rn) (i.e., the family of all bounded F0-measurable), there exists at least one solution of (1), which is denoted by u(t, t0, ), or, u(t) if no confusion occurs. As the solution u(t) is discontinuous at the moment tk, by theory of impulsive differential equations, we assume, as usual, that the solution is right continuous at tk, i.e., u(tk) = u(t+ k ). Obviously, (1) includes several well-known neural network models as its special cases and, thus, it is the generalization of the models presented in [6], [8], [9], [18], [20], [23], [26], [31], [33], and [34]. If impulses do not occur, i.e., Jk(u) 0, (1) turns into the general stochastic neural networks with delay [18], [20], [23], [26], [31], [33], [34] du (t) =  (C + C)u(t) + (A + A) f (u(t)) +(B + B) f (u(t )) + dt +[W0u(t) + W1u(t )]dW(t). (2) If no stochastic perturbation occur, i.e., W0 = W1 = 0, then (2) reduces to the following model [6]: u (t) = (C + C)u(t) + (A + A) f (u(t)) +(B + B) f (u(t )) + (3) which contains the classic form of recurrent neural networks with delays [8] [10] u (t) = Cu(t) + Af (u(t)) + B f (u(t )) + . (4) In the sequel, we make the following assumptions. (H1) The activation function fi ( ) is bounded, and satis es Lipschitz condition, namely, there exist positive scalars li(i = 1, 2, . . . , n) such that | fi( ) fi( )| li| |, for any , R. We denote L = diag(li, i = 1, . . . , n). (H2) The parameter uncertainties C, A, B, W0, and W1 are of the form [C, A, B, W0, W1] = DF(t) [ 1, 2, 3, 4, 5] (5) where D, i, i = 1, . . . , 5 are known real constant matrices with appropriate dimensions, and F(t) is the time-varying uncertain matrix that satis es FT (t)F(t) I (6) where I denotes an identity matrix with appropriate dimension. Remark 1: In (5), F(t) is the uncertainty, and D, i, i = 1, . . . , 5 are known real matrices that characterize the structure of the uncertainty. There are several reasons for assuming that the system uncertainty has such structure. First, a linear 868 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 23, NO. 6, JUNE 2012 interconnection of a nominal plant with the uncertainty F(t) leads to a structure of the form (5) [52]. In addition, there are many physical systems in which the uncertainty can be modeled in this manner, e.g., systems satisfying matching conditions [53]. The parameter uncertainty structure as in (5) and (6) has been widely exploited in the uncertain neural networks (e.g., [6], [18], [20], [23], [26], [31], [33], [34] and the references therein). It is known from stability theory of delay neural networks that the assumption (H1) implies the existence of equilibrium point of (3) (see [34]). Let u = (u 1, . . . , u n)T be an equilibrium point of (3). For the purpose of simplicity, we can shift the equilibrium u to the origin by the transformation x(t) = u(t) u , which converts system (3) to the following: dx(t) = [ (C + C)x(t) + (A + A)g(x(t)) +(B + B)g(x(t ))]dt +[W0x(t) + W1x(t )]dW(t). (7) Under the following assumption on jump operator. (H3) Jk(u) = diag( k1, . . . , kn)[u(t) u ], which implies Jk(u ) 0, (1) can be rewritten as the following form: dx(t) = [ (C + C)x(t) + (A + A)g(x(t)) +(B + B)g(x(t ))]dt +[W0x(t) + W1x(t )]dW(t), t = tk x(tk) = diag( k1, . . . , kn)x(t k ), t = tk (8) where g(x) = f (x + u ) f (u ). In the next section, we study the stochastic stability of (8). It is worth noting that the stability of equilibrium u of (1) is the same with that of the origin of (8). To end this section, let us introduce the required de nitions, lemmas, and notations. De nition 1: For the neural network (1) and every PCb F0 ([ , t0], Rn), the trivial solution (equilibrium point u ) is robustly, globally, and exponentially stable in the mean square if, for all admissible uncertainties satisfying (5) and (6), there exist positive constants > 0 and u > 0 such that every solution u(t, t0, ) of (1) satis es E  u(t, t0, ) u 2 u sup t0 s t0 E  | (s)|2 e (t t0). Lemma 1 (Ito Formula [48]): Let y(t) be an Ito process given by dy(t) = udt + vdw(t). Let V (t, y) C2[[0, + ) R, R] is an Ito process with second derivative respect to y, then dV (t, y) = V t dt + V y dy + 1 2 2V y2 (dy)2 where (dy)2 = (dy) (dy) is computed according to the rules dt dt = dt dw(t) = dw(t) dt = 0, dw(t) dw(t) = dt. De nition 2: The Dini s upper right-hand derivative of a continuous function f : R R is de ned by D+ f (t) = lim sup h 0+ f (t + h) f (t) h . Lemma 2 (Halanay Inequality for Stochastic Systems): Under assumption (H1), let x(t) be a solution of system (7) and assume that there exists a positive, continuous function V(t, x(t)) (for t t0 and x Rn) for which there exist positive constants c1, c2, and p >1, such that c1 |x (t)|p V (t, x (t)) c2 |x (t)|p and for some constants 0 < D+E (V (t, x (t))) E (V (t, x (t))) + E (V (t , x (t ))). Then, for t t0 E |x (t)|p c2 c1 E  sup s [t0 ,t0] | (s)|p  e v(t t0) where v (0, ) is the unique positive solution of the equation v = ev . Remark 2: This lemma is the generalization of Halanay inequality for delayed differential equations, which can be proved easily based on [49, Th. 7]. III. MAIN RESULTS Let min (P) and max (P) denote, respectively, the minimal and maximal eigenvalues of a symmetric matrix P. We now state our main results that are used to establish criteria for exponential stability for the class of neural networks considered. Theorem 1: Assume that (H1) (H3) hold. If there exist positive diagonal matrices P, Q1, Q2 and positive scalars , ( < ), 1, 2, 3, 4, 1, 2, and such that the following conditions are satis ed: 1) 1 + P 0; 2) 2 P 0; 3) DT PD 1I 0 and (DT PD)2 2I 0; 4) t j t j 1, ln d j/t j t j 1 <v, for any j = 1, 2, . . . where d j = max{ev , 2 j } with j = max1 i n{|1 ji|} 1 = PC C P + P AQ 1 1 AT P + LQ1L +PBQ 1 2 BT P + 1 1 PDDT P + 1 T 1 1 + 1 2 PDDT P + 2 max  T 2 2  L2 + 1 3 PDDT P + 1 T 4 4 + 4 T 4 4 2 = LQ2L + 3 max  T 3 3  L2 + 1 4 2 T 5 5 and v (0, ) is the unique positive solution of the equation v = ev . Then, the equilibrium u of system (1) is globally, robustly, and exponentially stable in the mean square for any admissible parameter uncertainty governed by (5) and (6). Proof : Let us consider a Lyapunov function candidate for (8) as V(x(t)) = xT (t)Px(t) (9) where P > 0 is a positive diagonal matrix to be determined. HUANG et al.: ROBUST EXPONENTIAL STABILITY OF UNCERTAIN DNNs 869 At the impulse moments, i.e., t = tk, we have V (x(tk)) = xT (tk)Px(tk) = xT (t k )(I R)T P(I R)x(t k )  max 1 j n(1 kj )2  V(x(t k )). (10) When t [tk 1, tk), by applying Lemma 1 (Ito formula), the stochastic derivative of V (x(t)) along the solution of (8) can be obtained as follows: dV(x(t))=  2xT (t)P  (C + C)x(t) + (A + A)g(x(t)) +(B + B)g((x(t ))  + [W0x(t) + W1x(t )]T P [W0x(t) + W1x(t )]  dt +2xT (t)P [W0x(t) + W1x(t )] dW(t) =  xT (t) [ PC C P] x(t) + 2xT (t)P Ag(x(t)) +2xT (t)PBg((x(t )) 2xT (t)PCx(t) +2xT (t)PAg(x(t)) + 2xT (t)PBg(x(t )) +xT (t)(W0)T P(W0)x(t) +2xT (t)(W0)T P(W)x(t ) +xT (t )(W1)T P(W1)x(t )  dt +2xT (t) [W0x(t) + W1x(t )] dW(t). (11) Note that 2xT (t) P Ag (x (t)) xT (t) P AQ 1 1 AT Px (t) + gT (x (t)) Q1g (x (t)) xT (t) [P AQ 1 1 AT P + LQ1L]x (t) (12) 2xT (t) PBg (x (t )) xT (t) PBQ 1 2 BT Px (t) +gT (x (t )) Q2g (x (t )) xT (t) PBQ 1 2 BT Px (t) +xT (t ) LQ2Lx (t ) (13) 2xT (t) PCx (t) = 2xT (t) PDF (t) 1x (t) xT (t) PD 1 1 DT Px (t) + 1xT (t) T 1 FT (t) F (t) 1x (t) xT (t) [PDDT P 1 1 + 1 T 1 1]x (t) (14) 2xT (t) PAg (x (t)) = 2xT (t) PDF (t) 2g (x (t)) xT (t) [ 1 2 PDDT P + 2 max  T 2 2  L2]x (t) (15) 2xT (t) PBg (x (t )) xT (t) PD 1 3 DT Px (t) + 3gT (x (t )) T 3 FT (t) F (t) 3g (x (t )) xT (t) PD 1 3 DT Px (t) + 3 max  T 3 3  xT (t ) L2x (t ) . (16) Based on condition 3), one observes that the following inequalities hold: xT (t) (W0)T P (W0) x (t) xT (t) [DF (t) 4]T P [DF (t) 4] x (t) 1xT (t) T 4 4x (t) (17) 2xT (t)(W0)T P(W1)x(t ) 4xT (t) T 4 FT (t)F(t) 4x(t) + 1 4 xT (t ) T 5 FT (t)DT PDDT PDF(t) 5x(t ) 4xT (t) T 4 4x(t)+ 1 4 2xT (t ) T 5 5x(t ). (18) and xT (t ) T 5 FT (t)DT PDF(t) 5x(t ) 1 xT (t ) T 5 5x(t ). (19) Using (12) (19), we obtain from (11) that dV (x(t))  xT (t)[ PC C P + P AQ 1 1 AT P + LQ1L +PBQ 1 2 BT P + PDDT P 1 1 + 1 T 1 1 + 1 2 PDDT P + 2 max( T 2 2)L2 + 1 3 PDDT P + 1 T 4 4 + 4 T 4 4]x(t) + xT (t )LQ2L + 3 max( T 3 3)L2 + 1 4 2 T 5 5  x(t )  dt +2xT (t)[W0x(t) + W1x(t )]dW(t) =  xT (t)1x (t) + xT (t ) 2x (t )  dt +2xT (t)[W0x(t) + W1x(t )]dW(t). Therefore, based on conditions 1) and 2), we have dV (x(t)) [ V (x(t)) + V(x(t ))]dt +2xT (t)[W1x(t) + W1x(t )]dW(t). (20) Taking the mathematical expectation of both sides of (20), we have d E[V(x(t))] dt E[V (x(t))] + E[V(x(t ))] t [tk 1, tk), k = 1, 2, . . . (21) Note that min(P)|x(t)|2 V(x(t)) max(P)|x(t)|2 which implies that min(P)E  x(t) 2 E  V (x(t))  max(P)E  x(t) 2 . (22) Therefore, by Lemma 2, one observes, for t [tk 1, tk) E [V(x(t))] E  V (x(tk-1)) e v(t tk 1) (23) where V (x(tk 1)) = suptk 1 s tk 1 V (x(s)), and v (0, ) is the unique positive solution of the equation v = evt. From (10), one observes that E [V (x(tk))] 2 k E  V (x(t k ))  (24) where k = max 1 j n |1 kj|. 870 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 23, NO. 6, JUNE 2012 In the sequel, we shall prove that the following claim is true E [V(x(t))] k 1 j=1 d j E[ V(x(t0))]e v(t t0) t [tk 1, tk), k = 2, 3, . . . (25) where d j = max{ 2 j , ev }. To show that claim (25) holds true, the mathematical induction method is used. When t [t0, t1), we have from (23) E[V(x(t))] E[ V(x(t0))]e v(t t0). Therefore E [V(x(t1))] 2 1 E  V (x(t 1 ))  2 1 E[ V (x(t0))]e v(t1 t0) (26) and E[ V (x(t1))] = sup t1 s t1 E[V (x(s))] = max  sup t1 s<t1 E[V(x(s))], E[V(x(t1))]  max  E  V (x(t0))  e v(t1 t0)ev 2 1 E [V(x(t0))] e v(t1 t0)  = E  V (x(t0)) e v(t1 t0) max  ev , 2 1  = d1E  V (x(t0)) e v(t1 t0) (27) where d1 = max{ev , 2 1 }. For t [t1, t2), i.e., k = 2, one observes from (23) and (27) that E[V (x(t))] E[ V (x(t1))]e v(t t1) d2 1 E[ V(x(t0))] e v(t t0). (28) This implies that claim (25) holds true for k = 2. Let us assume that claim (25) also holds true for k = 2, 3, . . . , k( k 2) E[V(x(t))] k 1 j=1 d j E[ V (x(t0))]e v(t t0) (29) for any t [tk 1, tk), k = 2, 3, . . . , k. Then, we have E[V(x(t k))] 2 k E[V (x(t k ))] 2 k E[ V (x(t0))] k 1 j=1 d j e v(t k t0) (30) and E  V (x(t k))  = sup t k s t k E [V (x(s))] = max sup t k s<t k E[V (x(s))], E[V(x(t k))] ! max  k 1 j=1 d j E  V (x(t0))  e v(t k t0)ev 2 k E  V (x(t0))   k 1 j=1 d j E  V (x(t0))  e v(t k t0) max{ev , 2 k } = k j=1 d j E  V (x(t0)) e v(t k t0). (31) Therefore, from (23) and (31), one obtains that, for any t [t k, t k+1) E [V(x(t))] E  V(x(t k))  e v(t t k) k j=1 d j E[ V(x(t0))]e v(t t0). This implies that claim (25) holds true for k = k + 1. Hence, by mathematical induction, claim (25) holds true for any k = 2, 3, . . . By (25) together with (22), one obtains E[|x(t)|2] max(P) min(P) E[| |2]e v(t t0) k 1 j=1 d j , t > t0. (32) By condition (iv) of the theorem, we also have e v(t t0) k 1 j=1 d j = exp v(t t0) + k 1 " j=1 ln d j = exp v (t tk 1) + k 1 " j=1 ln  d j v(t j t j 1)  exp { (v )(t t0)} . This implies that E  |x(t)|2 max(P) min(P) E  | |2 e (v )(t t0), t > t0. (33) By De nition 1, the origin of system (8) is globally, robustly, and exponentially stable in mean square and, therefore, the equilibrium point u of (1) is globally, robustly, and exponen- tially stable in mean square. The proof is thus complete. Remark 3: This theorem characterizes the hybrid effects of parameter uncertainties, stochastic perturbation, impulses, and time delay on the global exponential stability of (1). Conditions 1), 2), and 4) are to ensure the exponential stability with exponential convergence rate v for the impulse-free stochastic neural network system (2). In condition 4), d j HUANG et al.: ROBUST EXPONENTIAL STABILITY OF UNCERTAIN DNNs 871 depends on both the exponential stability property of impulse- free subsystem and impulse strength. Condition 4) combines the exponential convergence rate v of impulse-free subsystem, impulse strength kj , and impulse interval t j t j 1 and characterizes the relationship among them. Remark 4 (LMI Form): It is worth noting that condi- tions 1) 4) in this theorem can be formulated as a set of linear matrix inequalities (LMI) with respect to the variables P, Q1, Q2, 1, 2, 3, 4, and 1. Speci cally, condition 4) itself is an LMI, condition 2) is identical to the LMI of the form (here, we take 2 = 2 1)  LQ2L + 3 max( T 3 3)L2 P 1 T 5 1 T 5 4I  0 (34) and condition 1) can be rewritten as the following LMI: 11 P A PB PD PD Q1 Q2 1I 2I PD 3I 0 (35) where 11 = PC C P + LQ1L + 1 T 1 1 + 2 max  T 2 2  L2 + ( 1 + 4) T 4 4 + P. Remark 5: From this theorem, we can obtain several stability criteria for the special case of (1). For example, conditions 1) 3) ensure the global, robust, and exponential stability for impulse-free (2). Taking i = 0(i = 1 5) in Theorem 1, Theorem 1 is suitable to determine the global and exponential stability of the following neural networks with impulse: u (t)= Cu(t)+ Af (u(t)) + B f (u(t )) + .t = tk u(tk)= Jk(u(t k )), t = tk. (36) For computational simplicity, we derive the following corol- laries based on Theorem 1. Corollary 1: Assume that (H1) (H3) hold true. If < and condition (iv) is satis ed, then the equilibrium is globally, robustly, and exponentially stable in the mean square for all admissible parameter uncertainties, where = 2 c  2 a l + 2 , 1d + 2 , 2d l + b + d + 4 + d 4 = l2(1 + 3) + d2 5 in which c = min 1 i n(ci), b = max(BBT ), a = max(AAT ), d = max(DDT ), 1 = max( T 1 1), 2 = max( T 2 2), 3 = max( T 3 3), 4 = max( T 4 4), 5 = max( T 5 5), l = max 1 i n{li}. Proof: Let P = Q2 = I, Q1 = l 1 aI, 1 = , d/ 1, 2 = l 1, d/ 2, 3 = 4 = 1 and 1 = d. Obviously, condition 4) is satis ed. It is also easy to show that conditions 1) 2) are both satis ed and, therefore, we omit it here. Corollary 2: Assume that (H1) (H3) hold. If < , tk tk 1 T and v 1/T ln d > 0, where d = sup1 j< { d j}, v (0, ) is the unique positive solution of v = ev , and , are de ned in Corollary 1. Then, the equilibrium u is globally, robustly, and exponentially stable in the mean square. Proof: Since tk tk 1 T, for t [tk 1, tk), we have t t0 tk 1 t0 (k 1)T, which implies that k 1 1/T(t t0). Moreover e v(t t0) k 1 j=1 d j exp - v(t t0) + (k 1) ln d . = exp  v 1 T ln d (t t0)  . Similar to the proof of Theorem 1, one can conclude the proof of Corollary 2. Remark 6: Corollary 2 allows us to estimate the admissible interval of impulse rate ji(i = 1, . . . , n; j = 1, 2, . . .) by estimating the upper bound of d. In fact, since d < evT , with d = sup j d j = sup j max  ev , 2 j  = sup j max  ev , max 1 i n(1 ji)2  one easily obtains that 1 e 1 2 vT ji 1 + e 1 2 vT . (37) IV. NUMERICAL EXAMPLES In order to show the effectiveness of the theoretical results, we present two numerical examples in this section. Example 1: Consider the stochastic neural network model (1) with the following parameters: dx(t) = [ (C + C)x(t) +(B + B) f (x(t ))]dt +[W0x(t) + W1x(t )]dW(t), t = tk x(tk) = diag k1, k2, , kn  x t k  , t = tk, k = 1, 2, . . . (38) with C = 4I, D = diag (0.1, 0.5, 0.3) , 1 = 0.6I, 3 = 0.2I, 4 = 5 = 0.2I, F (t) = 0.6I fi ( ) = 0.5 tanh ( ) , i = 1, 2, 3. B = 0.3 0.8 0.5 0.1 0.6 0.1 0.6 0.4 0.3 where I denotes the identity matrix with appreciate dimension. By solving LMIs (34) and (35) and Condition 3) in Theorem 1, 872 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 23, NO. 6, JUNE 2012 Fig. 1. Time response curves of impulse-free version of (38) with initial function x( ) = [3, 2, 1]T , [ 1, 0]. Fig. 2. Relationship between the impulse strength and impulse interval. The region between two curves is the feasible region of (T, ki ). we obtain that = 6.44 and = 0.5 when P = 1.625969 0.274455 0.078403 0.274455 1.479031 0.153170 0.078403 0.153170 1.581636 Q1 = 1.189058 0.297209 0.113081 0.297209 0.880583 0.069861 0.113081 0.069861 1.182447 Q2 = 1.664753 0.099701 0.067295 0.099701 2.214071 0.412016 0.067295 0.412016 1.502604 1 = 1.06421021742691, 2 = 1.94254018041615 3 = 1.88286360430972, 4 = 1.51954215574645 1 = 1.23064946440599. From Theorem 1 and its corollaries, and Remark 6, we can obtain the following observations. Observation 1: The origin of impulse-free version of (38) is globally robust and exponentially stable in mean square for Fig. 3. Time response curves of impulsive model (38) with tj+1 t j T = 1.3, = 1, ki 5 and initial function x( ) = [3, 2, 1]T , [ 1, 0]. Fig. 4. Time response curves of impulsive model (38) with tj+1 t j T = 1.3, = 1, ki 10 and initial function x( ) = [3, 2, 1]T , [ 1, 0]. Large impulses lead to unstable dynamics. any delay. The time response curves of impulse-free model are shown in Fig. 1 where = 1. Observation 2: Based on Remark 6, the feasible bounds of impulse strength depend on the maximal impulse interval T. For this example, we estimate that v = 2.14954. Therefore, by Remark 6, we plot the feasible region of (T, ki), shown in Fig. 2. For any value of (T, ki) in this region, the origin of impulse (38) is globally robust and exponentially stable in mean square. Fig. 3 presents the time response curves of (38) with t j+1 t j T = 1.3, = 1, and ki 5, which are close to the upper limit of impulse stability region. As expected, the system response is exponentially stable. However, too large impulses will drive the system to unstable dynamics, as shown in Fig. 4. Example 2: Consider the quadruple-tank process presented in Fig. 5. Assuming perfect ow and cylindrical tanks, the non-dimensional differential equations representing the mass balances in this quadruple-tank process are modi ed as follows [51]: x (t)= A0x (t)+ A1x (t 1)+B0u (t 2)+B1u (t 3) (39) HUANG et al.: ROBUST EXPONENTIAL STABILITY OF UNCERTAIN DNNs 873 Fig. 5. Schematic representation of the quadruple-tank process (from [51]). where A0 = 0.021 0 0 0 0 0.021 0 0 0 0 0.424 0 0 0 0 0.424 A1 = 0 0 0.0424 0 0 0 0 0.0424 0 0 0 0 0 0 0 0 B0 =  0.1113 1 0 0 0 0 0.1042 2 0 0 T B1 =  0 0 0 0.1113 (1 1) 0 0 0.1042 (1 2) 0 T . In the sequel, for simplicity, we assume that 1 = 0, = 2, 1 = 0.333, 2 = 0.307, and u (t ) = K f (x (t )) with K =  0.1603 0.1765 0.0795 0.2073 0.1977 0.1579 0.2288 0.0772  and f (x (t )) = [ f1 (x1 (t )) , , f4 (x4 (t ))]T fi (xi) = 0.01 [|xi + 1| |xi 1|] . Thus, this system is further written as the form of x (t) = (C + C) x (t) + B f (x (t )) (40) where C = A0, C = A1, B = (B0 + B1) K. Moreover, it is assumed that this system is subject to impulsive pertur- bation described by x (tk) = 4x t k  , tk = 4k, k = 1, 2, . . . Hence, the model of this quadruple-tank process is exactly the form of (1) without stochastic perturbation, namely  x (t) = (C + C) x (t) + B f (x (t )) , t = 4k x (t) = x t  , t = 4k, k = 1, 2, . . . . (41) Fig. 6. Time response curves of quadruple-tank systems with impulse effects. Selecting D = 0.1I, F (t) = I, 1 = 10 A1, one observes that the conditions 1) 3) in Theorem 1 are feasible when = 0.04, = 0.0001, which implies that v = 0.0399. Note that t j t j 1 4. It is easy to show that condition 4) in Theorem 1 also holds provided that 0.0831 < < 2.0831. Namely, the considered system is globally and exponentially stable. Fig. 6 shows the trajectories of the system, from the initial condition x ( ) = [ 4.04.06.0 5.0]T . V. CONCLUSION Different from most of the existing publications dealing with the stochastic stability problem of continuous-time delayed neural networks, this paper has derived new criteria on stochas- tic stability with the impulse effects. We have characterized the destabilizing effects of impulse and then estimated the feasible bound of impulses. The main result shows that when the impulse-free DNN with stochastic perturbation is glob- ally and exponentially stable in mean square, the impulsive DNN with stochastic perturbation may preserve its stability property as long as the impulses are not beyond a speci ed bound. REFERENCES [1] J. J. Hop eld, Neural networks and physical systems with emergent collective computational abilities, Proc. Nat. Acad. Sci., vol. 79, no. 8, pp. 2554 2558, 1982. [2] J. J. Hop eld, Neurons with graded response have collective computa- tional properties like those of two-state neurons, Proc. Nat. Acad. Sci., vol. 81, no. 10, pp. 3088 3092, 1984. [3] G. Joya, M. A. Atencia, and F. Sandoval, Hop eld neural networks for optimization: Study of the different dynamics, Neurocomputing, vol. 43, nos. 1 4, pp. 219 237, 2002. [4] N. Kumaresan and P. Balasubramaniam, Optimal control for stochastic nonlinear singular system using neural networks, Comput. Math. Appl., vol. 56, no. 9, pp. 2145 2154, 2008. [5] S. Young, P. Scott, and N. Nasrabadi, Object recognition using multi- layer Hop eld neural network, IEEE Trans. Image Process., vol. 6, no. 3, pp. 357 372, Mar. 1997. [6] Z. Wang, Y. Liu, K. Fraser, and X. Liu, Stochastic stability of uncertain Hop eld neural networks with discrete and distributed delays, Phys. Lett. A, vol. 354, no. 4, pp. 288 297, 2006. [7] T. Roska and L. O. Chua, Cellular neural networks with nonlinear and delay-type template, Int. J. Circuit Theory Appl., vol. 20, no. 5, pp. 469 481, 1992. [8] S. Arik, Global asymptotic stability of a class of dynamical neural networks, IEEE Trans. Circuits Syst. I, vol. 47, no. 4, pp. 568 571, Apr. 2000. 874 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 23, NO. 6, JUNE 2012 [9] J. Cao, Global stability conditions for delayed CNNs, IEEE Trans. Circuits Syst. I, vol. 48, no. 11, pp. 1330 1333, Nov. 2001. [10] H. Zhang, Z. Wang, and D. Liu, Global asymptotic stability of recurrent neural networks with multiple time varying delays, IEEE Trans. Neural Netw., vol. 19, no. 5, pp. 855 873, May 2008. [11] T. Huang, A. Chan, Y. Huang, and J. Cao, Stability of Cohen-Grossberg neural networks with time-varying delays, Neural Netw., vol. 20, no. 6, pp. 868 873, 2007. [12] Z. Zeng and J. Wang, Design and analysis of high-capacity associative memories based on a class of discrete-time recurrent neural networks, IEEE Trans. Syst. Man Cybern., vol. 38, no. 6, pp. 1525 1536, Dec. 2008. [13] Z. Zeng and J. Wang, Associative memories based on continuous- time cellular neural networks designed using space-invariant cloning templates, Neural Netw., vol. 22, nos. 5 6, pp. 651 657, 2009. [14] Z. Zeng and J. Wang, Improved conditions for global exponential stability of recurrent neural networks with time-varying delays, IEEE Trans. Neural Netw., vol. 17, no. 3, pp. 623 635, May 2006. [15] D. Liu and A. Molchanov, Criteria for robust absolute stability of time- varying nonlinear continuous-time systems, Automatica, vol. 38, no. 4, pp. 627 637, 2002. [16] D. Liu, S. Hu, and J. Wang, Global output convergence of a class of continuous-time recurrent neural networks with time-varying thresh- olds, IEEE Trans. Circuits Syst.-II, Exp. Briefs, vol. 51, no. 4, pp. 161 167, Apr. 2004. [17] C. Li, G. Feng, and T. Huang, On hybrid impulsive and switching neural networks, IEEE Trans. Syst. Man Cybern. B, vol. 38, no. 6, pp. 1549 1560, Dec. 2008. [18] S. Blythe, X. Mao, and X. Liao, Stability of stochastic delay neural networks, J. Franklin Inst., vol. 338, no. 4, pp. 481 495, 2001. [19] Z. Zeng and J. Wang, Improved conditions for global exponen- tial stability of recurrent neural networks with time-varying delays, IEEE Trans. Neural Netw., vol. 17, no. 3, pp. 623 635, May 2006. [20] J. Zhang, P. Shi, J. Qiu, and H. Yang, A new criterion for exponential stability of uncertain stochastic neural networks with mixed delays, Math. Comput. Modell., vol. 47, pp. 1042 1051, 2008. [21] R. Rakkiyappan and P. Balasubramaniam, Delay-dependent asymptotic stability for stochastic delayed recurrent neural networks with time varying delays, Appl. Math. Comput., vol. 198, no. 2, pp. 526 533, 2008. [22] X. Lou and B. Cui, Delay-dependent stochastic stability of delayed Hop eld neural networks with Markovian jump parameters, J. Math. Anal. Appl., vol. 328, no. 1, pp. 316 326, 2007. [23] H. Huang and J. Cao, Exponential stability analysis of uncertain stochastic neural networks with multiple delays, Nonlinear Analy. Real World Appl., vol. 8, no. 2, pp. 646 653, 2007. [24] W. Zhou, H. Lu, and C. Duan, Exponential stability of hybrid stochastic neural networks with mixed time delays and nonlinearity, Neurocom- puting, vol. 72, pp. 3357 3365, 2009. [25] G. Peng and L. Huang, Exponential stability of hybrid stochastic recurrent neural networks with time-varying delays, Nonlinear Anal. Hybrid Syst., vol. 2, no. 4, pp. 1198 1204, 2008. [26] J. Yu, K. Zhang, and S. Fei, Further results on mean square exponential stability of uncertain stochastic delayed neural networks, Commun. Nonlinear Sci. Numer. Simulat., vol. 14, no. 4, pp. 1582 1589, 2009. [27] Z. Shu and J. Lam, Global exponential estimates of stochastic interval neural networks with discrete and distributed delays, Neurocomputing, vol. 71, nos. 13 15, pp. 2950 2963, 2008. [28] Y. Liu, Z. Wang, and X. Liu, On global stability of delayed BAM stochastic neural networks with Markovian switching, Neural Process. Lett., vol. 30, no. 1, pp. 19 35, 2009. [29] H. Huang, D. W. C. Ho, and Y. Qu, Robust stability of stochastic delayed additive neural networks with Markovian switching, Neural Netw., vol. 20, no. 7, pp. 799 809, 2007. [30] H. Liu, L. Zhao, Z. Zhang, and Y. Ou, Stochastic stability of Markovian jumping Hop eld neural networks with constant and dis- tributed delays, Neurocomputing, vol. 72, nos. 16 18, pp. 3669 3674, 2009. [31] B. Zhang, X. Sheng, G. Zong, and Y. Zou, Delay dependent exponential stability for uncertain stochastic Hop eld neural networks with time- varying delays, IEEE Trans. Circuits Syst. I, Reg. Papers, vol. 56, no. 6, pp. 1241 1247, Jun. 2009. [32] R. Yang, Z. Zhang, and P. Shi, Exponential stability on stochastic neural networks with discrete interval and distributed delays, IEEE Trans. Neural Netw., vol. 21, no. 1, pp. 169 175, Jan. 2010. [33] Y. Chen, A. Xue, X. Zhao, and S. Zhou, Improved delay-dependent stability analysis for uncertain stochastic Hop eld neural networks with time-varying delays, IET Control Theory Appl., vol. 3, no. 1, pp. 88 97, Jan. 2009. [34] R. Yang, H. Gao, and P. Shi, Novel robust stability criteria for stochastic hop eld neural networks with time delays, IEEE Trans. Syst. Man Cybern., vol. 39, no. 2, pp. 467 474, Apr. 2009. [35] Z. Wang, Y. Liu, M. Li, and X. Liu, Stability analysis for stochastic Cohen-Grossberg neural networks with mixed time delays, IEEE Trans. Neural Netw., vol. 17, no. 3, pp. 814 820, May 2006. [36] H. Zhang and Y. Wang, Stability analysis of Markovian jump- ing stochastic Cohen-Grossberg neural networks with mixed time delays, IEEE Trans. Neural Netw., vol. 19, no. 2, pp. 366 370, Feb. 2008. [37] X. Lou and B. Cui, Stochastic exponential stability for Markov- ian jumping BAM neural networks with time-varying delays, IEEE Trans. Syst. Man Cybern., vol. 37, no. 3, pp. 713 719, Jun. 2007. [38] J. Lu, D. W. C. Ho, J. Cao, and J. Kurths, Exponential synchro- nization of linearly coupled neural networks with impulsive distur- bances, IEEE Trans. Neural Netw., vol. 22, no. 2, pp. 329 335, Feb. 2011. [39] W. Allegretto, D. Papini, and M. Forti, Common asymptotic behavior of solutions and almost periodicity for discontinuous, delayed, and impulsive neural networks, IEEE Trans. Neural Netw., vol. 21, no. 7, pp. 1110 1125, Jul. 2010. [40] Z. Guan and G. Chen, On delayed impulsive Hop eld neural networks, Neural Netw., vol. 12, no. 2, pp. 273 280, 1999. [41] C. Li, S. Wu, G. Feng, and X. Liao, Stabilizing effects of impulses in discrete-time delayed neural networks, IEEE Trans. Neural Netw., vol. 22, no. 2, pp. 323 329, Feb. 2011. [42] Q. Song and J. Cao, Global robust stability of interval neural networks with multiple time-varying delays, Math. Comput. Simul., vol. 74, no. 1, pp. 38 46, 2007. [43] Q. Zhu and J. Cao, Robust exponential stability of Markovian jump impulsive stochastic Cohen-Grossberg neural networks with mixed time delays, IEEE Trans. Neural Netw., vol. 21, no. 8, pp. 1314 1325, Aug. 2010. [44] Y. Li, Global exponential stability of BAM neural networks with delays and impulses, Chaos Solitons Fractals, vol. 24, no. 1, pp. 279 285, 2005. [45] C. Li, Y. Shen, and G. Feng, Stabilizing effects of impulses in delayed BAM neural networks, IEEE Trans. Circuits Syst. II, vol. 55, no. 12, pp. 1284 1288, Dec. 2008. [46] Q. Song and Z. Wang, Stability analysis of impulsive stochastic Cohen- Grossberg neural networks with mixed time delays, Phys. A, vol. 387, nos. 13 15, pp. 3314 3326, 2008. [47] X. Wang, Q. Guo, and D. Xu, Exponential p-stability of impulsive stochastic Cohen-Grossberg neural networks with mixed delays, Math. Comput. Simul., vol. 79, no. 5, pp. 1698 1710, 2009. [48] B. Ksendal, Stochastic Differential Equations, 6th ed. New York: Springer-Verlag, 2003. [49] C. Baker and E. Buckwar, Exponential stability in pth mean of solutions, and of convergent Euler-type solutions, of stochastic delay differential equations, J. Comput. Appl. Math., vol. 184, no. 2, pp. 404 427, 2005. [50] A. N. Michel, L. Hou, and D. Liu, Stability of Dynamical Systems: Con- tinuous, Discontinuous and Discrete Systems. Boston, MA: Birkhauser, 2007. [51] F. El Haoussi, E. H. Tissir, F. Tadeo, and A. Hmamed, Delay-dependent stabilization of systems with time-delayed state and control: Application to quadruple-tank process, Int. J. Syst. Sci., vol. 42, no. 1, pp. 41 49, 2011. [52] P. P. Khargonekar, I. R. Petersen, and K. Zhou, Robust stabilization of uncertain linear systems: Quadratic stabilizability and H-in nite control theory, IEEE Trans. Autom. Control, vol. 35, no. 3, pp. 356 361, Mar. 1990. [53] J. S. Thorp and B. R. Barmish, On guaranteed stability of uncertain linear systems via linear control, J. Opt. Theory Appl., vol. 35, no. 4, pp. 559 579, 1981. HUANG et al.: ROBUST EXPONENTIAL STABILITY OF UNCERTAIN DNNs 875 Tingwen Huang received the B.S. degree in mathematics from Southwest Normal University, Chongqing, China, the M.S. degree in applied math- ematics from Sichuan University, Chengdu, China, and the Ph.D. degree in mathematics from Texas A&M University, College Station, in 1990, 1993, and 2002, respectively. He was a Lecturer with Jiangsu University, Zhen- jiang, China, from 1994 to 1998, and Visiting Assistant Professor from January to July 2003, an Assistant Professor from August 2003 to June 2009, and an Associate Professor since July 2009 with Texas A&M University. His current research interests include neural networks, complex networks, and chaos and dynamics of systems and operator semi-groups and their applications. Chuandong Li (SM 10) received the B.S. degree in applied mathematics from Sichuan University, Chengdu, China, in 1992, the M.S. degree in opera- tional research and control theory and Ph.D. degree in computer software and theory from Chongqing University, Chongqing, China, in 2001 and 2005, respectively. He has been a Professor with Chongqing Univer- sity since 2007. From November 2006 to November 2008, he served as a Research Fellow with the Department of Manufacturing Engineering and Engi- neering Management, City University of Hong Kong, Kowloon, Hong Kong. He has published more than 100 papers in high-impact journals. His current research interests include iterative learning control of time-delay systems, neural networks, chaos control and synchronization, and impulsive dynamical systems. Shukai Duan (M 10) received the B.Sc. degree in applied physics and the M.S. degree in condensed matter physics from Southwest University, Chongqing, China, in 1996 and 2003, respectively, and the Ph.D. degree in computer application technology from Chongqing University, Chongqing. He is the Associate Dean and Professor with the School of Physical Science and Technology, School of Electronics and Information Engineering, Southwest University. He is also a Visiting Professor with the Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor. He has published more than 40 academic papers since 2000. His current research interests include intelligent information processing, arti cial neural networks, chaos, nonlinear circuit theory and application, and memristor and memristive systems. He serves as a member of the IEEE Computational Intelligence Society, a Standing Committee Member of the National Information and Electronic Graduate Education Society of China, the peer review experts of the National Natural Science Foundation of China, the Science and Technology Gov- ernment Advisor of Chongqing Province, and the Deputy Director of the Chongqing Division, National Undergraduate Electronic Design Organizing Committee. Janusz A. Starzyk (SM 83) received the M.S. degree in applied mathematics and the Ph.D. degree in electrical engineering from the Warsaw University of Technology, Warsaw, Poland, in 1971 and 1976, respectively, and the Habilitation degree in electrical engineering from the Silesian University of Technol- ogy, Gliwice, Poland, in 2008. He was an Assistant Professor with the Institute of Electronics Fundamentals, Warsaw University of Technology, from 1977 to 1981. From 1981 to 1983, he was a Post-Doctorate Fellow and Research Engi- neer with McMaster University, Hamilton, ON, Canada. In 1983, he joined the Department of Electrical and Computer Engineering, Ohio University, Athens, where he is currently a Professor of electrical engineering and computer sci- ence. He has cooperated with the National Institute of Standards and Technol- ogy, Gaithersburg, MD. He has been a Consultant to AT&T Bell Laboratories, Sarnoff Research, Sverdrup Technology, Magnolia Broadband, and Magnetek Corporation. He was a Visiting Professor with the University of Florence, Florence, Italy, and Nanyang Technological University, Singapore. He was a Visiting Researcher with Redstone Arsenal, U.S. Army Test, Measurement, and Diagnostic Activity and Wright Laboratories, Advanced Systems Research and ATR Technology Development. He is the author or co-author of over 170 refereed journal and conference papers. His current research interests include motivated learning, embodied intelligence, computational models of cognition, goal creation in intelligent systems, hierarchically organized spatiotemporal memories, self-organizing learning, and neural networks. View publication stats