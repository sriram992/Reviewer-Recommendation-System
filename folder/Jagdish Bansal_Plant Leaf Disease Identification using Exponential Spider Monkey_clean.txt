Plant Leaf Disease Identi cation using Exponential Spider Monkey Optimization Sandeep Kumara, , Basudev Sharmab, Vivek Kumar Sharmab, Harish Sharmac, Jagdish Chand Bansald aAmity University Rajasthan, Jaipur, India bJagannath University, Jaipur, Rajasthan, India cRajasthan Technical University, Kota, Rajasthan, India dSouth Asian University, New Delhi, India Abstract Agriculture is one of the prime sources of economy and a large community is involved in cropping various plants based on the environmental conditions. However, a number of challenges are faced by the farmers including di erent diseases of plants. The detection and prevention of plant diseases are the serious concern and should be treated well on time for increasing the productivity. There- fore, an automated plant disease detection system can be more bene cial for monitoring the plants. Generally, the most diseases may be detected and classi ed from the symptoms appeared on the leaves. For the same, extraction of relevant features plays an important role. A number of meth- ods exists to generate high dimensional features to be used in plant disease classi cation problem such as SPAM, CHEN, LIU, and many more. However, generated features also include unrelated and inessential features that lead to degradation in performance and computational e ciency of a classi cation problem. Therefore, the choice of notable features from the high dimensional feature set is required to increase the computational e ciency and accuracy of a classi er. This paper introduces a novel exponential spider monkey optimization which is employed to x the signi cant features from high dimensional set of features generated by SPAM. Furthermore, the selected fea- tures are fed to support vector machine for classi cation of plants into diseased plants and healthy plants using some important characteristics of the leaves. The experimental outcomes illustrate that the selected features by Exponential SMO e ectively increase the classi cation reliability of the classi er in comparison to the considered feature selection approaches. Corresponding author Email address: sandpoonia@gmail.com (Sandeep Kumar) Preprint submitted to Sustainable Computing: Informatics and Systems October 26, 2018 Keywords: Plant Disease Identi cation, Feature Selection, Spider Monkey Optimization, Nature Inspired Algorithm, Subtractive Pixel Adjacency Model, Optimization 1. Introduction Agriculture is one of the important sources of earning for human beings in many countries. A di erent variety of food plants are harvested as per the need and environmental conditions of land. However, a number of problems are also faced by the farmers such as shortage of water, natural disasters, plant diseases and many more. However, some of the problems may be reduced by 5 providing technical facilities to the farmers. Automated plant disease identi cation and prevention system is one of such solutions that can aid the farmers. This type of system can overcome from the problems of lack of plants disease knowledge as there are very few experts for the same [1], [2]. Moreover, it may increase the food productivity by performing the on time prevention from the disease and there is no need to search for an expert. Such automated system will also be time and 10 cost e cient. Therefore, this manuscript proposes a novel strategy to recognize the various plant diseases. Generally, leaves of the plants are rst source to detect the most of the plant diseases. Yellow and brown spots, primary and late blister, and other ailments caused by bacteria, virus and fungus can be detected automatically through e cient image processing techniques [3], [4]. Therefore, this 15 paper focuses on the plant disease identi cation using leaves properties only. However, plant disease identi cation through image processing is not an easy job because of the huge disparities available in the leaves of di erent and similar plants for instance size, texture, color, shape, etc. Various image processing strategies have been anticipated to overcome from such problems and normally all methods have two steps [3]. In the rst phases prominent features are extracted from the input 20 images of the leaves and in second phase, a particular classi er is used which categorise the images into healthy or diseased images. Major classi cation techniques that are popularly used for disease identi cation in plant are k-nearest neighbor (kNN) [5], support vector machine (SVM) [6], sher linear discriminant (FLD) [7], neural network [8], random forest (RF) [9] and many more. The performance of a classi er is generally relies on the extraction of important features. As 25 per the contemporary, feature extraction methods for image analysis may be categorized into hand crafted and automatic generated (without human experts) features [10]. Hand crafted features may be shape-based, texture-based, color information-based features. To identify the four classes of 2 rice grains, Sakai et al. [11] used geometrical attributes namely maximum length, area, perimeter, and many others. For similar type of leaves such as texture and color, beetle and pepper features 30 were used [12]. In some of the literature, the combined texture and shape features have been used to identify the leaves. Although, hand crafted features shows good results for plant disease identi cation, however, it requires human expert knowledge and many features may be skipped or redundant features may be selected through this process. Therefore, many researchers proposed di erent machine learning based feature extraction methods that do not impose such constraint 35 such as intra and inter-block dependencies (CHEN) [13] for Markov features, for spatial domain subtractive pixel adjacency model (SPAM) [14], bag of visual words [15], convolutional neural network [16] and many more. These methods automatically generate the high dimensional features without human experts. However, high dimensionality [17] is a major concern in case of images. An expending order of training data is mandatory to engender the high dimensional features, 40 which increases the classi er s computational complexity. Moreover, the performance of a classi er may degraded due to generation of inappropriate and unnecessary features. Thence, there is a prerequisite for a competent technique for feature selection to solve the problem [18]. Generally an evaluation parameter is used by a feature selection method to obtain the optimal or sub-optimal feature subset. A number of search methods exists for selecting the prominent 45 features. In case of exhaustive search, 2N feature subsets are compared for N dimensional feature space. It shows a complexity of O(2N) which is impractical for large N [19]. For that reason, numerous approaches like lters, embedded methods and wrappers[20] have been introduced for feature selection to overcome these issue. The most e cient technique in these methods is lter technique that consider a set of features as class variables. On the other hand, for a speci c classi er, 50 it may do weakly [21]. The embedded methods use the information returned by a supervised classi er to pick the features like SVM with recursive feature elimination (SVM-RFE) [22] which eliminate the features, comprising minimum weight acquired from a trained SVM. Furthermore, Wrapper methods uses predictive models to appraise the feature subset and are more preferred than ltering techniques [20]. Greedy hill-climbing search approach is one of the popular wrapper 55 technique and repeatedly eradicates the smallest relevant features is Sequential backward selection (SBS) [23]. Though, both the embedded and wrapper techniques are computationally expensive procedures [24]. To overcome the limitations of above mentioned methods, nature inspired algorithms have widely 3 been used in the literature. Large number of methods have been evolved using nature inspired 60 algorithms [25, 26] for feature selection. Spider monkey optimization (SMO) [27], particle swarm optimization (PSO) [28], arti cial bee colony (ABC) [29], gravitational search algorithm (GSA) [30], and grey wolf optimization (GWO) [31] are few popular meta-heuristic useful in feature selection. The SMO is one the recently anticipated meta-heuristic based on the social activities of spider monkeys and is established by Bansal et al. [27]. As compared to other meta-heuristic algorithms, 65 SMO shows better performance in searching the relevant features from high dimensional feature space. SMO uses the concept of ssionfusion social system (FFSS) of spider monkeys. Initially it explore the feasible search reason and exploits slowly, by the means of social organization of spider monkeys. A number of variants of SMO are also available in literature such as modi ed position update in SMO [32], modi ed SMO [33], tness based position update in [34], SMO for constrained 70 optimization [35], improved SMO [36], hybrid of SMO and GA [37] and many more. Perturbation rate is one of the important parameter of SMO which a ects the convergence behavior of SMO. Generally, perturbation rate is a linearly increasing function. However, due to the availability of non-linearity in di erent applications, a non-linear function may a ect the performance of SMO. Therefore, to improve the performance of SMO, this manuscript recommends a novel alternative 75 of SMO, exponential spider monkey optimization (ESMO), with improved perturbation rate and desirable convergence precision, rapid convergence rate, and improved global search capability. The new variant ESMO, used in feature selection for plant disease identi cation. The SPAM method has been employed for extraction of features from the given database of leaf images. Further, the identi ed features are given to individual classi ers to categorise the leaves in the category of 80 healthy or diseased leaves. The results of the anticipated technique has been measured with PSO, GSA, DE, and SMO. In addition, SVM, kNN, LDA, and ZeroR classi ers are used to classify the images into their respective categories. The main contributions of this manuscript are listed here: 1. A novel exponential spider monkey optimization (ESMO) method has been introduced. 85 2. The extraction of relevant features from the considered leaf images done using SPAM. 3. A new approach for selection of feature subset has been anticipated based on ESMO. 4. For classifying the healthy leaf images and diseased leaf images, kNN, SVM, ZeroR, and LDA classi ers are analyzed. 4 The remaining manuscript is structured as follows. The SMO algorithm introduced in Section 90 2. Section 3 illustrates the anticipated image classi cation method. Experimental results of ESMO on standard benchmark problems and the proposed classi cation technique along with statistical analyses has been discussed in Section 4. Lastly the Section 5 conclude this manuscript. 2. Preliminaries This section describes the basic spider monkey optimization (SMO) algorithm that is used for 95 feature selection. 2.1. Spider Monkey Optimization (SMO) The SMO is comparatively new algorithm based on the mathematical model of intelligent be- havior of spider monkeys that follow the ssionfusion social structure (FFSS). According to FFSS, monkeys distribute themselves from bigger to minor groups and vice versa for foraging. The main 100 characteristics of the FFSS are as follows [27]: 1. Initially, all spider monkeys persist in the groups of 40 50 individuals. Each initial group has a leader under whom the food sources are explored. It is termed as a global leader of that group. 2. In case of insigni cant quantity of food, the global leader create smaller subgroups from larger 105 group with each subgroup containing three to eight members to forage independently and each subgroup headed by local leader. 3. The decision of searching food in each sub-group is also decided by a leader, known as local leader. 4. The group members maintain social bonds and defensive boundaries by communicating among 110 themselves and with other members of the group using a unique sound. The mathematical model of foraging behavior of SMO for optimization problem has six di erent phases discussed in subsequent sections. Initially, SMO randomly generates a population of N spider monkeys. A D-dimensional vector used to denote a spider monkey. Let Xij depicts the jth dimension of ith individual. In SMO, each Xij is initialized as follows: Xij = Xminj + U(0, 1) (Xmaxj Xminj) (1) 5 where Xminj and Xmaxj are lower and upper bounds in jth direction for Xi and U(0, 1) denotes a random number in the range [0, 1]. The next section describe all six phases of SMO in detail. 2.1.1. Local Leader Phase (LLP) In this phase, new position of an individual is attained on the basis of the knowledge from the 115 local leader and individuals of group using Eq. (2). Quality of solution decided by their tness value. The solution with higher tness (the new position is better than the current position) selected for next iteration. Xnewij = Xij + U(0, 1) (LLkj Xij) + U( 1, 1) (Xrj Xij) (2) where, Xkj and Xrj denote the positions of jth direction of the local group leader and randomly chosen rth spider monkey from kth group respectively. In order to manage the perturbation in the 120 present location, probability pr is used which is known perturbation rate. The steps of LLP are summarized in Algorithm 1. Algorithm 1 Local Leader Phase (LLP) [27] for each member Xi kth group do for each j {1, . . . , D} do if U(0, 1) pr then Xnewij = Xij + U(0, 1) (LLkj Xij) + U( 1, 1) (Xrj Xij) else Xnewij = Xij end if end for end for 2.1.2. Global Leader Phase (GLP) All individual update their position based on information from global leader and all member of group as shown in Eq. (3) during GLP. Xnewij = Xij + U(0, 1) (GLj Xij) + U( 1, 1) (Xrj Xij) (3) where, GLj shows the jth direction of the global leader. Furthermore, the probability probi is used to select the particular dimension for updating the Xi and is calculated using the tness values of 125 6 each individual as depicted in Eq. (4). probi = fitnessi PN i=1 fitnessi (4) Similar to LLP, the better solution from the newly generated position and old position of the SM are used for further processing. Algorithm 2 presents the steps of GLP. Algorithm 2 Global Leader Phase (GLP) [27] counter=0 while group size > counter do for Xi group do if U(0, 1) < pr then counter=counter+1 Arbitrarily choose j {1,. . . ,D} Arbitrarily choose Xr group and i = r Xnewij = Xij + U(0, 1) (GLj Xij) + U( 1, 1) (Xrj Xij) end if end for end while 2.1.3. Global Leader Learning (GLL) phase The global leader acquire position with overall best tness in this phase and a global limit 130 counter used to keep the record of change in the position of global leader. 2.1.4. Local Leader Learning (LLL) phase The position with best tness within group assigned to local leader. Similar to GLL phase, if local leader s new position is same as the previous position, then the local limit counter updated by one. 135 2.1.5. Local Leader Decision (LLD) phase If local limit counter of a local leader reaches to a threshold count, then the all group members re-initialized by using Eq. (5). The steps of LLD phase is presented in Algorithm 3. Xnewij = Xij + U(0, 1) (GLj Xij) + U( 1, 1) (Xrj Xij) (5) 7 Algorithm 3 Local Leader Decision [27] if Local Limit Count > Local Leader Limit then Local Limit Count = 0 for each j {1, . . . , D} do if U(0,1) pr then Xnewij = Xminj + U(0, 1)(Xmaxj Xminj) else Xnewij = Xij + U(0, 1) (GLj Xij) + U( 1, 1) (Xrj Xij) end if end for end if 2.1.6. Global Leader Decision (GLD) phase The global leader creates small size sub groups if her position not updated for a prede ned 140 number of iterations. In GLD, the local leaders of each group are elected by LLL process. The global leader merges all subgroups into a single group if its position not updated till pre decided threshold. This way, SMO mimics the FFS structure. Moreover, the complete SMO is presented in Algorithm 4. Algorithm 4 Spider Monkey Optimization [27] Initialize Population, pr, Global Leader Limit, and Local Leader Limit. Calculate tness. Identify local and global leaders by employing greedy selection. while Stopping condition is not contented do (i) Stimulate the new positions for the whole group by with knowledge of all individuals including themselves, local leader, group members with the help of Algorithm 1. (ii) Employ the greedy selection process for the whole group based on their tness. (iii) Compute the probability probi of the whole group using Eq. (4). (iv) Identify new locations for all the members of group, selected by probi, by own previous knowledge and experience of global leader and group members using Algorithm 2. (v) Local and global leader update their position using greedy selection strategy for all groups. (vi) All the members of a speci c group redirected for foraging by Algorithm 3 if its Local leader is not able to update her position after a prede ned number of times. (vii) A group further divided into smaller sub groups, with minimum size of each group four, if Global Leader not able to update her location for a prede ned number of times. end while 8 Figure 1: The proposed leaf image classi cation method. 3. Proposed Approach 145 The newly anticipated approach for image classi cation has three steps as depicted in Fig. 1: (1) 1st step uses SPAM for features extraction from the collection of leaf images, (2) 2nd step selects the distinguished features by the means of newly proposed feature selection algorithm using ESMO, and (3) Ultimately, the classi er is used to categorise the leaf images into diseased and healthy leaf images. Detailed description of these phases given in subsequent sections. 150 3.1. Feature Extraction The most important step in image analysis is feature extraction. The extracted features decide the success of a classi er. For an e cient image analysis algorithm, multi-dimension and divergent features must be extracted which can di erentiate healthy leaf images from diseased leaf images. For the same, a number of feature extraction methods have been proposed. This paper uses SPAM 155 for feature extraction from considered leaf images, which is discussed in the next sections. 3.1.1. Subtractive Pixel Adjacency Matrix (SPAM) Peny et al. [14] introduced SPAM to extract the features in spatial domain images. SPAM is the most e cient technique for feature extraction as compared to other existing methods and 9 Figure 2: The proposed ESMO based feature selection method is based on Markov chain features. It uses the information about the images that in general, an 160 image does not have noise. Furthermore, short-range dependencies amongst noise segments inside an image are used by SPAM to extract the features. It obtains the con ned interdependencies between dissimilarity of adjoining face rudiments and used them as a Markov chain to extract the features of images. This paper uses SPAM which extracts 686 features for leaf image data set. 3.2. Feature Selection 165 SPAM extracts 686 features from image data set which is a high dimensional feature vector and requires high computational cost. These features may have redundant or irrelevant features and sometimes degrade classi er s performance. For that reason, these features are given to the feature selection phase to reduce the unwanted and unrelated features. The overall steps of the ESMO based feature selection are shown in Fig. 2. 170 The exponential spider monkey optimization (ESMO) lessens the inappropriate and repetitive 10 features while selecting feature. The proposed ESMO rst initializes randomly the positions of each spider monkey in the population. Each individual has a dimension identical to the quantity of features extracted using SPAM. The ith entity denoted by Eq. (6) if there are total n features extracted and total spider monkeys are N. 175 Xi = {x1, x2, . . . , xn}; i = 1, 2, . . . , N (6) The value of each xi, having an arbitrary value in among 0 and 1, is xed to either 1 or 0 using prede ned threshold for computing the tness. The threshold value is xed at 0.7 in this manuscript, after empirically analysis. Therefore, the value of xi is xed to 1 if it is higher than or equivalent to 0.7 if not then set to 0. Hence, simply the features, having xi value one, are given to calculate the tness function. To calculate the tness value, SVM with ten times counter 180 con rmation is used. In order to select prominent features, the tness and actual value of spider monkeys are given to succeeding stages of ESMO. 3.2.1. Exponential Spider Monkey Optimization (ESMO) Perturbation rate is one of the important parameter of SMO which a ects the convergence behavior of SMO. Generally, perturbation rate is a linearly increasing function. However, due 185 to the availability of non-linearity in di erent applications, a non-linear function may a ect the performance of SMO. Therefore, to enhance the competency of SMO, this manuscript proposes a novel modi cation in SMO named as ESMO, with improved perturbation rate that leads to desirable e ciency for convergence, higher rate of convergence, and enhanced global search capability. For a meta-heuristic algorithm, intensi cation and diversi cation are two imperative stages to 190 achieve precise solution and escape from the local optima. In SMO, perturbation rate is one of the prime factors which a ect the convergence behavior of SMO. In general, it is a linearly in- creasing function with the iterations. On the other hand, it has been observed that sometimes poor divergence in SMO leads to entrapping into local optimum. For that reason, in the antici- pated optimization algorithm, the value of perturbation rate is customized exponentially in place 195 of linearly. In ESMO, the parameter, perturbation rate, is increased exponentially as shown in Eq. (7). prnew = (prinit) max it t (7) 11 where, max it and t symbolize the maximum and the current iteration counter respectively, N stand for the number of spider monkeys and prinit in initial perturbation, initialized randomly in between 0 and 1. Remaining steps of ESMO are similar to basic SMO as depicted in Algorithm 4. 3.3. Classi cation 200 Next step after selecting the relevant and non-redundant features is classi cation of the leaf images into healthy and diseased images and comparison of di erent classi cation techniques. For the same, SVM, kNN,LDA, and ZeroR classi ers are used. 4. Experimental Results The result analysis of newly developed feature selection approach based on ESMO for image 205 classi cation is given in a couple of phases. 1st phase shows the performance of new approach (ESMO) and second, analyses the e ect of feature selection method for plant diseased identi cation. 4.1. Result Analysis of ESMO The performance of ESMO has been simulated on 12 standard benchmarks which are repre- sented in Table 1 [38, 39, 40] along with their corresponding optimal value and range of decision 210 variables. Moreover, the proposed ESMO algorithm has been compared with GSA, DE, PSO, and SMO meta-heuristics over the considered benchmark functions. All the algorithms use default pa- rameter settings as mentioned in the corresponding literature except number of iterations (itr) and population size (N) which are taken 50 and 1000 respectively for all the methods. To reduce the inter-dependencies, mean tness values of 30 runs have been compared. 215 The mean tness values returned by the ESMO and considered methods have been presented in Table 2. The ESMO returns minimum mean values of all the benchmarks among PSO, GSA, and DE except F4 and F8 where, DE and GSA returns slightly better results. To con rm the outcome shown in Table 2, wilcoxon rank sum statistical test [41] has been conducted with NULL hypothesis that at 5% signi cance level, considered two algorithms are similar for respective benchmark. The 220 Table 3 shows wilcoxon rank sum test for proposed and existing methods. For p < 0.05, the null hypothesis is considered as discarded and denoted by + or otherwise accepted and symbolized as = . The + represents the signi cantly di erent result and ESMO returns better result while sign shows signi cantly di erent but ESMO gives competitively pitiable results. Table 3 shows 12 Table 1: Benchmark functions Sr. No. Equation Dimensions Range Optimal Value 1 F1(X) = Pd i=1 x2 i 30 [-100,100] 0 2 F2(X) = Pd i=1 | xi | + Qd i=1 | xi | 30 [-10,10] 0 3 F3(X) = maxi{| xi |, 1 i d} 30 [-100,100] 0 4 F4(X) = Pd i=1([xi + 0.5])2 30 [-100,100] 0 5 F5(X) = Pd i=1 ix4 i + random[0, 1) 30 [-1.28,1.28] 0 6 F6(X) = 20exp  0.2 q 1 d Pd i=1 x2 i  exp  1 d Pd i=1 cos(2 xi)  + 20 + e 30 [-32,32] 0 7 F7(X) = 0.1{sin2(3 x1) + Pd i=1(xi 1)2[1 + sin2(3 xi + 1)] + (xd 1)2[1 + sin2(2 xd)]} + Pd i=1 u(xi, 5, 100, 4) 30 [-50,50] 0 8 F8(X) = P11 i=1  ai x1(b2 i +bix2) b2 i +bix3+x4 2 4 [-5,5] 0.0003 9 F9(X) = 4x2 1 2.1x4 1 + 1 3 x6 1 + x1x2 4x2 2 + 4x4 2 2 [-5,5] -1.0316 10 F10(X) = [1+(x1+x2+1)2(19 14x1+3x2 1 14x2+6x1x2+3x2 2)] [30 + (2x1 3x2)2 (18 32x1 + 12x2 1 + 48x2 36x1x2 + 27x2 2)] 2 [-2,2] 3 11 F11(X) = P4 i=1 ci exp( P3 j=1 aij(xj pij)2) 3 [1,3] -3.86 12 F12(X) = P4 i=1 ci exp( P6 j=1 aij(xj pij)2) 6 [0,1] -3.32 that for maximum functions, ESMO returns signi cantly di erent and better results except F4 225 where DE shows comparatively better results. From the Table 2 and 3, it can be seen that the mean values for F8 with respect to DE is not signi cant. Therefore, it can be validated that the ESMO performs better than existing techniques for mean tness values. Moreover, the comparison of the computational time, taken by Exponential SMO and other considered methods, have been discussed in Table 4. From the table, it 230 can be visualized that by introducing the exponential k-best in basic SMO does not a ect the computational cost of the SMO. An average time taken by ESMO is 2.4385 seconds while SMO takes 2.3989 seconds average computational time. The computa- tional time of DE is also almost similar to ESMO. However, PSO and GSA takes more than 3 seconds for getting the best function values. Furthermore, the convergence 235 behavior of the proposed ESMO has also been compared with the considered state- 13 Table 2: Comparison of mean tness values Function ESMO PSO GSA DE SMO F1 9.69E-11 2.32E-07 1.12E-05 3.54E+03 1.88E-08 F2 3.94E-10 3.02E-07 5.27E-04 0.88E+02 3.03E-08 F3 1.11E-10 5.81E-05 2.47E+00 0.65E+02 2.16E-08 F4 2.69E-08 1.15E-07 2.811E-05 3.68E+03 9.55E-09 F5 2.01E-04 2.24E-01 7.13E-02 1.71E+00 2.18E-02 F6 3.79E-11 1.99E+01 1.40E-04 1.53E+01 4.15E-08 F7 1.80E-09 1.09E-02 3.39E-05 5.86E+07 1.83E-08 F8 4.07E-04 2.03E-02 1.68E-03 9.78E-04 3.14E-04 F9 4.69E-08 4.65E-08 6.23E-08 4.65E-08 4.83E-08 F10 3.00E+00 3.00E+00 3.00E+01 3.00E+00 3.00E+00 F11 -3.86E+00 -3.00E-01 -3.00E-01 -2.50E-11 -3.86E+00 F12 -3.04E+00 -2.98E+00 -3.04E+00 -2.43E+00 -3.04E+00 of-the-art algorithms. For the same, Figures 3-4 depict the convergence graphs for all the considered benchmark functions over 1000 iterations. The best tness values at every iteration are presented on y-axes in logarithmic scales. From the gures, it can be observed that the proposed ESMO has better convergence behavior as compared 240 to other methods for almost all the benchmark functions. Su cient iterations for exploration are taken by the proposed method before exploitation phase which help to achieve better objective values. Thus experimentally and statistically, it can be validated that the proposed ESMO returns the optimal solutions along with precise convergence behavior for various benchmark functions. 245 14 Table 3: The wilcoxon rank sum test. Function ESMO-PSO ESMO-GSA ESMO-DE ESMO-SMO p-value h-value SGFNT p-value h-value SGFNT p-value h-value SGFNT p-value h-value SGFNT F1 1.2E- 10 1 + 1.2E- 10 1 + 1.2E- 10 1 + 1.2E- 10 1 + F2 1.2E- 09 1 + 1.2E- 10 1 + 1.6E- 05 1 + 4.2E- 07 1 + F3 1.1E-08 1 + 1.2E- 10 1 + 1.2E- 10 1 + 1.9E- 07 1 + F4 1 0 = 2.4E- 10 1 + 1.3E- 10 1 + 1.1E- 08 1 - F5 2.2E- 07 1 + 6.5E- 07 1 + 8.3E- 08 1 + 2.4E- 10 1 + F6 1.2E- 10 1 + 1.2E- 10 1 + 1.2E- 10 1 + 1.2E- 10 1 + F7 1.7E- 08 1 + 2.4E- 12 1 + 2.3E- 06 1 + 8.3E- 08 1 + F8 1.4E- 06 1 + 1 0 = 1 0 = 1 0 = F9 1 0 = 1 0 = 1 0 = 1 0 = F10 1 0 = 1.3E-11 1 + 1 0 = 1 0 = F11 1.5E- 08 1 + 1.2E- 11 1 + 2.3E- 06 1 + 1 0 = F12 2.4E- 12 1 + 1 0 = 4.7E- 09 1 + 1 0 = Table 4: Comparison of computational time in seconds Function ESMO PSO GSA DE SMO F1 2.6828 4.8514 3.7373 2.5380 2.3048 F2 2.3369 4.0179 4.1698 2.7056 2.3397 F3 2.2932 4.7995 5.6281 2.3437 2.3291 F4 2.4616 4.4041 2.9849 2.3770 2.4155 F5 2.7494 3.2745 2.9529 2.9738 2.7474 F6 2.4834 3.2092 4.1470 2.4736 2.5012 F7 2.4275 4.7740 2.9563 2.7722 2.4045 F8 2.3595 3.6840 2.6673 2.4534 2.3460 F9 2.3661 3.3271 2.1040 2.3065 2.3288 F10 2.2383 3.4384 2.1209 2.3694 2.2408 F11 2.4322 3.1568 2.2077 2.0033 2.3858 F12 2.4321 3.6273 2.4316 2.0026 2.4432 Average 2.4385 3.8804 3.1757 2.4433 2.3989 15 0 100 200 300 400 500 600 700 800 900 1000 Iterations 10-15 10-10 10-5 100 105 Best Fitness Value (in log) ESMO PSO GSA DE SMO (a) Benchmark Function F1 0 100 200 300 400 500 600 700 800 900 1000 Iterations 10-10 10-5 100 105 1010 1015 Best Fitness Value (in log) ESMO PSO GSA DE SMO (b) Benchmark Function F2 0 100 200 300 400 500 600 700 800 900 1000 Iterations 10-10 10-5 100 105 Best Fitness Value (in log) ESMO PSO GSA DE SMO (c) Benchmark Function F3 0 100 200 300 400 500 600 700 800 900 1000 Iterations 10-10 10-5 100 105 Best Fitness Value (in log) ESMO PSO GSA DE SMO (d) Benchmark Function F4 0 100 200 300 400 500 600 700 800 900 1000 Iterations 10-3 10-2 10-1 100 101 102 103 Best Fitness Value (in log) ESMO PSO GSA DE SMO (e) Benchmark Function F5 0 100 200 300 400 500 600 700 800 900 1000 Iterations 10-10 10-8 10-6 10-4 10-2 100 102 Best Fitness Value (in log) ESMO PSO GSA DE SMO (f) Benchmark Function F6 Figure 3: The convergence behavior of proposed and existing methods for benchmark functions 16 0 100 200 300 400 500 600 700 800 900 1000 Iterations 10-10 10-5 100 105 1010 Best Fitness Value (in log) ESMO PSO GSA DE SMO (a) Benchmark Function F7 0 100 200 300 400 500 600 700 800 900 1000 Iterations 10-4 10-3 10-2 10-1 100 101 102 Best Fitness Value (in log) ESMO PSO GSA DE SMO (b) Benchmark Function F8 0 100 200 300 400 500 600 700 800 900 1000 Iterations 10-8 10-6 10-4 10-2 100 102 Best Fitness Value (in log) ESMO PSO GSA DE SMO (c) Benchmark Function F9 0 100 200 300 400 500 600 700 800 900 1000 Iterations 100 101 102 103 Best Fitness Value (in log) ESMO PSO GSA DE SMO (d) Benchmark Function F10 0 100 200 300 400 500 600 700 800 900 1000 Iterations -105 -100 -10-5 -10-10 -10-15 Best Fitness Value (in log) ESMO PSO GSA DE SMO (e) Benchmark Function F11 0 100 200 300 400 500 600 700 800 900 1000 Iterations -102 -101 -100 Best Fitness Value (in log) ESMO PSO GSA DE SMO (f) Benchmark Function F12 Figure 4: The convergence behavior of proposed and existing methods for benchmark functions 17 (a) (b) (c) (d) Figure 5: The represented diseased leaf images of (a) potato, (b) potato, (c) apple, and (d) apple leaves taken from [42]. (a) (b) (c) (d) Figure 6: The represented healthy leaf images of (a) potato, (b) potato, (c) apple, and (d) apple leaves taken from [42]. 4.2. Result Analysis of Feature Selection technique The performance of the anticipated diseased leaf identi cation system has been tested on 1000 images from PlantVillage dataset [42]. The dataset consists of 500 healthy leaf images and 500 diseased leaf images. Some of the representative images from diseased and healthy categories are delineated in Fig. 5 and 6 accordingly. For each image, 686 features are extracted using SPAM. 250 Furthermore, ESMO is employed for feature selection. The outcomes of ESMO for feature selection have been compared with PSO, GSA, DE, and SMO using the number of selected features and classi cation precision. Table 5 illustrate the outcome of both the feature selection technique and classi ers. ESMO based feature selection technique returns 82 features which is the least num- 255 ber of selected features as compared to other feature selection techniques. From Table 18 5, it can be observed that approximately 88% features are reduced by ESMO from the 686 extracted features. This reduction of features is highest with respective to PSO, GSA, DE, and SMO which reduce 85%, 87%, 86%, and 87% respectively. From the feature reduction rates, it can be stated that all the considered algorithms along with 260 ESMO eliminate almost same amount of features. However, relevancies of the selected features are tested by feeding them to a classi er for plant leaf disease identi cation. For the same, SVM, kNN, LDA, and ZeroR classi ers are used for comparative anal- ysis of accuracies. The results of ZeroR classi er is measured as a reference line for all the feature selection techniques. From Table 5, it can be obtained that without feature 265 selection method all the classi ers give lowest accuracies which validates the presence of redundant and irrelevant features in the set of extracted features from SPAM. After the applicability of feature selection method before identi cation of diseased leaves, the accuracy of each classi er increases. However, all the considered classi ers shows the best accuracies for the features identi ed by the anticipated ESMO. Among all the 270 classi ers for ESMO based feature selection method, SVM gives the best classi cation accuracy of 92.12% . Consequently, it can be speci ed that the anticipated ESMO based feature selection approach returns minimum number of optimal features which gives better classi cation accuracy. 5. Conclusion 275 This paper anticipated a feature selection approach using novel exponential spider monkey optimization (ESMO) for plant disease identi cation. For the same, diseased and healthy leaf images have been used from plant village dataset. Furthermore, 686 features extracted using SPAM method from the considered image dataset. The performance of ESMO has been compared with PSO, GSA, DE, and SMO methods in terms of mean tness values. The investigational and numerical outcome 280 authenticate that the anticipated ESMO outperforms the considered approaches. Additional, the performance of newly anticipated feature selection process using ESMO has been contrasted with PSO, GSA, DE, and SMO based feature selection techniques. The anticipated technique extracts the minimum 82 features. The classi cation results have been analyzed over SVM, kNN, LDA, and ZeroR classi ers. In the midst of all the classi ers, SVM outperforms to classify the plant leaf 285 images into diseased or healthy leaf images. Thus it can be validated that the anticipated feature 19 Table 5: Comparative analysis of classi ers and feature selection methods. Feature Selection Method Number of Fea- tures Selected Classi cation Method Accuracy SVM 80.26 None 686 LDA 72.37 kNN 76.34 ZeroR 42.29 SVM 89.54 PSO 97 LDA 78.20 kNN 82.13 ZeroR 47.31 SVM 87.54 GSA 87 LDA 77.66 kNN 83.82 ZeroR 46.54 SVM 87.45 DE 91 LDA 78.77 kNN 83.79 ZeroR 47.54 SVM 89.55 SMO 84 LDA 79.67 kNN 77.87 ZeroR 46.44 SVM 92.12 ESMO 82 LDA 80.79 kNN 84.76 ZeroR 49.32 20 selection technique minimizes the unrelated and super uous features while, maintains the elevated classi cation precision. In future, the anticipated technique can be used for multi-class problem where, the di erent plant disease categories can be identi ed. References 290 [1] Z. Wang, H. Li, Y. Zhu, T. Xu, Review of plant identi cation based on image processing, Archives of Computational Methods in Engineering 24 (3) (2017) 637 654. [2] V. Singh, A. Misra, Detection of plant leaf diseases using image segmentation and soft com- puting techniques, Information Processing in Agriculture 4 (1) (2017) 41 49. [3] S. Kaur, S. Pandey, S. Goel, Plants disease identi cation and classi cation through leaf images: 295 A survey, Archives of Computational Methods in Engineering (2018) 1 24. [4] D. Puja, M. Saraswat, K. Arya, et al., Automatic agricultural leaves recognition system, in: Proceedings of Seventh International Conference on Bio-Inspired Computing: Theories and Applications (BIC-TA 2012), Springer, 2013, pp. 123 131. [5] N. Guettari, A. S. Capelle-Laiz e, P. Carr e, Blind image steganalysis based on evidential k- 300 nearest neighbors, in: Image Processing (ICIP), 2016 IEEE International Conference on, IEEE, 2016, pp. 2742 2746. [6] S. Deepa, Steganalysis on images using svm with selected hybrid features of gini index feature selection algorithm, International Journal of Advanced Research in Computer Science 8 (5). [7] M. Ramezani, S. Ghaemmaghami, Towards genetic feature selection in image steganalysis, in: 305 Consumer Communications and Networking Conference (CCNC), 2010 7th IEEE, IEEE, 2010, pp. 1 4. [8] M. Sheikhan, M. Pezhmanpour, M. S. Moin, Improved contourlet-based steganalysis using binary particle swarm optimization and radial basis neural networks, Neural Computing and Applications 21 (7) (2012) 1717 1728. 310 [9] J. Kodovsky, J. Fridrich, V. Holub, Ensemble classi ers for steganalysis of digital media, IEEE Transactions on Information Forensics and Security 7 (2) (2012) 432 444. 21 [10] S. H. Lee, C. S. Chan, S. J. Mayo, P. Remagnino, How deep learning extracts and learns leaf features for plant classi cation, Pattern Recognition 71 (2017) 1 13. [11] N. Sakai, S. Yonekawa, A. Matsuzaki, H. Morishima, Two-dimensional image analysis of the 315 shape of rice and its application to separating varieties, Journal of Food Engineering 27 (4) (1996) 397 407. [12] A. R. Backes, O. M. Bruno, Plant leaf identi cation using multi-scale fractal dimension, in: International Conference On Image Analysis And Processing, Springer, 2009, pp. 143 150. [13] C. Chen, Y. Q. Shi, Jpeg image steganalysis utilizing both intrablock and interblock correla- 320 tions, in: Circuits and Systems, 2008. ISCAS 2008. IEEE International Symposium on, IEEE, 2008, pp. 3029 3032. [14] T. Pevny, P. Bas, J. Fridrich, Steganalysis by subtractive pixel adjacency matrix, IEEE Trans- actions on information Forensics and Security 5 (2) (2010) 215 224. [15] R. D. L. Pires, D. N. Gon calves, J. P. M. Oru e, W. E. S. Kanashiro, J. F. Rodrigues Jr, B. B. 325 Machado, W. N. Gon calves, Local descriptors for soybean disease recognition, Computers and Electronics in Agriculture 125 (2016) 48 55. [16] A. M. J. Hanson, A. Joy, J. Francis, Plant leaf disease detection using deep learning and convolutional neural network, International Journal of Engineering Science 5324. [17] M. Dash, H. Liu, Feature selection for classi cation, Intelligent data analysis 1 (3) (1997) 330 131 156. [18] S. Bhattacharyya, A. Sengupta, T. Chakraborti, A. Konar, D. Tibarewala, Automatic fea- ture selection of motor imagery eeg signals using di erential evolution and learning automata, Medical & biological engineering & computing 52 (2) (2014) 131 139. [19] R. Kohavi, G. H. John, Wrappers for feature subset selection, Arti cial intelligence 97 (1-2) 335 (1997) 273 324. [20] H. Deng, G. Runger, Feature selection via regularized trees, in: Neural Networks (IJCNN), The 2012 International Joint Conference on, IEEE, 2012, pp. 1 8. 22 [21] M. A. Hall, Correlation-based feature selection of discrete and numeric class machine learning. [22] I. Guyon, J. Weston, S. Barnhill, V. Vapnik, Gene selection for cancer classi cation using 340 support vector machines, Machine learning 46 (1-3) (2002) 389 422. [23] S. F. Cotter, K. Kreutz-Delgado, B. D. Rao, Backward sequential elimination for sparse vector subset selection, Signal Processing 81 (9) (2001) 1849 1864. [24] M. Saraswat, K. Arya, Feature selection and classi cation of leukocytes using random forest, Medical & biological engineering & computing 52 (12) (2014) 1041 1052. 345 [25] K. Hussain, M. N. M. Salleh, S. Cheng, Y. Shi, Metaheuristic research: a comprehensive survey, Arti cial Intelligence Review (2018) 1 43. [26] M. Saraswat, K. Arya, H. Sharma, Leukocyte segmentation in tissue images using di erential evolution algorithm, Swarm and Evolutionary Computation 11 (2013) 46 54. [27] J. C. Bansal, H. Sharma, S. S. Jadon, M. Clerc, Spider monkey optimization algorithm for 350 numerical optimization, Memetic computing 6 (1) (2014) 31 47. [28] R. R. Chhikara, P. Sharma, L. Singh, A hybrid feature selection approach based on improved pso and lter approaches for image steganalysis, International Journal of Machine Learning and Cybernetics 7 (6) (2016) 1195 1206. [29] F. G. Mohammadi, M. S. Abadeh, Image steganalysis using a bee colony based feature selection 355 algorithm, Engineering Applications of Arti cial Intelligence 31 (2014) 35 43. [30] E. Rashedi, H. Nezamabadi-Pour, S. Saryazdi, Gsa: a gravitational search algorithm, Infor- mation sciences 179 (13) (2009) 2232 2248. [31] E. Emary, H. M. Zawbaa, C. Grosan, A. E. Hassenian, Feature subset selection approach by gray-wolf optimization, in: Afro-European Conference for Industrial Advancement, Springer, 360 2015, pp. 1 13. [32] S. Kumar, V. K. Sharma, R. Kumari, Modi ed position update in spider monkey optimiza- tion algorithm, International Journal of Emerging Technologies in Computational and Applied Sciences 2 (2014) 198 204. 23 [33] G. Hazrati, H. Sharma, N. Sharma, J. C. Bansal, Modi ed spider monkey optimization, in: 365 Computational Intelligence (IWCI), International Workshop on, IEEE, 2016, pp. 209 214. [34] S. Kumar, R. Kumari, V. K. Sharma, Fitness based position update in spider monkey opti- mization algorithm, Procedia Computer Science 62 (2015) 442 449. [35] K. Gupta, K. Deep, J. C. Bansal, Spider monkey optimization algorithm for constrained opti- mization problems, Soft Computing 21 (23) (2017) 6933 6962. 370 [36] V. Swami, S. Kumar, S. Jain, An improved spider monkey optimization algorithm, in: Soft Computing: Theories and Applications, Springer, 2018, pp. 73 81. [37] A. Agrawal, P. Farswan, V. Agrawal, D. Tiwari, J. C. Bansal, On the hybridization of spider monkey optimization and genetic algorithms, in: Proceedings of Sixth International Conference on Soft Computing for Problem Solving, Springer, 2017, pp. 185 196. 375 [38] X.-S. Yang, Nature-inspired optimization algorithms, Elsevier, 2014. [39] D. Simon, Evolutionary optimization algorithms, John Wiley & Sons, 2013. [40] M. Jamil, X.-S. Yang, A literature survey of benchmark functions for global optimisation problems, International Journal of Mathematical Modelling and Numerical Optimisation 4 (2) (2013) 150 194. 380 [41] J. Derrac, S. Garc a, D. Molina, F. Herrera, A practical tutorial on the use of nonparametric statistical tests as a methodology for comparing evolutionary and swarm intelligence algo- rithms, Swarm and Evolutionary Computation 1 (1) (2011) 3 18. [42] S. P. Mohanty, D. P. Hughes, M. Salath e, Using deep learning for image-based plant disease detection, Frontiers in plant science 7 (2016) 1419. 385 24