Received August 12, 2016, accepted September 3, 2016, date of publication September 13, 2016, date of current version October 6, 2016. Digital Object Identifier 10.1109/ACCESS.2016.2608844 Direction Estimation for Pedestrian Monitoring System in Smart Cities: An HMM Based Approach RAHUL RAMAN, PANKAJ KUMAR SA, (Member, IEEE), BANSHIDHAR MAJHI, (Member, IEEE), AND SAMBIT BAKSHI, (Member, IEEE) Centre for Computer Vision and Pattern Recognition, Department of Computer Science & Engineering, National Institute of Technology Rourkela, Rourkela 769008, India Corresponding author: S. Bakshi (sambitbaksi@gmail.com) This work was supported by the Science and Engineering Research Board, Department of Science and Technology, Government of India, under Grant SB/FTP/ETA-0059/2014. ABSTRACT The paper proposes a novel approach for direction estimation of a moving pedestrian as perceived in a 2-D coordinate of eld camera. The proposed direction estimation method is intended for pedestrian monitoring in traf c control systems. Apart from traf c control, direction of motion estimation is also very important in accident avoidance system for smart cars, assisted living systems, in occlusion prediction for seamless tracking in visual surveillance, and so on. The proposed video-based direction estimation method exploits the notion of perspective distortion as perceived in monocular vision of 2-D camera co-ordinate. The temporal pattern of change in dimension of pedestrian in a frame sequence is unique for each direction; hence, the dimensional change-based feature is used to estimate the direction of motion; eight discrete directions of motion are considered and the hidden Markov model is used for classi cation. The experiments are conducted over CASIA Dataset A, CASIA Dataset B, and over a self- acquired dataset: NITR Conscious Walk Dataset. The balanced accuracy of direction estimation for these experiments yields satisfactory results with accuracy indices as 94.58%, 90.87%, and 95.83%, respectively. The experiment also justi es with suitable test conditions about the characteristic features, such as robustness toward improper segmentation, partial occlusion, and changing orientation of head or body during walk of a pedestrian. The proposed method can be used as a standalone system or can be integrated with existing frame-based direction estimation methods for implementing a pedestrian monitoring system. INDEX TERMS Visual surveillance, occlusion handling, pedestrian direction estimation, perspective distortion, hidden Markov model. I. INTRODUCTION Direction estimation of a moving subject is an important task during many video processing and computer vision ori- ented applications such as behaviour analysis, motion analy- sis, traf c control systems, smart cars, gait based pedestrian identi cation and visual surveillance at secure public places. Motion of a pedestrian in a 3D global plane can be completely analysed in 2D camera plane by three factors, i.e. direction of motion, velocity of motion, and depth information. Therefore, information about direction of motion of pedestrian is very signi cant in motion analysis. In many elds like accident avoidance mechanism in cars, traf c control system, and visual surveillance where dynamic decisions are needed to be taken, advance knowledge of direction of motion is very handy. This establishes the task of pedestrian direction estimation as an important domain of research. Speci cally, a prior knowledge of most proba- ble direction of pedestrian is crucial for accident avoidance and for different surveillance tasks as optimal camera place- ment [1] and for seamless object tracking [2]. Occlusion is a severe issue in pedestrian monitoring. Different authors have shown awareness towards problems arising due to occlusion and presented their views on treat- ment of occlusion. The survey by Yilmaz et al. [3] presents a section with some of the earlier research on treatment of occlusion. A recent survey on handling occlusion while object tracking is shown in [4]. Although a lot of research on occlusion treatment targets estimation of velocity, estimation of current location and its probable location of reappearance after occlusion, the other dimension of research towards occlusion has also been explored where the prediction of occlusion is targeted [2]. 5788 2169-3536 2016 IEEE. Translations and content mining are permitted for academic research only. Personal use is also permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. VOLUME 4, 2016 R. Raman et al.: Direction Estimation for Pedestrian Monitoring System in Smart Cities This prediction of occlusion will assist the existing meth- ods of occlusion handling and occlusion avoidance with a prior knowledge of occlusion. Information about pedestrian s direction of motion thus becomes vital in such situations. Apart from surveillance, direction of motion estimation has been used in many areas such as collision prediction mechanism in smart cars, traf c control system, assisted living, behaviour analysis and long term motion estimation. Markis and Ellis [5], Wakim et al. [6] and Antonini et al. [7], have used direction estimation for pedestrian behaviour model. Abramson and Steux [8] and Large et al. [9] have performed long term motion analysis and path prediction. In a similar work to occlusion prediction, Tsuji et al. [10] have used relative motions and used the knowledge of direction estimation for collision prediction. Many works for direction estimation of a moving pedestrian have extracted the orientation information of the pedestrian in each frame and observed the orientation in subsequent frames to estimate the direction of motion. To cope with the challenges of low resolution images, com- plex and dynamic background, and illumination variation, many researches have used low-level features like Histogram of Gradient (HoG), Scale Invariant Feature Transform (SIFT) or Haar like features with Support Vector Machine (SVM), Neural Network (NN), Regressions, and Adaboost classi ers. Shimizu and Poggio [11] have used SVM on Haar wavelet co-ef cient to distinguish different orientation and estimate direction of motion with study of orientation over subsequent frames. Gandhi and Trivedi [12] have used multi-class SVM on HoG based features to distinguish different orientations and further used Hidden Markov Model (HMM) for integra- tion from multiple frames to estimate direction of motion. Enzweiler et al. [13] have presented an integrated approach for single frame pedestrian classi cation and orientation esti- mation to predict the direction of motion. Zhao et al. [14] have used a Haar like feature vector subjected to Adaboost classi er for orientation. Cascaded orientation estimation is applied for body and head orientation estimations. Further, most frequent orientation estimate and rounded average of estimated sum are used for direction estimation. Many other researchers have also attempted to segment different body parts and study their orientations individually. Recently, Flohr et al. [15] have proposed probabilistic pedes- trian orientation system where head and body orientations are studied separately. The proposed research is intended to overcome faulty detection and provide robust orien- tation and direction estimation. Bensebaa et al. in their research [16], [17] have segmented different body parts i.e. head and shoulders, knees, feet, and body. The authors have attempted to study their orientation separately over their silhouettes to estimate the heading direction of pedestrian. The articles so far have utilised the static cues and gen- eralised it over multi frames. Liu et al. [18] have utilised RGB-D sensor where RGB sensing helps in illumination change and D sensing for depth of the subject. The article pro- vides insight about how only static cues (intra frame) are not suf cient and needs to be complemented with dynamic cues (inter frame) for orientation and direction of motion estima- tion. The authors have proposed Dynamic Bayesian Network System (DBNS) to effectively employ the complementary nature of both static and motion cues, which motivates to incorporate dynamic cues for direction estimation. A lot of research for direction and orientation estimation of pedestrian has been performed over videos exploiting dynamic cues along with appearance patterns. Goel and Chen [19] have called their classi cation method as Global Locale Motion Pattern Classi cation (GLMPC) where they have attempted to detect pedestrian in a video. In order to classify a pedestrian from a non-pedestrian, they have proposed 3 subclasses of pedestrian depending on 3 dif- ferent walk directions. The algorithm classi es motion of pedestrian into 3 discrete directions. Andriluka et al. [20] have exploited body pose but not the motion cues in the tem- poral pattern and hence can only estimate the orientation and not the direction of motion. Chen et al. [21] have proposed head and body orientation based calculation of direction estimation in low resolution videos. Authors in this article have exploited intra-frame features for body cues and then the position exploiting temporal patterns in particle ltering framework. Both the researches have discretized the direction to 8 levels. Baltieri et al. [22] have proposed an orienta- tion classi er approach exploiting only appearance model. Liu and Ma [23] have proposed an on-line orientation classi- er approach on eld camera. The article attempts to suppress the effect of perspective distortion and proposes a Reliable Motion Direction (RMD) determination method that assumes a constant apparent velocity of pedestrian walk. These meth- ods are very limited in cases when pedestrians are moving slow, are stationary or suffering occlusion, however they work ne during constant motion of pedestrian. Table 1 summarises a few landmark researches for direc- tion estimation of moving subject. The study of existing survey for direction of motion of a subject reveals following facts: Researchers have used features like Haar, HoG or exploited silhouette over frames to estimate their orientation. This process is extended over multiple frames and then the trend over multiple frames are either modelled [12] or statistically concluded for direction estimation like selecting most frequent orientation or applying rounded average [14]. The opposite pair of directions like approaching and departing from camera is always confusing if calculated over individual frame. A few articles [14], [24], [25] have reported them. The low resolution videos fail to capture many features, and factors like noisy environment, imprecise acquisi- tion, or improper segmentation may further make the cause dif cult. Most of the the researchers have considered 8 equiangular discrete directions as optimal VOLUME 4, 2016 5789 R. Raman et al.: Direction Estimation for Pedestrian Monitoring System in Smart Cities TABLE 1. A few landmark research on direction of motion estimation. 5790 VOLUME 4, 2016 R. Raman et al.: Direction Estimation for Pedestrian Monitoring System in Smart Cities TABLE 1. (Continued.) A few landmark research on direction of motion estimation. VOLUME 4, 2016 5791 R. Raman et al.: Direction Estimation for Pedestrian Monitoring System in Smart Cities TABLE 1. (Continued.) A few landmark research on direction of motion estimation. choice [2], [12], [14], [15], [18]. However, few articles have considered 4 discrete directions [26], [27], 16 dis- crete directions [11], and 24 discrete directions [17] as well. We propose an HMM based method that operates over a video for pedestrian direction of motion and exploits the temporal terrains i.e. the pattern of change of width and pattern of change of height of bounding box of the identi ed pedestrian blob in the frame. It uses these features to train different HMMs to classify the movements among different direction classes. The robustness of dynamic cues selection is already presented in earlier work [28]. In this article, we have used 8 different classes for 8 equiangular discrete directions ranging [0 , 360 ] from view axis (refer to Fig. 1). The sinusoidal wave pattern with perspective affected dis- tortion uniquely identi es a subject as pedestrian from any other moving object like vehicle or animal in the scene due to unique human gait patterns. Moreover, the method also FIGURE 1. 8 equiangular discrete directions with respect to field camera. overcomes confusion between cases where subjects moving towards and away from camera. The proposed method is robust to various other issues like illumination changes, envi- ronmental factors, partial occlusion for few frames and low resolution of surveillance videos. The proposed method can either be used alone or with existing methods of orientation 5792 VOLUME 4, 2016 R. Raman et al.: Direction Estimation for Pedestrian Monitoring System in Smart Cities estimation over consecutive frames to enhance the direction estimation results. Problem formulation, theoretical proposal, feature selec- tion, and HMM based training, testing and inferences are presented in the Section II. About the experimental envi- ronment and analysis which include assumptions, con- straints, database details, evaluation parameters and steps involved in the experiment are discussed in the Section III. In Section III-E, results related to classi cation over different databases using various performance metrics and comparison of proposed method with some landmark research are pre- sented. Section IV includes conclusion and scope for future work. II. PROPOSED METHOD We propose to exploit the dynamic cues of a surveillance video to estimate the direction of motion of a pedestrian. The temporal changes in the dimensions of bounding box are found to be unique as subject moves in different direc- tions. We have chosen 8 equiangular discrete directions in the context of our proposed method as presented in Fig. 1. Changes in width and height with respect to a few selected directions are plotted in Figs. 2(a) and 2(b) that demonstrates unique patterns with respect to different directions of motion. These patterns can be exploited to estimate the same. Much elaboration on these graphs and inferences are presented in detail in Section III-D. FIGURE 2. Plots representing unique pattern of change in dimensions as the pedestrian moves in different directions. (a) Pattern of change of width. (b) Pattern of change of height. As the pattern of change of height and width along with the displacement of centroid appears to be enough to estimate the direction of motion, yet considering such features alone can handle a limited type of cases. A few cases of direc- tion estimation that cannot be addressed with such features are: Partial static occlusion, where the pattern of change of width or pattern of change of height are not visible for few frames Segmentation errors, that can add noise and can also deform the shape of the pedestrian s blob Pedestrian s carrying conditions, clothing conditions, and unusual postures like putting hands in pocket etc can deform one of the feature Such situation demands to exploit both width and height feature simultaneously. Moreover, the modelling technique used for the human motion patterns as captured in eld cam- era has to be properly justi ed and robust enough to handle the cases of partial occlusion. To model this unique temporal pattern of the dimensions of moving subject in eld camera, authors are motivated to use HMM for direction classi cation. The justi cation to use HMM, formulation of feature and preprocessing are discussed next. The involved steps are: Preprocessing Feature formulation for HMM Problem modelling through HMM Machine learning and classi cation A. PREPROCESSING Field camera records video footage that is needed to be pre-processed before feature extraction. The step of pre-processing can be further divided into following sub-steps: Background subtraction for pedestrian selection Connected component generation through morphologi- cal operations Frame recti cation and unwanted blob removal Bounding box tting over pedestrian blob 1) BACKGROUND SUBTRACTION FOR PEDESTRIAN SELECTION Through the method of background subtraction, blobs of non- stationary pixels are separated from stationary pixels. Moving blobs are categorised as foreground while rest as background. The existing databases (as presented in Table 2), which are already background separated do not require this step. However, we have used Visual Background Extrac- tor (ViBe) [29] for background subtraction for self-acquired dataset to ensure fast and accurate computation, citing the in- time processing being the prime requirement of the proposed method. ViBe is a robust method of background subtraction. This is also an adaptive method as it works over different envi- ronmental constraints. Hence, this method has been adopted for the proposed method. The set of background pixels also contains undesired blobs other than that of pedestrian and frames need morphological operation followed by a certain recti cations as discussed next. VOLUME 4, 2016 5793 R. Raman et al.: Direction Estimation for Pedestrian Monitoring System in Smart Cities TABLE 2. Different databases used, their parametric properties, and special cases present. 2) CONNECTED COMPONENT GENERATION THROUGH MORPHOLOGICAL OPERATION After background subtraction, frame may contain undesired blobs identi ed as foreground. This may happen due to improper segmentation, partial occlusion or due to presence of noise. Dilation is performed to connect the nearby blobs to overcome the unwanted separation of connected foreground as different blobs. Different body parts of a pedestrian might be identi ed as different blobs in the binary frames. Disk dilation has been performed iteratively over such frames on different blobs until the nearby blobs form a single connected component. In the article, we have used dilation operation with disk as structuring element with radius 7 running for 5 iterations. The objective of this morphological operation is to ll unwanted holes in the foreground and to strengthen the foreground pixel near the body joint regions. However, for severely cluttered images the authors have adopted the background subtraction methods by Yao and Odobez [37] and Reddy et al. [38]. The resulting connected foreground helps in selection of single largest blob. 3) FRAME RECTIFICATION AND UNWANTED BLOB REMOVAL After morphological operation largest connected component has been chosen as desired foreground while deleting rest of the foreground blobs. With xed background and moving foreground, optical ow [39], [40] based methods may also be applied to overcome improper segmentation. Figs. 3(a) and 3(b) show two frame sequences where the pedestrians are moving in discrete direction 6 and direction 3 respectively. Figs. 3(c) and 3(d) show corresponding frame sequences after noise removal. 4) BOUNDING BOX FITTING OVER PEDESTRIAN BLOB The temporal change in the dimensions of a moving blob can be de ned ef ciently by the change in the dimension of bounding box tted over the blob. With this motive a rectangular bounding box has been put over the blob and their temporal change in subsequent frames are recorded. Figs. 3(e) and 3(f) present same set of frame sequences after tting rectangular bounding box over the identi ed pedes- trian blob. B. FEATURE FORMULATION FOR HMM The temporal change in consecutive frames is the output after pre-processing. 8 equiangular discrete directions of motion, D1 through D8 that a moving subject may achieve with respect to eld camera is considered. The change in the dimension of a moving object as perceived in a camera view is perspective in nature and unveils distinct patterns. This fact has been exploited to model the patterns for each of the 8 dis- tinct directions of motion using HMM. The temporal pattern of change of width and height of a moving subject is unique for each of the discrete direction. Figs. 2(a) and 2(b) show the pattern of change of width and height over the frames in different directions respectively. These unique patterns are now needed to be trained using a machine learning algorithm. The temporal changes in the pattern are stateless and hence follow Markovian property; this gives a good reason to use HMM for training. The features identi ed to be unique for each direction are the temporal change pattern in height and width of the bounding box along with the displacement direction of its centroid. Thus, the aggregated feature is formed as a 1D array 5794 VOLUME 4, 2016 R. Raman et al.: Direction Estimation for Pedestrian Monitoring System in Smart Cities FIGURE 3. Steps of preprocessing. (a) Sample frame sequence # 1 with noise. (b) Sample frame sequence # 2 with noise. (c) Sample frame sequence # 1 after noise removal. (d) Sample frame sequence # 2 after noise removal. (e) Sample frame sequence # 1 after boundary fitting. (f) Sample frame sequence # 2 after boundary fitting. which is linear concatenation of temporal pattern of change in height, padding (for displacement direction of centroid) and temporal pattern of change in width. Temporal change patterns of the dimensions of the bounding box exploits perspective distortion and can clas- sify directions among (direction 1), (directions 2 and 8), (directions 3 and 7), (directions 4 and 6), and (direction 5). The displacement direction of centroid of bounding box can differentiate between pair of directions with similar per- spective distortions but opposite displacement direction of centroid i.e. (directions 2 and 8), (directions 3 and 7), and (directions 4 and 6) (refer to Fig. 4). Thus the aggregated feature can classify among 8 direction classes as proposed in this article (cases available in CASIA Dataset A and NIT Conscious Walk Dataset, discussed in Section III-B). In cases where all the available discrete directions of pedestrian have same displacement direction of centroid (as in cases of CASIA Dataset B, discussed in Section III-B), the displacement direction of centroid is not required, as among the available direction classes in this database, pair of directions with similar perspective FIGURE 4. Genesis of the aggregated feature vector. distortion but opposite displacement direction of centroid are not present and padding may be removed from the aggregated feature. Corresponding feature vector can be formulated in two possible ways as follows: Case I: 1h|t + pd + 1w|t Case II: 1h|t + 1w|t where, 1h|t : temporal change of height (h) of the bounding box tted on the silhouette of the moving pedestrian over time t extracted from consecutive frames in video 1w|t : temporal change of width (w) of the bounding box tted on the silhouette of the moving pedestrian over time t extracted from consecutive frames in video pd : sequence denoting padding to discriminate 1h|t and 1w|t The padding (pd) can be represented as: pd = ( 0n if xf0 > xfc 1n if xf0 xfc where, in the 2D Cartesian coordinate system, xf0 : abscissa of the centroid of the foreground in rst frame xfc : abscissa of the centroid of the foreground in current frame Hence pd should be an array lled with n zeros when the pedestrian moves in directions 6, 7, and 8. In other cases (for movements along directions 1, 2, 3, 4, and 5), pd should be lled with ones (refer to Fig. 6). Thus pd will not only act as a unique separator between height ter- rain and width terrain, but will also bear partial discrim- inating power with respect to movement direction. Fig. 5 illustrates two typical sample cases (for directions 3, and 7) graphically depicting the change in centroid and selection of pd accordingly. However it is not suf cient only to choose whether pd will be lled with 0 or 1. The number of zeros or ones (n) has VOLUME 4, 2016 5795 R. Raman et al.: Direction Estimation for Pedestrian Monitoring System in Smart Cities FIGURE 5. Illustration for finding pd in different cases. FIGURE 6. Selection of pd reflecting classification through displacement of centroid. to be chosen wisely. The value of n is chosen satisfying two contradictory objectives: i. The n-unit padding should be long enough so that it can act as a unique separator between height and width terrain ii. The n-unit padding should be minimal in length to reduce the total feature vector length (as a long feature vector will cost more computation time) With a trade-off between the two factors outlined above, the value of n has empirically been found to be 5. This possibly happens due to the presence of partial occlusion or segmentation error (where height or width is not visible) persisting through consecutive 4 frames. Through this under- standing, the size of padding can be modelled based on the nature of the database (and the ef ciency of background subtraction applied on it) on which the method is supposed to work. Thus it can be well justi ed why an aggregate feature vector with padding (Case I shown above) will bear bet- ter directional discrimination ability than a feature vector constructed without padding (Case II shown above). Hence we have chosen a padded aggregate feature for proposed modelling. 1) PROBLEM MODELLING THROUGH HIDDEN MARKOV MODEL The time varying changes in the dimension of bounding box follows dependencies from the previous frame. i.e., P (qt+1 |qt, yt ) = P (qt+1 |qt ) where, qt+1 and qt are the dimensions at time t and (t + 1) respectively and yt being direction of motion to reach current dimensions. The problem thus satis es the Markovian property and this is the reason Hidden Markov Model is used for training the model for estimation of direction of motion. To model a problem to HMM, their observation states, hidden states, transition and emission probabilities are needed to be de ned. Following are the de nitions with respect to the proposed method. Observed State or visible state (denoted by V (t)): are the visible features that is accessible from an event sequence to be modelled. A sequence of observed state forms observed state sequence denoted as V t 1 = {V1, , Vt} In the proposed method, observable feature sequence is de ned as change in dimensions of the bounding box sur- rounding the foreground blob. As stated before, the observed state sequence is formed as 1h|t + pd + 1w|t. While the size of pd is already discussed to be 5 units, 1h|t and 1w|t are constructed with m (= f t) units (where, f is the number of frames elapsed per second in the video). In our implementation, we consider change of terrains within a time gap of t = 1s and the video is considered to move with 30 FPS. Hence, 1h|t and 1w|t are both 1 30 = 30 units long in size. Thus, the aggregated feature size turns out to be (30 + 5 + 30) = 65 units. Since the discrete directions of motion are mirror image of each other along the view axis, the pattern of change of dimensions for pair of directions 2 and 8, 3 and 7, and 4 and 6 are identical in camera projection, however their direction of motion is exactly opposite. To differentiate this a padding of 0 or 1 denoting rightwards or leftwards movement is introduced. Many of the existing researches are producing erroneous results while handling opposite pair of directions reported in the survey of Section I. 5796 VOLUME 4, 2016 R. Raman et al.: Direction Estimation for Pedestrian Monitoring System in Smart Cities Hidden State (denoted by (t)): are the states not observed directly rather needs interpretation from observed sequence. The perceiver does not have access to the hidden state, instead algorithm measures some property of the observed state to infer hidden state. In the proposed method 4 hidden states are taken due to sinusoidal nature observed at the selected feature vector. As the articles suggested [41], [42], brute force selection of number of hidden states near the estimated proximal value of number of hidden states, the simulation of the HMM are also performed with 3 and 5 hidden states how- ever the training lasted for 100-120 iterations as compared to 50-60 iterations in case of 4 hidden states. This empirically justi es the selection of number of hidden states. FIGURE 7. State transition diagram of the HMM of proposed method. 2) TRANSITION AND EMISSION PROBABILITIES As depicted in Fig. 7, the transition and emission prob- abilities are de ned as: Transition among hidden states, i.e., aij : P( j(t + 1)| i(t)) Emission of a visible state, i.e., bjk : P(vk(t)| j(t)) with limiting conditions as, P j aij = 1 i and P k bjk = 1 j. Projected problem is a learning problem of HMM. The problem states that given a set of observed state sequence V T and any hidden state as given by (t), the task is to determine the probabilities aij and bjk using forward backward algorithm. We start with the above de ned initial arbitrary values of aij and bjk and nd more accurate values of aij and bjk at the end of Baum-Welch or forward-backward algorithm as illustrated below. The probability that the model produces a sequence V T of visible states is P  V T  = rmax X r=1 P  V T | T r  P( T r ) (1) where each r indexes a particular sequence T r = { (1), (2), . . . , (t)} of T hidden states. In the general case of c hidden states, there will be rmax = cT possible terms in the sum of Eq. (1), with respect to all possible sequence of length T. As we are dealing with rst-order Markov process, the factors in Eq. (1) can be written as Eq. (2) and Eq. (3). P( T r ) = TY t=1 P( (t)| (t 1)) (2) P(V T | T r ) = TY t=1 P(v(t)| (t)) (3) Combining the results of Eq. (2) and Eq. (3), previously described Eq. (1) can be rewritten as Eq. (4). P(V T ) = rmax X r=1 TY t=1 P(v(t)| (t))P( (t)| (t 1)) (4) We denote our model - the a s and b s - by and using Bayes formula, probability of the model given observed sequence is given by Eq. (5). P( | V T ) = P(V T | ) P( ) P(V T ) (5) Now, j(t) and i(t) can be de ned as shown in Eq. (6) and Eq. (7). j(t) = 0 t = 0 and j = initial state 1 t = 0 and j = initial state P i i(t 1)aij]bjkv(t) otherwise (6) i(t) = 0 i(t) = 0 and t = T 1 i(t) = 0 and t = T P j j(t + 1)aijbjkv(t + 1) otherwise (7) Where, j(t) represents the probability that the model is in hidden state j(t) having generated rst t elements of V T and i(t) represents the probability that the model is in hidden state i(t) and will generate rest of the target sequence from (t + 1) to T. The notation bjkv(t) (as mentioned in Eq. (6)), and bjkv(t+1) (as mentioned in Eq. (7)) denotes the transition probabilities bjk selected by visible state emitted at time t and (t + 1) respectively. However, this way of determining j(t) and i(t) are mere estimates of their true values, as we do not know the actual values of aij and bjk in Eq. (6) and Eq. (7). We can calculate improved values of j(t) and i(t) by de ning ij(t) (shown in Eq. (8)) which is the probability of transition between i(t 1) and j(t), given the model generated the entire training sequence V T by any path. ij(t) = i(t 1)aijbjk j(t) P(V T | ) (8) VOLUME 4, 2016 5797 R. Raman et al.: Direction Estimation for Pedestrian Monitoring System in Smart Cities FIGURE 8. Block diagram of the proposed method. Hence we nd an improved estimation of aij and bjk as aij and bjk through Eq. (9) and Eq. (10) respectively. aij = TP t=1 ij(t) TP t=1 P k ik(t) (9) bjk = TP t=1,v(t)=vk P l jl(t) TP t=1 P l jl(t) (10) 3) PROBABILITY OF THE MODEL If we denote the a s and b s of our model by and use Bayes formula, probability of the model given observed sequence is P( | Ot 1) = P(Ot 1 | ) P( ) P(Ot 1) (11) where, a s and b s of HMM of the proposed model is denoted by . In this way 8 HMMs for each discrete direction are modeled. The successful supervised training of models are depicted by log likelihood graphs as shown further in Figs. 10, 11 and 12. Further, test sequences are classi- ed with highest probability and above a minimal threshold. The highest probability classi es the direction while mini- mal threshold segregates human from non-human based on motion patterns. Formal de nition and explanation of HMM in the context of proposed model is discussed among the evaluation parameters in Section III.1 1For general understanding of HMM, and its implementation in time sequential image data, readers may refer to Rabiner and Juang [43] and Yamato et al. [44]. C. MACHINE LEARNING AND CLASSIFICATION The proposed method of classi cation through HMM com- prises two phase viz. learning phase and direction estimation phase. In the learning phase training video samples undergo various steps to obtain feature vectors for each video that uniquely de nes respective classes. Hence different HMMs are formed each representing a unique direction. Log like- lihood graphs are used to represent their proper learning. Section III-C de nes log likelihood graph as an evaluation parameter and log likelihood graphs related to performed experiments are presented in Section III-E. Further, in the direction estimation phase, test videos are taken, their fea- ture vectors are extracted and are classi ed among different classes as de ned in the learning phase, thus resulting into an estimated direction for each subjects testing samples. From both the subsets (training as well as testing video) of database; steps of extracting silhouette of pedestrian, temporal change in dimension of their bounding box and their feature extract are performed. The set of training feature vectors for each of the direction classes are kept in the database. Newly arriv- ing test videos undergoes similar procedure for its feature extraction and are subjected to be classi ed among any of the direction classes. Fig. 8 presents a block diagram depicting overall description of the proposed method. III. EXPERIMENTAL RESULTS AND ANALYSIS The proposed method for direction estimation is experimen- tally justi ed. This section elaborates the experiment and related experimental environment prepared for conducting those experiments. This includes: Constraints and assumptions in the experiment Details about various databases used Evaluation parameters Experiments conducted for direction estimation Results Analysis 5798 VOLUME 4, 2016 R. Raman et al.: Direction Estimation for Pedestrian Monitoring System in Smart Cities A. CONSTRAINTS AND ASSUMPTIONS IN THE EXPERIMENT The experiments are conducted in a constrained environ- ment with certain assumptions. These constraints depicts the robustness as well as environmental limitations of the pro- posed method. They are presented as follows. The proposed method is constraint to work over binary image sequences with human subject as foreground silhouettes Preprocessing to achieve the same is not elaborated and assumed to be minimal Sudden and frequent changes in the direction of walk are not assumed in the experiment The proposed method is constraint to work with static eld camera The method is constraint to work in visible spectrum The proposed method is not robust to full and longer exposure to occlusion, such cases are not considered in the experiment The method is proposed to handle direction estimation of multiple human subjects present in a scene assuming no mutual occlusion however experiments demonstrated here have cases with only single pedestrian in the scene B. VARIOUS DATABASES USED With the limitations of existing databases with respect to our proposed experimental requirements, three differ- ent databases are used. They are CASIA dataset A and dataset B [45] and NITR conscious walk dataset [46]. These datasets are used with an intention to bring many scenarios in the purview of our proposed method. Subjects involved as pedestrian in the experiments have diversi ed cases as: Participating pedestrians are both female and male, and have diverse physical build, age group and ethnicity The velocity of walk within a video footage is not nec- essarily constant The velocity of walk in different video footage may be different Capturing environment and walking surfaces are different Pedestrians are not strictly following a direction during their walk Pedestrians have diverse carrying and clothing condi- tions that affects the pattern of temporal changes in the dimensions of silhouette Depth of the pedestrian with respect to eld camera is varying from video to video In few cases pedestrians are under partial static occlusion There are some cases with different orientation of head and body of a pedestrian to that of actual direction of their motion To further clarify the need of different databases used in the experiment, their parametric properties and special cases available in the database are presented in Table 2. Selected frames from different databases to support the diversity of special cases are presented in Fig. 9. FIGURE 9. Frame sequences depicting the diverse cases with pedestrian carrying bag (a, b, c, d), having different head orientation during a walk (e, f, g, h), walk with hands in the pockets (i, j), wearing coat (k, l), improper segmentations (m, n, o, p), and partial static occlusion with incomplete width or height information (q, r, s, t). Pedestrian frames are from CASIA dataset A, CASIA dataset B [45] and NITR conscious walk dataset [46] and have diverse physical build, walking velocity, age group, ethnicity and gender. Respective frame locations in the database available in .png format are: (a) CASIA/Gait Dataset B/012-bg-02-090-073 (b) CASIA/Gait Dataset B/007-bg-02-090-070 (c) CASIA/Gait Dataset B/ 124-bg-01-090-043 (d) CASIA/Gait Dataset B/001-bg-01-090-057 (e) CASIA/Gait Dataset A/fyc-45_1-092 (f) CASIA/Gait Dataset A/ fyc-45_1-085 (g) CASIA/Gait Dataset A/fyc-45_1-014 (h) CASIA/Gait Dataset A/fyc-45_1-011 (i) CASIA/Gait Dataset B/032-cl-02-000-069 (j) CASIA/Gait Dataset B/032-cl-02-000-059 (k) CASIA/Gait Dataset B/ 001-cl-02-090-059 (l) CASIA/Gait Dataset B/020-cl-02-090-060 (m) CASIA/Gait Dataset A/syj-90_3-089 (n) NITR conscious walk db/ 1001D2S1F065 (o) CASIA/Gait Dataset A/fyc-90_1-002 (p) CASIA/Gait Dataset A/fyc-90_1-006 (q) CASIA/Gait Dataset B/002-bg-01-018-042 (r) CASIA/Gait Dataset B/002-bg-01-018-034 (s) CASIA/Gait Dataset B/ 002-bg-01-018-059 (t) CASIA/Gait Dataset B/002-bg-01-018-050. C. EVALUATION PARAMETERS Log Likelihood Graph: Log Likelihood Graph in a supervised learning depicts the Log likelihood (on Y axis) of a learning model with the increase of expectation maximization iterations (on X axis). In the context of the proposed 4-state Hidden Markov Model based training, corresponding plots in different log like- lihood graph shows monotonic increase, followed by convergence. This depicts the successful training of all the HMMs ending up with maximum likelihood within VOLUME 4, 2016 5799 R. Raman et al.: Direction Estimation for Pedestrian Monitoring System in Smart Cities limited iterations and hence the suitability of the param- eter set to form aggregated features for training. Balanced Accuracy: Accuracy can be de ned as pro- portion of total number of correct predictions. The for- mula for accuracy is given by Accuracy = TP + TN TP + FP + TN + FN (12) Where, TP: True Positive FP: False Positive TN: True Negative FN: False Negative However, in multi-class classi cation, balanced accu- racy is used and is given by the arithmetic mean of class speci c accuracies. Balanced Accuracy = P all classes Accuracy Number of classes (13) Recall: Recall (also known as TP Rate, True Positive Rate, or Sensitivity) can be de ned as the proportion of positive cases that were correctly identi ed and is given by Recall = TP TP + FN (14) In multi-class classi cation, as in the proposed case the corresponding information is given by average recall gives, which is arithmetic mean of all positive cases that were correctly identi ed. Average Recall = P all classes Recall Number of classes (15) Precision: Precision can be de ned as the predicted positive cases that were correct and is given by Precision = TP TP + FP (16) In the proposed multi-class classi cation, precision of classi cation can be de ned with a more suitable param- eter called average precision which is given by: Average Precision = P all classes Precision Number of classes (17) F-Measure: F-Measure is the harmonic mean of preci- sion and recall, and it is a measure to judge the accuracy of a classi er. In the harmonic mean, when equal weight is given to recall and precision, it is more precisely called as F1-Measure. F1 Measure = 2 Precision recall Precision + Recall (18) In the proposed multi-class classi cation, F1-Measure of classi cation can be de ned with a more suitable parameter called average F1-Measure which is given by: Average F1 Measure = P all classes F Measure Number of classes (19) False Positive Rate: FP Rate can be de ned as the pro- portion of positive cases that were incorrectly identi ed and is given by FP Rate = FP FP + TN (20) In multi-class classi cation, as in the proposed sce- nario, a more suitable parameter called average FP Rate yields the corresponding information.which represents the average of all false rejections and is given by: Average FP Rate = P all classes FP Rate Number of classes (21) Error Rate: Error rate is given by (1 Accuracy). In the proposed multi-class classi cation, error rate is presented as percentage of incorrectly classi ed instances and is given by the average of misclassi cation for each individual classes. Average Error Rate = P all classes Error Rate Number of classes (22) Confusion Matrix: In supervised learning, a confusion matrix or an error matrix is a tool of statistical clas- si cation that lets the visualization of mislabelling of classi cation data in the form of false positive and false negative, while the correct labelling are present in the forms of true positive and true negative. V-Fold Cross Validation: In such validation method, database is randomly divided into V equal sized samples and each time any one of the sample is utilised as testing sample while all other samples are utilised as training sample. This technique of model veri cation also proves a dataset to be unbiased. D. EXPERIMENTS CONDUCTED FOR DIRECTION ESTIMATION The proposed method is validated with conducted experi- ments. As already shown in Fig 2, the temporal pattern of width and height of the bounding box of randomly selected video having directions of walk as 4, 5, 6 and 7 (refer to Fig. 1) are plotted. Study of the graph reveals following observa- tions: The temporal changes in the dimensions of bounding box are unique with respect to each direction Field camera follow perspective geometry, hence the temporal patterns shows perspective affected scaling distortion Height of the pedestrian remains constant as it moves orthogonal to the view axis (i.e. in direction 3 or 7). 5800 VOLUME 4, 2016 R. Raman et al.: Direction Estimation for Pedestrian Monitoring System in Smart Cities FIGURE 10. Log Likelihood graph presenting machine learning of 6 discrete directions using HMM over CASIA Dataset A. This is due to the fact that depth of pedestrian remains constant with respect to eld camera Temporal change in width of the pedestrian in the same pair of directions shows sinusoidal nature; this is due to cyclic motion of limbs during a gait cycle The temporal increment in the dimension of pedestrian s bounding box is maximal when it directly approaches towards the camera (i.e. direction 5) and minimal in the opposite case (i.e. direction 1) The temporal changes along the direction 4 and 6 were expected to show moderate increment in dimensions The temporal pattern of change in the dimensions of a pedes- trian moving along the direction 8, 1, 2, and 3 are mirror images of directions 5, 4, 6, and 7 respectively along x axis. They have not been included in order to avoid clumsiness in the graph. All the results discussed so far are consistent with the anticipated pattern. With this motivation, further experiments are conducted over different databases (refer to Table 2) that are discussed in subsequent paragraphs. Experiment # 1 is conducted over CASIA Dataset A. This database has overall 240 binary frame sequences where pedestrians are moving along 0 , 45 , and 90 with respect to view axis. This covers 6 of the 8 discrete directions that are discussed in the proposed method. The missing two directions (i.e. direction 4 and 8), are mirror images of directions 6 and 2 respectively and shows similar temporal pattern due to similar perspective distortion. The direction of pedestrian motion in this experiment are classi ed among 6 classes. Each of the directions have 40 frame sequences in the database. Further, 10 fold division of the database is created in such a way that each fold contains equal number of randomly chosen samples from all the walk directions. All the experiments are conducted with 9 folds for training and 1 fold for testing the result. These experiments are conducted 10 times with each fold tested exactly once. The random selection of samples uses entire database for training as well as testing, supports the unbiased nature of database. Fig. 10 presents a sample case where different conver- gent plots of log likelihood graph presents HMM based training of different classes over CASIA Dataset A. Related experimental environment is summarised in Table 4. Results related to this experiment are discussed in the Section III-E. CASIA Dataset A however does not contain enough discrete directions. TABLE 3. Merging 11 different walk directions of CASIA Dataset B into five discrete directions. Experiment # 2 is conducted over CASIA Dataset B. This dataset has overall 13640 binary frame sequences with 11 different walk directions. The dataset contains various carrying and clothing variations since the pedestrians are carrying bag, wearing coat or having a regular out t without carrying anything.Walk directions are ranging 0 to 180 from view axis of camera instead of varying 0 to 360 hence these directions are needed to be classi ed among 5 of the 8 discrete direction classes. Table 3 shows merging of the directions. Direction 3 which clubs 3 different walking cases has 3720 walk samples while rest of the directions have 2420 walk samples. However, for uniform training and testing, 2420 randomly selected samples are considered for direction 3 making an overall 12100 walk samples available for experiment. These samples further undergoes 10 fold cross validation where each fold are tested while other 9 folds are used for training the model. Fig. 11 presents a sample case where different convergent plots represents HMM based training over CASIA Dataset B. Related experimental setup VOLUME 4, 2016 5801 R. Raman et al.: Direction Estimation for Pedestrian Monitoring System in Smart Cities TABLE 4. Experimental environment for Experiment# 1. FIGURE 11. Log Likelihood graph presenting machine learning of 5 discrete directions using HMM over CASIA Dataset B. TABLE 5. Experimental environment for Experiment# 2. TABLE 6. Experimental environment for Experiment# 3. is summarised in Table 5 and related results are discussed in Section III-E. So far, the classi cation over existing database samples are limited to 5 and 6 different directions. To classify among all the proposed 8 discrete directions, a new outdoor database is acquired. Experiment # 3 is conducted with NITR Conscious Walk Database. This database consists of 21 subjects contributing 3 sample walks in each of the 8 directions spanning from [0 -360 ]. The pedestrians are consciously made to walk in the presumed discrete directions with a few little deviations, so as to have better machine training. 10 fold cross validation is again performed over this dataset. Fig. 12 presents a sample case where different convergent plots represents HMM based training over NITR Conscious Walk Dataset. Experimental environment of the same is presented in Table 6. Related results are discussed in the Section III-E. All the experiments are performed using MATLAB Simulink software over workstation having Intel Xeon pro- cessor with duel processing core where each core has a clock speed of 2.4 GHz. The system works on a 64-bit operating system and it has 8 GB of volatile memory. The detailed classi cation results over different databases are presented in Section III-E. 5802 VOLUME 4, 2016 R. Raman et al.: Direction Estimation for Pedestrian Monitoring System in Smart Cities FIGURE 12. Log Likelihood graph presenting machine learning of 8 discrete directions using HMM over NITR Conscious Walk Dataset. TABLE 7. Confusion matrix for casia dataset A. TABLE 8. Confusion matrix for Casia dataset B TABLE 9. Confusion matrix for NITR conscious walk dataset. E. RESULTS This section presents detailed evaluation of the proposed method and its comparison with state-of-the-art and few recent research in the domain. This results are further elab- orated in following sub-sections: 1) QUANTITATIVE EVALUATION AND COMPARISON The quantitative experimental results of the proposed method over different databases and its quantitative comparison with some existing work are presented in this subsec- tion. The frames are recti ed for feature extraction in the pre-processing stage, as already presented earlier in Fig.3. Tables 4, 5, and 6 summarises experimental environment. Results related to experiment # 1, experiment #2 and enper- iment #3 are presented in the form of confusion matrix in Tables 7, 8, and 9, respectively. The direction estimation accu- racy for CASIA Dataset A using a 10 fold cross validation with 9:1 training to testing ratio for 6 directions are found in the range of 90% to 100%, with an average balanced accuracy of 94.58%. The direction estimation accuracy for the experiment con- ducted over CASIA Dataset B with 10-fold cross validation VOLUME 4, 2016 5803 R. Raman et al.: Direction Estimation for Pedestrian Monitoring System in Smart Cities TABLE 10. Summary of evaluation results for direction estimation. TABLE 11. Quantitative comparison of direction estimation results over different database. with 5 discrete directions is found to be in the range of 89% to 94%, with an average balanced accuracy of 90.87%. This drop in the balanced accuracy is due to the merging of 11 intermediate directions of walk into 5 discrete directions. However, an overall balanced accuracy over such a large dataset with several walk directions justi es the robustness of the proposed method. These dataset does not possess all the 8 discrete directions to be classi ed among. Hence experiment 3 is conducted covering all the 8 proposed discrete directions. The direction estimation accuracy for the experiment conducted over NITR Conscious Walk Dataset with 10 fold cross validation with 8 directions are found to be in the range of 92 to 100% with an average balanced accuracy of 95.83%. In multi-class classi cation over different directions, results with different parameters of evaluation i.e. Balanced Accuracy, Precision, Recall, F1 Measure, Error Rate and False Positive Rate are presented in Table 10. The proposed method is further compared with a few parallel researches with common experimental platform. The methods compared in this section, works on video dataset captured from eld camera. All the methods are trained and tested over 3 different datasets (as discussed in Table 2) and uniformly underwent 10 fold cross validation for fair comparison. Their comparison results with two evaluation parameters: Balanced Accuracy and False Positive Rate is presented in Table 11. The quantitative comparison of the proposed method with some existing research in the domain shows that the proposed method out performs these existing methods with better Balanced Accuracy and False Positive Rate. 2) COMPARISON OF INTRINSIC PROPERTIES This subsection compares intrinsic properties of the proposed method from some of the existing method in the domain that witnesses a vast diversity towards approaching estima- tion of pedestrian direction. These diversities are mainly due to different research requirements. Some of them are view angles of the recording camera (top camera, eld camera), camera position (stationary, moving camera), extra sensor based requirement (infra-red sensor, depth sensor, monocular, binocular or multi-vision based camera), different data input requirement (frame based and video based) and different objectives (traf c, surveillance, home assistance). Due to this, a direct comparison from many other research is not feasible. However, we have attempted to compare their intrinsic prop- erties in this subsection. The proposed method can handle the situation of static partial occlusion since temporal patterns of both width and height contribute to the aggregate feature and at least one of the patterns remain available during partial occlusion. Fig. 9 shows a few example cases available in the dataset where only height or width information of the pedestrian blob is available in the frame and such cases are handled. Proposed method uses motion feature over the bounding box of the pedestrian blob and hence even if the head or body is oriented in other direction as that of actual direction of motion, the overall direction estimation is unaffected. The head and body orientation based direction estimation meth- ods [14], [15], [17], have reported to get affected in such scenario. Motion feature can capture the human gait motion and are used for human detection [19], [47] [49]. The set of feature selected in the proposed method for pedestrian direction esti- mation segregates human from a non-human under motion. This is due to the properties of the feature, like unique human gait pattern and height to width ratio of human, that it is segregated from vehicles, animals and other moving objects in the scene. Being less than a matching threshold, a non- human blob is not classi ed in any of the 8 direction classes; 5804 VOLUME 4, 2016 R. Raman et al.: Direction Estimation for Pedestrian Monitoring System in Smart Cities TABLE 12. Comparison of intrnsic properties of proposed method with some existing research on direction estimation. hence the direction estimation is performed only over those moving blobs which are identi ed as a human, based on their motion feature. Goel and Chen [19] have also attempted to perform human detection based on their motion feature using SVM and GLMPC over synthesized CASIA Dataset A claiming an accuracy of 97.4%, however, they have classi ed the directions only among 3 direction classes i.e. front, left and right (direction 3, 5, and 7). On the similar experimental conditions the proposed method is found to achieve estima- tion accuracy of 97.75% with all the 8 discrete direction classes. In the proposed method, exploiting motion feature gives a robust direction estimation result yet a sudden change in the direction of motion takes few frames to update and the overall result updates in such conditions are slower. This slowness in the update is comparable to other existing methods [14], [15], [26], [27], which generates direction estimation results by statistical combination of different orientation results over a few frames. However, such methods gives an orientation estimates in each frame and regards it as a probable direction of motion, which our proposed method does not. Due to large deviation in the environmental constraints, experimental set-up, and database types, a quantitative comparison of the proposed method with some landmark researches over a common platform is not feasible; however, an attempt has been made to compare the intrinsic features of the proposed method with few existing landmark researches across the platform. Table 12 presents the same. The table gives a fair idea about the robustness of the proposed model over existing researches and about its intrinsic features. F. ANALYSIS This subsection evaluates the claims of the proposed method with different results and comparisons. The quantitative results compared over common platform justi es the robust- ness of the proposed method over other researches. The result convincingly shows the proposed method to be working VOLUME 4, 2016 5805 R. Raman et al.: Direction Estimation for Pedestrian Monitoring System in Smart Cities FIGURE 13. Qualitative results with successful direction estimates. (a) Sample frame sequence with successful direction estimation by proposed method as direction 6. (b) Sample frame sequence with successful direction estimation by proposed method as direction 7. well even when the cases of static partial occlusion, varying head orientation, diverse clothing and carrying condition, and improper foreground segmentation are present. The proposed method out performs some of the existing methods in the domain with better Balanced Accuracy and False Positive Rate. Fig. 13 presents two frame sequences from CASIA dataset A for presentation of qualitative result of direction estimation by the proposed method. In Fig. 13, Dgt represents the ground truth direction and Dest represents estimated direction by the proposed method. The results on two different pedestrian videos (with frame gap of 30 and 15 frames) are shown respectively in Figs. 13(a) and 13(b), depicting the correct direction estimation at each frame in both the sequences. The result also shows the correct estimation of direction with different apparent velocity as both the frame sequences have different apparent velocity due to different depth from camera in two frame sequences. With different experimental results and comparisons, the accuracy of direction estimation and its intrinsic features are presented. The proposed method is robust since it uses motion features, on the other hand methods that performs direction estimation using orientation information gives an additional information about the orien- tation of pedestrian in each frame, that the proposed method does not. The proposed method can be used as a standalone system or can be integrated with orientation estimation based methods to produce faster yet accurate direction estimation results along with orientation information. IV. CONCLUSION Proposed method is a motion feature based direction estima- tion method. Due to motion feature, the proposed method is robust to partial occlusion, tolerable segmentation errors, and different head and body orientation. The method can estimate the direction of human motion through his gait patterns. The proposed method nds its usage in the domain of traf c safety and management, visual surveillance in smart cities, assisted living in smart homes and human computer interaction. Its potential usage in different aspects towards development of smart cities motivates to take the research further for more complicated and challenging environments like shopping malls, subways, and railway station to explore emerging issues like crowd behaviour analysis for business intelligence, monitoring, and surveillance. ABBREVIATIONS 3DPeS Dataset 3D People Surveillance Dataset CASIA Dataset Institute of Automation Chinese Academy of Sciences Dataset DBNS Dynamic Bayesian Network System FIND Feature Interaction Detector GLMPC Global Locale Motion Pattern Classi cation HMM Hidden Markov Model HoG Histogram of Gradient INRIA Dataset Institut National de Recherche en Informatique et en Automatique Dataset NITR Dataset National Institute of Technology Rourkela Dataset NN Neural Network PETS Dataset Performance Evaluation of Tracking and Surveillance Dataset PKU Dataset Peking University Dataset 5806 VOLUME 4, 2016 R. Raman et al.: Direction Estimation for Pedestrian Monitoring System in Smart Cities RGB-D Sensors Red Green Blue and Depth Sensor RMD Reliable Motion Direction SIFT Scale Invariant Feature Transform SVM Support Vector Machine TUD Dataset Technische Universitat Darmstadt Dataset ViBe Visual Background Extractor. REFERENCES [1] R. Raman, P. K. Sa, S. Bakshi, and B. Majhi, Towards optimized place- ment of cameras for gait pattern recognition, Procedia Technol., vol. 6, pp. 1019 1025, 2012. [2] R. Raman, P. K. Sa, and B. Majhi, Occlusion prediction algorithms for multi-camera network, in Proc. IEEE/ACM Int. Conf. Distrib. Smart Cameras, Oct./Nov. 2012, pp. 1 6. [3] A. Yilmaz, O. Javed, and M. Shah, Object tracking: A survey, ACM Comput. Surv., vol. 38, no. 4, pp. 1 45, 2006. [4] B. Y. Lee, L. H. Liew, W. S. Cheah, and Y. C. Wang, Occlusion handling in videos object tracking: A survey, in Proc. Int. Symp. Digit. Earth, vol. 18. 2014, pp. 12 20. [5] D. Makris and T. Ellis, Spatial and probabilistic modelling of pedestrian behaviour, in Proc. Brit. Mach. Vis. Conf., vol. 2. 2002, pp. 557 566. [6] C. F. Wakim, S. Capperon, and J. Oksman, A Markovian model of pedes- trian behavior, in Proc. IEEE Int. Conf. Syst., Man, Cybern., Oct. 2004, pp. 4028 4033. [7] G. Antonini, M. Bierlaire, and M. Weber, Discrete choice models of pedestrian walking behavior, Transp. Res. B, Methodol., vol. 40, no. 8, pp. 667 687, 2006. [8] Y. Abramson and B. Steux, Hardware-friendly pedestrian detection and impact prediction, in Proc. IEEE Intell. Vehicle Symp., Jun. 2004, pp. 590 595. [9] F. Large, D. Vasquez, T. Fraichard, and C. Laugier, Avoiding cars and pedestrians using velocity obstacles and motion prediction, in Proc. IEEE Intell. Veh. Symp., Jun. 2004, pp. 375 379. [10] T. Tsuji, H. Hattori, M. Watanabe, and N. Nagaoka, Development of night-vision system, IEEE Trans. Intell. Transp. Syst., vol. 3, no. 3, pp. 203 209, Sep. 2002. [11] H. Shimizu and T. Poggio, Direction Estimation of Pedestrian From Images, document AI Memo 2003-020, Massachusetts Institute of Technology, 2003, pp. 1 11. [12] T. Gandhi and M. M. Trivedi, Image based estimation of pedestrian orientation for improving path prediction, in Proc. IEEE Intell. Vehicles Symp., Jun. 2008, pp. 506 511. [13] M. Enzweiler, A. Eigenstetter, B. Schiele, and D. M. Gavrila, Multi- cue pedestrian classi cation with partial occlusion handling, in Proc. Int. Conf. Comput. Vis. Pattern Recognit. (CVPR), 2010, pp. 990 997. [14] G. Zhao, M. Takafumi, K. Shoji, and M. Kenji, Video based estima- tion of pedestrian walking direction for pedestrian protection system, J. Electron., vol. 29, nos. 1 2, pp. 72 81, 2012. [15] F. Flohr, M. Dumitru-Guzu, J. F. P. Kooij, and D. M. Gavrila, Joint probabilistic pedestrian head and body orientation estimation, in Proc. IEEE Intell. Vehicles Symp., Jun. 2014, pp. 617 622. [16] A. Bensebaa, S. Larabi, and N. M. Robertson, Head direction estimation from silhouette, in Proc. 17th Int. Conf. Image Anal. Process. (ICIAP), 2013, pp. 340 350. [17] A. Bensebaa, S. Larabi, and N. M. Robertson, Inferring heading direc- tion from silhouettes, in Developments in Medical Image Processing and Computational Vision, vol. 19. The Netherlands: Springer, 2015, pp. 319 334. [18] W. Liu, Y. Zhang, S. Tang, J. Tang, R. Hong, and J. Li, Accurate esti- mation of human body orientation from RGB-D sensors, IEEE Trans. Cybern., vol. 43, no. 5, pp. 1442 1452, Oct. 2013. [19] D. Goel and T. Chen, Pedestrian detection using global-local motion patterns, in Proc. Asian Conf. Comput. Vis. (ACCV), 2007, pp. 220 229. [20] M. Andriluka, S. Roth, and B. Schiele, Monocular 3D pose estimation and tracking by detection, in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2010, pp. 623 630. [21] C. Chen, A. Heili, and J. Odobez, Combined estimation of location and body pose in surveillance video, in Proc. IEEE Int. Conf. Adv. Video Signal Based Surveill. (AVSS), Aug./Sep. 2011, pp. 860 867. [22] D. Baltieri, R. Vezzani, and R. Cucchiara, People orientation recognition by mixtures of wrapped distributions on random trees, in Proc. Eur. Conf. Comput. Vis. (ECCV), 2012, pp. 270 283. [23] H. Liu and L. Ma, Online person orientation estimation based on classi er update, in Proc. IEEE Int. Conf. Image Process. (ICIP), Sep. 2015, pp. 1568 1572. [24] T. Gandhi and M. M. Trivedi, Pedestrian collision avoidance systems: A survey of computer vision based recent studies, in Proc. IEEE Intell. Transp. Syst. Conf., Sep. 2006, pp. 17 20. [25] S. Pierard and M. Van Droogenbroeck, Estimation of human ori- entation based on silhouettes and machine learning principles, in Proc. Int. Conf. Pattern Recognit. Appl. Methods (ICPRAM), 2012, pp. 51 60. [26] K. Goto, K. Kidono, Y. Kimura, and T. Naito, Pedestrian detection and direction estimation by cascade detector with multi-classi ers utilizing feature interaction descriptor, in Proc. IEEE Intell. Vehicle Symp. (IV), Jun. 2011, pp. 224 229. [27] J. Tao and R. Klette, Integrated pedestrian and direction classi ca- tion using a random decision forest, in Proc. IEEE Int. Conf. Comput. Vis. (ICCV) Workshops, Dec. 2013, pp. 230 237. [28] R. Raman, P. K. Sa, and B. Majhi, Direction prediction for avoiding occlusion in visual surveillance, Innov. Syst. Softw. Eng., vol. 12, no. 3, pp. 1 14, 2016. [29] O. Barnich and M. Van Droogenbroeck, ViBe: A universal background subtraction algorithm for video sequences, IEEE Trans. Image Process., vol. 20, no. 6, pp. 1709 1724, Jun. 2011. [30] H. Nisar and T.-S. Choi, Fast motion estimation algorithm based on spatio-temporal correlation and direction of motion vectors, Electron. Lett., vol. 42, no. 24, pp. 1384 1385, Nov. 2007. [31] INRIA Person Dataset. (Jan 05, 2016). [Online]. Available: http://pascal.inrialpes.fr/data/human/ [32] TUD Multiview Pedestrian Dataset. (Jan 15, 2016). [Online]. Available: https://www.d2.mpi-inf.mpg.de/node/428 [33] Daimler Mono Pedestrain Detection Benchmark Dataset. [Online]. (Feb 13, 2016). Available: http://www.gavrila.net/Datasets/Daimler_ Pedestrian_Benchmark_D/Daimler_Mono_Ped__Detection_Be/daimler_ mono_ped__detection_be.html [34] PETS 2009 Dataset. [Online]. (Jan 19, 2016). Available: http://www.cvg.reading.ac.uk/PETS2009/a.html [35] PKU Person Orientation Dataset. [Online]. (Feb 04, 2016). Available: https://github.com/mlq513773348/PKU-Person-Orientation.git [36] 3DPeS Dataset. [Online]. (Feb 19, 2016). Available: https://www.openvisor.org/3dpes.asp [37] J. Yao and J.-M. Odobez, Multi-layer background subtraction based on color and texture, in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2007, pp. 1 8. [38] V. Reddy, C. Sanderson, and B. C. Lovell, Improved foreground detection via block-based classi er cascade with probabilistic decision integration, IEEE Trans. Circuits Syst. Video Technol., vol. 23, no. 1, pp. 83 93, Jan. 2013. [39] B. K. P. Horn and B. G. Schunck, Determining optical ow, Artif. Intell., vol. 17, nos. 1 3, pp. 185 203, Aug. 1981. [40] B. D. Lucas and T. Kanade, An iterative image registration technique with an application to stereo vision, in Proc. DARPA Image Understand. Workshop, 1981, pp. 121 130. [41] S. M. Siddiqi, G. J. Gordon, and A. W. Moore, Fast state discovery for HMM model selection and learning, in Proc. Int. Conf. Artif. Intell. Statist., 2007, pp. 492 499. [42] G. Celeux and J.-B. Durand, Selecting hidden Markov model state number with cross-validated likelihood, Comput. Statist., vol. 23, no. 4, pp. 541 564, 2008. [43] L. Rabiner and B. Juang, An introduction to hidden Markov models, IEEE ASSP Mag., vol. 3, no. 1, pp. 4 16, Jan. 1986. [44] J. Yamato, J. Ohya, and K. Ishii, Recognizing human action in time- sequential images using hidden Markov model, in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 1992, pp. 379 385. [45] CASIA Dataset. [Online]. (Dec 05, 2015). Available: http://www.csbr.ia.ac.cn/english/Gait%20databases.asp [46] NITR Conscious Walk Dataset. [Online]. (Dec 22, 2015). Available: http://www.nitrkl.ac.in/Academic/Academic_Centers/Data_Computer _Vision.aspx [47] A. Mahapatra, T. K. Mishra, P. K. Sa, and B. Majhi, Human recognition system for outdoor videos using hidden Markov model, AEU-Int. J. Electron. Commun., vol. 68, no. 3, pp. 227 236, 2014. VOLUME 4, 2016 5807 R. Raman et al.: Direction Estimation for Pedestrian Monitoring System in Smart Cities [48] S. Zhang, D. A. Klein, C. Bauckhage, and A. B. Cremers, Fast moving pedestrian detection based on motion segmentation and new motion fea- tures, Multimedia Tools Appl., vol. 75, no. 11, pp. 6263 6282, 2016. [49] P. Viola, M. J. Jones, and D. Snow, Detecting pedestrians using patterns of motion and appearance, in Proc. IEEE Int. Conf. Comput. Vis. (ICCV), Oct. 2003, pp. 734 741. RAHUL RAMAN received the Master in Technol- ogy degree from the National Institute of Tech- nology Rourkela Rourkela, India in 2013. He is currently a Doctoral Research Scholar with the Department of Computer Science and Engineer- ing, National Institute of Technology Rourkela, India. His area of interest includes image process- ing, computer vision, visual surveillance, and bio- metrics. His research publications include inter- national journals and conferences, book chapters, and patents in the same domain. He has also served as the Guest Editor and Reviewer in many peer-reviewed international conferences and journals. PANKAJ KUMAR SA (M 07) received the Ph.D. degree in Computer Science in 2010. He is currently serving as an assistant profes- sor with the Department of Computer Science and Engineering, National Institute of Technol- ogy Rourkela, India. His research interests include computer vision, biometrics, visual surveillance, and robotic perception. He has coauthored a num- ber of research articles in various journals, confer- ences, and book chapters. He has co-investigated some research and development projects that are funded by SERB, DRDO- PXE, DeitY, and ISRO. He was a recipient of prestigious awards and honors for his excellence in academics and research. Apart from research and teach- ing, he conceptualizes and engineers the process of institutional automation. BANSHIDHAR MAJHI (M 07) is a Professor with the Department of Computer Science and Engineering, National Institute of Technology Rourkela, India. He has successfully executed var- ious Research and Development projects being funded by agencies such as MHRD, ISRO, DRDO, and DeitY. He has authored hundreds of articles in reputed journals and conferences. His current research interests include image processing, com- puter vision, biometric security, and pattern recog- nition. He has been conferred with prestigious awards and honors for his contribution towards scienti c research and academic excellence. SAMBIT BAKSHI (M 13) is currently with the Centre for Computer Vision and Pattern Recog- nition, National Institute of Technology Rourkela, India. He also serves as an Assistant Profes- sor with the Department of Computer Science and Engineering, National Institute of Technol- ogy Rourkela, India. He received the Ph.D. degree in computer science and engineering in 2015. He serves as an Associate Editor of the Interna- tional Journal of Biometrics (2013). He is a mem- ber of the IEEE Computer Society Technical Committee on Pattern Analysis and Machine Intelligence. He received the prestigious Innovative Student Projects Award - 2011 from the Indian National Academy of Engineering for his master s thesis. He has authored or coauthored over 30 publications in journals, reports, and conferences. 5808 VOLUME 4, 2016