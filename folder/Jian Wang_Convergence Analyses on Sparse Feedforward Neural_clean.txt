Convergence Analyses on Sparse Feedforward Neural Networks via Group Lasso Regularization $ Jian Wanga,d, , Qingling Caib, Qingquan Changc, Jacek M. Zuradad,e aCollege of Science, China University of Petroleum, Qingdao, 266580, China bSchool of Engineering, Sun Yat-sen University, Guangzhou, 510275, China cSchool of Mathematics and Statistics, Lanzhou University, Lanzhou, 730000, China dDepartment of Electrical and Computer Engineering, University of Louisville, Louisville, KY, 40292, USA eInformation Technology Institute, University of Social Sciences, L od z 90-113, Poland Abstract In this paper, a new variant of feedforward neural networks has been proposed for a class of nonsmooth optimization problems. The penalty term of the pre- sented neural networks stems from the Group Lasso method which selects hidden variables in a grouped manner. To deal with the non-di erentiability of the original penalty term (l1-l2 norm) and avoid oscillations, smoothing techniques have been used to approximate the objective function. It is as- sumed that the training samples are supplied to the networks in a speci c incremental way during training, that is, in each cycle samples are supplied in a xed order. Then, under suitable assumptions on learning rate, penaliza- tion coe cients and smoothing parameters, the weak and strong convergence of the training process for the smoothing neural networks have been proved. The convergence analysis shows that the gradient of the smoothing error function approaches zero and the weight sequence converges to a xed point, respectively. We demonstrate how the smoothing approximation parameter $This workwas supported in part by the National Natural Science Foundation of China (No. 61305075), the China Postdoctoral Science Foundation (No. 2012M520624), the Nat- ural Science Foundation of Shandong Province (No. ZR2013FQ004), the Specialized Re- search Fund for the Doctoral Program of Higher Education of China (No. 20130133120014) and the Fundamental Research Funds for the Central Universities (No. 13CX05016A, 14CX05042A, 15CX02079A, 15CX05053A, 15CX08011A). Corresponding author Email addresses: wangjiannl@upc.edu.cn (Jian Wang), jacek.zurada@louisville.edu (Jacek M. Zurada) Preprint submitted to Information Sciences October 14, 2016 *Manuscript (including abstract) Click here to view linked References can be updated in the training procedure so as to guarantee the convergence of the procedure to a Clarke stationary point of the original optimization problem. In addition, we have proved that the original nonsmoothing algo- rithm with l1 l2 norm penalty converges consistently to the same optimum solution with the corresponding smoothed algorithm. Numerical simulations demonstrate the convergence and e ectiveness of the proposed training algo- rithm. Keywords: Clarke gradient; Convergence; Feedforward neural networks; Group Lasso; Non-di erentiability. 1. Introduction Arti cial neural networks have been widely used in various applications, such as pattern recognition, machine learning, data mining and signal pro- cessing [37, 27, 24]. The feedforward networks are one of the most popular architectures for their strong structural exibility, good representation ability and compatibility with di erent training algorithms. A reasonable architecture is one of the key aspects to guarantee better generalization of the trained neural networks [2]. Generally, the number of neurons in the input and output layers is xed and represents the attributes and the target values of the dataset, respectively. Whereas the number of neurons in the hidden layer (or layers) depends on the complexity of the problem to be modeled, it is essential how to optimize the number of hidden neurons. Typically, there are two approaches to determine the number of hidden neurons. One is the growing method, which starts with a small initial network and then adds hidden neurons stepwise during training [58, 35, 39]. The other is a pruning way, which starts with a large initial network and then removes the unnecessary neurons or weights [29, 40, 38, 56, 45, 1]. Researchers have developed a number of pruning algorithms to optimize net- work architectures. An interesting comparative study for pruning algorithms of neural networks has been presented in [2]. Based on the elimination tech- niques, pruning methods could be categorized as penalty term methods, cross validation methods [30], magnitude base methods [21], evolutionary prun- ing methods [47], mutual information (MI) [18], signi cance based pruning methods and sensitivity analysis (SA) method [3, 1]. This paper focuses on 2 pruning technique by using a novel penalty term for backpropagation (BP) neural networks. For standard networks, the error function is de ned as the sum of the squared errors E = 1 2 X j Outputj Targetj 2, (1) where j represents the j-th sample of the dataset. When a network is to be pruned, it is common to add a penalty term to the error function during training E = 1 2 X j Outputj Targetj 2 + (w). (2) The penalty term is to suppress the unnecessary connections between neu- rons. The parameter > 0 is the regularization coe cient, which balances the relative importance of the penalty term and the pure error expression. There are three typical regularization forms for feedforward networks: weight decay [9, 26, 41, 45], weight elimination [40, 46, 34, 31] and approxi- mate smoother [36, 15]. When the backpropagation method is employed to train the network, uniform weight decay in [26] has a disadvantage that large weights are de- caying at the same rate as small weights [22]. To remedy this problem, the weight elimination method has been suggested in [46]. Unfortunately, it does not distinguish between the large and very large weights [40]. The approximate smoother [50] appears to be more accurate than weight decay or weight elimination from the complexity point of view. It is designed for a multilayer perceptron with a single hidden layer and a single output neuron. However, it is more complicated than weight decay or weight elimination and has additional computational cost [23]. We notice that the above penalty methods discourage speci c connec- tions among neurons. To improve the interpretability and sparsity of neural networks, we will borrow the idea of Group Lasso to optimize the network architecture in this paper. Lasso, the least absolute shrinkage and selection operator was rst pro- posed for linear regression problem as a new technique that reduces some coe cients and sets others to zero [44]. It has been popular for simultaneous estimation and variable selection. However, lasso often results in selecting more factors than necessary and the solution depends on how the factors are represented. Then, a new version of the lasso, the adaptive lasso, was 3 proposed in [59] by employing adaptive weights with l1 penalty term. In addition, it enjoys the oracle properties. An extension of lasso known as Group Lasso has been developed in [57] which selects the nal models on the solution paths and encourages sparsity at group level. The penalty function (l1-l2 norm) is intermediate between the l1 penalty in lasso and the l2 penalty in ridge regression (weight decay). In addition, a more general form, sparse Group Lasso, has been investigated in [19] which blends the lasso with the Group Lasso. Its main advantage is that it yields the sparse solutions at both the group and individual feature levels. A novel idea of this paper is to replace the common penalty terms in [26] with a Group Lasso counterpart for BP networks. We expect that this can enhance some of the desirable properties of Group Lasso and improve the pruning performance with better generalization. For a multi-layer network, we denote by Wi the weight matrix connecting the i-th and (i + 1)-th layers. We suggest using the following expression (w) = nl 1 X i=1 cli X l=1 w(i) l , (3) where nl is the number of layers, w(i) l is the l th column vector of matrix Wi, cli is the number of column vectors in matrix Wi. Thus the function of the optimization problem for pruning the hidden layer can be formulated as follows E = 1 2 X j Outputj Targetj 2 + nl 1 X i=1 cli X l=1 w(i) l , (4) where is the l2 norm (Euclidean). The above penalty term form is identi- cal to the counterpart in Group Lasso, namely, the l1-l2 norm penalty, where the norm of the weight vectors, w(i) l , is one of the components of the whole weight matrices. It is obvious that the penalty term of the above cost function (4) is not di erentiable at the origin. This may lead to di culties for both theoretical analysis and numerical simulations, especially when the norm of weight vector is very close to zero, because the gradients of the objective function are prerequisite for common BP networks [23]. Much attention has recently been attracted on how to e ciently solve this problem [4, 33, 5, 12]. One of the popular e ective solutions is to use the smoothing approximate techniques for solving the nonsmooth optimization problems [20, 13, 14, 6]. 4 By using the Clarke generalized gradient of the objective functions, a gen- eralized nonlinear programming circuit was introduced in the framework of nonsmooth analysis and the theory of di erential inclusions [20]. To solve the nonsmooth and nonconvex optimization problems in image restorations, an improved smoothing nonlinear conjugate gradient method was suggested in [14]. In [6], a smoothing quadratic regularization algorithm was then de- veloped for solving a class of nonsmooth, nonconvex minimization problems, which has been widely used in statistics and sparse reconstruction. Two recent papers focused on nonconvex and nonsmooth penalization methods of neural networks with smoothing approximate techniques [7, 8]. By adopting smooth techniques, a continuous network was established for solving a non-Lipschitz optimization problem in [7]. Under the bounded level set condition of the initial point, it was demonstrated that the uniform boundedness of the solutions and the global convergence of the proposed smoothing neural network. A novel smooth neural network was presented in [8] that nds a Clarke stationary point of the non-smooth constrained optimization problem. Some more attractive features of this study include that it relaxed the restriction of the initial point to be in a feasible set and can exactly specify the prior penalty parameters of the smooth neural network. Inspired by the above smoothing approximate techniques, we replace the original penalty term (l1-l2 norm) in (4) with smoothing ones. Consequently, it is bene cial for both conquering the numerical oscillations and avoiding the di culties in theoretical analysis. One of the main topics of this paper is to ll the gap and compare the convergence for the original non-smooth methods and its smoothing counterparts. According to di erent orders of sampling, there are mainly three in- cremental learning approaches [25]: online learning (completely stochastic order), almost-cyclic learning (special stochastic order) and cyclic learning ( xed order). To be speci c, in this paper we only implement the cyclic mode. The existing convergence results for online gradient method are mostly asymptotic convergence with a probabilistic nature since the sampling order is completely random [10, 43, 56]. The deterministic convergence of almost- cyclic and cyclic learning has been presented with a similar proof in [49, 55, 48, 45], separately. We note that the almost-cyclic learning of BP neural networks performs numerically better than the cyclic learning algorithm since the stochastic nature survives in the almost-cyclic training process [32]. The aim of this paper is to present a comprehensive study on the weak 5 and strong convergence of the proposed smooth neural networks with cyclic learning. We particularly show that the gradient of the cost function with respect to weight vectors can approach zero and the weight updating sequence can go to a xed point, respectively. Moreover, the rigorous proofs will be provided to ensure the convergence consistency between the smoothed neural networks and the original neural networks in terms of the Clarke di erential theory. The main contributions of this paper are as follows: 1) A novel penalty term has been presented as a part of the cost function for BP network incremental training, which e ectively prunes the connections among neurons at group level. We note that the penalty term borrows the idea from Group Lasso method which is bene cial to eliminating the unnecessary weights at group level. This essentially stems from the fact that the penalty term is the summation of the l2 norm (not squared) of the weight vectors, i.e., all of the weights connecting with the same neuron. The simulations in Section 5 shows the better pruning performance of the proposed algorithm than those of the common BP neural networks and BP algorithm with weight decay penalty (WDBP). 2) Smoothing techniques have been applied to approximate the penalty term of the cost function instead of using the original nonsmooth penalty. It is easy to know that the original proposed penalty term is non-di erentiable at the origin which may lead to the numerical oscillations and result in the obstacle on theoretical analysis. The proposed smoothing techniques en- courage the smooth weight updates and show sparse properties. In addition, we provide an e ective way of setting suitable smoothing parameters in the training process to bridge the gap between the convergence analysis based on the two di erent cost functions. 3) The weak and strong convergence of the proposed algorithm with smooth- ing approximation have been proved under mild assumptions on learning pa- rameters. We show that the weak convergence indicates that the norm of the gra- dients of the smooth cost function goes to zero, and the strong convergence implies that the weight sequence tends to a xed point (accumulation point). 4) By selecting reasonable smoothing parameters, the proved weak and strong convergence for the proposed smoothing networks have been shown to be consistent with those of the original networks. For weak convergence, we prove that any accumulation point of weight sequence is both the stationary point of the smoothed neural networks and 6 the Clarke stationary point of the original neural networks. Corresponding to the strong convergence, a restricted proof shows that the weight updating sequences converge to the same unique point as the iteration goes to in nity. The rest of this paper is organized as follows. In the next section, we present a smoothing approximate technique to induce the new training al- gorithm for BP neural networks based on the use of smoothing l1 l2 norm penalty. In Section 3, we introduce the de nitions of Clarke di erential and present the main convergence results. The rigorous proof of the results is provided in Section 4. In Section 6, we conclude the research with some useful remarks. 2. Algorithm Description We consider a feedforward neural network with three layers. The numbers of neurons for the input, hidden and output layers are p, n and q, respectively. Suppose that the training sample set is {xj, yj}J 1 j=0 Rp Rq, where xj and yj are the input and the corresponding ideal output of the j-th sample, respectively. Let V = (vij)n p be the weight matrix connecting the input and hidden layers, and write vi = (vi1, vi2, ..., vip) for i = 1, 2, ..., n. The weight matrix connecting the hidden and output layers is denoted by U = (uki)q n. Let urk = (uk1, uk2, ..., ukn) Rn and uci = (u1i, u2i, ..., uqi)T Rq be the k th row vector and i th column vector of weight matrix U, where k = 1, 2, ..., q and i = 1, 2, ..., n. To simplify the presentation, we combine the weight matrix U and V, and write w = (ur1, ..., urq, v1, ..., vn) Rn(p+q). Let g, f be the given activation functions for the hidden and output layers, respectively. For convenience, we introduce the following vector valued functions G(z) = (g(z1), g(z2), ..., g(zn))T, z Rn, (5) and F(UG(Vxj)) =  f(ur1 G(Vxj)), , f(urq G(Vxj)) T . (6) For any given input x Rp, the output of the hidden neurons is G(Vx). 7 For any xed weights w, the error of the neural networks is de ned as E (w) = 1 2 J 1 X j=0 F  UG  Vxj yj 2 + q X k=1 urk + n X i=1 vi ! ! = J 1 X j=0 Fj  UG  Vxj + J 2 q X k=1 urk + n X i=1 vi ! , (7) where Fj (UG (Vxj)) = 1 2  yj F  UG  Vxj2, j = 0, 1, ..., J 1. The gradients of the error function with respect to urk and vi (when urk = 0 and vi = 0) are, separately, given by Eurk = J 1 X j=0  f  urk G  Vxj yj k  f  urk G  Vxj G  VxjT + J urk urk , (8) Evi = J 1 X j=0 q X k=1  f  urk G  Vxj yj k  f  urk G  Vxj ukig  vi xj (xj)T + J vi vi . (9) We denote that Ew = (Eur1, Eur2, ..., Eurq, Ev1, Ev2, ..., Evn) (10) We note that the group lasso penalty in (7) is an intermediate case of the l1 norm penalty and the squared l2 penalty. From the mathematical expression, it distinguishes the di erent weight vectors, however, it equally evaluates the components of the same weight vector. This is the essential reason that the built learning system may prune the weights at a group level. It is easy to nd that (7) is non-di erentiable at the origin, which is prone to oscillate the numerical simulations. To solve the problems caused by the nonsmoothness, we will apply smoothing approximation method in this paper. There are many di erent smooth functions that can be used to approximate the l2 norm in the training process of BP neural networks. For any nite dimensional vector z and a xed positive constant , we can de ne 8 a smoothing function of z as follows: h(z, ) = z , z > . z 2 2 + 2, z . (11) We notice that this function approximates h(z) = z closer and closer as the parameter approaches to zero. Therefore, we will apply (11) to approximate the l2 norm of the penalty term in (7). In addition, the gradient of h(z, ) with respect to vector z is zh(z, ) = z z , z > z , z (12) The error function (7) can then be rewritten as eE(w) = 1 2 J 1 X j=0 F(UG(Vxj)) yj 2 + J 2 q X k=1 h(urk, ) + n X i=1 h(vi, ) ! . (13) Based on (13), we construct a smoothing Group Lasso BP network (SGLBP) as follows. Starting from an arbitrary initial guess w0, we proceed to re ne the weight sequence iteratively by the following formulae umJ+j+1 rk = umJ+j rk m jumJ+j rk (14) vmJ+j+1 i = vmJ+j i m jvmJ+j i , (15) where m > 0 is the m-th cycle learning rate, jumJ+j rk = Hm,j,jGm,j,j + J urkh  umJ+j rk , m  (16) jvmJ+j i = q X k=1 Hm,j,jumJ+j ki g  vmJ+j i xj (xj)T + J vih  vmJ+j i , m  , (17) 9 Hm,l,j (f  umJ+l rk Gm,l,j yj k)f (umJ+l rk Gm,l,j) (18) Gm,l,j = G  VmJ+lxjT (19) and, m = 1 m , m N, (20) where 0 < < 1 3 is a positive constant, l, j = 0, 1, , J 1. Then the weight vectors are updated by wmJ+j+1 = wmJ+j m jwmJ+j (21) 3. Main Results In this section, we state the related de nitions and main results of the present paper. These de nitions are referred to [42]. De nition 3.1. The directional derivative of f at x in the direction d is f (x; d) = lim t 0 f(x + td) f(x) t . (22) De nition 3.2. Let X be a Banach space, f be a locally Lipschitz continuous function at x, and let d be any vector in X. The generalized directional derivative of f at x in the direction d is f o(x; d) = lim sup t 0 y x f(y + td) f(y) t , (23) where y is a vector in X and t is a positive scalar, and t 0 denotes that t tends to zero monotonically and downward. De nition 3.3. Let f(x) be locally Lipschitz at x. Then we say that the generalized di erential (or Clarke di erential) of f at x is the set f(x) = { X | f o(x; d) , d d X} (24) The is said to be a generalized gradient of f at x. The norm in conjugate space X is de ned by = sup{ , d : d X, d 1} 10 De nition 3.4. A point x is called a Clarke stationary point of f if for all d, f o(x ; d) 0, (25) i.e.,0 f(x ). To analyze the convergence of the weight sequence (21) in the training we make the following assumptions (A1) The activation functions f and g are such that f C1(R) and g C1(R), f, g, f and g are uniformly bounded on R. And f and g are local Lipschitz functions. (A2) The learning rate m satis es that m = O( 1 m ), where 2 3 1. (A3) All of Clarke stationary points of the error function (7) are isolated. (A4) There is a bounded open set Rn(p+q) such that {wm} (m N). Our main results are as follows: Theorem 3.1. (Weak Convergence) Suppose that the assumptions (A1), (A2) and (A4) are valid. Then the weight sequence {wm} generated by (14) and (15) is weakly convergent in the sense that lim m eEw(wm) = 0, m N, (26) where eEw(wm) represents the gradient of eE(wm) with respect to w. In ad- dition, we have that 0 E(wm), m N. (27) Theorem 3.2. (Strong Convergence) If assumptions (A1)-(A4) are valid, then there holds the strong convergence: w Rn(p+q) such that lim m wm = w . (28) Theorem 3.3. (Consistency) If the assumptions (A1) (A4) hold, then the smooth error function eE(wm) and the original error function E(wm) converge to the same value as m goes to in nity, that is, lim m eE(wm) = lim m E(wm) = E(w ). (29) 11 Remark: We show that the above assumptions (A1) (A4) are su - cient but not necessary conditions for the convergence results of Theorem 3.1 Theorem 3.3. Stochastic gradient descent (SGD) method is one of the most common used strategies in dealing with large -scale machine learning problems. Actually, the training algorithm proposed in this paper is one of the speci c cases of SGD, the fed sequence of the samples are with xed or- der in the total training procedure. We note that the practical strategies of learning rates in [51, 52, 53] are of the special cases of presented assumption (A2). To our best knowledge, it rstly shows the strict convergence analysis for incremental gradient neural networks with a constant learning rate in [54]. However, its theoretical analysis is limited for two-layer neural networks, which can not simply extend to a multilayer network. The convergence anal- yses of our work are bene cial to expanding the results of [54] to more general cases in the future. 4. Proofs Some necessary lemmas for the proofs are given as follows. Lemma 4.1. Suppose that f(x) = x , x Rn, and ef(x, ) = x , if x > , x 2 2 + 2, if x . (30) Then lim 0 ef(x, ) f(x). Proof. We consider the following two cases to verify the result. Case (I). If n = 1, the function f(x) corresponds to the simple absolute value function, that is, f(x) = |x|. It is clear that the function f is Lipschitz in terms of the triangle inequality. If x > 0, we get the following by De nition 3.2 f o(x; d) = lim sup y x t 0 y + td y t = d. (31) According to De nition 3.3, f(x) = { |d d, d R} (32) 12 includes the singleton {1}. Similarly, we obtain that f(x) = { 1} , if x < 0. (33) For the case x = 0, we can calculate f o(0; d) = ( d, if d 0, d, if d 0. (34) We then immediately have that f(0) = [ 1, 1]. Therefore, we conclude that f(x) = 1, if x > 0, 1, if x < 0, [ 1, 1], if x = 0. (35) We note that the derivative of (30) with respect to x can be obtained as following ef(x, ) = ( x x , if x > , x , if x . (36) Then we have that lim 0 ef(x, ) 1. (37) This then implies that lim 0 ef(x, ) f(x). Case (II). When n > 1, f(x) = x . We let x = (x1, x2, , xn)T, then the Clarke di erential is as follows f(x) = ( x x , if x > 0, { Rn : 1} , if x = 0. (38) The gradient of ef(x, ) with respect to x can be expressed with ef(x, ) = ( x x , if x > , x , if x . (39) It is clear that lim 0 ef(x, ) f(x). This then completes the proof. 13 Lemma 4.2. Suppose that the learning rate m > 0 satis es that the as- sumption (A2). And the sequence {am} (m N) satis es that am > 0, P m=0 ma m < and limm |am+1 am| = 0 for a xed positive constant . Then we have lim m am = 0. (40) The details of the proof has been done in [48], and omitted here. Lemma 4.3. Suppose that the assumptions (A1), (A2) and (A4) are valid. Then the weight sequence {wm} generated by (14) and (15) satis es the following weak convergence lim m eEw(wm+1) eEw(wm) = 0. (41) Proof. Due to the similarity, we just prove the result of lim m eEurk(wmJ+j) = 0 and omit the proof of lim m eEvi(wm) = 0. By virtue of (13), we know that eEurk(wmJ+j) = J 1 X j=0 Hm,j,jGm,j,j + J urkh(umJ+j rk , m), (42) eEvi(wmJ+j) = J 1 X j=0 q X k=1 Hm,j,jukig  vi xj (xj)T + J vih(vmJ+j i , m), (43) which yields eEurk  w(m+1)J eEurk  wmJ = J 1 X j=0  Hm+1,0,jGm+1,0,j Hm,0,jGm,0,j + J urkh(u(m+1)J rk , m) J urkh(umJ rk , m) J 1 X j=0  Hm+1,0,jGm+1,0,j Hm,0,jGm,0,j + J urkh(u(m+1)J rk , m) urkh(umJ rk , m) . (44) 14 From Assumption (A4), we can see that there are C1 > 0 and C2 > 0 such that max 0 j J 1  xj , yj = C1, sup m N wm = C2. (45) First of all, we know that f, g are continuously di erentiable. And f and g are Lipschitz continuous and uniformly bounded on R. Thus, there exist three positive constants C3, C4 and C5, such that |f (x)| C3, x R; (46) G(z) C4, z Rn; (47) |f (x)| C5, x R; (48) |f (x) f (y)| Lf |x y|, x, y R. (49) where Lf represents the Lipschitz constant of f . jumJ+j rk = Hm,j,jGm,j,j + urkh(umJ+j rk , m) (sup x R |f(x)| + |yj k|)C5C4 + C6, (50) Similarly, jvmJ+j i C7, for some C7 > 0. Now let s consider the rst part of (44): Hm+1,0,jGm+1,0,j Hm,0,jGm,0,j ( 1 + 2 + 3) (51) where 1 = f  u(m+1)J rk Gm+1,0,j f  umJ rk Gm,0,j f  u(m+1)J rk Gm+1,0,j Gm+1,0,j , (52) 2 = f  u(m+1)J rk Gm+1,0,j f  umJ rk Gm,0,j f  umJ rk Gm,0,j yj k Gm+1,0,j , (53) 3 = Gm+1,0,j Gm,0,j f(umJ rk Gm,0,j) yj k f (umJ rk Gm,0,j) . (54) 15 Then by Newton-Leibniz formula, we nd 1= Z u(m+1)J rk Gm+1,0,j umJ Gm,0,j f (x)dx f  u(m+1)J rk Gm+1,0,j Gm+1,0,j C5|u(m+1)J rk Gm+1,0,j umJ rk Gm,0,j|C5C4 C8  u(m+1)J rk umJ rk Gm+1,0,j + umJ rk Gm+1,0,j Gm,0,j  C8  C4 J 1 X j=0 m jumJ+j rk + umJ rk sup x R |g (x)| xj n X i=1 J 1 X j=0 m jvmJ+j i  m  JC4C6C8 + nJC1C2 sup x R |g (x)| C7C8  C9 m, (55) where C8 = C4C2 5, C9 = JC4C6C8 + nJC1C2 supx R |g (x)| C7C8). By the same way, we can obtain the following by (49) 2 Lf u(m+1)J rk Gm+1,0,j umJ rk Gm,0,j  sup x R |f(x)| + |yj k|  C4 Lf u(m+1)J rk Gm+1,0,j umJ rk Gm,0,j (C3 + C1)C4 C10 m, (56) where Lf denotes the Lipschitz coe cient of f , and 16 C10 = Lf (JC4C6 + nJC1C2 supx R |g (x)| C7)(C1 + C3)C4. 3 sup x R |g (x)| xj n X i=1 J 1 X j=0 m jvmJ+j i (sup x R |f(x)| + |yj k|)C5 sup x R |g (x)|C1nJ mC7(C3 + C1)C5 C11 m, (57) where C11 = supx R |g (x)|C1nJC7(C3 + C1)C5. Now we study the second part of (44) with the following four cases: u(m+1)J rk u(m+1)J rk umJ rk umJ rk , for u(m+1)J rk > m+1, umJ rk > m. u(m+1)J rk u(m+1)J rk umJ rk m , for u(m+1)J rk > m+1, umJ rk m. u(m+1)J rk m+1 umJ rk umJ rk , for u(m+1)J rk m+1, umJ rk > m. u(m+1)J rk m+1 umJ rk m , for u(m+1)J rk m+1, umJ rk m. (58) We just study the rst case because of the similarity of analyzing the other cases. From (A2), m = o( m). u(m+1)J rk u(m+1)J rk umJ rk umJ rk u(m+1)J rk umJ rk u(m+1)J rk + umJ rk u(m+1)J rk umJ rk u(m+1)J rk umJ rk = 2 u(m+1)J rk umJ rk u(m+1)J rk 2 u(m+1)J rk umJ rk m C12 m m , (59) 17 where C12 = 2JC6. Thus we obtain that J hurk(u(m+1)J rk , m) hurk(umJ rk , m) C13 m m . (60) where C13 = J C12. In terms of (44), (51), (55), (56), (57) and (60), we conclude that eEurk  w(m+1)J eEurk  wmJ JC9 m + JC10 m + JC11 m + C13 m m C14(3 m + m m ) 0, (m ) (61) where C14 = max{JC9, JC10, JC11, C13}. Lemma 4.4. Let q(x) be a function de ned on a bounded closed interval [a, b], such that q (x) is Lipschitz continuous with Lipschitz constant K > 0. Then q(x) is di erentiable almost everywhere in [a, b] and |q (x)| K, a.e.[a, b]. (62) Moreover, there exists a constant C > 0 such that q(x) q(x0) + q (x0)(x x0) + C(x x0)2, (63) where x0, x [a, b]. The details are referred to [48] for interested readers. The next lemma plays an essential role in assuring the weak convergence theorem 3.1. It also reveals an almost monotonicity of the smoothing error function during training procedure. Certainly, the rigorous proof of this lemma is a little complicated in which the Taylor expansion and inequality tricks have been frequently used. Lemma 4.5. Let the sequence  wmJ+j be generated by (14) and (15). Un- der assumptions (A1), (A2) and (A4), there holds eE(w(m+1)J) eE(wmJ) m eEw(wmJ) 2 + m + J(n + q) m m+1 2 . (64) 18 Proof. By virtue of (13), we have eE(wmJ+j) = 1 2 J 1 X j=0 F(UmJ+jG(VmJ+jxj)) yj 2 + J q X k=1 h(umJ+j rk , m) + n X i=1 h(vmJ+j i , m) ! = 1 2 J 1 X j=0 q X k=1 f(umJ+j rk G(VmJ+jxj)) yj k 2 + J ( q X k=1 h(umJ+j rk , m) + n X i=1 h(vmJ+j i , m)), (65) where j = 0, 1, , J 1 and m N. From Lemma 4.4, (16), (17) and assumption (A1), there is a constant C15 > 0 such that  f(u(m+1)J rk Gm+1,0,j) yj k 2  f(umJ rk Gm,0,j) yj k 2 + 2Hm,0,j  u(m+1)J rk Gm+1,0,j umJ rk Gm,0,j + C15  u(m+1)J rk Gm+1,0,j umJ rk Gm,0,j2 . (66) Let 4 =  f(umJ rk Gm,0,j) yj k 2 , (67) 5 = 2Hm,0,j  u(m+1)J rk Gm+1,0,j umJ rk Gm,0,j , (68) 6 = C15  u(m+1)J rk Gm+1,0,j umJ rk Gm,0,j2 . (69) Then from (68), we nd 5 = 2Hm,0,j (u(m+1)J rk umJ rk ) Gm,0,j + umJ rk (Gm+1,0,j Gm,0,j) + (u(m+1)J rk umJ rk ) (Gm+1,0,j Gm,0,j)  . (70) 19 By virtue of the integral Taylor expansion and assumption (A1), g (x) exits almost everywhere which implies umJ rk (Gm+1,0,j Gm,0,j) = n X i=1 umJ rk,ig (vmJ i xj)  v(m+1)J i vmJ i  xj + n X i=1 umJ rk,i h v(m+1)J i vmJ i  xji2 Z 1 0 (1 t)g (vmJ x + t(v(m+1)J i vmJ i ))dt, (71) where umJ rk,i is the i-th element of the weight vector umJ rk . h(u(m+1)J rk , m+1) h(umJ rk , m) = 7 + 8, (72) where 7 = h(u(m+1)J rk , m) h(umJ rk , m), (73) 8 = h(u(m+1)J rk , m+1) h(u(m+1)J rk , m). (74) Using Taylor formula, h(u(m+1)J rk , m) = h(umJ rk , m) + urkh(umJ rk , m)  u(m+1)J rk umJ rk  + 1 2  u(m+1)J rk umJ rk  urkh(u , )  u(m+1)J rk umJ rk T , (75) where u = u(m+1)J rk + (1 )umJ rk , = m+1 + (1 ) m, 0 < < 1. Since h(z, ) is smooth and continuous di erential, it is easy to nd that urkh(u , ) is bounded by O  1 m  . So we have h(u(m+1)J rk , m) h(umJ rk , m) + urkh(umJ rk , m)  u(m+1)J rk umJ rk  + 1 2 sup u R urkh(u , )  u(m+1)J rk umJ rk 2 h(umJ rk , m) + urkh(umJ rk , m)  u(m+1)J rk umJ rk  + Cm 16 u(m+1)J rk umJ rk 2 . (76) 20 where Cm 16 = sup  1 2 sup u R urkh(u , )  . It is obvious that 7 urkh(umJ rk , m)  u(m+1)J rk umJ rk  + Cm 16 u(m+1)J rk umJ rk 2 . (77) By (11), we obtain the following 8 = 0, u(m+1)J rk m,  u(m+1)J rk m 2 2 m , u(m+1)J rk ( m+1, m), m, u(m+1)J rk m+1, (78) where m = ( m m+1)  u(m+1)J rk 2 m m+1  2 m m+1 . This implies that | 8| m m+1 2 . (79) Thus, we have h(u(m+1)J rk , m+1) h(umJ rk , m) + urkh(umJ rk , m)  u(m+1)J rk umJ rk  + Cm 16 u(m+1)J rk umJ rk 2 + m m+1 2 . (80) By the same way, we can nd h(v(m+1)J i , m+1) h(vmJ i , m) + vih(vmJ i , m)  v(m+1)J i vmJ i  + Cm 17 v(m+1)J i vmJ i 2 + m m+1 2 . (81) By virtue of (14) and (15), we then obtain u(m+1)J rk umJ rk = J 1 X j=0 m jumJ+j rk = J 1 X j=0 m jumJ rk + J 1 X j=0 m   jumJ+j rk jumJ rk  , (82) 21 v(m+1)J i vmJ i = J 1 X j=0 m jvmJ+j i = J 1 X j=0 m jvmJ i + J 1 X j=0 m  jvmJ+j i jvmJ i  . (83) Summing (66) from k = 1 to k = q and j = 0 to j = J 1, and then multiple 1 2, summing (76) from k = 1 to k = q and summing (81) from i = 1 to i = n, and also, from (16), (17), (67)-(81), (82), (83), (42), (43), we then obtain eEw(w(m+1)J) 1 2 J 1 X j=0 q X k=1 4 + J 1 X j=0 q X k=1 Hm,0,j(u(m+1)J rk umJ rk ) Gm,0,j + J 1 X j=0 q X k=1 Hm,0,j  n X i=1 umJ rk,ig (vmJ i xj)  v(m+1)J i vmJ i  xj + n X i=1 umJ rk,i h v(m+1)J i vmJ i  xji2 Z 1 0 (1 t)g (vmJ x + t(v(m+1)J i vmJ i ))dt  + J 1 X j=0 q X k=1 Hm,0,j(u(m+1)J rk umJ rk ) (Gm+1,0,j Gm,0,j) + J 1 X j=0 q X k=1 C15 6 + J q X k=1 h(umJ rk , m) + J q X k=1 urkh(umJ rk , m)  u(m+1)J rk umJ rk  + J q X k=1 Cm 16 u(m+1)J rk umJ rk 2 + J n X i=1 h(vmJ i , m) + J n X i=1 vih(vmJ i , m)  v(m+1)J i vmJ i  + J n X i=1 Cm 17 v(m+1)J i vmJ i 2 + J(n + q) m m+1 2 22 = eEw(wmJ) + q X k=1  u(m+1)J rk umJ rk  eEurk(wmJ) + n X i=1  v(m+1)J i vmJ i  eEvi(wmJ) + m + J(n + q) m m+1 2 = eEw(wmJ) q X k=1 J 1 X j=0 m jumJ rk ! eEurk(wmJ) n X i=1 J 1 X j=0 m jvmJ i ! eEvi(wmJ) + m + J(n + q) m m+1 2 = eEw(wmJ) m q X k=1 eEurk(wmJ) 2 m n X i=1 eE2 vi(wmJ) 2 + m + J(n + q) m m+1 2 = eEw(wmJ) m eEw(wmJ) 2 + m + J(n + q) m m+1 2 , m = J 1 X j=0 q X k=1 Hm,0,j(u(m+1)J rk umJ rk ) (Gm+1,0,j Gm,0,j) + J 1 X j=0 q X k=1 Hm,0,j n X i=1 umJ rk,i h v(m+1)J i vmJ i  xji2 Z 1 0 (1 t)g (vmJ x + t(v(m+1)J i vmJ i ))dt + J 1 X j=0 q X k=1 C15 6 + J q X k=1 Cm 16 u(m+1)J rk umJ rk 2 + J n X i=1 Cm 17 v(m+1)J i vmJ i 2 , (84) 23 and m = J 1 X j=0 m   jumJ+j rk jumJ rk  ! eEurk(wmJ) + J 1 X j=0 m  jvmJ+j i jvmJ i ! eEvi(wmJ) + m. (85) By a similar argument with (51)-(61) and (14), we can nd that jumJ+j rk jumJ rk C18 m (86) jvmJ+j i jvmJ i C19 m, (87) for some C18, C19 > 0. By virtue of (45)-(48), (42) and (43), it is easy to see that the rst term of m can be controlled by J 1 X j=0 m   jumJ+j rk jumJ rk  ! eEurk(wmJ) mC18 m  (sup x R |f(x)| yj k)C5C4 +  C20 2 m (88) In the same way, we can estimate the other terms of m with corresponding constants C21, ..., C26 > 0. Finally, the desired estimate (64) could be proved by setting C27 = P26 =21 C . Proof of Theorem 3.1. It is easy to see that the series P m=1 m m+1 2 < . In addition, by Lemma 4.5, we can conclude that X m=0 m eEw(wmJ) 2 = X m=0 m q X k=1 eEurk(wmJ) 2 + n X i=1 eEvi(wmJ) 2 ! < , (89) which shows that X m=0 m eEurk(wmJ) 2 < (90) 24 From Lemma 4.2 and (61), it then follows that lim m eEurk  wmJ = 0. (91) Similarly we have lim m eEurk  wmJ+j = 0, (92) and lim m eEvi  wmJ+j = 0, (93) which then implies that lim m eEw(wm) = 0. (94) There exists a bounded subsequence {wmk} {wm} which converges to w , since {wm} is bounded. From the argument above, we have E(w ) = lim s eE(wms) (95) which yields eEw(w ) = lim s eEw(wms) = 0. (96) Thus here we get 0 eE(w ). From de nition of eE(wm), we can nd eE(wm) E(wm) by Lemma 4.1. So 0 E(w ). Proof of Theorem 3.2. From Theorem 3.1, we have 0 = eE(w ) E(w ). (97) This means that w is a Clarke stationary point of E(w ). If {wm} has more than two stationary points. Without loss of generality, we can assume there are two di erent stationary points w1 and w2. Without loss of generality, we assume that the rst components w11 = w12, (0, 1), denote w = w11 + (1 )w12. Then, there exits a subsequence wmj of wm such that lim j wmj = w , where w is the rst component of w . This contradicts the assumption (A3). Thus, w must be the unique Clarke stationary point of {wm}. 25 Proof of Theorem 3.3. The above Theorem 3.2 has implied that the weight sequence {wm} converges to a xed point w . By the de nitions of (7) and (13), we can nd that E(wmJ+j) eE(wmJ+j) = q X k=1   umJ+j rk h(umJ+j rk , m)  + n X i=1  vmJ+j i h(vmJ+j i , m)  q X k=1 umJ+j rk umJ+j rk 2 2 m m 2 + n X i=1 vmJ+j i vmJ+j i 2 2 m m 2  Here umJ+j rk m and vmJ+j i m  q X k=1   umJ+j rk m 2 2 m + n X i=1  vmJ+j i m 2 2 m q m 2 + n m 2 0. (since m 0 as m ) (98) Consequently, we obtain lim m E(wm) eE (wm) = 0. (99) This completes the proof. 5. Simulations In this section, we provide simulations to compare the performance of the proposed new training algorithm (14)-(21), SGLBP, with the common BP 26 and BP with Weight Decay (WDBP) on two problems: function approxima- tion and nonlinear autoregression. The simulations support the convergence assertion made in Section 3 as well. 5.1. Approximation of SinC Function with Noise In this example, the three algorithms, BP, WDBP and SGLBP, are used to approximate the SinC function, a popular example to demonstrate the performance of intelligent algorithms for regression problems, de ned as fol- lows y(x) = ( sin(x)/x, x = 0, 1, x = 0. (100) The training set with 2, 000 data (xi = 1, 2, , 2, 000) is randomly generated on the interval [ 10, 10] with outputs y(xi)+ i, where i is the uniform noise distributed in [ 0.04, 0.04]. The testing set, however, is uniformly created on the interval [ 10, 10] with 2, 000 noise-free samples. A l gor i t h m s T r a i n i n g T i me ( s ) T r a i n i n g R M SE T es t in g R M SE P ru n e d N eu r o n s BP 1 31 . 6 0 . 0 58 23 0 .075 7 4 0 W DBP 1 4 5 . 3 0 . 0 3 4 71 0 .0 4 6 4 1 2. 4 523 SG LBP 1 53 . 7 0 . 0 19 05 0 .028 00 4 . 6 4 29 Figure 1: Performance comparison for learning of the noisy function SinC. To compare the three di erent algorithms, we have designed the identical networks (2 20 1 number of neurons of input, hidden and output layers, separately) with the same initial training parameters to the regression prob- lem. The activation functions of hidden and output layers have been assigned with tansig( ) and purelin( ) functions, respectively. The initial weights (in- cluding bias) have been generated with the Nguyen-Widrow algorithm. The learning rate and penalty coe cient are separately set to be = 0.006 and = 0.009. The stop criteria for the training is at 40, 000 iterations. For the sake of comparing the generalization and pruning abilities, the simulations have been repeated 10 times for all the three algorithms. Fig. 1 shows the average results in terms of Training Time, Training RMSE, Testing RMSE and Pruned Neurons, where RMSE means the root mean squared error. 27 Due to the additional computation burden of the penalty terms, WDBP and SGLBP are more time consuming than the common BP algorithm. How- ever, we observe that the training RMSE of SGLBP is less than those of WDBP and BP, which shows that SGLBP performs a better approximation for the SinC function. The lower testing RMSEs of WDBP and SGLBP demonstrate that they have the stronger generalization ability than BP. From the last column of Fig 1, it shows that SGLBP has the best pruning ability since the group sparse due to the use of Group Lasso penalty. 1 0 8 6 4 2 0 2 4 6 8 1 0 0 . 6 0 . 4 0 . 2 0 0 . 2 0 . 4 0 . 6 0 . 8 1 1 . 2 x y= Si n C( x ) S i n C w i t h o u t N o i se S i n C w i t h N o i s e B P W D B P S G L B P Figure 2: Comparison of the approximation performance for the algorithms: BP, WDBP and SGLBP. After training, the outputs of the noise-free testing set for the three neural networks have been graphed in Fig. 2. Based on the observation, it is clear that SGBP shows the best approximation performance for SinC function than WDBP and BP. 5.2. NAR Problem We consider the following two-dimensional nonlinear time series predic- tion problem [11], de ned by y(k) =  0.8 0.5 exp( y2(k 1))  y(k 1)  0.3 + 0.9 exp( y2(k 1))  y(k 2) + 0.1 sin( y(k 1)), (101) where k = 3, 4, , 1002. 28 1. 5 1 0. 5 0 0.5 1 1.5 1. 5 1 0. 5 0 0. 5 1 1. 5 y (k 1) y (k 2) N A R w i t hou t N o i s e N A R w i t h N oi s e Figure 3: NAR problem: Training data with noise and the original data without noise. This is a noise-free system which was speci ed by a limit circle. One thou- sand samples were generated under the initial condition that y(1) = y(2) = 0.1. To verify the stability and noise resistance of the proposed algorithm, the rst 500 training samples were added noise (k), where the noise (k) is Gaussian with zero mean and variance 0.05 (cf. Fig. 3). The remaining 500 noise-free samples were used for testing data. According to the previous ob- servations y(k 1) and y(k 2), the proposed algorithm, SGLBP, is trained to predict y(k). In this experiment, the network is designed with three layers 1. 5 1 0. 5 0 0.5 1 1.5 1. 5 1 0. 5 0 0. 5 1 1. 5 y (k 1) y (k 2) T rainin g D a t a A c t ual Ou t pu t Figure 4: Training data and the corresponding actual output of the trained neural network. with the architecture 3 13 1 (number of input, hidden and output neu- rons which include the bias in the input and hidden layers). Based on the obtained samples, the functions tansig( ) and purelin( ) have been selected as the activation functions of hidden and output layers, respectively. The 29 initial weights have been randomly chosen on the closed interval [ 1.0, 1.0] with learning rate = 0.006 and the penalty coe cient = 0.008. The stop criteria is that the maximum training iterations reach 40, 000 or the training error is below 0.001. In Fig. 4, the blue color points ( ) represent the rst noisy 500 training samples, while the red color points ( ) stand for the corresponding actual outputs of the trained neural network. It shows that the actual outputs are a good approximation for the time series prediction problem (101) except for some of the inner training points. 1. 5 1 0. 5 0 0.5 1 1.5 1. 5 1 0. 5 0 0. 5 1 1. 5 y (k 1) y (k 2) T e s t Da t a A c t ual Ou t pu t Figure 5: Test data and the corresponding actual output based on the trained neural network. Generalization ability is one of the important indexes to measure the performance of neural networks. Fig. 5 shows clearly that the proposed algorithm, SGLBP, predicts the testing dataset very well, where ( ) and ( ) are the ideal test outputs and the predictive outputs, respectively. Fig. 6 demonstrates the convergence behavior of SGLBP. This simulation shows that SGLBP outperforms the normal BP and WDBP. Furthermore, all these simulations demonstrate the error function of SGLBP decreases to a stable minimum value. Furthermore, the norm of the gradient of error function does approach to zero as iteration increases large enough, which supports the convergence results (cf. Theorem 3.1). 5.3. Classi cation In this simulation, the following classi cation datasets were downloaded from UCI Machine Learning Repository 7, which include 4 binary classi ca- tion and one multi-class classi cation problems [28]. Each dataset is ran- 30 1 0 0 1 0 1 1 0 2 1 0 3 1 0 4 0 0 .5 1 1 .5 2 2 .5 3 3 .5 4 4 .5 N u m b e r o f i t e r a t io n s N o r m o f g r a d ie n ts E r r o r Figure 6: The training error and the norm of gradient of error with respect to weight vector. D a t a S e t Da t a S iz e I n p u t F e a t u r es Cl a s s es 1 . I ri s 15 0 4 2 2 . V e hi c l e 9 46 1 8 4 3 . W i s c o n s i n B . C. 1 9 8 3 4 2 4 . W i n e 178 1 3 3 5 . A d u l t 4 88 42 1 4 2 Figure 7: Datasets for classi cation performance comparison. domly split into training and testing subsets with a xed percentage, 80% and 20% . For each dataset, we adopt the following normalization technique xnew = 2(x x) (xmax xmin) to preprocess the training and testing samples. In terms of the classi cation datasets, we establish ve di erent FNNs for the two algorithms (Fig. 8). Each of them di ers by network architectures, initial weight intervals, number of maximum iterations, learning rates and penalty coe cients, respectively. To compare the generalization and pruning abilities of WDBP and the proposed SGLBP, each simulation was repeated 10 times with three fold cross validation process. For the numerical results, we mainly focus on the three performance met- rics which have been listed in the last columns of Fig. 9: the average training accuracy, average testing accuracy and the average number of pruned hidden 31 D a t a Se t s A r c h i t ec t u r e In i t i a l In t e r v a l M a x I t e r a t io n L ea r n i n g R a t e P e n a l t y C o e f f i c i e n t 1 . B a l a n c e 5 1 8 3 [ 0 . 5 , 0 . 5 ] 1 5 , 0 0 0 0 . 0 9 0 . 0 1 2 2 . E c o l i 5 2 0 3 [ 0 . 4 ,0 . 4 ] 3 0 , 0 0 0 0 .1 2 0 . 0 0 5 3 . F e r t i l i t y 1 0 2 0 1 [ 0 . 2 , 0 . 2 ] 1 0 , 0 0 0 0 . 0 6 0 . 0 0 8 4 . G l as s 1 1 2 2 3 [ 0 . 5 , 0 . 5 ] 4 0 , 0 0 0 0 . 0 7 0 . 0 0 4 5 . I r i s 5 2 0 3 [ 0 . 3 , 0 . 3 ] 1 2 , 0 0 0 0 .1 2 0 . 0 0 5 6 . L i v e r 7 2 2 2 [ 0 . 5 , 0 . 5 ] 2 0 , 0 0 0 0 . 0 7 0 . 0 0 3 7 . S o n a r 6 1 2 0 2 [ 0 . 8 , 0 . 8 ] 1 5 , 0 0 0 0 . 0 8 0 . 0 0 8 8 . V e h i c l e 1 9 3 0 2 [ 0 . 5 , 0 . 5 ] 4 0 , 0 0 0 0 .1 0 0 . 0 1 0 9 . V e r t e br a l 7 1 2 3 [ 0 . 4 ,0 . 4 ] 2 3 , 0 0 0 0 . 0 8 0 . 0 0 5 Figure 8: Network architecture and the corresponding learning parameters for di erent datasets. D a t a S e t s A l go r i t h m T r a i n A c c . T e st A c c . P r u n e d N e u r o n s 1 . I r i s W D BP 0 . 9 2 7 8 0 . 9 0 3 6 0 .1 SG L B P 0 . 9 9 4 2 0 . 9 5 6 3 1 4 .1 7 3 2 2 . V e h i c l e W D BP 0 . 7 8 9 0 0 . 7 2 3 7 0 . 3 1 6 3 SG L B P 0 . 9 6 7 2 0 . 7 7 8 5 6 . 0 6 1 7 3 . Wi s c o n s i n ( B C ) W D BP 0 . 9 4 7 9 0 . 9 3 8 9 0 . 2 1 0 6 SG L B P 0 . 9 5 8 1 0 . 9 4 0 8 5 . 9 8 1 2 4 . Wi n e W D BP 0 . 9 4 9 1 0 . 9 3 8 7 0 . 3 6 2 1 SG L B P 0 . 9 6 0 8 0 . 9 5 3 4 4 . 8 6 1 9 5 . A d u l t W D BP 0 . 8 3 1 6 0 . 8 2 0 2 3 . 0 2 5 1 SG L B P 0 . 8 5 3 6 0 . 8 4 6 7 1 3 .1 2 8 6 Figure 9: Performance comparison for the two pruning algorithms. neurons. Fig. 9 demonstrates two aspects of the algorithms. The rst two performance measures show the network classi cation capabilities, while the last one, Pruned Neurons , evaluates the pruning e ect. It is clear to see that the proposed SGLBP algorithm performs much better than the common WDBP algorithm on both classi cation accuracy and pruning ability. 6. Conclusions We have proposed a new type of penalty functions for neural networks that e ectively prunes the neurons at the group level. To overcome the numerical oscillations and nonsmooth problems, smoothing techniques have been applied to approximate the l1 l2 norm penalty term. The weak and strong convergence of the suggested smoothing algorithms have been proved 32 which results in the consistent convergence properties for the proposed algo- rithms with non-di erentiable penalty terms. The simulations indicate the usefulness of the new suggested penalization approach. An important future research topic for incremental FNNs is to analyze the convergence behavior with a constant learning rate. The existing literature on converge analysis shows that the learning rates during training are heavily dependent on the iteration number and decrease to zero as iteration goes on. This then directly leads to reduced e ciency for real experiments. [54] shows an interesting attempt to obtain the convergence results with constant learn- ing rate. Unfortunately, it is only valid for no-hidden layer networks under some complicated constraints. How to guarantee the convergence statements for multilayer networks with constant learning rate and apply for the real applications is one of our next challenging works. Acknowledgment The authors wish to thank the anonymous reviewers for their insightful comments and suggestions which greatly improved this work. References References [1] M. G. Augasta, and T. Kathirvalavakumar, A novel pruning algorithm for optimizing feedforward neural network of classi cation problems, Neural processing letters, vol. 34, no. 3, pp. 241-258, 2011. [2] M. G. Augasta, and T. Kathirvalavakumar, Pruning algorithms of neu- ral networks-a comparative study, Central European Journal of Com- puter Science, vol. 3, no. 3, pp. 105-115, 2013. [3] L. M. Belue, and K. W. Bauer, Determining input features for multi- layer perceptrons, Neurocomputing, vol. 7, no. 2, pp. 111-121, 1995. [4] D. P. Bertsekas, Nondi erentiable optimization via approximation, Mathematical Programming Studiy, vol. 3 pp. 1-25, 1975. [5] W. Bian, and X. Xue, Subgradient-based neural networks for non- smooth nonconvex optimization problems, Neural Networks, IEEE Transactions on, vol. 20, no. 6, pp. 1024-1038, 2009. 33 [6] W. Bian, and X. Chen, Worst-Case Complexity of Smoothing Quadratic Regularization Methods for Non-Lipschitzian Optimization, SIAM Journal on Optimization, vol. 23, no. 3, pp. 1718-1741, 2013. [7] W. Bian, and X. Chen, Smoothing neural network for constrained non- Lipschitz optimization with applications, Neural Networks and Learn- ing Systems, IEEE Transactions on, vol. 23, no. 3, pp. 399-411, 2012. [8] W. Bian, and X. Chen, Neural Network for Nonsmooth, Nonconvex Constrained Minimization Via Smooth Approximation, Neural Net- works and Learning Systems, IEEE Transactions on, vol. 25, no. 3, pp. 545-556, 2014. [9] C. M. Bishop, Pattern recognition and machine learning. Springer, 2006. [10] D. Chakraborty, and N. R. Pal, A novel training scheme for multilay- ered perceptrons to realize proper generalization and incremental learn- ing, Neural Networks, IEEE Transactions on, vol. 14, no. 1, pp. 1-14, 2003. [11] S. Chen, Local regularization assisted orthogonal least squares regres- sion, Neurocomputing, vol. 69, nos. 4-6, pp. 559-585, 2006. [12] X. Chen, Smoothing methods for nonsmooth, nonconvex minimiza- tion, Mathematical Programming, vol. 134, no. 1, pp. 71-99, 2012. [13] X. Chen, F. Xu, and Y. Ye, Lower Bound Theory of Nonzero Entries in Solutions of 2- p Minimization, SIAM Journal on Scienti c Com- puting, vol. 32, no. 5, pp. 2832-2852, 2010. [14] X. Chen, and W. Zhou, Smoothing Nonlinear Conjugate Gradient Method for Image Restoration Using Nonsmooth Nonconvex Minimiza- tion, SIAM Journal on Imaging Sciences, vol. 3, no. 4, pp. 765-790, 2010. [15] Z. Chen, and S. Haykin, On Di erent Facets of Regularization Theory, Neural Comput., vol. 14, no. 12, pp. 2791-2846, 2002. [16] F. H. Clarke, Optimization and Nonsmooth Analysis. New York: Wiley, 1983 34 [17] A. P. Engelbrecht, A new pruning heuristic based on variance analysis of sensitivity information, Neural Networks, IEEE Transactions on, vol. 12, no. 6, pp. 1386-1399, 2001. [18] L. Fletcher, V. Katkovnik, F. E. Ste ens, and A. P. Engelbrecht, Op- timizing the number of hidden nodes of a feedforward arti cial neural network, in Proc. IEEE world congress on computational intelligence, The international joint conference on neural networks, 1998, pp. 1608- 1612 [19] J. Friedman, T. Hastie, and R. Tibshirani, A note on the group lasso and a sparse group lasso, Preprint arXiv:1001.0736, 2010. [20] M. Forti, P. Nistri, and M. Quincampoix, Generalized neural network for nonsmooth nonlinear programming problems, Circuits and Systems I: Regular Papers, IEEE Transactions on, vol. 51, no. 9, pp. 1741-1754, 2004. [21] M. Hagiwara, A simple and e ective method for removal of hidden units and weights, Neurocomputing, vol. 6, no. 2, pp. 207-218, 1994. [22] S. J. Hanson, and L. Y. Pratt, Comparing biases for minimal network construction with back-propagation, in Advances in Neural Informa- tion Processing Systems, 1989, 177-185. [23] S. S. Haykin, Neural networks : a comprehensive foundation, 2nd, En- glewood Cli s, NJ, USA: Prentice Hall, 1999. [24] S. Haykin, Neural networks and learning machines. McMaster Univer- sity. Hamilton, Ontario, Canada.: Prentice Hall, 2009. [25] T. Heskes, and W. Wiegerinck, A theoretical comparison of batch- mode, on-line, cyclic, and almost-cyclic learning, Neural Networks, IEEE Transactions on, vol. 7, no. 4, pp. 919-925, 1996. [26] G. E. Hinton, Connectionist learning procedures, Arti cial intelli- gence, vol. 40, no. 1, pp. 185-234, 1989. [27] W. W. Hsieh, Machine Learining Methods in the Envriomental Sciences. Cambridge, U.K.: Cambridge Univ. Press, 2009 35 [28] http://archive.ics.uci.edu/ml/ [29] S. C. Huang, and Y. F. Huang, Bounds on the number of hidden neu- rons in multilayer perceptrons, Neural Networks, IEEE Transactions on, vol. 2, no. 1, pp. 47-55, 1991. [30] T. Q. Huynh, and R. Setiono, E ective neural network pruning using cross-validation, in Proc. IEEE international joint conference on neural networks (IJCNN),2005, pp. 972-977. [31] M. Z. Iskandarani, A novel Approach to System Security using Derived Odor Keys with Weight Elimination Neural Algorithm (DOK-WENA), Trans. Mach. Learn. Artif. Intell., vol. 2, no. 2, pp. 20-31, 2014. [32] Z. Li, W. Wu, and Y. Tian, Convergence of an online gradient method for feedforward neural networks with stochastic inputs, Journal of Computational and Applied Mathematics, vol. 163, no. 1, pp. 165-176, 2004. [33] W. Lu, and J. Wang, Convergence analysis of a class of nonsmooth gra- dient systems, Circuits and Systems I: Regular Papers, IEEE Transac- tions on, vol. 55, no. 11, pp. 3514-3527, 2008. [34] P. May, E. Zhou, and C. Lee, A comprehensive evaluation of weight growth and weight elimination methods using the tangent plane algo- rithm, Int. J. Adv. Comput. Sci. & Appl.,, vol. 4, no. 6, pp. 149-56, 2013. [35] J. O. Moody, and P. J. Antsaklis, The dependence identi cation neural network construction algorithm, Neural Networks, IEEE Transactions on, vol. 7, no. 1, pp. 3-15, 1996. [36] J. E. Moody and Thorsteinn S. R ognvaldsson, Smoothing regularizers for projective basis function networks, CSETech. 1996. [37] B. D. Ripley, Pattern Recognition and Neural Network. Cambridge, U.K.: Cambridge Univ. Press, 2008. [38] D. Sabo, and X.-H. Yu, Neural network Dimension Selection for dy- namical system identi cation, in Proc. IEEE international conference on control applications, 2008, pp. 972-977. 36 [39] R. Setiono, and L. C. K. Hui, Use of a quasi-Newton method in a feedforward neural network construction algorithm, Neural Networks, IEEE Transactions on, vol. 6, no. 1, pp. 273-277, 1995. [40] R. Setiono, A penalty-function approach for pruning feedforward neural networks, Neural computation, vol. 9, no. 1, pp. 185-204, 1997. [41] J. Sum, C. S. Leung, and K. Ho, Convergence analyses on on-line weight noise injection-based training algorithms for MLPs, Neural Networks and Learning Systems, IEEE Transactions on, vol. 23, no. 11, pp. 1827- 1840, 2012. [42] W. Sun, and Y.-X. Yuan, Optimization theory and methods: nonlinear programming. Springer Science & Business Media, 2006. [43] V. Tadic, and S. Stankovic, Learning in neural networks by normalized stochastic gradient algorithm: local convergence, in Proc. Seminar neu- ral networks application electronic engineering, 2000, pp. 11-17. [44] R. Tibshirani, Regression shrinkage and selection via the lasso, Jour- nal of the Royal Statistical Society. Series B (Methodological), pp. 267- 288, 1996. [45] J. Wang, W. Wu, and J. M. Zurada, Boundedness and convergence of MPN for cyclic and almost cyclic learning with penalty, in Proc. IEEE International Joint Conference on Neural Networks (IJCNN), 2011, pp. 125-132. [46] A. S. Weigend, D. E. Rumelhart, and B. A. Huberman, Generalization by weight-elimination applied to currency exchange rate prediction, in Proc. IEEE International Joint Conf. Neural Netw. (IJCNN), 1991, pp. 837-841. [47] D. Whitley, T. Starkweather, and C. Bogart, Genetic algorithms and neural networks: Optimizing connections and connectivity, Parallel computing, vol. 14, no. 3, pp. 347-361, 1990. [48] W. Wu, J. Wang, M. Cheng, and Z. Li, Convergence analysis of online gradient method for BP neural networks, Neural Networks, vol. 24, no. 1, pp. 91-98, 2011. 37 [49] W. Wu, and Y. Xu, Deterministic convergence of an online gradient method for neural networks, Journal of Computational and Applied Mathematics, vol. 144, no. 1, pp. 335-347, 2002. [50] L. Wu, and J. Moody, A smoothing regularizer for feedforward and recurrent neural networks, Neural Computation, vol. 8, no. 3, pp. 461- 489, 1996. [51] L. Bottou, Large-Scale Machine Learning with Stochastic Gradient De- scent, Proceedings of COMPSTAT 2010. Physica-Verlag HD, pp. 177- 186, 2010. [52] W. Xu, Towards Optimal One Pass Large Scale Learning with Aver- aged Stochastic Gradient Descent, Computer Science, 2011. [53] S. Shalev-Shwartz, Online Learning and Online Convex Optimization, Foundations & Trends in Machine Learning, vol. 4, no.2, pp. 107-194, 2012. [54] L. Xu, J. Chen, D. Huang, J. Lu, and L. Fang, Analysis of bounded- ness and convergence of online gradient method for two-layer feedfor- ward neural networks, Neural Networks and Learning Systems, IEEE Transactions on, vol. 24, no. 8, pp. 1327-1337, 2013. [55] Z. B. Xu, R. Zhang, and W.-F. Jing, When does online BP training converge?, Neural Networks, IEEE Transactions on, vol. 20, no. 10, pp. 1529-1539, 2009. [56] H. Zhang, W. Wu, F. Liu, and M. Yao, Boundedness and convergence of online gradient method with penalty for feedforward neural networks, Neural Networks, IEEE Transactions on, vol. 20, no. 6, pp. 1050-1054, 2009. [57] M. Yuan, and Y. Lin, Model selection and estimation in regression with grouped variables, Journal of the Royal Statistical Society: Series B (Statistical Methodology), vol. 68, no. 1, pp. 49-67, 2006. [58] J. Zhang, and A. J. Morris, A sequential learning approach for single hidden layer neural networks, Neural networks, vol. 11, no. 1, pp. 65-80, 1998. 38 [59] H. Zou, The adaptive lasso and its oracle properties, Journal of the American statistical association, vol. 101, no. 476, pp. 1418-1429, 2006. 39