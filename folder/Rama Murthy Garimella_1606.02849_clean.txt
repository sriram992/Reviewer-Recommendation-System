arXiv:1606.02849v1 [cs.IT] 9 Jun 2016 1 Time Optimal Spectrum Sensing Garimella Rama Murthy1, Rhishi Pratap Singh2, Samdarshi Abhijeet3, and Sachin Chaudhary4 1,2,3,4Signal Processing and Communication Research Center International Institute of Information Technology, Hyderabad, India 1Email: rammurthy@iiit.ac.in 2Email: rhishi.pratap@research.iiit.ac.in 3Email: samdarshi.abhijeet@research.iiit.ac.in 4Email: sachin.c@iiit.ac.in Abstract Spectrum sensing is a fundamental operation in cognitive radio environment. It gives information about spectrum availability by scanning the bands. Usually a xed amount of time is given to scan individual bands. Most of the times, historical information about the traf c in the spectrum bands is not used. But this information gives the idea, how busy a speci c band is. Therefore, instead of scanning a band for a xed amount of time, more time can be given to less occupied bands and less time to heavily occupied ones. In this paper we have formulated the time assignment problem as integer linear programming and source coding problems. The time assignment problem is solved using the associated stochastic optimization problem. Index Terms Spectrum Sensing, Pareto Front, Integer Pro- gramming, Source Coding, Stochastic Optimization. I. INTRODUCTION In recent years, Cognitive Radio technology [1] is proposed for making ef cient utilization of electromagnetic spectrum. At the physical layer of cognitive radio networks, various techniques are proposed for Spectrum Sensing [2]. One of the basic approaches to spectrum sensing is based on Energy Detection. In earlier efforts of spectrum sensing, the temporal record/history of spectrum utilization has been completely ignored. Some researchers realized that such approach to spectrum sensing is sub-optimal[3]. The authors particularly proposed Doubly Cognitive Network Architecture in which Intelligent Spectrum Sensing is carried out by taking the historical data of spectrum utilization into account. In this research paper, we make precise mathematical formulation of time optimal spectrum sensing and propose an interesting solution. II. TIME OPTIMAL SPECTRUM SENSING: INTEGER LINEAR PROGRAMMING Consider a band of EM spectrum available for wireless communication. Let this band be subdivided into sub-bands labeled 1, 2, . . . , M. In traditional spectrum sensing based on, say, energy detection, all the sub-bands are scanned with a xed, constant time irrespective of the historical data about packet traf c. It is logically clear that the sub bands which are heavily occupied(based on historical traf c data) can be scanned faster(sensing time is chosen to be smaller) while the less occupied sub-bands can be scanned using larger sensing time. The total available time for spectrum sensing of the entire band is assumed to be constant, say L seconds. The sensing time allocated for each of the sub bands is assumed to be integer valued. Note: The time optimal spectrum sensing problem formulated below does not depend on the spectrum sensing approach. Joint Detection-Estimation Approach to Spectrum Sensing: In the following discussion we formulate the problem of prediction of packet traf c based on historical data as a Linear Mean Square Estimation problem. Also as in traditional spectrum sensing primary user detection is formulated as hypothesis testing based detection problem. As discussed earlier, we take the historical traf c data on various sub-bands into account for choosing the spectrum sensing time. In this direction we model the historical traf c data as an Auto Regressive(AR) process. In time, the unit on which the data is collected, could be an hour, day, month etc. Speci cally, we t a pth order AR process to the traf c data, i.e. x(n + p) = a1x(n) + a2x(n + 1) + . . . + apx(n + p 1) + w(n + p) (1) where using LMSE(Linear Mean Square Error estimation i.e. Solving Yule-Walker equations) method, the coef cients are estimated and the traf c data is predicted(on certain time unit). Note: The prediction tool(model) can be chosen to be more sophisticated (arti cial Neural network based approach). Let the predicted data in M sub-bands be denoted by n1, n2, . . . , nM. We normalize the number of packets in various sub-bands in the following manner qi = ni PM j=1 nj for 1 i M (2) Thus {q1, q2, . . . , qM} is a probability mass function, associated with packet traf c data in various sub-bands. Now, we formulate the time-optimal spectrum sensing problem . Our goal is to allocate the total time for sensing the entire band ( say L seconds ) into time for sensing sub- 2 bands( i.e.T1, T2, . . . , TM) such that the average sensing time i.e. T = M X i=1 Tiqi with M X i=1 Ti = L (3) is minimized. We reason below that if no constraints are imposed on {Ti}, then we have a trivial problem. Case1: In this case order {qi} from smallest value to largest value, i.e. label them as { q1, q2, . . . , qM} Set T1 = L, T2 = 0, . . . , TM = 0. With such a trivial allocation, T is minimized. Case 2: Minimum sensing time in any of the bands is lower bounded by T1(i.e smallest sensing time is at-least T1). Allocation can be as following: T1, T1, . . . , T1, (L (M 1)T1) with (L MT1 + T1) T1 Case 3: Smallest sensing time is at-least T1 and other sensing times differ by at-least 1 time unit. Allocation can be as following: T1, T1 + 1, T1 + 2, . . . , T1 + M 2, (L S)) with (L S) T1, where S=(T1) + (T1 + 1) + (T1 + 2) + . . . , (T1 + M 2). Case 4: Smallest sensing time is at-least T1 and other sensing times differ by at-least d time units. Allocation can be as following: T1, T1 +d, T1 +2d, . . . , T1 +(M 2)d, (L S)) with (L S) T1, where S=(T1) + (T1 + 1) + (T1 + 2) + . . . , (T1 + M 2). Thus we are naturally led to imposition of realistic (practical) constraints on the integer valued Ti s. Case A: Ti s are in arithmetic progression. i.e. T1, T1 + d, . . . , T1 + (M 1)d. These times must add up to total sensing time, L. Thus, we have MT1 + dM(M 1) 2 = L 2MT1 + dM(M 1) = 2L (4) Note: In the above equation M, the number of sub-bands and L , the total sensing time are known. T1, d are unknown variables. Since T1, d are always constrained to be integers, we have a linear Diophantine equation of the form aT1 + bd = 2L , where a=2M and b=M(M-1) There are standard techniques for solving such an algebraic equation. Case B: Ti s are in Geometric progression. i.e. T1, (T1)(d), (T1)(d2), . . . , (T1)(dM 1). They must add up to total sensing time L. T1 + (T1)(d) + (T1)(d2) + . . . + (T1)(dM 1) = L T1(1 + d + d2 + . . . + (dM 1) = L T1 (dM 1) d 1 = L (5) As discussed earlier M, L are known and T1, d are unknown. Thus we need to solve the following algebraic equation T1dM Ld (T1 L) = 0 (6) Goal: To solve the above algebraic equation for T1, d suppose we assume that d = 2. Thus we have to decide T1 for T1(2M 1) L = 0 Thus, for a given M ; T1, (2M 1) must be divisors of L. If not, no solution exists. Suppose M is such that 2M 1 is a prime i.e. A Mersenne prime. If L happens to be a prime number, no solution exists. (It should be noted that M must necessarily be a prime for 2M 1 to be a Mersenne prime). Thus in this case for a solution to exist L must be such that its prime factorization contains the Mersenne prime 2M 1. For a given M if L is divisible by 2M 1, we have T1 = L (2M 1) (7) Signi cance of this solution: Energy detection is facilitated by the use of FFT of certain length/size. Typically the FFT sizes are power of 2. Thus, d can be chosen to be a power of 2, leading to explicit solution for T1, i.e. T1 = L d 1 (dM 1) (8) General Solution in Case B: Factoring L gives all the possibilities for T1. A short computation will give the desired solutions, if any. Justi cation of AP/GP for sensing times: As the probabilities decrease, the increase in sensing times assume values in an AP i.e. the rate of increase of sensing times is linear, or Sensing times increase geometrically (implemented by an FFT of suitable frequency resolution.) e.g. a, 2a, 4a, 8a, 16a,. . . Case C: Ti s are in Arithmetico-geometric sequence. i.e. T1, (T1 + d)(r), (T1 + 2d)(r2), . . . , [T1 + (M 1)d](rM 1). They must add up to total sensing time L. T1 + (T1 + d)(r) + (T1 + 2d)(r2) + . . . + (T1 + (M 1)d)(rM 1) = L T1[1 + r + r2 + . . . + rM 1] + dr[1+ 2r + 3r2 + . . . + (M 1)rM 2] = L T1 (1 rM) (1 r) + dr[(1 MrM 1) (1 r) + (r rM) (1 r)2 ] = L (9) If common difference is equal to common ratio i.e d = r T1 (1 dM) (1 d) + d2[(1 MdM 1) (1 d) + (d dM) (1 d)2 ] = L (10) Thus, the Diophantine equation whose solutions are of inter- est to us are given by above equations. Solutions must be feasible/Non-negative integer values of {T1, d}. Note: It can easily be reasoned that if d=0, the above equation reduces to (4) and if r=1, (by using L Hospitals rule) the equation reduces to (8). 3 III. TIME OPTIMAL SPECTRUM SENSING: SOURCE CODING In this section we relate the problem of Time Optimal Spectrum Sensing to the source coding problem. Let Si = ni for 1 i M Compute pi = Si PM j=1 Sj for 1 i M Let T = M X j=1 Tj pj (11) Let X be the random variable assuming values { T1, T2, . . . , TM} with probabilities { p1, p2, . . . , pM}. Shannon Entropy of X is given by H(X) = M X j=1 pjlog (pj) (12) Suppose we require the spectrum sensing times in various sub- bands i.e. { Ti}M i=1 to satisfy the Kraft inequality. i.e. M X i=1 2 Ti 1 (13) Then we necessarily have the following lower bound on average sensing time i.e. T H(X) , where H(X) is the entropy of the random variable assuming values { Ti}M i=1 with the probabilities { qi}M i=1. In this connection we have following interesting lemma. Lemma: If the sensing times { Ti}M i=1 are increasing at-least in an arithmetical progression with common difference 1 i.e. T2 = T1 +1, T3 = T1 +2, . . . , TM = T1 +(M 1) then Kraft inequality is satis ed. Proof: Refer [4] Note: It is immediate that if Kraft inequality is satis ed with D = 2 i.e. M X i=1 2 Ti 1 then M X i=1 D Ti 0 1 (14) for any D0 > 2. Also if { Ti}M i=1 increases faster than Arithmetic progression with common difference ONE (i.e. AP with common difference strictly greater than one or geometric progression etc) then Kraft inequality is satis ed. Using Huffman coding we determine the values { Ti}M i=1. Suppose they i.e. { Ti}M i=1must add up to L . Then the values of { Ti}M i=1 are scaled/ normalized such that PM i=1 Ti = 1. IV. NUMERICAL EXPERIMENTS We now consider case A in section 2. We invoke the following theorem on computing the solution of linear Diophantine Equation [5]. Theorem: The linear Diophantine equation ax + by = c has a solution if and only if d|c (d divides c), where d is G.C.D.(a,b). Furthermore if (x0, y0) is a solution for this equation, then the set of solutions of the equations consist of all integer pairs (x, y), where x = x0 + t(b/d) and y = y0 + t(a/d) ,where t=. . .,-2,-1,0,1,2,. . . Note: We can compute any one solution discussed in the above theorem using Euclidean (G.C.D. Computation) algorithm. Q: How do we select the required solution? i.e. {T1, d} should be non-negative. Q: What if there are multiple solutions for {T1, d}? Examples: Case of linear Diophantine Equation case 1: aT1 + bd = 2L, where a=2M, b=M(M-1) and L = Total sensing time. let M=10, L=100 20T1 + 90d = 200 GCD (20, 90) = 10. (200 is divisible by 10.) T1 = 1 + t(90/10) d = 2 t(20/10) (15) For t=. . ..-2,-1,0,1,2,. . . there are multiple solutions but there is only one interesting solution is with t=0, T1 = 1 and d=2 case 2: if GCD(a,b) in aT1 + bd = 2L is a. let M=15, 30T1 + 210d = 1800 GCD (30, 210) = 30. (1800 is divisible by 30.) T1 + 7d = 60 (16) One solution can be d=8 and T1 =4 T1 = 4 + t(210/30) d = 8 t(30/30) (17) For t= 0,1,2,. . .,7 there are solutions. So there are multiple solutions. Note:The solution for {T1, d} in case A and B is always a matching pair. Problem: Suppose the number of solutions i.e. {T1, d} in case A, case B is strictly more than One. Goal: We would like to arrive at solutions that minimize both the mean and variance of sensing time random variable. Suppose even after such optimization procedure, we arrive at multiple solutions. Heuristically, some solutions are eliminated on the basis of {T1, d} that are too low or too high. V. TIME OPTIMAL SPECTRUM SENSING : STOCHASTIC OPTIMIZATION Case A: {q1, q2, q3, . . . , qM} are unsorted probabilities. {p1, p2, p3, . . . , pM} are sorted increasing probabilities. Mean = T = M X i=1 Tiqi = X j R Tjpj = E[Z] (18) where Z is spectrum sensing time random variable, Tj s are sorted sensing time values and R is a suitable index set. E[Z2] = X j R Tj 2pj V ariance[Z] = E[Z2] [E[Z]]2 (19) Note: Minimizing E[Z] maximizes variance [Z]. Our goal is to minimize E[Z] as well as variance [Z] (Joint Optimization 4 Problem). We would like to arrive at a PARETO Optimal Solution. Note: Suppose qi s are all equal. Then Ti = T1 , for 1 i M E[Z] = M X j=1 Tjpj = T1 (20) First Approach: Suppose Ti s are in arithmetical progression. E[Z] = M X j=1 [ T1 + (j 1)d]pj = T1(1) + M X j=1 (j 1)pjd E[Z] = T1 + ( )(d) (21) where = PM j=1(j 1)pj E[Z2] = M X j=1 ( Tj)2pj = M X j=1 [ T1 + (j 1)d]2pj = M X j=1 [ T1 2 + (j 1)2d2 + 2(j 1) T1d]pj = T1 2 + d2 M X j=1 (j 1)2pj + 2 T1d M X j=1 (j 1)pj = T1 2 + ( )d2 + (2 T1d)( ) (22) where = PM j=1(j 1)2pj var[Z] = T1 2 + ( )d2 + (2 T1d) ( T1 2 + 2d2 + 2 T1d) = ( )(d2) 2d2 = ( 2)d2 (23) Note:Optimal choice of {T1, d} are decoupled. Thus, the problem boils down to minimize E[Z] as well as var[Z]. How can we select the best solution? E[Z] = T1 + ( )(d) var[Z] = ( 2)d2 (24) where = PM j=1(j 1)pj and = PM j=1(j 1)2pj i.e.{ , } are determined by probabilities { pj}M j=1 and are xed / constants. Problem: Determine T1 and d from possibly non-unique solutions for { T1, d} (determined by Diophantine equation) Note: T1 does not effect var[Z] and only affects E[Z]. So choose minimum possible positive solution for T1. Simultaneously minimize E[Z], var[Z] with respect to d (treating T1 as constant.) E[Z] = f(d) = T1 + ( )(d) var[Z] = ( 2)d2 hence( 2) 0 (25) Note:If only mean needs to be minimized, choose the smallest T1 and matching value for d among pairs of solution of (4). Note:It can easily be reasoned that, with T1 being chosen as smallest feasible value, d is chosen to be smallest matching value from among all solutions of Diophantine equation 4. Lemma:Unique optimal solution for d exists where E[Z]=var[Z]. Proof: For an optimal solution E[Z] = var[Z] T1 + ( )(d) = ( 2)d2 ( 2)d2 d T1 = 0 ad2 + bd + c = 0 (26) where a = ( 2), b = , c = T1 b2 4ac > 0 for d to be real. 2 4( 2)( T1) > 0 2 + 4( 2) T1 > 0 since ( 2) > 0 (27) The zeros are distinct, thus we are interested in the value of d in the rst quadrant. Thus,a unique optimal solution for d is achieved. Q.E.D. Note: We expect the optimization problem formulated in the time-optimal spectrum sensing to arise in other applications. The above lemma provides solution. Case B: Suppose Ti s are in geometrical progression. E[Z] = M X j=1 Tjpj E[Z] = M X j=1 ( T1dj 1)pj = T1( M X j=1 dj 1pj) E[Z2] = M X j=1 ( Tj)2pj = M X j=1 ( T1 2d2(j 1))pj var[Z] = E[Z2] (E[Z])2 = T1 2[ M X j=1 d2j 2pj] T1 2[ M X j=1 djpj]2 = T1 2[( M X j=1 d2j 2pj) ( M X j=1 djpj)2] (28) Note: E[Z] = T1f(d) var[z] = T1 2R(d) (29) 5 where f(d) = M X j=1 djpj and R(d) = ( M X j=1 d2j 2pj) ( M X j=1 djpj)2 = f(d2) [f(d)]2 Thus the optimal choice of minimal T1 will be optimal for both E[Z] and var[Z]. But minimization of E[Z] with respect to d will maximize var[z]. Thus we are interested in Pareto Optimal Solution i.e. jointly optimal choice for d for minimizing E[Z] as well as var[z]. We now prove that if f(d) is minimized, R(d) is maximized. Claim: If f(t) is minimized, then f(t2) is maximized, which leads to Pareto optimal solution. Suppose we consider the unconstrained optimiza- tion/minimization of f(t) then Let K(t) = t2 f(t2) = f(K(t)) df(K(t)) dt = df dk dk dt = (df dt )(2t) d2f(K(t)) dt2 = d2f dt2 (2t) + (df dt )(2) df(K(t)) dt = 0 if and only if df dt = 0 (30) Further, the minima of f(.) are in the left half plane. Suppose they are real valued, e.g. to d2f dt2 |t=t0 > 0 with t0 < 0 d2f(K(t)) dt2 |t=t0 = d2f dt2 (2t)|t=t0 + 0 Since t < t0 d2f(K(t)) dt2 |t=t0 < 0 (31) f(t2) is maximized, when f(t) is minimized. Thus, we look for Pareto optimal solution for d i.e. denoted t here. So use closest solution of 7 pair of T1, d to Pareto optimal solution. Pareto Optimal Solution: Fixed Point Equation: E[Z] = T1f(d) E[Z2] = T1 2g(d) E[Z] = var[Z] note that g(d) = f(d2) T1f(d) = T1 2g(d) T1 2(f(d))2 T1f(d2) T1(f(d))2 f(d) = 0 (32) Since f(d) is a polynomial in d, we have a polynomial equation which has multiple zeros. Q: How can we determine optimal d ? Choose smallest real d that is feasible. Example: Let M=3 f(d) = 3 X j=1 d(j 1)pj = p1 + dp2 + d2p3 g(d) = f(d2) = 3 X j=1 d2j 2pj = p1 + d2p2 + d4p3 (33) Replace values in equation T1f(d2) T1(f(d))2 f(d) = 0 (34) Let T1 = 1, p1 = .5, p2 = .3, p3 = .2 (p1+d2p2+d4p3) (p1+dp2+d2p3)2 (p1+dp2+d2p3) = 0 After solving equations values for d = 2.43, -1.26, -0.20 0.68i Let T1 = 1, p1 = .2, p2 = .3, p3 = .5 values for d = 2.53, -1.19, -0.06 0.21i Let T1 = 1, p1 = .6, p2 = .3, p3 = .1 values for d = 2.69, -1.33, -0.34 0.99i In the above examples, the solution contains only one positive real value that is of our interest. Rest of the values are not useful. Take the closest integer value of d which is real positive optimal solution. Summary: Step1: Based on data, predict the Probabilities related to spectrum band occupancy. Step2: Allocate Sensing Times in the order of probability values i.e. if a band is highly occupied (probabilistic), allocate smaller sensing time and vice-versa. Step3: Assume that the sensing times are in arithmetic/ Geometrical progression. Compute solution to the Integer programming problem (or the Diophantine equation.) If there is more than one solution, we need to decide the solution that must be chosen. Step4: Find solution/solutions which minimize the mean, variance (assuming that the sensing time values are in AP/GP) and nd unique/multiple solutions. In summary if the allocated times are in AP: 1) If E[Z] and Var[Z] both require minimization, choose smallest {T1, d} pair solution to (4). 2) If E[Z] is maximized and Var[Z] require minimization, it will lead to unique Pareto optimal solution. 3) If Var[Z] is maximized and E[Z] require minimization, it will lead to unique Pareto optimal solution. if the allocated times are in GP: 1) If E[Z] and Var[Z] both require minimization, choose Pareto solution to d rounded & closest matching pair {T1, d} solution to (8), with T1 chosen as small as possible. 2) If E[Z] is maximized and var[Z] require minimization, choose Pareto solution to d rounded & closest matching pair {T1, d} solution to (8), with T1 chosen as large as possible.. 6 Case C: Suppose Ti s are in Arithmetico-geometric progres- sion. E[Z] = M X j=1 Tjpj E[Z] = M X j=1 [ T1 + (j 1)d]rj 1pj = T1 M X j=1 rj 1pj + d M X j=1 (j 1)rj 1pj = T1f1(r) + df2(r) where f1(r) = M X j=1 rj 1pj and f2(r) = M X j=1 (j 1)rj 1pj E[Z2] = M X j=1 ( Tj)2pj = T1 2 M X j=1 r2(j 1)pj + d2 M X j=1 [(j 1)rj 1]2pj+ 2T1d M X j=1 (j 1)r2(j 1)pj = T1 2f3(r) + d2f4(r) + 2T1df5(r) where f3(r) = M X j=1 r2(j 1)pj , f4(r) = M X j=1 [(j 1)rj 1]2pj and f5(r) = M X j=1 (j 1)r2(j 1)pj (35) Case: If r = d E[Z] = T1 M X j=1 dj 1pj + M X j=1 (j 1)djpj = T1f1(d) + df2(d) where f1(d) = M X j=1 dj 1pj and f2(d) = M X j=1 (j 1)djpj E[Z2] = T1 2 M X j=1 d2(j 1)pj + M X j=1 [(j 1)dj]2pj+ 2T1 M X j=1 (j 1)r2j 1pj = T1 2f3(d) + f4(r) + 2T1f5(r) where f3(d) = M X j=1 d2(j 1)pj , f4(d) = M X j=1 [(j 1)dj]2pj and f5(d) = M X j=1 (j 1)r2j 1pj Variance var[Z] = E[Z2] (E[Z])2 (36) For a Pareto Optimal Solution E[Z] = var[Z] E[Z] = E[Z2] E[Z]2 (37) Keep values from equation (35) and (36) and solve the functional equation in {T1, d}. TODO: Numerical Experiments Case D: Generalization: Sensing times form an increasing sequence (not necessarily AP/GP). T1, T2, . . . , TM are such that T1 + T2 + . . . + TM = L (38) where Ti > 0 for 1 i M We have a constrained partition problem (as in Number Theory.) i.e. Find all possible solutions of partition problem and prune out unsuitable solutions based on some criterion. Ti < Tj if j > i With this constraint only, the number of possible solution need to be computed. Case 1: M < L (Most interesting case) L(L 1) . . . (L (M 1)) = L! (L M + 2)! (39) possible solutions when there is no further constraint on values of . We don t worry about other case M > L. Note: We can have a lower bound of sensing time allocated in any of the sub-bands i.e. Ti > s for 1 i M Max number of solutions = (L s)(L s 1) . . . (L s (M 1)) = (L s)(L s 1) . . . (L s M + 1) = (L s)! (L s M + 2)! (40) Effective Idea: The most general choice of sensing times (increasing numbers) leads to the constrained partition problem. Further the sensing times must minimize the mean as well as variance of the sensing time random variable. The above discussion naturally leads to the following more interesting optimization problems (related to joint optimiza- tion of moments of a discrete random variable.) Let Z be a random variable assuming values {T1, T2, . . . , TM} with probabilities {q1, q2, . . . , qM} respectively. E[Z2] = M X i=1 T 2 i qi (41) Let {Ti}M i=1 be the unknowns and {qi}M i=1 are known con- stants. Then the mean and variance of the random variable are given by E[Z] = M X i=1 Tiqi = f(T1, T2, . . . , TM) = f( T) var[Z] = E[Z2] (E[Z])2 = g(T1, T2, . . . , TM) = g( T) (42) 7 Goal: To see if we can optimize E[Z], var[z] jointly. Q: Do we have an interesting functional equation arising in the joint optimization of E[Z], var[Z] ? E[Z] = var[Z] E[Z] = E[Z2] (E[Z])2 E[Z2] E[Z] (E[Z])2 = 0 letting E[Z2] = h(T1, T2, . . . , TM) = f(T 2 1 , T 2 2 , . . . , T 2 M) (43) The multivariate functional equation that must be solved is given by f(T 2 1 , T 2 2 , . . . , T 2 M) f(T1, T2, . . . , TM) (f(T1, T2, . . . , TM))2 = 0 (44) Is there a solution to such a functional equation? Mostly it constitutes the Pareto Front(Non-Dominating solution set). VI. MULTI-OBJECTIVE OPTIMIZATION: LINEAR AND QUADRATIC PROGRAMMING (HYBRID PROGRAMMING) Objective Functions: C = [p1, p2, . . . , pM]T T = [T1, T2, . . . , TM]T D = diag{p1, p2, . . . , pM} E[Z] = CT T = T TC = C.T V ar[Z] = T T DT (CT T )2 = T T DT (T TC)2 = T T DT T T CCT T = T T ( D CT C)T = T T GT ,where G = D CCT (45) Example: G = D CCT G = p1 0 0 0 p2 0 0 0 p3 p12 p1p2 p1p3 p1p2 p22 p2p3 p1p3 p2p3 p32 = p1(1 p1) p1p2 p1p3 p1p2 p2(1 p2) p2p3 p1p3 p2p3 p3(1 p3) (46) Inferences: G is a laplacian like matrix. - G is a symmetric generator matrix. Function of interest for arriving at solutions where E[Z]=var[Z]: J(T ) = V ar[Z] E[Z] = T T GT T TC = T T 1  G 0 CT 0  T 1  (47) Note: G e = 0 G = GT Note:We use Laplacian and Laplacian like matrix interchange- ably. Theme: Laplacian matrix arising in variance optimization of a discrete random variable. Q: Can (linear algebraic) properties of matrix G be capitalized to derive new results on variance minimization? Goal: To study properties of laplacian like matrix G = D CCT where D = diag{p1, p2, . . . , pM}, C = [p1, p2, . . . , pM]T , G = GT 1) Eigen values are all real. 2) G is positive semide nite, with an eigen value at zero ( e . . . all ones vector). e is in the null space of G. 3) 0 is the smallest eigen value and all other eigen values lie on real axis. 4) Bounds on spectral radius of G n X j=1 Gij = n X j=1 Dij pj X (p1 + . . . + pj 1 + pj + . . . + pM) = pj pj = 0 n X j=1 |Gij| = pj(1 pj) + |pj(p1 + . . . + pj 1 + pj+1 + . . . + pM)| = pj(1 pj) + |pj(1 pj)| = 2pj(1 pj) min j {2pj(1 pj)} Spectral radius(G) max j {2pj(1 pj)} (48) All eigen values of G lie in the interval [0,1). Note: G = G is a generator matrix. G/ + I = P , stochastic matrix. I G/ = P . . . eigen value of G. . . . eigen value of P. . . . largest diagonal element of G. . . . eigen value of G. 0 . . . Sp(G). / + 1 = and = = 0 , so = 1 (49) Thus, by Perron Frobenius theorem, the dimension of null space of G is one (with e = 1 1 . . . 1T ) i.e. all ones column vector using null space. 8 Computation of determinant and trace of G: G C = D C C( M X j=1 pi2) = p12 p22 ... pM 2 p1 p2 ... pM = p12 p1 p22 p2 ... pM 2 pM G = D C CT = D[I D 1 C CT ] Det( G) = Det( D)Det[I D 1 C CT ] Note: Det( G) = 0 D 1 C CT = 1/p1 0 . . . 0 0 1/p2 . . . 0 ... ... ... ... 0 0 . . . 1/pM C = p1 p2 ... pM ; D 1 C = p1/p1 p2/p2 ... pM/pM = 1 1 ... 1 D 1 C CT = 1 1 ... 1 p1 p2 . . . pM  = F (50) It is a rank one matrix. From Kailath ( Linear Systems ), page 658, we have that if A is a rank one matrix Det(I + A) = 1 + trace(A) Determinant: Det( (I) + ( F)) = 1 + trace( F) = 1 (p1 + p2 + . . . + pM) = 0 Det(G) = Det(D)Det(I D 1 (C) CT ) = N Y i=1 i N Y i=1 i =  p1 p2 . . . pM  (1 p1 p2 . . . pM) = 0 (51) Trace: T race(G) = T race(D) T race( C CT ) N X i=1 i = (p1 + p2 . . . + pM) (p2 1 + p2 2 . . . + p2 M) = N X i=1 i > 0 (52) Properties of laplacian type matrix arising in variance expression of a Discrete Random Variable Z: V ar[Z] = T TG T 0 1) Global minimum occurs at the eigen vector(s) of G corresponding to zero eigen value(null space of G). 2) Let 1 2 . . . N 1 N = 0. Non-zero minimum value of Var[Z] is determined by the eigen vector R, corresponding to second smallest eigen value N 1. G R = N 1 R RT R = N 1 RT R = N 1(|| R||)2 ,where || R|| is the L2-norm of G (53) Note:Optimization of Var[Z] requires speci cation of constraint set on T. Example: || T|| = 1 RT G R = N 1 > 0 (54) 3) Using Rayleigh s theorem, when constraint set is eu- clidean hyper sphere 1 is the maximum value. 4) Similar results are derived when the constraint set is unit hypercube, lattice. Results related to Laplacian like matrix G: G = D C CT is symmetric laplacian like matrix. 1) T race( G) = N X i=1 (pi pi 2) = N X i=1 pi N X i=1 pi 2 = 1 N X i=1 pi 2 = 1 Tsallis entropy of ( P = (p1, p2, . . . , pN)) Tsallis entropy and Shannon entropy are related. T race( G) = 1 N X i=1 pi 2 0( Note that i (0, 1]) = N 1 X i=1 i (55) 2) Det(G) = 0 , other coef cients of characteristic polyno- mial may easily be computed. 3) Computation of eigen values of G G = PN 1 i=1 i fi fi T , where fi s are the right eigen vectors of G. Q:How do we compute fi s. 4) G is sub-stochastic since i [0, 1) Gnn 0 ,where G = D C CT Using matrix binomial theorem, Gn can be explicitly 9 computed. G = D C ,where C is rank one matrix. C2 = C CT C CT = ( CT C) C CT = ( N X i=1 pi2)( C CT ) = ( C CT ) Cm can be computed for m 2 Cm = ( )m 1( C CT ) AlsoDm = diag{p1m, p2m, . . . , pN m} Using expression for GM, we can compute T race(Gm) = N 1 X i=1 ( i)m ,for m 1 (56) 5) Using Leverrier - Fadeev algorith, all the coef cients of characteristic polynomial of G could be computed ef ciently. They involve {p1, p2, . . . , pN}. VII. FUTURE WORK Consider the packet arrivals to each secondary user con- stitute a Poisson process. Let these packet streams be in- dependent. Also, let there be K channels available for communication. Let the service times (for transmitting the packets from the secondary users) be exponential random variables. Thus, we model the associated Queuing system to be an M/M/K queue. Using standard results in queuing theory, various performance measures can be computed and interpreted. More General Stochastic Model: By associating channel states, a more general model based on Quasi Birth and Death process is being developed and analyzed. VIII. CONCLUSION In the paper, information theoretic and integer linear pro- gramming approach for time optimal spectrum sensing is discussed. The problem is also discussed as stochastic op- timization problem and how Pareto Front helps solving the issue. We expect the optimization problem formulated here can arise in other applications. REFERENCES [1] J. Mitola , and G. Maguire, Cognitive radio: making software radios more personal , Personal Communications, IEEE 6.4 (1999): 13-18 [2] Tev k Ycek, and Hseyin Arslan, A survey of spectrum sensing al- gorithms for cognitive radio applications , Communications Surveys & Tutorials, IEEE 11.1 (2009): 116-130 [3] Sumit Kumar, Deepti Singhal and G. Rama Murthy, Doubly Cognitive Architecture based Cognitive Wireless Sensor Networks , International Journal of Wireless Networks and Broadband Technologies (IJWNBT), Volume 1, Issue 2 (April-June 2011), IGI Global Publications [4] Samdarshi Abhijeet, and Garimella Rama Murthy, Doubly Optimal Secure and Protected Multicasting in Hierarchical Sensor Networks. International Journal of Wireless Networks and Broadband Technologies (IJWNBT) 2.4 (2012): 51-63 [5] G.E. Andrews, Number Theory , Dover Publications Inc, New York, 1994