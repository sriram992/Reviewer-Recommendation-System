Towards Designing Computer Vision-based Explainable-AI Solution: A Use Case of Livestock Mart Industry Devam Dave1, Het Naik1, Smiti Singhal1, Rudresh Dwivedi1 and Pankesh Patel2 1 School of Technology, Pandit Deendayal Energy University (PDEU), Gandhinagar, India {devam.dce18,het.nce18,smiti.sce18,rudresh.dwivedi}@sot.pdpu.ac.in 2 AI Institute, University of South Carolina, Columbia, South Carolina, USA dr.pankesh.patel@gmail.com Abstract. The objective of an online Mart is to match buyers and sellers, to weigh animals and to oversee their sale. A reliable pricing method can be developed by ML models that can read through historical sales data. However, when AI models suggest or recommend a price, that in itself does not reveal too much (i.e., it acts like a black box) about the qualities and the abilities of an animal. An interested buyer would like to know more about the salient features of an animal before making the right choice based on his requirements. A model capable of explaining the di erent factors that impact the price point is essential for the needs of the market. It can also inspire con dence in buyers and sellers about the price point o ered. To achieve these objectives, we have been working with the team at MartEye, a startup based in Portershed in Galway City, Ireland. Through this paper, we report our work-in-progress research towards building a smart video analytic platform, leveraging Explainable AI techniques. Keywords: Explainable AI Video Analytics Internet of Things vision based feature extraction ML based price prediction. 1 Introduction With the growth of AI and ML and deployment of advanced models in novel application domains, hitherto unasked questions and challenges come to the fore. One such question relates to comparing the models to black boxes where it is not easy to understand their inner-workings, their algorithms and prediction reasoning seemingly stand unexplained. This is also fuelled by the need for increasing e orts at removing bias where discriminatory features are found to be This is extended work of our demonstrations at ACM India Joint International Conference on Data Science & Management of Data 8th ACM IKDD CODS and 26th COMAD, 2021. The authors thank Ciaran Feeney from MartEye, Prof. John Breslin from NUIG, Galway and Dr. Muhammad Intizar Ali from DCU, Ireland. arXiv:2103.03096v1 [cs.CV] 8 Feb 2021 2 D. Dave et al. used by the model in the process of prediction. These two perceptions have led to increasing levels of distrust on ML/AI systems that are in use. Interpretability has also played an important role in applied ML over the years, but as black-box techniques such as DL take wings, it has assumed the form of concerns requiring urgent attention. As such, the eld that attempts to x these problems goes by the name of Explainable AI (XAI) [3]. An objective of this paper is to report our work-in-progress research and early results towards building a smart video analytics platform, leveraging Explainable AI technology. We present a motivating scenario rst and then report early results. 2 Motivating Scenario The objective of an online Mart3 is to match buyers and sellers, to weigh animals and to oversee their sale. Valuation of animals lack standards and transparency in the Livestock Marts Industry. It is also a playground for agents who indulge in pro teering by selling or purchasing animals. The signi cant factors that should determine the price of the animals at the Mart are their lineage, physical attributes such as age, weight, and health. To ensure fair pricing of the animals, it is essential to understand data that determine their true value. Careful modelling and appropriate transparency consideration for valuation and pricing of animals should aid in stopping kickbacks, exorbitant commissions and price manipulations. A reliable pricing method can be developed by ML models that can read through historical sales data. Moreover, it is possible for the visual recognition models of AI to recognize the features of animals and use that data for application of the pricing model. Collectively, it provides a valuable and unbiased tool for pricing of the animals at a Livestock Mart. However, when AI models suggest or recommend a price, that in itself does not reveal too much (i.e., it acts like a black box) about the qualities and the abilities of an animal. An interested buyer would like to know more about the salient features of an animal before making the right choice based on his requirements. A model capable of explaining the di erent factors that impact the price point is essential for the needs of the market. It can also inspire con dence in buyers and sellers about the price point o ered. To achieve these objectives, we have been working with the team at MartEye4, a startup based in Portershed in Galway City, Ireland. Through this paper, we report our work-in-progress research towards building a smart video analytic platform, leveraging Explainable AI techniques [3]. 3 Our approach and early results Figure 1 presents an overview of our approach. In the following section, we present the software components and their implementation brie y: 3 https://tinyurl.com/yyeytfc8 4 https://www.marteye.ie/ Title Suppressed Due to Excessive Length 3 Fig. 1. An Overview of Our Approach. Edge Device. A real-time video stream is captured by a camera and pre- processed at the Edge device (Circled 1 in Figure 1) [6] [5] [2]. The edge hosts frame sampling components that samples a frame o a live video stream from the attached camera and video pre-processing components to remove redundant and uninteresting parts of a video stream. Both the components are implemented using OpenCV library in Python. The pre-processed video stream is packaged at the edge camera and transmitted (Circled 2 in Figure 1) to the AWS Cloud. Vision-based Feature Extraction. A video stream is pre-processed at the Edge camera and transmitted to AWS. AWS Kinesis ingests this video stream and it is analysed using AWS Rekognition, which lets developers build several computer vision capabilities on top of scalable and reliable Amazon infrastructure. We build feature extraction smart module using AWS Rekognition (Circled 3 in Figure 1). It extracts several features from an animal image (such as weight, age, height) by applying the classical and CNN-based vision techniques [10]. ML-based Price Prediction. It takes features (extracted by the vision-based feature extraction module) and predicts the price of an animal. We have trained a multivariate linear regression model using the historical dataset provided by the MartEye team. The team has collected large datasets from di erent marts across Ireland. The dataset contains 20+ features (e.g., Price per kilo (PPK/Weight), Weight of an animal (WT)). This model is trained by considering the total price of an animal as a target variable and the remaining features as independent variables. This model is implemented using AWS SageMaker, which provides the ability to build, train and deploy ML models on AWS. The proposed approach generates price recommendations with explanations, describing how the recommended price is derived with explanations, instead of just predicting the price (Circled 5 in Figure 1). We employ several existing model interpretation techniques to generates insights from the multivariate linear regression model. Figure 2 illustrates the explanations, generated by LIME (Local Interpretable Model-agnostic Explanations) technique. The output of LIME is a list of explanations, considering the contribution of each feature to the predicted value of a dataset. Explainable AI Module. The proposed solution goes beyond prediction. It generates price recommendations with explanations, describing how the recom- mended price is derived at with explanations, instead of just predicting the price. We employ several existing model interpretation techniques to generate insights from the ML-based price prediction model. Figure 2 illustrates the explana- 4 D. Dave et al. tions generated by the LIME (Local Interpretable Model-agnostic Explanations) technique, using the ML-based price prediction model. Fig. 2. Model explanations generated by LIME. The output of Figure 2 is a list of explanations, considering the contribu- tion of each feature to a predicted price. The left part shows the range of a maximum (1487.18) and minimum (240.07) value, which is predicted by the ML-based price prediction module. The middle part shows the features (i.e., WT and PPK), which contribute the most in the predicted price of an animal. We can nd that when the weight is in a range between 308.00 < WT <= 327.00 it is contributing in a negative direction of the prediction. We can also clearly see that when PPK is in a range 210.50 < PPK <= 214.10 it is contributing to the positive side of the total price. The right part shows the actual value of a particular feature (i.e., Weight = 327.00, PPK = 214.10). 4 Video Analytics for Weight estimation One of key objective of our approach is to estimate a weight value using video analytics. This section presents our approach to estimate the weight. 4.1 Weight estimation using Teachable Machine Teachable Machine is a web-based tool [8] that is used for creating machine learning models in a simple way with no prerequisites required. Images, audios, and poses can be categorized using this tool, after which the model can be used in your own software. The main idea was to give a designated range of weights for the input image. We de ned classes based on images for selected ranges and then gave an almost equal number of images for all the classes. The images used for testing included the cows with a familiar and unfamiliar background. Phase 1: Single POV (fewer images with larger range) In the rst testing phase, we took 5 classes in the range of 200 KGs each, i.e 0-200, 201-400, etc. The images were taken from a single point of view, i.e side view. Approximately 30 images of each class were taken. The accuracy turned out to be approximately 92%. In Figure 3, the left screenshot below shows the image in which the model predicted the correct range of cow s weight while the right image shows the one with the incorrect output predicted by the model. Title Suppressed Due to Excessive Length 5 Phase 2: Single POV (more images with smaller range) In the second testing phase, we increased the number of classes to make the predicted value concise, keeping the view of the images the same. We took 8 classes in the range of 100 KGs each, i.e 0-100, 101-200, etc. The images were taken from a single point of view, i.e side view. Approximately 25 images of each class were taken. The accuracy turned out to be approximately 95%. In Figure 4, the left screenshot below shows the image in which the model predicted the correct range of cow s weight i.e. 500-600, while the right one shows the one with the incorrect output predicted by the model. Phase 3: Multiple POV (more images with smaller range) In the third testing phase, we added di erent viewpoints of the cow ( i.e. front, back, and cross), keeping the number of classes and images the same as the previous phase, in order to make the predicted value concise. We took 8 classes in the range of 100 KGs each, i.e 0-100, 101-200, etc. Approximately 25 images of each class were taken. The accuracy turned out to be approximately 65%. Similar to the above 2 phases, Figure 5 shows the image in which the model predicted the correct range of cow s weight i.e. 400-500, while the right one shows the one with the incorrect output predicted by the model. The Key observations from the method is that the accuracy varies based on training image s Point of View . If the image of the cow is sideways, the model Fig. 3. Prediction by teachable machine with 5 classes. 6 D. Dave et al. Fig. 4. Prediction by teachable machine with 8 classes. predicts almost the accurate weight, but when it is a front or back view, the model fails to predict the weight. Further, the color of the cow doesn t matter in this case, as the training data used contains a mix of images of cows of every breed (and color). 4.2 Body Mass Index In this method, our main objective was to estimate the weight using a video frame that shows the front, side, and back view of the cows. The approach followed to nd the weight was to capture an image of the face of the cow which was then given as input to the model to estimate the weight based on it. A ResNet based method [7,9,4,1] was used for preparing the model and estimating the body weight from the input face, the reason being that ResNets not only have a signi cantly high accuracy of object classi cation, object detection and segmentation but also high training speed. Although the approach gives signi cantly good results for human beings, it fails to infer weight and deliver accurate results in our case. The following limitations arise with this approach: Unlike a human face that shows features such as age, gender, and a person s skin being thick or thin depending on the facial fat, this is not the case with cows. All cows have similar facial features and they do not exhibit information Title Suppressed Due to Excessive Length 7 Fig. 5. Prediction by teachable machine with di erent viewpoints. that can be useful in weight estimation. Thus, the major issue is that the face won t be not enough for weight estimation. If we train by using the weight of each face of the cow, the model can get biased according to color, which will totally mislead the model and will give a set of vague predictions. The video quality provided is not good enough to capture the face in the video as zooming the video to capture the face does not give enough resolution to train the model on it. 5 Acknowledgement This publication has emanated from research supported by grants from the European Union s Horizon 2020 research and innovation programme under grant agreement number 847577 (SMART 4.0 Marie Sklodowska-Curie actions CO- FUND) and from SFI under grant numbers SFI/16/RC/3918 (Con rm) and SFI/16/RC/3835 (VistaMilk) cofunded by the European Regional Development Fund. 8 D. Dave et al. References 1. Angelov, P., Soares, E.: Towards explainable deep neural networks (xdnn). Neural Networks (2020) 2. Chauhan, S., Patel, P., Sureka, A., Delicato, F.C., Chaudhary, S.: Demonstration abstract: Iotsuite - a framework to design, implement, and deploy iot applications. In: 2016 15th ACM/IEEE International Conference on Information Processing in Sensor Networks (IPSN). pp. 1 2 (2016). https://doi.org/10.1109/IPSN.2016.7460669 3. Doshi-Velez, F., Kim, B.: Towards a rigorous science of interpretable machine learning (2017) 4. Du, M., Liu, N., Hu, X.: Techniques for interpretable machine learning. Communi- cations of the ACM 63(1), 68 77 (2019) 5. Gyrard, A., Serrano, M., Patel, P.: Chapter 11 - building interoperable and cross-domain semantic web of things applications. In: Sheng, Q.Z., Qin, Y., Yao, L., Benatallah, B. (eds.) Managing the Web of Things, pp. 305 324. Mor- gan Kaufmann, Boston (2017). https://doi.org/https://doi.org/10.1016/B978- 0-12-809764-9.00014-7, https://www.sciencedirect.com/science/article/pii/ B9780128097649000147 6. Intizar, M., Patel, P., Kanti Datta, S., Gyrard, A.: Multi-Layer Cross Domain Reasoning over Distributed Autonomous IoT Applications. Open Journal of Internet of Things 3 (2017), https://hal-emse.ccsd.cnrs.fr/emse-01644333 7. Kocabey, E., Camurcu, M., O i, F., Aytar, Y., Marin, J., Torralba, A., Weber, I.: Face-to-bmi: Using computer vision to infer body mass index on social media. In: Proceedings of the International AAAI Conference on Web and Social Media. vol. 11 (2017) 8. Mothilal, R.K., Sharma, A., Tan, C.: Explaining machine learning classi ers through diverse counterfactual explanations. CoRR abs/1905.07697 (2019), http://arxiv. org/abs/1905.07697 9. Society, T.R.: Explainable ai: the basics, policy brie ng, the royal so- ciety. Policy Projects https://royalsociety.org/topics-policy/projects/ explainable-ai/ (2019) 10. Song, X., Bokkers, E., van der Tol, P., Groot Koerkamp, P., van Mourik, S.: Automated body weight prediction of dairy cows us- ing 3-dimensional vision. Journal of Dairy Science 101(5), 4448 4459 (2018). https://doi.org/https://doi.org/10.3168/jds.2017-13094, http://www.sciencedirect.com/science/article/pii/S0022030218301693