Training Data Compression Algorithms and Reliability in Large Wireless Sensor Networks Vasanth Iyer, Garimella Ram Murthy, M.B. Srinivas Computer Science and Engineering, International Institute of Information Technology, Gachibowli, Hyderabad, India 500032 Email: vasanth@research.iiit.ac.in, rammurthy@iiit.ac.in, srinivas@iiit.net Abstract With the availability of low-cost sensor nodes there have been many standards developed to integrate and network these nodes to form a reliable network allowing many different types of hardware vendors to coexist. Most of these solutions however have aimed at industry-speci c interoperability but not the size of the sensor network and the large amount of data which is collected in course of its lifetime. In this paper we use well studied data compression algorithms which optimize on bringing down the data redundancy which is related to correlated sensor readings and using a probability model to ef ciently compress data at the cluster heads. As in the case of sensor networks the data reliability goes down as the network resource depletes and these types of networks lacks any central synchronization making it even more a global problem to compare different reading at the central coordinator. The complexity of calibrating each sensor and using an adaptable measured threshold to correct the reading from sensors is a severe drain in terms of network resources and energy consumption. In this paper we separate the task of comparative global analysis to a central coordinator and use a reference PMax which is a normalized probability of individual source which re ects the current lifetime reliability of the sensors calculated at the cluster heads which then is compared with the current global reliability index based on all the PMax of cluster heads. As this implementation does not need any synchronization at the local nodes it uses compress once and stamp locally without any threshold such as application speci c calibration values (30oC) and the summarization can be application independent making it more a sensor network reliability index and using it independent of the actual measured values. I. INTRODUCTION The lifetime of sensor networks is typically factored into the resources it is deployed with, as by design it is unattended (i.e. no replacement of batteries) it coexists for many months to some years. The numbers of sensor nodes are typically run into hundreds to thousands in a large environmental monitoring application. As the number of nodes in such applications are enormous than typical networks it uses a clustering algorithms in which typically 20%-30% [5] of the nodes aggregate the data of the remaining 70%-80% [5] of the connected nodes. These cluster heads are data concentrators which can be modeled as a device CODEC, compressor/decompressor. The sensors which are attached to the nodes typically sense temperature, humidity and light. It is true, however, that the sensor measurements in the operation region are spatially correlated (since many environmental phenomena are) they tend to be very similar. In a CODEC a probability model is used which gives the highest probability to the most frequently occurred values reported by the sensors within the same cluster. This allows transmitting peak values with least amount of bits as the underlying compression algorithm assigns least number of bit for frequently occurring values. This probability distribution is send with the data values to the central coordinator. So each cluster head has a unique PMax [1] but not all cluster heads have the same measured value. As in recent development of VLSI and MEMS technologies have made it possible to package self-powered sensors and wireless radio components which together is capable of collecting and processing new sensor data for a period of many months to few years without replacing the internal batteries. The miniaturized sensors are sensitive to the available effective range to the energy consumed per bit. The instantaneous drain on the internal batteries is evident and the study shows that INTERNATIONAL JOURNAL ON SMART SENSING AND INTELLIGENT SYSTEMS, VOL. 1, NO. 4, DECEMBER 2008 INTERNATIONAL JOURNAL ON SMART SENSING AND INTELLIGENT SYSTEMS, VOL. 1, NO. 4, DECEMBER 2008 912 Energy consumed per bit to transmit = 100pj bit m2 Energy consumed to receive a bit = 50nj bit Transmit Energy = Eamp + d2 i,j (1) Where d is the distance to transmit between sensors i to sensor j, from this we get the Power rule based on the distance d of nearest sensor to the farthest away sensor, substituting in the above equation (1) and summing up the total energy required for all transmissions within one meter, two meters, three meters, four meters and extending up to (d-1) meters to a progressive sequence in equation (1.1) (as shown in Figure 1(a)). (a) (b) (c) Fig. 1. (a) Shows xed energy overhead with distance in transmission, (b) Plot of the theoretical expected lifetime using Power Law, being used to demonstrate ranking of popularity. To the right is the long tail, to the left are the few that dominate (also known as the 80-20 rule) and (c) Simulation results with sensor nodes up to 100 using routing algorithms during lifetime calculation. PowerLaw = 12 + 22 + 32 + 42 + ... + (d 1)2 + d2 (1.1) To sum up the total energy consumption we can write it in the form of Power Law equation (1.1) PowerLaw = f(x) = ax2 + o(x)2 (1.2) Substituting d-distance for x and k number of bits transmitted, we equate as in equation (1.2). PowerLaw = f(d) = kd2 + o(d)2 (1.3) Taking Log both sides of equation (1.2), log(f(d)) = 2 log d + log k (1.4) Notice that the expression in equation (1.4) has the form of a linear relationship with slope k, and scaling the argument induces a linear shift of the function, and leaves both the form and slope k unchanged. Plotting to the log scale as shown in Figure 1(b) Vasanth Iyer, Garimella Ram Murthy, M.B. Srinivas, Reliability in Large Wireless Sensor Networks 913 we get a long tail showing a few nodes dominate the transmission power compared to the majority, similar to the Wikipedia reference 80-20 rule of Power Law [4]. A. Scale invariance property in clustering for energy dissipation in RF based applications. As novel sensor applications are deployed to provide reliable data over the life-time [2] of the sensor network, with current routing algorithms [5] which are dependent to communicate with a central coordinator the instantaneous drain on the sensors are very demanding. A typical 9V battery communication for an RF sensor to transmit over 10 meters range will drain out as per the capacity table [3]. As shown in the previous equation in logarithmic scale for point to point transmission, we can extend this by clustering C nodes in the same range as shown in equation (1.6). f(d) = kd2 + o(d2) (1.5) f(cd) = k(cd2) = ckf(d) f(d) (1.6) From the equation (1.6) we can infer that the property is scale invariant even with clustering c nodes in a given radius k. This is validated from the simulation results [5] obtained in Fig 1 (c) which show optimal results(minimum loading per node[7]) when clustering is 20% as expected in theory(80-20 rule) from Fig 1 (b). It is true, however, that the sensor measurements in the operation region are spatially correlated, to be ef cient in a large sensor network partitioning the network into special clusters in done periodically and data needs to be aggregated locally by fusing all sensor reading at the cluster head. This data is periodically routed to a central coordinator which is a collaborative effort of all the active nodes in the sensor network. II. TRAINING DATA COMPRESSION ALGORITHMS A. Probability Model Most of the compression algorithms use a probability model based on the entropy of the source. Entropy of general source is given by H(S) = lim n 1 nGn, where (2) Gn = X X ... X P(X1 = i1, X2 = i2...Xn = in) log P(X1 = i1, X2 = i2...Xn = in) And is a sequence of length n from the source. In sensor each element in the sequence is independent and identically distributed (i.i.d.), then we can modify the entropy to the rst order to equation (2) H(S) = X P(X1) log P(X1) (2.1) B. Aggregation Model If the cluster size in n, from the cluster equation (8) then the entropy of data aggregation [2.1] is H(S) = n X i=0 P(X1) log P(X1) (2.2) In a lossless mode if there are no faults in the sensor network then we can show that the highest probability given by PMax is ambiguous if its frequency is n 2 otherwise it can be determined by a local function. C. Local Pmax functions |Pmax| = ( local, for Pmax n 2 (2.3a) global, for Pmax < n 2 (2.3b) INTERNATIONAL JOURNAL ON SMART SENSING AND INTELLIGENT SYSTEMS, VOL. 1, NO. 4, DECEMBER 2008 INTERNATIONAL JOURNAL ON SMART SENSING AND INTELLIGENT SYSTEMS, VOL. 1, NO. 4, DECEMBER 2008 914 Where n is total number of sensors placed in a cluster head. Here the probability of sampling similar values are highly correlated as in the case of environmental sensing the Pmax 0.5 then the entropy can be re-calculated as Hgood = 0.6 log2 0.6 0.4 log2 0.4 = 0.956 (2.4) per cluster head. For a good distributed clustering algorithm it uses 20% cluster heads [5] then the total entropy of the network will be 0.958 20 = 19.16 per round. To further calculate the algorithm ef ciency the most popular being Huffman coding [1] which has a lower and upper bound for a given Pmax. The Kraft-McMillan inequality there exist a uniquely decodable code with code word li. The average length of the code can be upper-bounded by using the right inequality. lavg = n X i=0 P(ai)li X P(ai)  log2 1 P(ai) + 1  (2.5) In fact it can be shown that if Pmax is the largest probability in the probability model then for Pmax < 0.5, the upper bound is H(S) = Pmax (2.6) While for Pmax < 0.5 then the upper bound is H(S) = Pmax + 0.086 (2.7) Now to calculate the average numbers needed for both Pmax using Huffman coding, when Pmax > 0.5 then using the above equations we get H(S) = Pmax + 0.086 = 0.958 + 0.4 + 0.086 = 1.44bps (2.8) If the symbol distribution is highly skewed then it takes few extra bits which are certainly the case in sensor networks with no faults. To nd the ef ciency of the coding we use Efficiency = lavg H(S) = 1.6 0.78 100 = 45% (2.9) more than the source entropy using Huffman coding. The data payload which is aggregated for each round with source entropy which needs a minimum of 0.78 bits at the cluster heads, actually it represents 1.6 bits still reducing the total number of bits to be transmitted to the coordinator after coding. III. PMAX, LOCAL VALUE-INDEPENDENT REDUNDANCY FILTER FOR DATA AGGREGATIONS AT THE CLUSTER HEADS Sensors networks when deployed has a predictable energy resource and uses a well distributed routing algorithm to aggregate its data to periodically send the sensed data to a central coordinator for further processing by using minimum resources. The goal of all the aggregation algorithms it to maximize the network reliability index which is a global threshold and re ects the health of the network. In the central coordinator mode we like to implement a classi er which allows maximizing on the redundancy of correlated data from each node as it learns during the lifetime of the network and maximizes on the fault by uniformly distributing the load on the node. This extends the useful lifetime of the sensor network by decreasing the number of energy holes in the network and corrupting good sensors readings. A. Localized Classi er - Fault = 0 In the life-time of sensor networks when it has no faults then the case (fault=0) the classi er s view will be as shown in Figure 2. This given the training model of the classi er a good partition of the life-time of the sensor network. As this information is needed latter when faults happen in speci c areas the cluster head transmit this data periodically to the central coordinator so it can send it to the host for latter comparisons. B. Localized Classi er - Fault <= n From the gure 3 it is clear that the localized aggregation function Pmax is effective and the classi er rule will be able to differentiate the good and bad readings ef ciently. As the fault rate increases (fault < n) then we have differentiation inside Vasanth Iyer, Garimella Ram Murthy, M.B. Srinivas, Reliability in Large Wireless Sensor Networks 915 Fig. 2. Simple classi cation of faulty sensors. Fig. 3. Cluster level classi cation of faults. one cluster as the sensor reading are correlated the cluster head is able to differentiate within the cluster boundaries. The classi er uses a local rule for this case. C. Localized Classi er - Fault >= n Now considering cases (fault > n) it uses sampled values from border nodes as well as some distributed nodes to compare and obtains a new global fault function. It is a signi cant task as shown in gure 4 to compare and correct the values to an expected value. The classi er uses a Bayesian approach in which it has to maximize on the posterior probability with existing prior probability. The classi er uses Pmax as a reference if it is not able to resolve then, it takes all the faulty nodes and uses the highest Pmax of the classi ed nodes and extracts the value as the best approximation for all the correlated sensors. We will say that we are trying to nd the correction c, out of all possible corrections, that maximizes the probability of c given the original measurement M: PMAX = P  C M  (3) By Bayes Theorem this is equivalent to equation below PMAXC = P M C  P(C) (3.1) P(c) the probability that a proposed correction c stands on its own. This is called the correlated cluster model. PMAXC = P M C  P(C) (3.2) This is called the correlated cluster model. P   M C  P(C) the probability that M would be measured by itself when the network meant c. This is the error model which is given by equation (3.2). Pmaxc,the control mechanism, which says to enumerate all feasible values of c, and then choose the one that gives the best combined probability score. Fig. 4. Classi cation boundary of Faulty sensors. Fig. 5. Simulation setup of K-neighborhood algorithms. INTERNATIONAL JOURNAL ON SMART SENSING AND INTELLIGENT SYSTEMS, VOL. 1, NO. 4, DECEMBER 2008 INTERNATIONAL JOURNAL ON SMART SENSING AND INTELLIGENT SYSTEMS, VOL. 1, NO. 4, DECEMBER 2008 916 IV. PMAX GLOBAL, FEATURE EXTRACTION USING PAST VALUE FILTER OF ALL THE DATA AGGREGATED BY THE CLUSTER HEADS AT THE CENTRAL CO-ORDINATOR The off-line process helps to train the model with the frequency of Pmax generated by the sensors as in equation (2.3a) and (2.3b) which are in the fault mode but could have good readings. Below is the pseudo-code for the training the features. Input: Measured Values from Sensors Output: Weighted Measured Values in snapshot-sensor.db Model = collection.defaultTopologyID() foreach f in features do Model[f] += 1 end Return model NSENSORS=train(Pmax( le( snapshot-sensor.db ).read()) At this point, NSENSORS [M] holds a count of how many times the measured value M has been seen. Now let s look at the problem of enumerating the possible correction c of a seen measured value M. It is common to talk of k-neighborhood distance between two sensors, this is shown in gure 5, the number of comparisons it would take to con rm its relative measurement. If Pmax does not get an instant match then it has to nd it in K-neighborhood distance-2. In the expanded search if found then it can approximate to measured value which closely matches with an existing sensor of the same Pmax. The process stops if there was no match for the seen frequency which is termed unrecoverable fault(unknown) as shown in gure 5. The next section deals with such cases using training database. Here is the pseudo code to return all measured corrections c that are K-neighbor distance away from sensor with a measure M: Input: Measured Value PMAX from Sensors Output: Corrected Value (C) using K-Neighborhood distance vector N1,N2... foreach i in range(n) for c in the sampled live measurement do KND1(Pmax) N+ (Pmax) return [Pmax [0:i] + c + Pmax [i+1] end With the implementation of this classi er we get an fault rate of 10%. The input and outputs corrections are shown in TABLE I. V. TRAIN CLUSTERING FEATURES In simple cases the sensor network uses local cluster head functions to predict the best correction but as the fault rate increases which is the case in large sensor network deployments. The central coordinator needs a way to cross validate the measured data to accept it or to make possible corrections by using the nearest correction found by using a global function. As this performed at the coordinator is not limited to any energy constraints and can use sophisticated methods such as training and feature classi ers. The main goal of the cross-validation logic as shown in gure 6 is to compare with live values as the primary factor, the more connected the network the better is its reliability. This factor would correct most of the anomalies which could occur due to bad calibration or external noise. Even the secondary factor which is to nd an equivalent connected path further away from the cluster head and use its measured value to correct the currently seen value at the faulty sensor. This correction process is used even more as the sensor network becomes widely faulty [8] where the existing of full clusters are minimum or none. VI. MULTI-FEATURE TRAINING We like to train the feature vectors in a way that it can handle most of the aggregation locally and minimally globally. The two features are PMAX which takes off the redundancy based on value locally equation (2.3a, 2.3b) and PMAXR which is based on the relevance of the current measurement when exceeding a given threshold based on the trained data. Table II shows example local temperature reading in this case the PMAX= 0.40, hence a high measured value of 75.0 can be safely ignored. This can be adapted ef ciently using a linear discriminate analysis (LDA) as shown in Figure 7, 8. As this needs a lot of computing resources we differ such machine learning techniques to be implemented globally. In table III the trained values are corrected and are shown in C and the new measured data set is shown as M . Here again the sensor measures a high value Vasanth Iyer, Garimella Ram Murthy, M.B. Srinivas, Reliability in Large Wireless Sensor Networks 917 TABLE I CLASSIFIER PERFORMANCE COMPARISON Sample K-Neighbor K-Neighbor Fault Rate Distance- 1 Distance- 2 Sim Run-1 98% 1% 1% Sim Run-1 91% 1% 8% TABLE II CLASSIFIER PERFORMANCE USING PMAX LOCAL Classi cation Temperature Value PMAX PMAXC M 40.00 0.10 M 32.00 0.20 M 44.00 0.30 0.40 (M) 75.00 0.10 Aggregated 35.00 0.40 at 75.0 with a corresponding of PMAX= 0.10 the classi er s other feature vector is matching similar readings reported earlier or newly available fused values from boarder nodes, in this case there is a such high relevant values in the training data set so comparing the measured data set the training data set we have equation (24). Substituting P(B) = 0.30 and P(A) = 0.1, the conditional probability of P   A B  = 0.30 which is relevant to being a reportable event with high probability. PMAXC = 0.04, PMAXR = 0.30 (6) P  A B  = P A B P(B)  (6.1) The global classi er updates the new measured value over Pmaxc, as it is highly a probable event to be reported at the central coordinator. In the faulty case which is given in table IV we similarly shown as before. Pmaxc = 0.40, Pmaxr = 0.30. Substituting the values of P(A)= 0.30 and P(B)=0.0 in equation (6. 1) we get a faulty case as Pmaxr = not effected. The high value measured is treated as faulty as in this case the training data set it cannot nd any matching value. Figure 10, shows LDA based classi er using dot product. Fig. 6. Learning and training during correction as a global function. Fig. 7. Measured values using local PMAX local LDA, with 1, 2. VII. SORTING OUT FAULTY EVENTS The above methods allow dividing good measured value and the faulty values measured data locally and globally. As the network becomes large there is a lot of redundant data and it hard to scale which makes it harder to provide a good solution. One of the alternate methods [7] used to have a better scale is using fuzzy logic. But here we like to de ne an event. An event can be de ned in terms of entropy or self information or surprisal of message m. Where p(m) = Pr(M = m) is the probability that message m is chosen from all possible choices in the message space M. From equation (7) we can de ne an INTERNATIONAL JOURNAL ON SMART SENSING AND INTELLIGENT SYSTEMS, VOL. 1, NO. 4, DECEMBER 2008 INTERNATIONAL JOURNAL ON SMART SENSING AND INTELLIGENT SYSTEMS, VOL. 1, NO. 4, DECEMBER 2008 918 Fig. 8. Classi cation aggregated values using PMAXLDA 1, 2. Fig. 9. Measured data set across sensor clusters. Fig. 10. Classifying local measured data with PMAXR global thresholds at the central coordinator which is based on feature extraction and has unlimited processing power. event as the lowest probability reported by a sensor in a clustered environment. In our data aggregation process we use Pmax which typically has a higher probability of 0.3. If the sensor measured probability has a lower probability such as 0.1 then it is a highly probable event from equation (7). Now that we can have such events reported across many clusters we need to determine if it is a faulty event or an actual true event. To make it fault-tolerant we extend the same Bayesian learning method such as equations (3.1,3.2) to come with a fault scoring rule. This allows the algorithm to more fault-tolerant in a distributed network. So for this we assume that the sensors have xed register based accuracy(8-16 bits) and we can assign a weighted score based on a probability that a expected bit is set or not set for an measurement to be accurate compared to Pmax mask (measured value) bits. From equation (7.1) we have that given an event has been reported in a cluster, i.e. P(B). I(m) = log2 1 p(m) = log2(p(m)) (7) Then we can rewrite this as P  A B  = P AggrV alue Event  = P V alueAggr Event Event  (7.1) From the previous de nition of an event we know Pevent = 0.1,so we need to nd out the numerator PV alueaggr Event. As the measured values are internally stored in bits, let us assume for a register with m bits, then it is equiprobable for any bit to set giving a probability of 1 m1 for every position of the bits in the register. Let us say the pattern of expected bits from a measurement has a probability p, and also an event has been reported then the fault probability of the event can be calculated by each bit with the faulty bit at the positions 1 m1 , 1 m2 , ... 1 m3 the mismatched bits have a (1-p) probability. This is the intermediate fault score for each and every mis-matched bit present in the current set of measured value. Then the fault conditional probability given the expected pattern is calculated by summing up the score of the faulty bits as shown in equation Vasanth Iyer, Garimella Ram Murthy, M.B. Srinivas, Reliability in Large Wireless Sensor Networks 919 (7.2) which is then ranked to get a nal fault score. Fault Score = 1 m1 (1 p)faultybit(m1)+ 1 m1 (1 p)faultybit(m2)+ 1 m1 (1 p)faultybit(m3)+ ... 1 m1 (1 p)faultybit(mn) (7.2) In a clustering environment Pmax = p, P(event local) = 0.1, also the average self-information that is an event is equiprobable in the whole network con gured into c clusters is given by P(event global) = 1 c This allows to evenly scoring fault bits which have high signi cance such as MSB compared to least signi cant bits which have least signi cance. If a fault score is > 0.5 then the event is ignored otherwise it is reported even though is carries a self-information of as low as 0.1. This event detection and reporting algorithm is adaptive as it is based on, if the number of clusters in the sensor network is large then an event detected is more precise at the central coordinator as shown in equation (7) by substituting the P(event global)value. TABLE III CLASSIFIER PERFORMANCE USING PMAX- PMAXR-RELEVANT Classi cation Temperature Value PMAX PMAXC PMAXR C 57.20 0.10 C 53.60 0.10 C 55.60 0.20 0.30 75.00 C 75.00 0.30 0.30 75.00 C 55.40 0.20 M 40.00 0.10 M 32.00 0.20 M 44.00 0.30 M 35.00 0.40 0.40 (M) 75.00 0.10 Relevant 75.00 0.10 0.30 TABLE IV CLASSIFIER PERFORMANCE USING PMAX - PMAXR -FAULTY Classi cation Temperature Value PMAX PMAXC PMAXR C 57.20 0.10 C 53.60 0.10 C 55.40 0.20 C 58.40 0.30 0.30 58.00 C 55.00 0.20 M 40.00 0.10 M 32.00 0.20 M 44.00 0.30 M 35.00 0.40 0.40 (M) 75.00 0.10 Fault 35.00 0.40 0.0 VIII. SUMMARY Theoretically we show that study of wireless sensor network energy management is a signi cant part of solving the reliability issue. Also we show that the energy constraint is network size invariant and converges to an optimal cluster size. To achieve a INTERNATIONAL JOURNAL ON SMART SENSING AND INTELLIGENT SYSTEMS, VOL. 1, NO. 4, DECEMBER 2008 INTERNATIONAL JOURNAL ON SMART SENSING AND INTELLIGENT SYSTEMS, VOL. 1, NO. 4, DECEMBER 2008 920 balance we use compression algorithms to bring down the transmitted bits to known entropy at the cluster head as in equation (2.9). The same model serves as the reliability index for a central classi er which uses multi-feature and brings down the fault rate to a minimum in our case around 10%. This technique can also be shown that in training data, as the data is measured with respect to a given source entropy at each cluster head then the training data set over time for a large sensor network has the self mutual-information of all the nodes in the network with respect to the new measured values. This helps to further classify the current data set by reducing any faults due to measurements locally by using conditional probability of the new measured value given a similar measured value is reported by adjacent cluster heads. To scale a machine learning system to adapt to faulty sensors we have designed a rank based fault scoring system to classify good and bad events in a widely faulty environment and also has a precession factor calculated by the number of clusters. The classi er s design is based on the probability model and is independent of the actual measured value, some of the future work [7] extends this work into a fault-tolerant classi er for the class of distributed WSN clustering algorithms. ACKNOWLEDGMENT The authors would like to thank for the tools made available in the area of Data Compression and Pattern Recognition at International Institute of Information Technology, Hyderabad, India. We like to thanks Prof. Subash Mukhopadhyay from University of Massey, New Zealand for this constructive comments [9] in time that improved the quality of the paper for submission to the new sensor journal. REFERENCES [1] Introduction to Data compression. Khalid Sayood. 2nd Edition, Morgan Kaufmann Series in Multimedia Information and Systems (Hardcover) [2] Software Stack Architecture for Self-Organizing Sensor Networks, Vasanth Iyer, G.Rama Murthy and M.B. Srinivas- ICST 2007, Palmerston North New Zealand. [3] Battery drain (http : //www.techlib.com/reference/batteries.html) [4] Power Law math (http : //en.wikipedia.org/wiki/P ower law). [5] Environmental measurement OS for a Tiny CRF-STACK Used in Wireless Network. Vasanth Iyer, G.Rama Murthy and M.B. Srinivas- Sensors & Transducers Journal-April 2008, ISSN 1726-5479 2006 by IFSA. [6] Distributed Wireless Sensor Network Architecture: Fuzzy Logic based Sensor Fusion, G.Rama Murthy, Vasanth Iyer. ISBN 978-80-7368-387-0 pages 71-78 VOL II, Proceedings of the 5th EUSFLAT Conference, Ostrava, Czech Republic, 2007. [7] Min Loading Max Reusability Fusion Classi ers for Sensor Data Model, Vasanth Iyer, G.Rama Murthy and M.B. Srinivas, 2008 IEEE SENSORCOMM, August 25, 2008 - Cap Esterel, France. [8] B.Krishnamachari, S.S. Iyengar, IEEE.,Distributed Bayesian Algorithms for Fault-Tolerant Event Region Detection in WSN, MARCH 2004. [9] Vasanth Iyer, Garimella Rammurthy and M.B. Srinivas., Training Data Compression Algorithms and Reliability in Large Wireless Sensor Networks. IEEE International Workshop on Embedded Processors, Sensors, and Actuators (EPSA-2008) to be held in Taichung, Taiwan, June 11-13, 2008. Vasanth Iyer, Garimella Ram Murthy, M.B. Srinivas, Reliability in Large Wireless Sensor Networks 921