Machine Learning and Dataming Algorithms for Predicting Accidental Small Forest Fires Vasanth Iyer , S. Sitharama Iyengar , N. Paramesh , Garmiela Rama Murthy , Mandalika B. Srinivas International Institute of Information Technology, Hyderabad, India - 500 032 Louisiana State University, Baton Rouge, LA 70803, USA University of New South Wales, Sidney, Australia Brila Institute of Technology & Science, Hyderabad Campus, Hyderabad-500078, India vasanth,rammurthy{@research.iiit.ac.in},iyengar@csc.lsu.edu,paramesh@cse.unsw.edu.au,srinivas@bits-hyderabad.ac.in Abstract Extracting useful temporal and spatial patterns from sensor data has been seen before, the technical basis of Machine learning with Data mining is studied with the evidence collected uniformly over many years and which allow using users perspective in collected evidence. This model helps in probabilistically forecasting res and help forest department in planing day to day schedules. Using a model to predict future events reliably one needs to collect samples from sensors and select a feature, which does have any particular bias. Due to practicable problems most of the collected data have 80% of attributes missing and the remaining has numeric values, which are hard to discretization. To adapt to such limitations, we use nominal data type, which allows better understanding of the temporal and spatial features, which are learnt. We encounter several practicable limitations as forest res events are very rare and manual classi cation is extremely costly. Another is the unbalanced nature of the problem of the many forest re events many are of the burnt area is very small and gives skewed distribution. Most of the examples naturally group into batches, which are collected from evidence satellite photography and collaborative reports from national parks departments. The second set of database was collected from the meteorological weather station about several weather observations, which are located very close to the reported res. Finally, the compiling task is to serve as a lter and provide the user to vary the false alarm rate. We show by regression analysis of the compiled dataset that the forest re classi er has a minimum false alarm rate when including temporal features. The machine learning algorithms success- fully classi es accidental small res with 85% reliably and large res by a much lower accuracy of 30%. Index Terms Machine Learning, Datamining, Naive Bayes, Forest res, Fire Weather Index (FWI), Temporal Patterns, WEKA machine learning framework. I. INTRODUCTION Accidental small forest re can lead to heavy loss of precious natural reserves in protected lands in which many different species thrive due to their balanced ecol- ogy. Tropical rain forests are also a factor in keeping the sensitive balance global warming trends seen recently due to heavy deforestation due to human needs. One of objective of Datamining is to allow modeling the users perspective such as temporal properties, which are cause and effect of forest res, which allows in reducing false detection. The hidden patterns are mined, which allows to nd the underlying hidden structure of the data. This allows learning the concepts needed for forest res classi cation. The features extracted of the predicted class by means of datamining allows to apply many machine learning algorithms to the transformed data. This framework forms the technical basis for the supervised and unsupervised classi cation. Temporal query properties like weekday and week- ends help probabilistically bias the predicted outcome of class variables to be classi ed. As a small human accidental re or a possibility of occurrence of large natural re disasters can further be classi ed according to the users choice. Attribute value transformations are equally important when formulating attribute depen- dencies within a weather class [4,5,6,7] nominal values such as cool, windy and high humidity for successful formulation of machine learning rules. Following motivation the rest of the paper with is organized as follows. In Section III, we evaluate the per- formance of machine learning algorithms and develop a weak learner for temporal features. Section IV presents initial basis for user queries without signi cant error analysis, that is without any ranking criteria. In Section V, we evaluate Naive Bayes [1] with Tree based classi ers and compare the method on the task of accidental re prediction. Section VI investigates alternative feature and computational aspects of the method respectively and explains the results. Section VII concludes the paper. II. STATE OF THE ART The historic information recorded by it does not reveal any hidden patterns to calculate the likelihood of forest res. Classi ers model depends on accurate class condi- tional probabilities but in practice, samples are limited, 116 SENSORCOMM 2011 : The Fifth International Conference on Sensor Technologies and Applications Copyright (c) IARIA, 2011. ISBN: 978-1-61208-144-1 most of the estimates are approximate which further biases the sample search space. With limited samples, the Bayesian method does the better estimation of the class conditional probabilities, when compared to maximum likelihood method. The internal representation of the classi er data model uses a weighted term, and best evaluates the quantity purity of the labeled class. This provides classi cation of the same-labeled pattern with insuf cient samples, with pure and impure groups and helps its internal ranking. The internal representation captures the hidden pattern of the training samples, once the hidden patterns are quantitatively veri ed with a base classi er such as Nave Bayes, these representative patterns are further classi ed by user speci ed attributes such as which month and which day the particular pattern has maximized the likelihood of a phenomenon such as re event. There are standard benchmarks for performance com- parison of classi ers and Bayes gives the lowest error rate compared to others. We also study the kappa score, which compares our classi er with a J48 tree classi er for the same input data set and normalizes using the results of the confusion matrixes. A high kappa score is generally preferred for a classi er to be ef cient, which needs using of good pre-processing algorithms. Since sensor data are, highly unreliable most of well- designed classi ers perform badly and cannot adapt to the sensor data stream. By using post-processing of miss-classi ed samples and identifying falsely classi ed data also called outliers, we further improve the relia- bility. From authors previous work [1], we have shown data aggregation eliminate redundancies and improves reliability in sensor network performance. The current ML algorithm focuses on event aggregation over a long period of time from user reports and collaborative sensor network stream, which have been further classi ed to a particular application. We study the effects of predicting forest res in a given region using sensor aggregated data. III. MACHINE LEARNING RULES Consider the concept leaning, in particular the learner considers some nite hypothesis [6] space H de ned over instance space X, in which the task is to learn some target concepts c : X > 0, 1. As we are building a re event predictor from the sensed data, we assume that the network learner is given some sequence of training measurements ((X1, d1)...(xm, dm)) where xi is some instance from X and where di is the target value of di = c(xi). As we are learning from a knowledge base such as data repository the sequence of instances (x1...xm is held xed, so that the training data D can be written as the sequence of target values D = d1...dm. P(h|D) = P(D|h)P(h) P(D) (1) X Y Fig. 1. location of inventorised geomorphosites in the montesinho natural park. hMAP = argmaxh HP(h|D) (2) The assumptions for the concept learning follows: The training data D is noise free. The target concept c is contained in the hypothesis space H. We have no a priori reason to believe that any hypothesis is more probable than any others. Since we assume noise free samples, the rst hypothesis can be formed for the detection of forest res [2] for an approximate target function V as shown in equation (3) V (Fire Location) = XPos + Y Pos (3) where every incident of forest re is documented and its location in terms of <X,Y> [2] are recorded. The learning algorithm uses a boosting [5,6] method to learn from the forest re events and its corresponding correlated sensor measurements. The model does not use real-time sensor inputs and data samples to classify but on the other hand it uses recorded re events and probabilistically predict the new sensor input closest to the already seen training sample (data aggregated over time) using Naive Bayes or Tree classi ers. This computational model can further post-processed using supervised learning to improve on the purity of the classes and detect any outliers which may create false alarms. The unreliability of accurately detection real and outlier events is an open problem in sensor networks. The above equation is dependent on <X,Y> positions in Figure 1 map, for grid of 10x10 it may need 100 combinations of every other dependent variable making the model unfriendly. The estimated formula of the above equation (3) can estimated in temporal terms for the Fire Event as shown in equation (4). Vtrain(FireEventDay of week) (4) V (FireEventDay of week)Temporal Variable + V (FireEventDay of week)Correlated measurements Temporal Variables = Month of the year + (5) Day of the week 117 SENSORCOMM 2011 : The Fifth International Conference on Sensor Technologies and Applications Copyright (c) IARIA, 2011. ISBN: 978-1-61208-144-1 Correlated measurement = temperature + (6) humidity + wind + rain Class res = {accidental; small, medium, large} (7) A. Estimating training values with sample data Sample datasets are based on UCI forest re repos- itory. The equation representing the Bayes probability model of the hypothesis is given in equation (1). In our case the hypothesis to be maximized is shows in equation (2) for a four class classi cation, as shown in equation (7). The assumption here is that the training set D is a unbiased representation to learn the concept c and can estimate the inputs xi. The previously de ned dependent variable Fire Location, which is used to es- timate given the independent correlated measurements and its relation to the temporal attributed are given in equations (4), (5) and (6). The target concepts are present in the training samples and, we like to see the in uence of adding sensor measurements to further accurately learn the concepts of the human induced accidental res versus the more natural accruing types of the medium and large res. For the sake of clarity of machine learning domain we convert the correlated sensor data to nominal [5] types, as illustrated below. temperature = {cool; mild; hot} (8) humidity = {low; medium; high} (9) wind = {true; false} (10) The model estimation of the the target function with weights w1, w2 as shown allows to minimize the training error, where x1, x2 are temporal and correlated measure- ments. V = w1x1 + w2x2 (11) The learning algorithm needs to de ne the best t for the given hypothesis and adjust the weights to minimizing the error and miss classi cations. E (Vtrain(FireEvent) V (FireEvent))2 (12) B. Algorithm complexity Search space consists of all the possible patterns of the features, given our data model, 3 3 2 4 = 72 possibilities for each rule when using attributes 3 for temperature, 3 for humidity, 2 for wind for 4 classes of re categories. As there are 517 rules from the collected dataset instances the complete search space [5] will have nPr = 72522 10969 different possibilities. To minimize the complexity of search space, we can further cut down on the sample instances by using spatial clustering and removing any redundancies in similar features. Given the <X,Y> positions, we can cluster into groups the possible res types into accidental small res and others which have medium and larger burnt area as large res. FIRE TYPES RECORDED Accidental (AF) 247 Small (SF) 175 Medium (MF) 71 Large (LF) 24 TABLE II TARGET VARIABLE OCCURRENCES. As measuring ambient phenomena are correlated, we expect them to be independent, then all the i.i.d s can be aggregated to nCr = 517 C72 1089 possibilities, were in this case the combination is calculated. As these methods are used with pre-processing to reduce data overloads in the model further real valued dataset search space optimization is possible. Domain knowledge as in the case of WSN can be used practically to reduce the complexity of machine learning algorithms if well studies and calibrated. To judge the affectiveness of the model and the classi cation effectiveness, we initially rely on real-valued numeric model such as [1] to estimate the errors. In contrast to the previous approach, we use nominal values as de ned in equations (8), (9) and (10) to build a tree classi er and further reduce errors. IV. NAIVE BAYES One can use Naive Bayes [5], which by design pre- sumes the class densities, which have been determined and accurate. The model calculates the class conditional probabilities of the input feature vectors. To understand the underlying skewed structure of the dataset, we fur- ther create thresholds for accidental small res compared to medium and large res as shown in Table II. So we have the four possible values for the target variable as shown in equation (7). A. User query To validate the model let us predict the outcome of a peak month, from the dataset [2] August has signi cant number of reported res compared to other months. Esti- mating the probabilities of re events given the attribute values for the class ? = {Month = August; Day = Monday} {Temprature = Cool; Humidity = High; Wind = True} The estimated class conditional densities for the inde- pendent variables temperature, humidity and wind con- ditions are calculated using temporal attributes month for the dataset are shown in Table II. The datasets further is explored using two temporal variables, which are month and the day of the week as shown in Table I and Table IV. The temporal variables introduced into the dataset helps gain the insight of users dependencies with re prediction model. gi(x) = P( i x) = p(x i)P( i) i=4 i=0 p(x j)P( j) (13) 118 SENSORCOMM 2011 : The Fifth International Conference on Sensor Technologies and Applications Copyright (c) IARIA, 2011. ISBN: 978-1-61208-144-1 Burnt Area(hectors) AUG MON TEMP HUMIDITY WINDY PRIOR PROB PREDICTOR VAR GT 1h 0.34 0.14 0.46 0.17 0.42 0.47 57% GT 1h LEQ 10h 0.39 0.15 0.35 0.13 0.38 0.33 25.0% GT 10h LEQ 50h 0.30 0.14 0.43 0.19 0.50 0.13 17.0% GT 50h 0.33 0.08 0.16 0.08 0.37 0.04 0.02% TABLE I POSTERIORS PROBABILITIES FOR BACKGROUND WEATHER DATA FOR THE PEAK MONTH AUGUST. FIRE TYPE MONTH=AUG Accidental 0.004 Small 0.002 Medium 0.001 Large 0.00004 TABLE III LIKELIHOOD OF FIRES FOR THE MONTH OF AUGUST. DAYS ACCIDENTAL SMALL MEDIUM LARGE MON 35 27 10 2 TUE 28 21 11 4 WED 22 24 5 3 THU 30 21 9 1 FRI 42 31 12 0 SAT 42 24 11 7 SUN 48 27 13 7 TOTAL 247 175 71 24 TABLE IV POSTERIORS PROBABILITIES FOR TEMPORAL FEATURE DAY OF THE WEEK. Substituting the corresponding highlighted values from Table 1 through to Table IV in the above equation (13), we get the posterior probability of accidental small re fireaccidental = 0.0007547 0.003565 = 57% (14) fireSmall = 0.000333 0.003565 = 25% (15) firemedium = 0.000223 0.003565 = 0.17% (16) firelarge = 0.000000287 0.003565 = 0.02% (17) The posterior probabilities for the month of August for the data collected in Portugal [2], the likelihood of ac- cidental small res are very high. From cross-validating from the known fact that in summer the likelihood of wild res are higher the Bayes rule is able to classify the dataset for accidental and small res with high accuracy. We use a simulation framework in the next sections to further prove our intial conclusion from the datasets, it is shown that the training time for Naive Bayes scales linearly in both the number of instances and number of attributes. V. TREE CLASSIFIER In this section, we will focus on the domain rules, which are applicable to the learning system. Tree clas- si ers lend itself to use ML rules [6] when searching the hypothesis by further branching on speci c attributes. The design of such a classi er needs to sort the weights or entropies [5] of the attributes, which is the basis of its classi cation effectiveness. ID3 is a popular tree classi er algorithm, to implement ID3 as illustrated in Figure 2 and Table X with our attributes. Let (S) be a collection of samples then using the tree algorithm, which uses entropy to split its levels is given by Entropy(S) = i=c i=0 p(i) log2 p(i) (18) Let us assume a collection (S) has 517 samples [2] with 248, 246, 11 and 12 of accidental, small, medium, large res respectively. The total entropy calculated from equa- tion (18) is given by Entropy(S) = 248 517 log2 248 517 + 246 517 log2 246 517 + 11 517 log2 11 517 + 12 517 log2 12 517 = 1.23 A. Attribute selection ID3 uses a statistical property called information gain to select the best attribute. The gain measures how well the attribute separates training targeted examples, when classifying them into re events. The measure of purity that we will use is called information and is measured in units called bits. It represents the expected amount of information that would be needed to specify whether a new instance should be classi ed accidental, small, medium or large res, given that the example reached that node. The gain of an attribute is de ned by and illustrated in Table V. Using the calculated attribute for information gain we show that temp attribute is used before the wind attribute to split the tree after the tree root. Gain(S, A) = Entropy(S) i=c i=0 Sv |S|Entropy(Sv) (19) Entropy(SHot) = 9 36 log2 9 36 + 23 36 log2 23 36 + 3 36 log2 3 36 + 1 36 log2 1 36 = 119 SENSORCOMM 2011 : The Fifth International Conference on Sensor Technologies and Applications Copyright (c) IARIA, 2011. ISBN: 978-1-61208-144-1 1.282 Entropy(SMedium) = 23 96 log2 23 96 + 65 96 log2 65 96 + 3 96 log2 3 96 + 5 96 log2 5 96 = 1.175 Entropy(SCool) = 117 269 log2 117 269 + 146 269 log2 146 269 + 2 269 log2 2 269 + 4 269 log2 4 269 = 1.05 Entropy(temp) = 43 517 1.282 + 139 517 1.175 + 335 517 1.05 = 1.08 Gain(S, temp) = 1.23 1.08 = 0.192 Entropy(SHIGH) = 162 249 log2 162 249 + 72 249 log2 72 249 + 8 249 log2 8 249 + 7 249 log2 7 249 = 1.1952 Entropy(SLOW ) = 68 133 log2 68 133 + 59 133 log2 59 133 + 2 133 log2 2 133 + 4 133 log2 4 133 = 1.24 Entropy(wind) = 361 517 1.1952 + Fig. 2. Tree classi er and attribute view. Month Temp Wind Not shown info: 1.08 info: 1.20 Not shown gain: 1.23-1.08 gain: 1.23-1.08 = 0.192 = 0.025 TABLE V GAIN RATIO CALCULATION FOR TREE IN FIGURE 2. 156 517 1.24 = 1.20 Gain(S, wind) = 1.23 1.20 = 0.025 The internal tree representation for m attributes from n samples will have a complexity of O(lg n), with in- creasing inputs, given by parameter n, the height of the tree will not grow linearly as in the case of Naive Bayes. On the other hand complexity of building a tree will be O(mn lg n) VI. SIMULATION Open-source workbench called WEKA [3] is a useful tool to quantify and validate results, which can be duplicated. WEKA can handle numeric attributes well, so we use the same values for the weather data from the UCI [4] repository datasets. The class variable has to be a nominal one, to allow WEKA [3], we convert all re types to 0 or 1 . Where 0 is of accidental small re and 1 is for large res making it a two class classi er, the results are shown as confusion matrix in Table VIII and Table IX. Naive Bayes correctly classi es accidental and small res(209 out of 247) were as the J48 Tree classi er does far more, 219 out of 247. As WEKA uses kappa [3] stats for evaluating the training sets, a standard score of > 60% means training set is correlated, using J48 simulation, we get 53.56% just below the standard. The comparison on results shows that tree classi er does better than Naive Bayes by 25% overall and equally well for accidental and small res as shown in Table VI and Table VII, when randomly tested it falls just short of the expected 60%. Therefore using sensor network measurements accidental and small res can be predicted reliably. 120 SENSORCOMM 2011 : The Fifth International Conference on Sensor Technologies and Applications Copyright (c) IARIA, 2011. ISBN: 978-1-61208-144-1 WEKA Stats Results Summary Correctly Classi ed Instances 267 51.64% Incorrectly Classi ed Instances 250 48.35% Kappa statistic 0.1371 Mean absolute error 0.3022 Root mean squared error 0.3902 Relative absolute error 94.86% Root relative squared error 97.84% Total Number of Instances 517 TABLE VI EVALUATION ON TRAINING SET FOR NAIVE BAYES. WEKA Stats Results Summary Correctly Classi ed Instances 373 72.14% Incorrectly Classi ed Instances 144 27.85% Kappa statistic 0.5356 Mean absolute error 0.1938 Root mean squared error 0.3113 Relative absolute error 60.83% Root relative squared error 78.04% Total Number of Instances 517 TABLE VII EVALUATION ON TRAINING SET FOR J48 TREE CLASSIFIER. A. Simulation analysis WEKA attribute statistics and its effective correlation score. Table VI and Table VII show kappa and other com- parison statistics for Naive Bayes and J48 tree classi er. B. Error analysis Equation (12) speci es the model error and the Confu- sion matrix from the simulation score are shown in Table VIII and Table IX, upper bound of small re(AF+SF) has over 80% accuracy for J48-Tree and 61% for Naive Bayes. The corresponding baseline performances including all res categories is 72.1% for J48-Tree and Naive Bayes is 51.64%, which is due to large res not correlated. 1) Correlation of attributes: From statistical point of view if the attributes have similar values then it creates high bias creating what is called over- tting error during learning. In our case temp and humidly may have similar values and needs to be avoided and substituted with a suitable attribute. To pre-process and analyze, we use all the available in the dataset and WEKA provides the attribute selection as illustrated in Table X. We use the attribute selection wizard of WEKA to nd out the best match. The analysis shows from Table X that LF MF SF AF LF 0 1 7 16 MF 0 5 12 54 SF 0 7 53 115 AF 0 0 38 209 TABLE VIII CONFUSION MATRIX FOR NAIVE BAYES USING TRAINING SET. LF MF SF AF LF 7 0 7 10 MF 0 29 15 27 SF 1 7 118 49 AF 0 5 23 219 TABLE IX CONFUSION MATRIX ON TRAINING SET FOR J48 TREE CLASSIFIER. Number of folds (%) No. Attribute 10(100 %) 1 month 1( 10 %) 2 day 0( 0 %) 3 temp 0( 0 %) 4 RH 0( 0 %) 5 wind TABLE X ATTRIBUTE SELECTION 10 FOLD CROSS-VALIDATION (STRATIFIED) the Month(100%), Day(10%) and Wind(0%) are highly dependent on the precision. As most of the attributes are nominal it lends more to a tree classi er, which are more exibility in handling nominal types by design. VII. CONCLUSION AND FUTURE WORK The future research work will focus on how to rank sensor queries with high reliability which otherwise be biased due to unveri able outliers present in the form of noise, spikes and false positives in the time- series data. The training sample sorting allows to weigh the precession versus relevant evidence based on the ranking criteria, such has F-scores and correlated Fire Weather Index (FWI) to further compare the likelihood of predicting large re events reliably. The statistical analysis of the data collection helps in exploring the higher and lower bounds of the FWI ranges and its corresponding robustness to predict large res using our implemented algorithms. VIII. ACKNOWLEDGEMENT One of the authors like to thank Shailesh Kumar of Google, Labs, India for suggesting the machine learning framework WEKA and UCI for providing the most needed datasets on forest res. Dean P.J. Narayanan has been a mentor in introducing dimension reduction and attribute selection. The rst author like to express appre- ciation and support from Dr. S.S Iyengar for funding this work under his LSU research grant. Authors also like to thank the anonymous reviewer s comments which has improved the nal quality of the submission. REFERENCES [1] Vasanth Iyer, S.S. Iyengar, G. Rama Murthy, and M.B. Srinivas. INSPIRE-DB: Intelligent Networks Sensor Processing of Infor- mation using Resilient Encoded-Hash DataBase. SENSORCOMM 2010, The Fourth International Conference on Sensor Technologies and Applications, Venice, pp. 363-368. [2] Paulo Cortez and Anibal Morais. A Data Mining Approach to Predict Forest Fires using Meteorological Data. Department of Information Systems-R&D Algoritmi Centre, University of Minho, Portugal. [3] WEKA Machine learning software. http : //www.cs.waikato.ac.nz/ ml/weka [Accessed May 15th, 2011]. [4] Frank, A. and Asuncion, A. (2010). UCI Machine Learning Reposi- tory [http://archive.ics.uci.edu/ml]. Irvine, CA: University of Cal- ifornia, School of Information and Computer Science. [Accessed May 20th, 2011]. [5] Ian H. Witten and Eibe Frank. Datamining, Pratical machine learn- ing. Elsevier 2005. [6] Tom M. Mitchell. Machine Learning. MaGRAW-Hill Publications 1997. 121 SENSORCOMM 2011 : The Fifth International Conference on Sensor Technologies and Applications Copyright (c) IARIA, 2011. ISBN: 978-1-61208-144-1