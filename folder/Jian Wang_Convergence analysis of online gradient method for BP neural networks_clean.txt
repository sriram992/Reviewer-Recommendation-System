This article appeared in a journal published by Elsevier. The attached copy is furnished to the author for internal non-commercial research and education use, including for instruction at the authors institution and sharing with colleagues. Other uses, including reproduction and distribution, or selling or licensing copies, or posting to personal, institutional or third party websites are prohibited. In most cases authors are permitted to post their version of the article (e.g. in Word or Tex form) to their personal website or institutional repository. Authors requiring further information regarding Elsevier s archiving and manuscript policies are encouraged to visit: http://www.elsevier.com/copyright Author's personal copy Neural Networks 24 (2011) 91 98 Contents lists available at ScienceDirect Neural Networks journal homepage: www.elsevier.com/locate/neunet Convergence analysis of online gradient method for BP neural networks Wei Wu a, , Jian Wang a,b, Mingsong Cheng a, Zhengxue Li a a School of Mathematical Sciences, Dalian University of Technology, Dalian, 116024, PR China b School of Mathematics and Computational Sciences, Petroleum University of China, Dongying, 257061, PR China a r t i c l e i n f o Article history: Received 28 March 2010 Received in revised form 8 September 2010 Accepted 9 September 2010 Keywords: Neural networks Backpropagation learning Online gradient method Weak convergence Strong convergence a b s t r a c t This paper considers a class of online gradient learning methods for backpropagation (BP) neural networks with a single hidden layer. We assume that in each training cycle, each sample in the training set is supplied in a stochastic order to the network exactly once. It is interesting that these stochastic learning methods can be shown to be deterministically convergent. This paper presents some weak and strong convergence results for the learning methods, indicating that the gradient of the error function goes to zero and the weight sequence goes to a fixed point, respectively. The conditions on the activation function and the learning rate to guarantee the convergence are relaxed compared with the existing results. Our convergence results are valid for not only S S type neural networks (both the output and hidden neurons are Sigmoid functions), but also for P P, P S and S P type neural networks, where S and P represent Sigmoid and polynomial functions, respectively. 2010 Elsevier Ltd. All rights reserved. 1. Introduction Artificial neural network has been a hot topic in recent years in cognitive science, computational intelligence and intelligent information processing. Backpropagation (BP) is the most broadly used learning method for feedforward neural networks. It was first proposed by Werbos (1974) in his Ph.D. thesis, and has been rediscovered several times (LeCun, 1985; Parker, 1982; Rumelhart, Hinton, & Williams, 1986). There are two practical ways to implement the backpropagation algorithm: batch updating approach and online updating approach. Corresponding to the standard gradient method, the batch updating approach accumulates the weight correction over all the training samples before actually performing the update. On the other hand, the online updating approach updates the network weights immediately after each training sample is fed. Some authors compare the two different training schemes for feedforward neural networks (Heskes & Wiegerinck, 1996; Nakama, 2009; Wilson & Martinez, 2003). Heskes and Wiegerinck (1996) reveal several asymptotic properties of the two schemes. Wilson and Martinez (2003) explain why batch training is almost always slower than online training (often orders of magnitude slower) especially on large training sets. Nakama (2009) theoretically analyzes the convergence properties of the two schemes applied to quadratic Project supported by the National Natural Science Foundation of China (No. 10871220). Corresponding author. E-mail address: wuweiw@dlut.edu.cn (W. Wu). loss functions and shows the exact degrees to which the training set size, the variance of the per-instance gradient, and the learning rate affect the rate of convergence for each scheme. There are three approaches for online training of BP neural networks according to different fashions of sampling. The first approach is OGM-CS (completely stochastic order): At each learning step, one of the samples is drawn at random from the training set and presented to the network (Finnoff, 1994; Heskes & Wiegerinck, 1996; Terence, 1989; Wilson & Martinez, 2003). The second approach is OGM-SS (special stochastic order): In each training cycle, each sample in the training set is supplied in a stochastic order to the network exactly once (Heskes & Wiegerinck, 1996; Li & Ding, 2005; Li, Wu, & Tian, 2004; Nakama, 2009). The third approach is OGM-F (fixed order): In each training cycle, each sample in the training set is supplied in a fixed order to the network exactly once (Heskes & Wiegerinck, 1996; Mangasarian & Solodov, 1994; Wu & Xu, 2002; Wu, Feng, Li, & Xu, 2005; Xu, Zhang, & Jin, 2009). Naturally, the existing convergence results for OGM-CS are mostly asymptotic convergence with a probabilistic nature as the size of training samples goes to infinity (Bertsekas & Tsitsiklis, 1996; Chakraborty & Pal, 2003; Fine & Mukherjee, 1999; Finnoff, 1994; Liang, Feng, Lee, Lim, & Lee, 2002; Tadic & Stankovic, 2000; Terence, 1989; Zhang, Wu, Liu, & Yao, 2009). Deterministic convergence can be obtained for OGM-SS and OGM-F (Li et al., 2004; Mangasarian & Solodov, 1994; Shao, Wu, & Liu, 2007; Wu & Xu, 2002; Wu et al., 2005; Wu, Feng, & Li, 2002; Wu & Shao, 2003; Wu, Shao, & Qu, 2005; Xu et al., 2009). It is interesting to see that the learning method OGM-SS with stochastic nature enjoys deterministic convergence. The convergence result is a 0893-6080/$ see front matter 2010 Elsevier Ltd. All rights reserved. doi:10.1016/j.neunet.2010.09.007 Author's personal copy 92 W. Wu et al. / Neural Networks 24 (2011) 91 98 bit easier to prove for OGM-F than for OGM-SS. But we have reason to believe, and our experience shows, that OGM-SS behaves numerically better than OGM-F since the stochastic nature of the learning procedure survives in OGM-SS (Li & Ding, 2005; Li et al., 2004). To guarantee the convergence, it is commonly required that the learning rate m satisfies the assumptions m=1 m = and m=1 2 m < as in Bertsekas and Tsitsiklis (1996) and Tadic and Stankovic (2000) for OGM-CS. An extra assumption limm m/ m+1 = 1 was introduced by Xu et al. (2009) for OGM- F. A special condition which is basically m = O(1/m) was required in Li et al. (2004), Shao et al. (2007), Wu and Xu (2002), Wu et al. (2005), Wu et al. (2002), Wu and Shao (2003) and Wu et al. (2005) for OGM-F and OGM-SS. To obtain the strong convergence result, which means that the weight sequence converges to a fixed point, Wu et al. (2005) introduced an additional assumption: the number of the stationary points of the error function is finite. A more relaxed condition is used in Xu et al. (2009): the gradient of the error function has at most countably infinite number of stationary points. The aim of this paper is to present a comprehensive study on the weak and strong convergence for OGM-F and OGM-SS, indicating that the gradient of the error function goes to zero and the weight sequence goes to a fixed point, respectively. These convergence results improve the existing results in Li et al. (2004), Shao et al. (2007), Wu and Xu (2002), Wu et al. (2005), Wu et al. (2002), Wu and Shao (2003), Wu et al. (2005) and Xu et al. (2009) such that the conditions on the activation function and the learning rate to guarantee the convergence are much relaxed. Specifically, we make the following contributions: The extra condition limm m/ m+1 = 1 for the learning rate is removed which is a requisite in Xu et al. (2009). The convergence results are valid for both OGM-F and OGM-SS. The convergence results apply not only to S S type neural networks (both the output and hidden neurons are Sigmoid functions), but also to P P, P S and S P type neural networks, where S and P represent Sigmoid and polynomial functions, respectively. The restrictive assumptions for the strong convergence in Wu et al. (2005) and Xu et al. (2009) are relaxed such that the stationary points set of the error function is only required not to contain any interior point. We assume that the derivative g of the activation function is Lipschitz continuous on any bounded closed interval. This improves the corresponding conditions in Wu et al. (2005), which require the boundedness of the second derivative g , and in Xu et al. (2009), which require g to be Lipschitz continuous and uniformly bounded on the whole R. Let us make a few remarks on the above contribution points. For the first contribution point, as an example, we recall a well-known adaptive technique for the learning rate m: m = (1 + a) m 1 if the error is decreasing, and m = (1 a) m 1 otherwise, where a < 1 is a positive number. Xu s condition limm m/ m+1 = 1 (Xu et al., 2009) is not valid in this case, while our convergence results remains valid. For the second contribution point, it is interesting to see that the learning method OGM-SS with stochastic nature enjoys deterministic convergence. We observe that OGM-F is actually a deterministic iteration procedure in that the iteration sequence is determined uniquely by the initial value and the fixed order of the samples. The convergence result is a bit easier to prove for OGM-F than for OGM-SS. We have reason to believe, and our experience shows, that OGM-SS behaves numerically better than OGM-F since the stochastic nature of the learning procedure survives in OGM-SS (Li & Ding, 2005; Li et al., 2004). Our convergence results are generalizations of both the results of Xu et al. (2009), which considers OGM-F, and the results of Li et al. (2004), which considers OGM-SS with an unpleasant condition m = O(1/m) on the learning rate. Our third contribution allows the activation functions for both hidden and output layers to be more flexible. Here we remark that typically, S S type networks are used for classification problems, and S P type networks with Sigmoid hidden neurons and linear output neurons are used for approximation problems. The existing convergence results (Li et al., 2004; Shao et al., 2007; Wu & Xu, 2002; Wu et al., 2005, 2002; Wu & Shao, 2003; Wu et al., 2005; Xu et al., 2009) are mostly for either S S type or S P type alone but not for both of them. In this paper, we give a uniform treatment for all types of networks. The fourth and fifth contribution points are mainly of theoretical interest. From a theoretical point of view, we mention that different analytical tools are employed in Wu et al. (2005) and Xu et al. (2009) and this study for the convergence analysis, might explain, at least in part, why different conditions are obtained for the convergence. The differential Taylor expansion is used in Wu et al. (2005), which requires the boundedness of the second derivative g of the activation function g; the mean value theorem of integrals is employed in Xu et al. (2009), which requires g to be Lipschitz continuous and uniformly bounded; and in this paper, we use the integral Taylor expansion and hence require the Lipschitz continuity of g on any bounded closed interval. Finally, we point out that Xu et al. (2009) is a big step forward for the convergence study of OGM-F and that Xu et al. (2009) also includes another convergence result under the condition that the error function is directionally convex. This convex condition is not considered in this paper. The rest of this paper is organized as follows. In Section 2, online updating methods including OGM-F and OGM-SS are introduced. The main convergence results are presented in Section 3 and their proofs are gathered in Section 4. Some conclusions are drawn in Section 5. 2. OGM-F and OGM-SS Let us begin with an introduction of a feedforward neural network with three layers. The numbers of neurons for the input, hidden and output layers are p, n and 1, respectively. Suppose that the training sample set is xj, Oj J j=1 Rp R, where xj and Oj are the input and the corresponding ideal output of the jth sample, respectively. Let V = vi,j n p be the weight matrix connecting the input and the hidden layers, and write vi = (vi1, vi2, . . . , vip)T for i = 1, 2, . . . , n. The weight vector connecting the hidden and the output layers is denoted by u = (u1, u2, . . . , un)T Rn. To simplify the presentation, we combine the weight matrix V with the weight vector u, and write w = uT, vT 1, . . . , vT n T Rn(p+1). Let g, f : R R be given activation functions for the hidden and output layers, respectively. For convenience, we introduce the following vector valued function G (z) = (g (z1) , g (z2) , . . . , g (zn))T , z Rn. (1) For any given input x Rp, the output of the hidden neurons is G(Vx), and the final actual output is y = f (u G (Vx)) . (2) For any fixed weights w, the error of the neural networks is defined as E(w) = 1 2 J j=1 (Oj f (u G(Vxj)))2 = J j=1 fj(u G(Vxj)), (3) where fj(t) = 1 2(Oj f (t))2, j = 1, 2, . . . , J, t R. The gradients of the error function with respect to u and vi are, respectively, Author's personal copy W. Wu et al. / Neural Networks 24 (2011) 91 98 93 given by Eu(w) = J j=1 Oj yj f (u G(Vxj))G(Vxj) = J j=1 f j (u G(Vxj))G(Vxj), (4) Evi(w) = J j=1 Oj yj f (u G(Vxj))uig (vi xj)xj = J j=1 f j (u G(Vxj))uig (vi xj)xj, (5) where yj = f (u G(Vxj)), i = 1, 2, . . . , n; j = 1, 2, . . . , J. (6) Write EV(w) = Ev1(w)T, Ev2(w)T, . . . , Evn(w)T T , (7) Ew(w) = Eu(w)T, EV(w)T T . (8) First, let us consider the case that the training samples are supplied to the network in a fixed order (OGM-F) in the training process. Hence, starting from an arbitrary initial guess w0, we proceed to refine it iteratively by the formulas umJ+j+1 = umJ+j + jumJ+j, (9) v mJ+j+1 i = v mJ+j i + jv mJ+j i , (10) where kumJ+j = m(Ok ymJ+j, k)f (umJ+j GmJ+j, k)GmJ+j, k = mf k umJ+j GmJ+j, k GmJ+j, k, (11) kv mJ+j i = m Ok ymJ+j, k f (umJ+j GmJ+j, k) u mJ+j i g (v mJ+j i xk)xk = mf k umJ+j GmJ+j, k u mJ+j i g (v mJ+j i xk)xk, (12) GmJ+j, k = G(VmJ+jxk), ymJ+j, k = f (umJ+j GmJ+j, k), m N; i = 1, 2, . . . , n; j, k = 1, 2, . . . , J. (13) Here the parameter m is the learning rate, whose value may be changed after each cycle of the training procedure. We can also choose training samples in a special stochastic order (OGM-SS) as follows: For the mth training cycle, let xm, 1, xm, 2, . . . , xm, J be a stochastic permutation of the set x1, x2, . . . , xJ . Similar to (9) and (10), the weights are iteratively updated in the following fashion umJ+j+1 = umJ+j + m j umJ+j, (14) v mJ+j+1 i = v mJ+j i + m j v mJ+j i , (15) where m k umJ+j = m(Ok ymJ+j, m, k)f (umJ+j GmJ+j, m, k)GmJ+j, m, k = mf k umJ+j GmJ+j, m, k GmJ+j, m, k, (16) m k v mJ+j i = m Ok ymJ+j, m, k f (umJ+j GmJ+j, m, k) u mJ+j i g (v mJ+j i xm, k)xm, k = mf k umJ+j GmJ+j, m, k u mJ+j i g (v mJ+j i xm, k)xm, k, (17) GmJ+j, m, k = G(VmJ+jxm, k), ymJ+j, m, k = f (umJ+j GmJ+j, m, k), m N; i = 1, 2, . . . , n; j, k = 1, 2, . . . , J. (18) We mention that OGM-F and OGM-SS are also called cycle learning and almost-cycle learning in Heskes and Wiegerinck (1996), respectively. 3. Main results For any x Rn, we write x = n i=1 x2 i , where stands for the Euclidean norm in Rn. Let 0 = {w : Ew(w) = 0} be the stationary point set of the error function E(w), where Rn(p+1) is a bounded region satisfying (A3) below. Let 0,s R be the projection of 0 onto the sth coordinate axis, that is, 0,s = ws R : w = (w1, . . . , ws, . . . , wn(p+1))T 0 (19) for s = 1, 2, . . . , n(p + 1). To analyze the convergence of the algorithm, we need the following assumptions. (A1) g (t) and f (t) are Lipschitz continuous on any bounded closed interval; (A2) m > 0, m=0 m = , m=0 2 m < ; (A3) There exists a bounded open set Rn such that {wm} (m N); (A3 ) There exists a bounded open set Rn such that {um} (m N), and the derivative of the activation function g in (1) is uniformly bounded and Lipschitz continuous on R. (A4) 0,s does not contain any interior point for every s = 1, 2, . . . , n(p + 1). Theorem 3.1. Assume that conditions (A1) (A3) are valid. Then, starting from an arbitrary initial value w0, the learning sequence {wm} defined by (9) and (10) or by (14) and (15) satisfies the following weak convergence lim m Ew wm = 0. (20) Moreover, if assumptions (A1) (A4) are valid, there holds the strong convergence: There exists w 0 such that lim m wm = w . (21) Let us make three remarks on the convergence result: (1) We claim that the weak convergence remains valid if the activation function g of the hidden layer is a commonly used sigmoid function and assumptions (A3 ) (instead of (A3)) and (A2) are valid. This is due to the fact that the sigmoid function g is uniformly bounded on R and that (37) is valid even if the weight vectors vi (i = 1, 2, . . . , n) are unbounded. (2) In the numerical analysis of an iterative method for a class of nonlinear problems, the iterative sequence is often required to be bounded in order to prove its convergence. This is what we do in conditions (A3) and (A3 ). We mention that the weights will be automatically bounded in the network training with the help of a penalty term (cf. Zhang et al., 2009). (3) For the strong convergence, our condition (A4) on 0 allows it to be finite set, countably infinite set, nowhere dense set or even some uncountable dense set. Hence, the corresponding assumptions that the set 0 contains finite points and at most countably infinite points in Wu et al. (2005) and Xu et al. (2009), respectively, are special cases of assumption (A4). This relaxed condition makes it much easier to verify the strong convergence in practice. 4. Proofs For convenience of presentation, we present in detail the convergence proof for OGM-F in the following Section 4.1. Then, in Section 4.2, we briefly point out how to extend the result to OGM-SS. 4.1. Convergence analysis for OGM-F We first present four useful lemmas for the convergence analysis. Author's personal copy 94 W. Wu et al. / Neural Networks 24 (2011) 91 98 Lemma 4.1. Let q(x) be a function defined on a bounded closed interval [a, b] such that q (x) is Lipschitz continuous with Lipschitz constant K > 0. Then, q (x) is differentiable almost everywhere in [a, b] and q (x) K, a.e. [a, b]. (22) Moreover, there exists a constant C > 0 such that q(x) q(x0) + q (x0)(x x0) + C(x x0)2, x0, x [a, b]. (23) Proof. Since q (x) is Lipschitz continuous on [a, b], q (x) is absolutely continuous and the derivative q (x) exists almost everywhere on [a, b]. Hence, for almost every x [a, b], q (x) = lim h 0 q (x + h) q (x) h = lim h 0 q (x + h) q (x) h K. (24) Using the integral Taylor expansion, we deduce that q(x) = q(x0) + q (x0)(x x0) + (x x0)2 1 0 (1 t)q (x0 + t(x x0))dt q(x0) + q (x0)(x x0) + (x x0)2 1 0 K(1 t)dt = q(x0) + q (x0)(x x0) + C(x x0)2, C = K 2 , x0, x [a, b] . (25) Lemma 4.2. Suppose that the learning rate m satisfies (A2) and that the sequence {am} (m N) satisfies am 0, m=0 ma m < and |am+1 am| m for some positive constants and . Then we have lim m am = 0. (26) Proof. According to (A2), we know that m 0 as m . We claim that limk infm>k am = 0. Otherwise, if a limk infm>k am (0, ], then by the definition of the inferior limit, there exists an integer M > k such that am a 2 > 0 for m M, which leads to m=0 ma m a 2 m M m = . (27) This contradicts m=0 ma m < and confirms the claim. Next, we claim that limk supm>k am = 0. Otherwise, there exists (0, ] such that limk supm>k am = . Then, for any 0 < < , we can choose two subsequences aik and ajk of {am} to satisfy (1) aik 0, 4 , ajk ( , ); (2) ik + 1 < jk < ik+1; (3) aik+1 [ 4, 2]. (This can be done because limk infm>k am = 0, limk supm>k am = , and |am am+1| m 0 as m .) For any ik < m < jk, we have am [ 4, ]. Thus, we conclude that 2 ajk aik+1 |ajk ajk 1| + + |aik+2 aik+1| jk 1 m=ik+1 m jk m=ik+1 m. Therefore, we have for all large enough integer k that jk m=ik ma m jk m=ik+1 ma m 4 jk m=ik+1 m 2 4 +1 . But this contradicts m=0 ma m < and implies our second claim. Finally, the above two claims together clearly lead to the desired estimate (26). Lemma 4.3. Let {bm} be a bounded sequence satisfying limm (bm+1 bm) = 0. Write 1 = limn infm>n bm, 2 = limn supm>n bm and S = {a R: There exists a subsequence {bik} of {bm} such that bik a as k }. Then we have S = [ 1, 2]. (28) Proof. It is obvious that 1 2 and S [ 1, 2]. If 1 = 2, then (28) follows simply from limm bm = 1 = 2. Let us consider the case 1 < 2 and proceed to prove that S [ 1, 2]. For any a ( 1, 2), there exists > 0 such that (a , a + ) ( 1, 2). Noting that limm (bm+1 bm) = 0, we observe that bm travels to and from between 1 and 2 with very small pace for all large enough m. Hence, there must be infinite number of points of the sequence {bm} falling into (a , a + ). This implies a S and thus( 1, 2) S. Furthermore,( 1, 2) S immediately leads to [ 1, 2] S. This completes the proof. Let the sequence wmJ+j (m N, j = 1, 2, . . . , J) be generated by (9) and (10). We introduce the following notations: Rm, j = jumJ+j jumJ, (29) r m, j i = jv mJ+j i jv mJ i , (30) dm, l = umJ+l umJ = l j=1 jumJ+j = l j=1 jumJ + l j=1 Rm, j, (31) hm, l i = v mJ+l i v mJ i = l j=1 jv mJ+j i = l j=1 jv mJ i + l j=1 r m, j i , (32) m, l, j = GmJ+l, j GmJ, j, (33) m N, j = 1, 2, . . . , J, l = 1, 2, . . . , J, i = 1, 2, . . . , n. Then, (9) and (10) can be rewritten as umJ+j = umJ + j k=1 kumJ + Rm, k , (34) v mJ+j i = v mJ i + j k=1 ( kv mJ i + rm, k i ). (35) Let constants C1 and C2 be defined by (cf. assumption (A3)) max 1 j J xj , |Oj| = C1, sup m N wm = C2. (36) By assumption (A1), f j (t) also satisfies the Lipschitz condition for j = 1, 2, . . . , J. Furthermore, g(t), f (t) and fj(t) are all uniformly continuous on any bounded closed interval. Lemma 4.4. Let conditions (A1) and (A3) be valid, and let the sequence wmJ+j be generated by (9) and (10). Then there are constants C3 C7 such that GmJ+j, k C3, (37) dm, l C4 m, m, l, j C5 m, (38) Rm, j C6 2 m, r m, j i C7 2 m, (39) where m N; j, k = 1, 2, . . . , J; l = 1, 2, . . . , J; i = 1, 2, . . . , n. Author's personal copy W. Wu et al. / Neural Networks 24 (2011) 91 98 95 Proof. According to (36), we have |v mJ+j i xk| v mJ+j i xk C1C2 D1. (40) Thus, there exists a positive constant C3,1 such that max |t| D1 |g(t)| = C3,1, (41) GmJ+j, k = G VmJ+jxk nC3,1 C3. (42) It follows from (36) and (42) that umJ+j GmJ+j, k umJ+j GmJ+j, k C2C3 D2. (43) Then, there is a positive constant C4,1 such that max |t| D2 f j (t) C4,1. (44) Furthermore, a combination of (A1), (11), (37) and (40) gives dm, l = umJ+l umJ = l j=1 jumJ+j C4 m, (45) where C4 = JC4,1C3. Employing (40), we find that max |t| D1 g (t) = C5,1, (46) where C5,1 is a positive constant. Moreover, we observe that m, l, j = GmJ+l, j GmJ, j max 1 i n g (ti) xj n i=1 hm, l i max 1 i n g (ti) xj n i=1 l k=1 kv mJ+k i C5 m, (47) where C5 = nlC4,1C5,1 max1 i n |g (ti)| xj supm N wn max1 k J xk , in which ti = v mJ i xj + i(v mJ+l i v mJ i ) xj, i (0, 1), and |ti| v mJ i xj + |(v mJ+l i v mJ i ) xj| 3C1C2. By virtue of (A1), we see that g (ti) (i = 1, 2, . . . , n) is bounded. Combining f j (t) s Lipschitz continuity, (36) and (37), we have f j (umJ+j GmJ+j, j) f j (umJ GmJ+j, j) L umJ+j GmJ+j, j umJ GmJ+j, j L dm, j GmJ+j, j LC3 dm, j , (48) f j (umJ GmJ+j, j) f j (umJ GmJ, j) L umJ GmJ+j, j umJ GmJ, j L umJ m, j, j LC2 m, j, j , (49) where L > 0 is the Lipschitz constant. By the definition of Rm, j, we see that Rm, j = jumJ+j jumJ = m f j (umJ+j GmJ+j, j)GmJ+j, j f j (umJ GmJ, j)GmJ, j = m f j (umJ+j GmJ+j, j) m, j, j + f j (umJ+j GmJ+j, j) f j (umJ GmJ+j, j) GmJ, j + f j (umJ GmJ+j, j) f j (umJ GmJ, j) GmJ, j . (50) Therefore, it follows from (37), (38), (48) and (49) that Rm, j m LC2 3 dm, j + (C4,1 + LC2C3) m, j, j C6 2 m,(51) where C6 = max LC2 3 C4, (C4,1 + LD2)C5 . Similarly, we can show the existence of a constant C7 > 0 such that r m, j i C7 2 m. (52) The next lemma reveals an almost monotonicity of the error function during the training process. Lemma 4.5. Let the sequence wmJ+j be generated by (9) and (10). Under assumptions (A1) and (A3), there holds E w(m+1)J E wmJ m Ew wmJ 2 + C8 2 m, (m = 0, 1, . . .) (53) where C8 > 0 is a constant independent of m and m. Proof. By virtue of assumption (A1) and Lemma 4.1, we know that the derivative g (v mJ i xj + t(h mJ i xj)) is integrable almost everywhere on [0, 1] and f j umJ GmJ, j umJ m, J, j = f j umJ GmJ, j n i=1 u mJ i g (v mJ i xj)h m, J i xj + f j umJ GmJ, j n i=1 u mJ i (h m, J i xj)2 1 0 (1 t) g (v mJ i xj + t(h m, J i xj))dt. (54) By virtue of Lemma 4.1, (11), (12) and (54), there is a constant C9 > 0 such that fj u(m+1)J G(m+1)J, j fj umJ GmJ, j + f j umJ GmJ, j u(m+1)J G(m+1)J, j umJ GmJ, j + C9 u(m+1)J G(m+1)J, j umJ GmJ, j 2 = fj(umJ GmJ, j) + f j (umJ GmJ, j) dm, J GmJ, j + umJ m, J, j + dm, J m, J, j + C9 u(m+1)J G(m+1)J, j umJ GmJ, j 2 = fj(umJ GmJ, j) 1 m jumJ dm, J 1 m n i=1 ( jv mJ i h m, J i ) + f j umJ GmJ, j n i=1 u mJ i (h m, J i xj)2 1 0 (1 t) g (v mJ i xj + t(h m, J i xj))dt + f j (umJ GmJ, j)dm, J m, J, j + C9 u(m+1)J G(m+1)J, j umJ GmJ, j 2 . (55) Summing (55) from j = 1 to j = J and noting (3) (5), (31) and (32), we have E w(m+1)J E wmJ 1 m J j=1 jumJ 2 + n i=1 J j=1 jv mJ i 2 + m = E wmJ m Eu wmJ 2 + n i=1 Evi wmJ 2 + m = E wmJ m Ew wmJ 2 + m, (56) Author's personal copy 96 W. Wu et al. / Neural Networks 24 (2011) 91 98 where m = 1 m J j=1 jumJ J j=1 Rm, j 1 m n i=1 J j=1 jv mJ i J j=1 r m, j i + J j=1 n i=1 u mJ i f j umJ GmJ, j (h m, J i xj)2 1 0 (1 t) g (v mJ i xj + t(h m, J i xj))dt + J j=1 f j (umJ GmJ, j)dm, J m, J, j + C9 J j=1 u(m+1)J G(m+1)J, j umJ GmJ, j 2 . It now follows from (36) and (37) that GmJ, j = G VmJxj C3, umJ GmJ, j umJ GmJ,j C2C3 = D2. (57) By (11), (42) (44) and (51), the first term of m can be estimated as follows. 1 m J j=1 jumJ J j=1 Rm, j 1 m J j=1 jumJ J j=1 Rm, j C8,1 2 m, (58) where C8,1 = J2C3C4,1C6 = JC4C6. Similar estimates for the other terms of m can be obtained with corresponding constants C8,t > 0 for t = 2, . . . , 5. Finally, the desired estimate (53) is proved by setting C8 = 5 t=1 C8,t. Now, we are ready to prove the convergence theorem. Proof of Theorem 3.1 for OGM-F. The proof is divided into two parts, dealing with (20) and (21), respectively. Proof of (20). By (A2) and Lemma 4.5, we conclude that m=0 m Ew wmJ 2 = m=0 m Eu wmJ 2 + EV wmJ 2 < , (59) m=0 m Eu wmJ 2 < . (60) Employing the integral Taylor expansion, we deduce that f j (u(m+1)J G(m+1)J, j)G(m+1)J, j f j (umJ GmJ, j)GmJ, j = f j (u(m+1)J G(m+1)J, j) m, J, j + f j (u(m+1)J G(m+1)J, j) f j (umJ G(m+1)J, j) GmJ, j + f j (umJ G(m+1)J, j) f j (umJ GmJ, j) GmJ, j = f j (u(m+1)J G(m+1)J, j) m, J, j + (dm, J G(m+1)J, j)GmJ, j 1 0 (1 t) f j (umJ G(m+1)J, j + t(dm, J G(m+1)J, j))dt + (umJ m, J, j)GmJ, j 1 0 (1 t) f j (umJ GmJ, j + t(umJ m, J, j))dt. (61) Note (A2) and let c > 0 be an upper bound of { m} m=0. It follows from (36) (38) that umJ G(m+1)J, j + t dm, J G(m+1)J, j umJ + dm, J G(m+1)J, j (C2 + C4 c) C3, t (0, 1), (62) umJ GmJ, j + t umJ m, J, j umJ GmJ, j + m, J, j C2(C3 + C5 c) = D2 + C2C5 c, t (0, 1). (63) According to (62), (63) and the proof of Lemma 4.1, there are positive constants C10, C11 > 0 such that 1 0 (1 t) f j umJ G(m+1)J, j + tdm, J G(m+1)J, j dt C10, (64) 1 0 (1 t) f j umJ GmJ, j + tumJ m, J, j dt C11. (65) By (43), we obtain u(m+1)J G(m+1)J, j C2C3 = D2. Employing (4) and (37), (38), (44), (64) and (65), and summing (61) from j = 1 to j = J, we conclude that Eu w(m+1)J Eu wmJ Eu w(m+1)J Eu wmJ C10J max 1 j J m N G(m+1)J, j GmJ, j dm, J + JC4,1 + C11J max 1 j J m N umJ GmJ, j max 1 j J m N m, J, j C12 m, (66) where C12 = JC2 3 C4C10 + JC4,1C5 + JD2C5C11. Combining (59), (66) and Lemma 4.2 results in limm Eu wmJ = 0. Similarly as in the proof to (66), there exists a positive constant C13 such that Eu wmJ+j Eu wmJ C13 m. (67) Since Eu wmJ+j Eu wmJ+j Eu wmJ + Eu wmJ C13 m + Eu wmJ , (68) we have limm Eu wmJ+j = 0 for j = 1, 2, . . . , J. Similarly, we deduce that limm Evi wmJ+j = 0 for i = 1, . . . , n, j = 1, 2, . . . , J, and lim m Ew wmJ+j = 0, j = 1, 2, . . . , J. (69) This immediately gives lim m Ew wm = 0. (70) Proof of (21). According to (A3), the sequence {wm} (m N) has a subsequence {wmk} (k N) that is convergent to, say, w 0. It follows from (20) and the continuity of Ew (w) that Ew w = lim k Ew wmk = lim m Ew wm = 0. (71) This implies that w is a stationary point of E (w). Hence, {wm} has at least one accumulation point and every accumulation point must be a stationary point. Author's personal copy W. Wu et al. / Neural Networks 24 (2011) 91 98 97 Next, by reduction to absurdity, we prove that {wm} has precisely one accumulation point. Let us assume to the contrary that {wm} has at least two accumulation points w = w. We write wm = (wm 1 , wm 2 , . . . , wm n(p+1))T . It is easy to see from (9) (12) that limm wm+1 wm = 0, or equivalently, limm |wm+1 i wm i | = 0 for i = 1, 2, . . . , n(p + 1). Without loss of generality, we assume that the first components of w and w do not equal to each other, that is, w1 = w1. For any real number (0, 1), let w 1 = w1+(1 ) w1. By Lemma 4.3, there exists a subsequence w mk1 1 of wm 1 converging to w 1 as k1 . Due to the boundedness of w mk1 2 , there is a convergent subsequence w mk2 2 w mk1 2 . We define w 2 = limk2 w mk2 2 . Repeating this procedure, we end up with decreasing subsequences {mk1} {mk2} {mkn(p+1)} with w i = limki w mki i for each i = 1, 2, . . . , n(p+1). Write w = (w 1, w 2, . . . , w n(p+1))T . Then, we see that w is an accumulation point of {wm} for any (0, 1). But this means that 0,1 has interior points, which contradicts (A4). Thus, w must be a unique accumulation point of {wm} m=0. This completes the proof of the strong convergence. 4.2. Convergence analysis for OGM-SS Now, let the sequence wmJ+j (m N, j = 1, 2, . . . , J) be generated by (14) and (15), and let Rm, j = m j umJ+j m j umJ, (72) r m, j i = m j v mJ+j i m j v mJ i , (73) dm, l = umJ+l umJ = l j=1 m j umJ+j = l j=1 m j umJ + l j=1 Rm, j, (74) hm, l i = v mJ+l i v mJ i = l j=1 m j v mJ+j i = l j=1 m j v mJ i + l j=1 r m, j i , (75) m, l, j = GmJ+l, m, j GmJ, m, j, (76) m N, j = 1, 2, . . . , J, l = 1, 2, . . . , J, i = 1, 2, . . . , n. It is obvious that Lemmas 4.1 4.3 are not influenced by the new definitions. In place of Lemmas 4.4 and 4.5, we now have the following two Lemmas. Lemma 4.6. Let conditions (A1) and (A3) be valid, and let the sequence wmJ+j be generated by (14) and (15). Then, there hold the following estimates with the same constants C3 C7 as in Lemma 4.4: GmJ+j, m, k C3, (77) dm, l C4 m, m, l, j C5 m, (78) Rm, j C6 2 m, r m, j i C7 2 m, (79) where m N; j, k = 1, 2, . . . , J; l = 1, 2, . . . , J; i = 1, 2, . . . , n. Proof. According to (36), we have |v mJ+j i xm, k| v mJ+j i max 1 k J xk C1C2 D1. (80) Thus, there exists a positive constant C3,1 such that max |t| D1 |g(t)| = C3,1, (81) GmJ+j, m, k = G VmJ+jxm, k nC3,1 C3. (82) Similarly, (78) and (79) can be proved after adjusting the corresponding superscripts in the proof to Lemma 4.4. Lemma 4.7. Let the sequence wmJ+j be generated by (14) and (15). Under assumptions (A1)and (A3), there holds E w(m+1)J E wmJ m Ew wmJ 2 + C8 2 m, (m = 0, 1, . . .) (83) where C8 > 0 is the same constant defined in Lemma 4.5. Proof. As in the proof to Lemma 4.6, we only need to adjust some superscripts. For example, corresponding to (54), we change the related superscripts and get f j umJ GmJ, m, j umJ m, J, j = f j umJ GmJ, m, j n i=1 u mJ i g (v mJ i xm, j)h m, J i xm, j + f j umJ GmJ, m, j n i=1 u mJ i (h m, J i xm, j)2 1 0 (1 t)g (v mJ i xm, j + t(h m, J i xm, j))dt. (84) The details are left to the interested readers. Proof of Theorem 3.1 for OGM-SS. We can use Lemmas 4.1 4.3 and Lemmas 4.6 4.7 to obtain the weak and strong convergence results for OGM-SS precisely as in the proof to Theorem 3.1 for OGM-F. 5. Conclusions In this paper, we present a comprehensive study on the weak and strong convergence for three-layer BP neural networks. Com- pared with the existing convergence results, the corresponding as- sumptions are more relaxed. Our convergence analysis holds for more extensive BP neural networks, e.g., S S, S P, P P and P S type neural networks. References Bertsekas, D. P., & Tsitsiklis, J. N. (1996). Neuro-dynamic programming. Athena Scientific. Chakraborty, D., & Pal, N. R. (2003). A novel training scheme for multilayered perceptrons to realize proper generalization and incremental learning. IEEE Transactions on Neural Networks, 14, 1 14. Fine, T. L., & Mukherjee, S. (1999). Parameter convergence and learning curves for neural networks. Neural Computation, 11, 747 769. Finnoff, W. (1994). Diffusion approximations for the constant learning rate backpropagation algorithm and resistance to local minima. Neural Computation, 6, 285 295. Heskes, T., & Wiegerinck, W. (1996). A theoretical comparison of batch-mode, on- line, cyclic, and almost-cyclic learning. IEEE Transactions on Neural Networks, 7, 919 925. LeCun, Y. (1985). Une procedure d apprentissage pour reseau seuil asymmetrique. A la Frontieredel Intelligence Ariticielle des Sciences de la Connaissance des Neurosciences, 85, 599 604. Li, Z. X., & Ding, X. S. (2005). Prediction of stock market by bp neural networks with technical indexes as input. Numerical Mathematics: A Journal of Chinese Universities, 27, 373 377. Li, Z. X., Wu, W., & Tian, Y. L. (2004). Convergence of an online gradient method for feedforward neural networks with stochastic inputs. Journal of Computational and Applied Mathematics, 163, 165 176. Liang, Y. C., Feng, D. P., Lee, H. P., Lim, S. P., & Lee, K. H. (2002). Successive approx- imation training algorithm for feedforward neural networks. Neurocomputing, 42, 11 322. Mangasarian, O. L., & Solodov, M. V. (1994). Serial and parallel backpropagation convergence via nonmonotone perturbed minimization. Optimization Methods and Software, 4, 117 134. Nakama, T. (2009). Theoretical analysis of batch and on-line training for gradient descent learning in neural networks. Neurocomputing, 73, 151 159. Parker, D. B. (1982). Learning-logic, invention report. Stanford University, Stanford, Calif. Author's personal copy 98 W. Wu et al. / Neural Networks 24 (2011) 91 98 Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Learning representations by back-propagation errors. Nature, 323, 533 536. Shao, H. M., Wu, W., & Liu, W. B. (2007). Convergence of BP algorithm for training MLP with linear output. Numerical Mathematics: A Journal of Chinese Universities (English Series), 16, 193 202. Tadic, V., & Stankovic, S. (2000). Learning in neural networks by normalized stochastic gradient algorithm: local convergence. In Proceedings of the 5th seminar neural networks application electronic engineering. Terence, D. S. (1989). Optimal unsupervised learning in a single-layer linear feedforward neural network. Neural Networks, 2, 459 473. Werbos, P. J. (1974). Beyond regression: new tools for prediction and analysis in the behavioral sciences. Ph.D. thesis. Harvard University, Cambridge, MA. Wilson, D. R., & Martinez, T. R. (2003). The general inefficiency of batch training for gradient descent learning. Neural Networks, 16, 1429 1451. Wu, W., & Xu, Y. S. (2002). Deterministic convergence of an on-line gradient method for neural networks. Journal of Computational and Applied Mathematics, 144, 335 347. Wu, W., Feng, G. R., Li, Z. X., & Xu, Y. S. (2005). Deterministic convergence of an online gradient method for BP neural networks. IEEE Transactions on Neural Networks, 16, 533 540. Wu, W., Feng, G. R., & Li, X. (2002). Training multilayer perceptrons via minimization of sum of ridge functions. Advances in Computational Mathematics, 17, 331 347. Wu, W., & Shao, Z. Q. (2003). Convergence of online gradient methods for continuous perceptrons with linearly separable training patterns. Applied Mathematics Letters, 16, 999 1002. Wu, W., Shao, H. M., & Qu, D. (2005). Strong convergence of gradient methods for BP networks training. In Proceedings of 2005 international conference on neural networks and brains (pp. 332 334). Xu, Z. B., Zhang, R., & Jin, W. F. (2009). When on-line BP training converges. IEEE Transactions on Neural Networks, 20, 1529 1539. Zhang, H. S., Wu, W., Liu, F., & Yao, M. C. (2009). Boundedness and convergence of online gradient method with penalty for feedforward neural networks. IEEE Transactions on Neural Networks, 20, 1050 1054.