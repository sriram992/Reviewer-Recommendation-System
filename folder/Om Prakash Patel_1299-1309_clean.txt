Novel quantum inspired binary neural network algorithm OM PRAKASH PATEL* and ARUNA TIWARI Department of Computer Science and Engineering, Indian Institute of Technology Indore, Indore 453552, India e-mail: oppatel13@gmail.com; artiwari@iiti.ac.in MS received 16 April 2015; revised 5 April 2016; accepted 5 June 2016 Abstract. In this paper, a quantum based binary neural network algorithm is proposed, named as novel quantum binary neural network algorithm (NQ-BNN). It forms a neural network structure by deciding weights and separability parameter in quantum based manner. Quantum computing concept represents solution proba- bilistically and gives large search space to nd optimal value of required parameters using Gaussian random number generator. The neural network structure forms constructively having three number of layers input layer: hidden layer and output layer. A constructive way of deciding the network eliminates the unnecessary training of neural network. A new parameter that is a quantum separability parameter (QSP) is introduced here, which nds an optimal separability plane to classify input samples. During learning, it searches for an optimal separability plane. This parameter is taken as the threshold of neuron for learning of neural network. This algorithm is tested with three benchmark datasets and produces improved results than existing quantum inspired and other clas- si cation approaches. Keywords. Quantum computing; neural network; quantum gates; classi cation; separability plane. 1. Introduction Arti cial neural networks have been successfully applied to problems in pattern classi cation, pattern matching, asso- ciative memories, optimization and function approximation [1 3]. Several architectures have been proposed like per- ceptron, backpropagation, recurrent network, etc. to solve the problem from various elds like mathematics, medi- cine, economics, computer science and many more [4 6]. The performance of neural network in the mentioned area depends upon several parameters such as network archi- tecture, input data, number of neurons, the number of hidden layers, activation function and weights [7 10]. In this paper, with the help of quantum computing concept and constructive formation of neural network, all these param- eters have been optimized. The proposed algorithm uses the quantum computing concept for the selection of weights required to establish the connection at hidden and output layers. Quantum computing concept was, rstly, introduced in classical computing by Narayanan and Moore [11]. The signi cant work has been done by Han and Kim to solve the knapsack problem using the quantum computing concept with and without termination criteria [12, 13]. Here, qubit q is de ned as a smallest unit of information which have better characteristic of the population diversity than other representations. Since qubits are linear superposition of states of probabilistic thus, with the help of Gaussian random generation it gives diversity to select the optimal value of parameters from large subspace. Lu et al [2] proposed an algorithm for optimizing arti cial neural net- works by deciding connection weight and architecture through the quantum computing concept. The quantum computing concept has also been used in several applica- tions. Gandhi et al [14] proposed an algorithm to lter EEG signal for brain computer interface. A novel scheme has been proposed by Li and Xu [15] for speech enhancement based on quantum feed-forward neural network, which produces better results than traditional spectral subtraction and Wiener ltering method. Caraiman and Manta [16] proposed an image processing technique using the quantum concept. As network architecture plays an important role in the performance of the system, therefore, selection of an appropriate network architecture is required. There are many algorithms exist which constructively form the net- work architecture. As [17] proposed an algorithm to design neural network architecture constructively for data classi- cation. Huang and Huang [18] proposed a method to bound number of hidden layer neuron in multilayer per- ceptron. Similarly, [19] proposed an algorithm to bound on the number of hidden layer neurons for the binary neural network classi er. It offers a high degree of parallelism in the hidden layer formation. To overcome the issue of nding proper weights and selection of network architecture, recently a quantum inspired binary neural network algorithm is presented by Patel and Tiwari [20, 21]. In this algorithm, the weights of *For correspondence 1299 Sa dhana Vol. 41, No. 11, November 2016, pp. 1299 1309  Indian Academy of Sciences DOI 10.1007/s12046-016-0561-0 the network are decided by quantum computing concept, hence each weight space is decomposed into subspaces in terms of quantum bits. The quantum bits that represent the probability of weight subspaces rather than a speci c structure and weight values. Thus using quantum comput- ing the partitioning subspace strategy nds the near-optimal weights. It explores each weight space region-by-region and rapidly nds the promising subspace for further exploitation using Gaussian random generation. The net- work architecture formed constructively. Here, threshold of a neuron is decided by evaluating 10 15% sample input with weight basis. This method produces good results on benchmark dataset, but deciding the threshold of a neuron may lead to a local optima problem. Therefore, to over- come this issue, in this paper a novel quantum inspired binary neural network algorithm (NQ-BNN) is presented. This paper presented a quantum separability parameter (QSP) or quantum threshold, which help to classify non- linear separable data into separable data. The selection of quantum threshold or quantum separability parameters using quantum computing concept gives the same advan- tages as selection of weight through quantum computing. These parameters are updated by the quantum computing concept. Here objective function sum has been used to de ne the quantum states. It means, when the neural net- work has better objective function in current iteration, the probability of the corresponding connections weights and quantum separability parameters being adopted in the next iteration is increased. If neural network has a bad objective function, then the probability of the corresponding con- nection weights and quantum separability parameters being adopted in the next iteration is decreased. This proposed approach has been tested on three benchmark dataset, and it is found that it gives better results than existing known methods [17, 20 23]. The paper is organized as follows: section 2 describes preliminaries. Section 3 is presented with proposed methodology. Section 4 is presented with the experimental work and shows the results on three benchmark datasets: breast cancer data, PIMA Indian diabetes and liver disease diagnosis. Section 5 is presented with the concluding remarks. 2. Preliminaries In this paper a quantum based binary neural network is proposed, which construct a binary neural network using the quantum computing concept. The quantum computing concept has been used to decide the parameters like weights of neural networks and quantum separability parameter (QSP) or threshold. All the parameters have been brie y described throughout this section. The proposed method forms a neural network structure, which consists of three layers, input layer, hidden layer and output layer. Let X X1; X2; X3; . . .; Xl denote the input samples, where l is the number of input samples and Xi xl 1; xl 2; xl 3; . . .; xl n where n is the number of attributes in one instance of input sample. Therefore, the number of input layer nodes is equal to n. For jth hidden layer neuron, connection weights are denoted as follows: Wj wj1; wj2; wj3; . . .; wjn   1 Here, the number of neurons in the hidden layer is decided constructively. The Output layer contains only one neuron which gives a binary output. In the proposed neural network, the connection weights Wj and thresh- old or quantum separability parameter kj of neuron are decided using quantum computing concept. The required preliminaries for the quantum computing concept are presented subsequently. The core idea of quantum inspired algorithm is to present solution of a problem in terms of so-called quantum bits Q rather than classical bits. The weight matrix corresponding to Eq. (1) of jth hidden layer neuron in the form of quantum bit Q can be represented as W0 j Qj1; Qj2; Qj3; . . .; Qjn   ; 2 The quantum bit (Qj) can be represented by several qubits (q). The weight W0 j space is decomposed into sub- space using quantum bit (Qj). Qj qj1jqj2j. . .jqjk   : 3 Here, the k number of qubits which represent quantum bit (Q). A single qubit (qji) where i = 1,2 ,k, is the smallest unit of representing information. A qubit is fun- damentally different from the binary bit used in traditional digital computers in the sense of representing data. A single binary bit can represent only two states, 0 and 1 , whereas the qubit (qji) has the capability to represent the linear superposition of two states simultaneously, which is determined by probability model [12]. Thus, qubit qji can be represented as qji aji j 0i bji j 1i aji bji  4 where a and b is a complex number representing the probability of qubit in 0 state and in 1 state. A probability model is applied here to decide to qubit in 0 state by a2 and in 1 state by b2, where a2 ji b2 ji 1; 0  aji  1; 0  bji  1: As discussed above, a quantum bit (Qj) formed using qubits (qj1, (k = 1)) which represent two states e.g. 0 state or 1 state. An individual quantum bit (Qj) having two qubits (qj1jqj2, (k 1; 2)) in it represents four states, e.g. 00 , 01 , 10 and 11 . In the same way, three-qubits ((qj1jqj2jqj2), (k 1; 2; 3)) system represent eight states, 1300 Om Prakash Patel and Aruna Tiwari thus n qubits (qjn, (k 1; . . .; n)) will have 2n states. For example, an individual quantum bit Qj having two qubits can be represented as follows: Qj aj1jaj2 bj1jbj2   : 5 The below example shows representation of the four states of quantum bit (Qj) having two qubits (qj2). Qj aj1  aj2   h00i aj1  bj2   h01i bj1  aj2   h10i bj1  bj2   h11i: 6 It is noted that qubit (qji) is made of two components aji and bji, where each component value lies between 0 and 1 . Let us assume that a quantum bit (Qj) having 2 qubits (qj2) and any random value can be initialized for parameter mentioned in Eq. (4). Here aj1, aj2, bj1 and bj2 are initial- ized as follows: Qj 1= 2 p j1= 2 p j1= 2 p 1= 2 p j1= 2 p j1= 2 p   : 7 With respect to Eq. (5) and Eq. (6), the state represen- tation of quantum bits (Qj). Qj 1=2 2 p h000i 1=2 2 p h001i 1=2 2 p h010i 1=2 2 p h011i 1=2 2 p h100i 1=2 2 p h101i 1=2 2 p h110i 1=2 2 p h111i: 8 2.1 Real coded value generation The proposed algorithm is inspired by quantum concept which makes use of qubit to represent data. As the classical computer operates on bits and discrete value rather than qubits, therefore, there is the need to convert qubit (q) into real coded value qreal. The conversion of real coded value from quantum bits has been done with the help of the observation process. This process starts by taking random number matrix Rj, where Rj rj1rj2. . .rjk, corresponding to Qj aj1 j aj2 j . . . j ajk . The value of rji is selected with the help of random function which generates uniform number between 0 to 1. Then, further mapping is done by using binary matrix Sj where Sj sj1sj2. . .sjk. The value of matrix Sj is generated as follows: if rj  aji 2 then sj 1 else sj 0: 9 To select weights from binary values, the Gaussian random generator has been used with mean value l and variance r, represented as N l; r . The observation process shows the process of conversion of a single qubit. As shown in Eq. (4) the qubit (qji) is formed by using two components aji and bji. For processing of qubits, generally aji component is considered because the value of second component bji will be 1  a2 ji q [12]. This observation performed for all qubits of Qj. Thus, this Qj is utilized for getting real coded value of the weight and separability parameter. Novel quantum inspired binary neural network algorithm 1301 The observation process can be understood with the help of an example. Let a quantum bit of length two qubits is represented as Q = h0:707j0:707i, therefore a random number matrix is generated using random number R = [0.85 0.02]. Now using Eq. (9) the binary matrix is generated as S = [01]. Once the binary matrix is achieved then the formula used to convert a binary number to a decimal value (bin2dec(S)?1) is used here. This return a number between 1 to 4 and corresponding four Gaussian random values are also mentioned for example N(0.25, 0.03), N(0.40, 0.03), N(0.55, 0.03), and N(0.70, 0.03). As a binary value achieved here return 2 as decimal value, therefore the real coded value corresponding to quantum bit Q is selected from N(0.40, 0.03). 2.2 Qubit updation This observation process helps to convert qubits W0 into real coded W matrix for g iteration. However, the proposed method nds the best value of weight from the large sub- space provided by the quantum concept. The appropriate value of weight can nd out during several iterations. Therefore, there is a need of updating weight values. The new value of weight (W) is generated through quantum bits W0 with quantum update function. To update W0 g 1 from W0 g quantum rotation gates as required, which is described as follows: U Dh cos Dh  sin Dh sin Dh cos Dh 10 where Dh is a rotation angle which use to generate W0 g 1 from W0 g. In the proposed algorithm, the length of indi- vidual qubit Qg i ag i;1 j ag i;2 j . . . j ag i;k is considered as 2 (k 2). It means that W will be selected from four subspaces. ag 1 i bg 1 i cos Dh  sin Dh sin Dh cos Dh  ag i bg i : 11 As presented in Lu et al [2], Dh is calculated on the basis of the objective function. sum and sumg are the objective function values which depend on parameters count1 and count2 showing the number of learnt samples in the pro- posed algorithm discussed in the next section. This objec- tive function gives better exploitation in the next iteration. The objective function parameters are sum represent as the best objective function value in g iterations and sumg objective function value of current iteration. sum and sumg are evaluated based on the number of samples learnt. As shown in the observation process, each qubits aji is asso- ciated with binary value sji, therefore, a mapping is done between sum and binary string s to update individual qubit. If objective function values sumg corresponding to s is worse than that of objective function value in sum, and state of s is zero and best objective function state of s is one, then decrement in probability of aji may produce the worst result. Therefore, to increase the probability of aji to one, Dh made negative. While, if the objective function values sumg corresponding to s is better than the objective function in sum, and state of s is one and best objective function state is zero, then increasing probability of aji to one, may produce the worst result. Therefore, to update aji, angular displacement made positive Dh. In other cases, angular displacement will remain zero. The value of angular displacement must be selected in such a way so that it can cover the maximum value of a in the range of (0 1) and also should not take many iterations to cover these values. Therefore, Dh must be initialized between 0:01  p; 0:05  p [2]. The evaluation of objective function function is done in the proposed algorithm that is discussed next. For preventing the quantum bit ag i from acquiring values 0 or 1, following constraints are applied: ag i  p ; if ag i \  p ag i if  p  ag i  1   p 1   p if ag i [ 1   p 8 > < > : 12 where the value that assigned to  is very small (approxi- mately approaching to zero), so that it can cover maximum value in the range of (0, 1). Based on these basic concepts, a novel quantum binary neural network learning algorithm (NQ-BNN) algorithm is designed, which forms three layer network structure. Network structure has input layer, hid- den layer and output layer. This works on binary form of input, and also generates binary form of output. It works for two-class classi cation problem. 3. Proposed approach In this section, a novel quantum binary neural network learning algorithm (NQ-BNN) is proposed, which makes use of a novel quantum separability parameter or threshold. The quantum computing concept helps in evolving process of connection weight (Wg) and separability parameter k . First, the basic principle of using quantum bits in learning is discussed, then the quantum separability parameter is pro- posed. Finally, learning algorithm NQ-BNN is modelled by making use of this quantum separability parameter. 3.1 Basic principle The proposed algorithm constructs neural network archi- tecture by using the quantum computing concept. It forms a three-layer network structure having input, hidden and output layer. Here, X X1; X2; X3; . . .; Xc1 and Y Y1; Y2; Y3; . . .; Yc2 denotes the instance of input samples of two different classes, which is present in the binary form. 1302 Om Prakash Patel and Aruna Tiwari Let, Xi and Yi are one of the instances of class A and class B respectively. Each instance of Xi x1; x2; x3; . . .; xn and Yi y1; y2; y3; . . .; yn has n attributes. As each instance has n attributes, therefore input nodes will be equal to n. The number of neurons in the hidden layer is decided constructively. As proposed system deals with two-class problem, therefore only one neuron required at the output layer. Figure 1 shows the basic architecture of the proposed system. In the proposed learning algorithm, we make use of step function as an activation function for forming the neuron which is given as follows. netj X n i 1 Wj  Xi or X n i 1 Wj  Yi 13 f netj 1 if netj  kj 0 if netj [ kj: 14 Training starts by taking a single neuron in hidden layer rst. The weights of this neuron is initialized as W0 g Qg w1; Qg w2; Qg w3; . . .; Qg wn , where Qwi 0:707 j 0:707 , with k = 2 and g = 1. Here K is a subspace selection of weights, and g (user de ned variable) is the maximum number of iterations to update quantum weights to get the optimized result. After initialization of quantum bits, vector R is generated with random values. Now the observation process starts. It will generate vector Sg wi in terms of bits. With the help of Sg wi and Gaussian random value generator, weight matrix is nalized for rst iteration g = 1. Then all the samples of class A and class B are applied to the neuron along with weight Wg (real coded number with respect to W0 g . Thus, after nalizing a neuron, it is checked against samples of both the classes A and B. For proper separation of samples of different class quantum separability parameter is proposed next. 3.2 Quantum separability parameter (QSP) (k0) To produce better separating plane for two classes, quan- tum separability parameter or quantum threshold is proposed. This separability parameter (k 0 t) is initialized, which takes a quantum value denoted as k 0 t ai j ai 1 ; 15 The value is set to (0.358|0.586) for selecting the value from four subspaces. Quantum separability parameter is updated for the time t (user de ned variable) in each iteration (g). After initialization of quantum bits for separability parameter (k 0 t), the observation process is used. The obser- vation process generates a random number matrix R. This random number and quantum value with the help of step 2 of observation process function, generates binary value matrix St. Now, the real coded value of kt (corresponding to quantum value k 0 t) is achieved with the help of matrix St and Gaussian random generator. The value of quantum separa- bility parameter is nalized by exploring in 2k subspace thus introducing diversity to reach the optimal value. The real coded value kt is compared with netA and netB at g 1 to nd out the optimal separability plane for two class data. This kt quantum separability parameter is updated for t iteration for weight value Wg at iteration g 1. After completion of all iterations t, best value of kt is selected corresponding to the best value of the objective function. Now quantum weights are updated for iteration g 2 and again the same process is implemented to nd out best value of kt in all iterations of t. The process will continue for the iteration g to nd out best value of weights W and quantum separability parameter k. To update quantum weights W0 g and quantum separability parameter (k 0 t), Eq. (10), Eq. (11) and Eq. (12) along with quantum updation process is used. The overall updation process is presented in the form of a pseudo-code as quantum update function. 3.3 Design of NQ-BNN The NQ-BNN is designed to classify two-class problem using quantum computing concept. The quantum bits are used to evaluate weights of network termed as quantum weights. These quantum weights are evaluated using an observation process with the help of the Gaussian random number generator. To classify an input sample more accurately, a novel concept is proposed termed as quantum separability parameter. Furthermore, the designed algo- rithm constructively form the network and work only for binary data. In this algorithm, some necessary parameters have been used, which is described as follows: Xi and Yj are an input sample of class A and class B respectively for the learning process. Two values count1 and count2 have been taken, which describe the number of samples of class A and B that are learnt. Here S vector is used which stores best value from Sg corresponding to the best result or value of the sum denoted by sum. The overall process is explained in the form of an algorithm which is presented next: Figure 1. Basic architecture. Novel quantum inspired binary neural network algorithm 1303 Note: The observation process is common to both, in generating real coded value of weight Wg and quantum separability kt parameters. The above algorithms use quantum update function to generate new quantum value of weights W and quantum separability parameter k. Table 1 is shown for updation of qubits for weights, the same process will be implemented to update qubits of separability parameter kt by using objective function as sumt, sum k and binary bits accordingly. Eqs. (10) (12) Eqs. (10) (12) 1304 Om Prakash Patel and Aruna Tiwari 3.4 Illustration of NQ-BNN The illustration of (algorithm 2) NQ-BNN with quantum separability parameter (QSP) is done with the help of one small numerical example as follows: Let us consider a dataset consist of 16 samples, where each sample having four features (0000, 0001, 0010, 0011, 0100, 0101, 0110, 0111, 1000, 1001, 1010, 1011, 1100, 1101, 1110, 1111). For class A, the set of samples are divided into training and testing set. Let us take training samples for class A as (0000, 0001, 0010, 0011, 0100, 0101, 0110) and only one test sample as (0111). Similarly for class B set of samples are divided, the training samples are (1001, 1010, 1011, 1100, 1101, 1110, 1111) and testing sample is (1000). The proposed NQ-BNN with QSP is illustrated with the help of following steps. Step 1: To start designing hidden layer rst we take a neuron and initialize its quantum weights in terms of quantum bits as W0 1 0:352j0:725; 0:907j0:854; 0:524j 0:542; 0:924j0:222 . Also initialize parameters like number of iterations to update quantum weights (g), objective function sum and binary value S as g 1 : 10, sum 0 and S 0. Step 2: For rst iteration g 1, observation process (algorithm 1) is called to convert quantum weights W0 1 into real coded value matrix W1. This is being done by rst generating random number matrix as R = (0.5|0.32, 0.89|0.25, 0.37|0.97, 0.39|0.95) correspond- ing to each qubit qji of quantum weights W0 1. With the random number matrix R, quantum weight matrix W0 1 and Eq. (9), the binary bit matrix can be formed as S = (11, 01, 10, 10). With the help of matrix S and Gaussian random number generator, the real coded value can be formed as W1 0:45; 0:25; 0:63; 0:73 . Now this real coded W1 is applied into quantum separability function. Step 3: In quantum separability function, rst initialize parameters, the number of iteration to update QSP is t = 1:20, count1 = 0; count2 = 0; objective function parameter of quantum separability parameter sum k 0; and quantum separability parameter in terms of qubits as k 0 1 0:907j0:562 . This, qubit representation of k 0 1, is then converted to real coded value by applying observation process. Let the real coded value achieved as k0 1 0:52 . Note: In the process of converting quantum weights and quantum separability parameter into real coded value, there is need to keep track of binary bits represented by S for each qubit. These binary bits are required at the time of updating qubit as mentioned in qubit updation process and table 1. This updation of qubit is further explored and resumed. Step 4: After getting real coded value weight W1 and separability parameter k1, the QSP is further proceed to evaluate the number of samples learnt as netA and netB and compared with separability parameter. Accordingly, value of count1, count2 and sum1 (t = 1) is calculated. After this the sum1 is compared with sum k and maximum value is assigned to sum k. After assigning the maximum value to sum k, qubits are updated. Thus with the help of k 0 1, k 0 2 is evaluated. Step 5: The process in step 4 is repeated to keep on evolving k 0 t for 20 iterations i.e. t = 1:20. The nal value of kt is selected for which maximum number of samples are learnt or for the best value objective function sum k is achieved. The best objective function maximum value corresponding to separability parameter is assigned to sum g. Let us say out of 14 number of samples, total eight number of samples are learnt by using best value of sepa- rability parameter among k1; . . .; k20 and weight value W1. Therefore the process will continue until network is learnt for all 14 number of samples or until all iterations (t) are over, to update k 0 t. Step 6: If all the samples are not learnt then, for the next iteration (g 2) for the same neuron, quantum weight value W0 2 is evolved. By repeating step number 5, with the help of real coded value of weight W2; k0 t is evaluated. The quantum weight W0 g evolvement process will be continued till G = 1:10 iterations or neural network learn for all training set samples. Thus, after 10 iterations G = 1:10, if all samples are not learnt then add a new neuron and follow step 1 to step 6. Step 7: The process of adding new neuron would be continued till all samples are learnt or in two successive neurons, number of unlearnt samples remains same. 4. Experimental results The proposed NQ-BNN algorithm is tested on three benchmark datasets: breast cancer dataset, PIMA Indian diabetes dataset and BUPA liver dataset, which is taken from UCI Machine learning repository [24]. 4.1 Experimental setup The experiment is carried on Intel core, I-5 processor with 4 GB RAM on windows 7 operating system. Initially, for the given input dataset quantum weights W0 g are initialized Table 1. Qubits updation. sg ji s ji sum g\sum Dh 0 0 False 0 0 0 True 0 0 1 False 0:03  P 0 1 True 0 1 0 False 0:03  P 1 0 True 0 1 1 False 0 1 1 True 0 Novel quantum inspired binary neural network algorithm 1305 with quantum values. The qubits of the quantum separa- bility parameter (k0 t) are also initialized as 0.707|0.707. Using Eqs. (10) (12) and qubit updation procedure the quantum separability parameter and quantum weights are updated, where displacement angle (Dh) has been used as 0:03  P. During this update process, it is insured that quantum weights should not converge to 0 and 1 . Therefore, the limiting parameter  has been taken as 0.001. The proposed approach is tested using tenfold cross vali- dation scheme. As the proposed algorithm takes input in terms of binary bits, therefore, real value is converted in terms of binary bits. The number of binary bits to represent each data is decided by observing highest numerical value in dataset. 4.1a Classi cation of breast cancer dataset: The breast cancer dataset has total 699 instances and 9 attributes in terms of real value. The dataset has converted into binary values for applying on the proposed algorithm. Every sample consists of 9 values corresponding to 9 attributes. For converting into binary form, each value would be represented by 3 bits according to maximum value in dataset. Thus every sample is converted into 27 bits and these binary values of every samples are taken as input to NQ-BNN. The description of breast cancer dataset has been given in table 2. In this experiment, dataset have been splitted into tenfolds, where 629 instances (ninefolds) have been used for the training purpose, and 70 instances (one- fold) are used for the testing purpose. The training of neural network with cancer dataset produces best results with two numbers of hidden layer neurons. The separability value that achieved from the rst and the second neuron is 0.52 and 0.36 respectively. The best results have been achieved in g 75 number of iterations. Table 3 shows the classi- cation result in terms of various parameters as training accuracy, generalization accuracy and number of neurons in the hidden layer. As it can be seen that NQ-BNN achieves good training and generalization accuracy with only two neurons in the hidden layer. Table 3 shows the best training accuracy as 98.6354% and worst training accuracy 95.0325%. The best generalization accuracy is 99.9586% and worst generalization accuracy is 98.6584%. The average training accuracy and generalization accuracy is 97.0816% and 99.6501% respectively. Table 4 shows the comparison of the proposed approach with a novel dis- cretization technique using class attribute interval average and Q-BNN in terms of generalization accuracy [20, 22]. Table 4 shows that the proposed approach produces gen- eralization accuracy as 99.6501%, which is far better than other available methods. 4.1b Classi cation of PIMA Indian diabetes dataset: The PIMA Indian diabetes dataset has 768 instances and every sample has 27 real values corresponding to each 27 attri- butes. In the same way as done for cancer dataset each real value is represented by 3 bits. Thus every sample is con- verted into 81 bits. The description of the PIMA Indian diabetes dataset has been given in table 5. For the training and the testing purpose, the dataset has been splitted into tenfold, where 692 instances (ninefold) have been used for training and 76 instances (onefold) have been used for the testing purpose. In the training phase, the best results are achieved with three numbers of hidden layer neurons. The separability values of hidden layer neurons are achieved as 0.45, 0.86. and 0.66 respectively. The neural network got Table 2. Class distribution of breast cancer dataset. Index Class name Class size Class distribution (%) C1 Negative 458 65.52 C2 Positive 241 34.47 Table 3. Results of NQ-BNN for breast cancer dataset. Breast cancer dataset Hidden layer neuron Training accuracy Testing_accuracy Set 1 2 95.0325 99.9574 Set 2 2 97.1854 99.8415 Set 3 2 95.2587 99.9558 Set 4 2 97.0325 99.9586 Set 5 2 98.6354 99.8248 Set 6 2 97.2576 99.8124 Set 7 2 98.1254 98.6584 Set 8 2 97.0221 99.5876 Set 9 2 98.0124 99.9477 Set 10 2 97.2546 98.9568 Average 97.08166 99.6501 Table 4. Comparison of various learning algorithms with NQ- BNN on breast cancer dataset. Generalization accuracy (%) Methods MLP classi er Naive bayes classi er Decision tree classi er Radial basis function classi er NQ- BNN* 99.6501 Q-BNN* 99.63 EW 94.57 97.28 91.3 95 EF 94.71 96.28 90.8 95 ChiMerge 92.89 91.88 93 92 IEM 74.69 81.68 93.6 80.98 CAIM 93.56 93.99 93.8 93.42 CACC 95.14 95.28 94.1 94.85 CAIA 95.42 96.57 96.57 95.99 Bold numerical values show the best accuracies for particular classi ers like MLP classi er, Naive bays classi er, Decision tree classi er, and Radial basis function classi er. The other bold value shows the proposed results for NQ-BNN *NQ-BNN and Q-BNN does not use any inbuilt classi er 1306 Om Prakash Patel and Aruna Tiwari the best results in g 90 number of iterations. Table 6, shows the classi cation results in terms of various param- eters as training accuracy, generalization accuracy and number of neurons in the hidden layer. Table 6, shows the best training accuracy as 93.5546%, and worst training accuracy 92.2547%. The best generalization accuracy is 89.5428%, and worst testing accuracy is 87.2541%. The average training accuracy and generalization accuracy is 92.7485% and 88.2865% respectively. It is observed that the proposed approach produces good results in terms of training and generalization accuracy. The proposed approach is compared with MTiling-real algorithm [17, 20]. Table 7 shows the training and classi cation accuracy as compared to MTiling-real algorithm. It is observed that it produces better results that is 88.28657%. 4.1c Classi cation of BUPA liver dataset: The BUPA liver dataset has total 340 instances and every sample has 6 real values corresponding to each 6 attributes. For con- verting into binary form, each value would be represented by 7 bits because the maximum possible value of instance is 128. Thus, total 42 attributes are achieved in terms of binary bits, these binary bits are taken as input to neural network. The description of the BUPA liver dataset has been given in table 8. Total 306 instances (ninefold) have been used to train the system and remaining 34 instances (onefold) have been used for the testing purpose. The training of neural network formed for BUPA liver dataset produces the best results with three numbers of hidden layer neurons. The separability achieved for neurons is 0.21, 0.95, and 0.68 respectively. The number of iterations required to achieve best result is at g 70 for this dataset. Table 9 small value of Dh may increase the number of iterations. The appropriate value of Dh can be selected from the range of Dh 2 0:01  p; 0:05  p . Therefore, to make a tradeoff between the maximum number of values ( which is between 1 and 0 ) and number of iterations for the faster con- vergence of a problem, the angular displacement Dh has been selected as 0:03  p [2, 12]. The quantum based updation process performs well, even with a small population, without premature convergence as compared to the conventional genetic algorithm [12]. As weight selection process using quantum computing concept is compared to another method as fuzzy neural network, in which weights are generally center point of the input dataset. In fuzzy, the parameter selection process is done through random selection, which may or may not belong to dataset point. Due to which algorithm may converge to local maxima and minima problem, whereas quantum approaches gives better search space with the help of Gaussian random generator. 5. Conclusion This paper presents a novel quantum inspired binary neural network algorithm. This algorithm forms a neural network constructively by using the quantum computing concept for deciding connection weights and separability parameter. The proposed quantum separability parameter helps to nd out the proper plane to classify the input data. The NQ-BNN algorithm automatically forms an optimal network structure with the above-mentioned parameters. The algorithm has been tested on the breast cancer dataset, PIMA Indian diabetes dataset and liver disease diagnosis dataset. The results show improvement in classi cation accuracy when compared to the methods like [17, 20 23]. References [1] Gupta A K and Singh Y P 2011 Analysis of bidirectional associative memory of neural network method in the string recognition. In: Proceeding of 2011 International Confer- ence on Computational Intelligence and Communication Networks (CICN), pp 172 176, IEEE [2] Lu T-C, Yu G-R and Juang J-C 2013 Quantum-based algo- rithm for optimizing arti cial neural networks. IEEE Trans. Neural Netw. Learn. Syst. 24(8): 1266 1278 [3] Wang T and Wang Y 2010 Pattern classi cation with ordered features using MRMR and neural networks. In: Proceeding of 2010 International Conference on Information, Network- ing and Automation (ICINA), pp 2128 2131, IEEE [4] Aydin M and Celik E 2013a Assamese character recognition with arti cial neural networks. In: Proceeding of 2013 21st International Conference on Signal Processing and Com- munications Applications Conference (SIU), pp 1 4, IEEE Table 7. Comparison of various learning algorithms with NQ- BNN on PIMA Indian dataset. Methods Training accuracy (%) Generalization accuracy (%) NQ-BNN 92.74853 88.28657 Q-BNN 85.3 85.6 MTiling-real 85.3 80.6 MPyramid- real 81.3 80.3 Perceptron 81.2 80.9 Bold values show the best result corresponding to the proposed approach Table 8. Class distribution of BUPA liver dataset. Index Class name Class size Class distribution (%) C1 Negative 198 58.23 C2 Positive 142 41.76 Table 9. Results of NQ-BNN for BUPA liver dataset. BUPA liver dataset Hidden layer neuron Training accuracy Testing_accuracy Set 1 3 91.5524 95.34 Set 2 3 91.2457 95.2485 Set 3 3 91.3658 95.2475 Set 4 3 90.4578 95.6845 Set 5 3 92.1247 94.6235 Set 6 3 91.4583 94.7851 Set 7 3 91.7452 94.2519 Set 8 3 91.4578 95.2156 Set 9 3 91.2457 94.3615 Set 10 3 90.1458 93.4715 Average 91.27992 94.82296 Table 10. Comparison of various learning algorithms with NQ- BNN on BUPA liver dataset. Classi cation algorithm Accuracy (%) Precision NQ-BNN 94.82296 95.35 QBNN-L 90.35 94.00 Logistic 67.39 75.00 Linear logistic regression 69.57 74.70 Gaussian processes 73.91 79.01 Logistic model trees 68.12 73.49 Multilayer perceptron 68.84 76.32 K-STAR 59.42 71.43 RIPPER 64.49 71.25 Neural net 73.91 77.65 Rule induction 64.49 76.56 Support vector machine 69.23 75.00 Classi cation and regression trees 66.35 77.36 Bold values show the best result corresponding to the proposed approach 1308 Om Prakash Patel and Aruna Tiwari [5] Aydin M and Celik E 2013b Assamese character recognition with arti cial neural networks. In: Signal Processing and Communications Applications Conference (SIU), 2013 21st, pp 1 4, IEEE [6] Turnip A, Hong K-S and Ge S S 2010 Backpropagation neural networks training for single trial eeg classi cation. In: Proceeding of 2010 29th International Conference on Chi- nese Control Conference (CCC), pp 2462 2467, IEEE [7] Chan L-H, Salleh S-H and Ting C-M 2009 Pca, lda and neural network for face identi cation. In: Proceeding of 2009 4th IEEE Conference on Industrial Electronics and Appli- cations (ICIEA), pp 1256 1259, IEEE [8] Ou G and Murphey Y L 2007 Multi-class pattern classi - cation using neural networks. Pattern Recognit. 40(1): 4 18 [9] Sarikaya R, Hinton G E and Deoras A (2014) Application of deep belief networks for natural language understanding. IEEE/ACM Trans. Audio Speech Lang. Process. 22(4): 778 784 [10] Sun B 2013 Analysis and detection of nonlinear analogue based on variable threshold value neuron. In: Proceedings of 2013 Fifth International Conference on Measuring Tech- nology and Mechatronics Automation (ICMTMA), pp 225 228, IEEE [11] Narayanan A and Moore M 1996 Quantum-inspired genetic algorithms. In: Proceeding of 1996 International Conference on Evolutionary Computation, pp 141 144, IEEE [12] Han K-H and Kim J-H 2002 Quantum-inspired evolutionary algorithm for a class of combinatorial optimization. IEEE Trans. Evolut. Comput., 6(6): 580 593 [13] Han K-H and Kim J-H 2004 Quantum-inspired evolutionary algorithms with a new termination criterion, He gate, and two-phase scheme. IEEE Trans. Evolut. Comput. 8(2): 156 169 [14] Gandhi V, Prasad G, Coyle D, Behera L and McGinnity T M 2013 Quantum neural network-based eeg ltering for a brain- computer interface. IEEE Trans. Neural Netw. Learn. Syst. 25(2): 278 288 [15] Li F and Xu G 2009 A novel scheme of speech enhancement based on quantum neural network. In: Proceeding of 2009 International Asia Symposium on Intelligent Interaction and Affective Computing (ASIA), pp 141 144, IEEE [16] Caraiman S and Manta V 2012 Image processing using quantum computing. In: Proceeding of 2012 16th Interna- tional Conference on System Theory, Control and Computing (ICSTCC), pp 1 6, IEEE [17] Parekh R, Yang J and Honavar V 2000 Constructive neural- network learning algorithms for pattern classi cation. IEEE Trans. Neural Netw. 11(1): 436 451 [18] Huang S-C and Huang S-C 1991 Bounds on the number of hidden neurons in multilayer perceptrons. IEEE Trans. Neural Netw. 2(1): 47 55 [19] Chaudhari N S and Tiwari A 2010 Binary neural network classi er and it s bound for the number of hidden layer neurons. In: Proceeding of 2009 Control 11th International Conference on Automation Robotics & Vision (ICARCV), pp 2012 2017, IEEE [20] Patel O P and Tiwari A 2014 Quantum inspired binary neural network algorithm. In: Proceeding of 2014 International Conference on Information Technology (ICIT), pp 270 274, IEEE [21] Patel O P and Tiwari A 2015 Liver disease diagnosis using quantum-based binary neural network learning algorithm. In: Proceedings of 2015 Fourth International Conference on Soft Computing for Problem Solving, pp 421 430, Springer [22] Abdulloh Baka S V and Wiphada Wettayaprasit 2014 A novel discretization technique using class attribute interval average. In: Proceeding of 2014 Fourth International Con- ference on Digital Information and Communication Tech- nology and it s Applications (DICTAP), pp 95 100, IEEE [23] Bahramirad S, Mustapha A and Eshraghi M 2013 Classi - cation of liver disease diagnosis: Acomparative study. In Proceeding of 2013 Second International Conference on Informatics and Applications (ICIA), pp 42 46, IEEE [24] Lichman M 2013 UCI machine learning repository.http:// archive.ics.uci.edu/ml [25] Islam M M, Yao X and Murase K 2003 A constructive algorithm for training cooperative neural network ensembles. IEEE Trans. Neural Netw. 14(4): 820 834 Novel quantum inspired binary neural network algorithm 1309