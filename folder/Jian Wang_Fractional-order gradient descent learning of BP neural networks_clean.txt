Neural Networks 89 (2017) 19 30 Contents lists available at ScienceDirect Neural Networks journal homepage: www.elsevier.com/locate/neunet Fractional-order gradient descent learning of BP neural networks with Caputo derivative Jian Wang a, , Yanqing Wen a, Yida Gou a, Zhenyun Ye b, Hua Chen a, a College of Science, China University of Petroleum, Qingdao, 266580, China b College of Computer & Communication Engineering, China University of Petroleum, Qingdao, 266580, China a r t i c l e i n f o Article history: Received 22 June 2016 Received in revised form 4 February 2017 Accepted 14 February 2017 Available online 22 February 2017 Keywords: Fractional calculus Backpropagation Caputo derivative Monotonicity Convergence a b s t r a c t Fractional calculus has been found to be a promising area of research for information processing and modeling of some physical systems. In this paper, we propose a fractional gradient descent method for the backpropagation (BP) training of neural networks. In particular, the Caputo derivative is employed to evaluate the fractional-order gradient of the error defined as the traditional quadratic energy function. The monotonicity and weak (strong) convergence of the proposed approach are proved in detail. Two simulations have been implemented to illustrate the performance of presented fractional-order BP algorithm on three small datasets and one large dataset. The numerical simulations effectively verify the theoretical observations of this paper as well. 2017 Elsevier Ltd. All rights reserved. 1. Introduction Fractional differential calculus has been a classical notion in mathematics for several hundreds of years. It is based on differ- entiation and integration of arbitrary fractional order, and as such it is a generalization of the popular integer calculus. Yet only re- cently it has been applied to the successful modeling of certain physical phenomena. A number of papers in the literature (Kvitsin- skii, 1993; Love, 1971; McBride, 1986; Miller, 1995; Nishimoto, 1989; Oldham & Spanier, 1974) have recently reported fractional differential calculus as a model which describes much better than the conventional integer calculus on some certain selected natu- ral phenomena. Descriptions of viscosity of liquids, diffusion of EM waves and fractional kinetics are such a few examples of dynamics of certain systems that can be successfully expressed in fractional This work was supported in part by the National Natural Science Foundation of China (No. 61305075), the China Postdoctoral Science Foundation (No. 2012M520624), the Natural Science Foundation of Shandong Province (Nos. ZR2013FQ004, ZR2013DM015), the Specialized Research Fund for the Doctoral Program of Higher Education of China (No. 20130133120014) and the Fundamental Research Funds for the Central Universities (Nos. 13CX05016A, 14CX05042A, 15CX05053A, 15CX08011A). Corresponding authors. E-mail addresses: wangjiannl@upc.edu.cn (J. Wang), chenhua@upc.edu.cn (H. Chen). formats. A good literature review in support of this statement is in Wang, Yu, Wen, Zhang, and Yu (2015). As a consequence, the study of dynamics based on fractional- order differential systems has attracted considerable research interest. Novel results include solutions for chaos and stability analysis in fractional-order systems (Delavari, Baleanu, & Sadati, 2012; Deng & Li, 2005; Wu, Li, & Chen, 2008). In Wu et al. (2008), fractional multipoles, fractional solutions for Helmholtz, and fractional image processing methods were effectively studied and promising results have been produced in electromagnetics. By using the Laplace transform theory, chaos synchronization of the fractional L system with suitable conditions has been meticulously proved (Deng & Li, 2005). In Delavari et al. (2012), the Lyapunov direct method was extended to describe the Caputo type fractional-order nonlinear systems, and the comparison theorem of these systems was proposed by using Bihari s and Bellman Gronwall s inequalities. The fractional differential calculus has also been successfully adopted by the field of neural networks. Some remarkable research of fractional-order neural networks has been presented in Chen (2013), Chen and Chen (2016), Rakkiyappan, Cao, and Velmurugan (2015); Rakkiyappan, Sivaranjani, Velmurugan, and Cao (2016) and Zhang, Yu, and Wang (2015). In Chen (2013), fractional calculus was used for the Backpropagation (BP) (Rumelhart, Hinton, & Williams, 1986) algorithm for feedforward neural networks (FNNs). The simulation results demonstrated that the convergence speed based on fractional-order FNNs was much faster than integer-order FNNs. By extending the second method of Lyapunov, http://dx.doi.org/10.1016/j.neunet.2017.02.007 0893-6080/ 2017 Elsevier Ltd. All rights reserved. 20 J. Wang et al. / Neural Networks 89 (2017) 19 30 the Mittag-Leffler stability analysis was performed for fractional- order Hopfield neural networks (Zhang et al., 2015). In Zhang et al. (2015), the stability conditions have been used to achieve complete and quasi synchronization in the coupling case of these networks with constant or time-dependent external inputs. The global Mittag-Leffler stability and global asymptotical periodicity of the fractional-order non-autonomous neural networks were successfully investigated in Chen and Chen (2016) by using a fractional-order differential and integral inequality technique. For fractional-order complex-valued neural networks, the existence and stability analyses have been studied in detail in Rakkiyappan et al. (2015, 2016). In Xiao, Zheng, Jiang, and Cao (2015), a fractional-order recurrent neural network model was studied with commensurate or incommensurate orders to exhibit the dynamic behaviors. The simulation results demonstrate that the dynamics of fractional-order systems are not invariant, in contrast to integer- order systems. However, most research findings for fractional-order systems have been limited to studies of fully coupled recurrent networks of Hopfield type (Chen & Chen, 2016; Rakkiyappan et al., 2015, 2016; Wang, Yu, & Wen, 2014; Wu & Zen, 2013; Wu, Zhang, & Zen, 2011a; Xiao et al., 2015; Zhang et al., 2015). The vast majority of pa- pers have been focused on studying properties of fixed points for non-integer order differential equations that describe such net- works. The researched networks vary in their properties: they are with or without delay in the feedback loop, while other extensions have provided generalizations to complex-valued neurons. In con- trast, this work concerns fractional-order error BP in FNNs. Gradient descent method is commonly used to train FNNs by minimizing the error function, being the norm of a distance be- tween the actual network output and the desired output. There exist other optional methods to implement the BP algorithm for FNNs, such as conjugate gradient, Gauss Newton and Leven- berg Marquardt. We note that all of the above optimal methods are typically employed to train integer-order FNNs. To our best knowledge, there is very limited literature focused on the convergence analysis of fractional-order FNNs. The existing convergence results are mainly concentrated on integer-order gradient-based FNNs. For batch mode training, the BP algorithm of FNNs with penalty was proven to be deterministic convergent (Wu, Shao, & Li, 2006). In addition, the weight sequence is uniformly bounded due to the influence of the penalty term. For incremental training, weak and strong convergence results are obtained with or without penalty terms based on different assumptions for the activation function, learning rates and the stationary points of the objective function (Shao, Wu, & Liu, 2010; Wu, Wang, Cheng, & Li, 2011b; Xu, Zhang, & Jin, 2009). Only very recently, fractional neural networks have been eval- uated in a broader context of training and the minimization of an objective function (Pu et al., 2015). This paper has offered a de- tailed analysis of fractional gradient descent learning conditions, and has supported it with initial convergence studies of minimiza- tion of quadratic energy norm and also with numerical illustra- tion of searching for extreme points during the Fractional Adaptive Learning (FAL). Inspired by Chen (2013) and Pu et al. (2015), we apply the frac- tional steepest descent algorithm to train FNNs. In particular, we employ the Caputo derivative formula to compute the fractional- order gradient of the error function with respect to the weights and obtain the deterministic convergence. The main contributions of this paper are as follows: (1) Under suitable conditions for activation functions and the learning rate, the monotonic decreasing property of the error function has been guaranteed. For the activation functions of hidden and output layers, we assume that the first and second derivatives of the activation functions are uniformly bounded. This condition can be easily satisfied since the most common sigmoid activation functions are uniformly bounded on R and infinitely differentiable. (2) The deterministic convergence of the proposed fractional- order training algorithm has been rigorously proved, which prevents the divergence behavior from a theoretical point of view. The weak convergence means that the fractional-order gradient of the error function approaches zero when the iter- ation number tends to infinity, while the strong convergence means the weight sequence goes to a fixed point. (3) Numerical simulations are reported to illustrate the effective- ness of the proposed factional-order neural networks and sup- port the theoretical results in this paper. Selected benchmark UCI datasets have been used to compare the performances of fractional-order vs. integer-order based FNNs. The simulations demonstrate that the training and testing accuracies for fractional-order FNNs are better than those for first-order gradient based FNNs. In addition, the monotonicity of error function and weak convergence have been figured out through the MNIST dataset. We note that the error function in Pu et al. (2015) is quite different from that in this paper. In Pu et al. (2015), the error function is defined as the sum of the first-order extreme values of the error function and the quadratic norm of the parameters and its first-order extreme value. In this work, we consider the error function to be the squared error being the difference between the actual output and the desired output. Another difference between (Pu et al., 2015) and this paper is in the definition of the convergence itself. In Pu et al. (2015), it focuses on the regular convergence rate of the proposed algorithm, that is, on the exponential convergence. In this paper the convergence behavior of fractional-order BP algorithm is evaluated. That is, the norm of the gradient of the error function approaches zero (weak convergence). Authors also provide strong convergence proof. The structure of the paper is as follows: in Section 2, the definitions of three commonly used fractional-order derivative are introduced. The traditional BP algorithm and our novel algorithm of fractional-order BP neural networks training based on Caputo derivative are presented in Section 3. In Section 4, the convergence results are presented, and the detailed proofs of the main convergence results are stated as Appendix. Numerical simulations are presented to illustrate the effectiveness of our results in Section 5. Finally, the paper is concluded in the last section. 2. Fractional-order derivative Unlike the situation with integer-order derivatives, several definitions are used for fractional-order derivatives. The three most common definitions are by Grunwald Letnikov (GL), Rie- mann Liouville (RL), and Caputo (Love, 1971; McBride, 1986; Nishimoto, 1989; Oldham & Spanier, 1974). Definition 2.1 (GL Fractional-Order Derivative). The GL derivative with order of function f (t) is defined as GL aD t f (t) = n k=0 f (k)(a)(t a) +k ( + k + 1) + 1 (n + 1) t a (t )n f (n+1)( )d , (1) where GL aD t is the GL fractional derivative operator, > 0, n 1 < < n, n N, f (t) is the objective function under consideration and [a, t] is the interval of f (t). ( ) is the Gamma function, that is, ( ) = 0 t 1e tdt. J. Wang et al. / Neural Networks 89 (2017) 19 30 21 Definition 2.2 (RL Fractional-Order Derivative). The Riemann Liouville derivative with order of function f (t) is defined as RL aD t f (t) = dn dtn aD (n ) t f (t) = 1 (n ) dn dtn t a (t )n 1f ( )d , (2) where RL aD t is the RL fractional derivative operator. Moreover, the GL fractional derivative can be deduced from the definition of the RL fractional derivative. Definition 2.3 (Caputo Fractional-Order Derivative). The definition of the Caputo fractional-order derivative of order is defined as follows Caputo aD t f (t) = 1 (n ) t a (t )n 1f (n)( )d , (3) where Caputo aD t is the Caputo derivative operator, is the fractional order. Particularly, when (0, 1), the expression for Caputo derivative is as follows Caputo aD t f (t) = 1 (1 ) t a (t ) f ( )d . (4) When (0, 1) and a = 0, there is a visible difference in the derivation with respect to a constant between RL derivative and Caputo derivative. Suppose that K is a constant, it is easy to conclude that RL aD t K = K (1 )x = 0 and Caputo aD t K = 0. Since the initial value of fractional differential equations with the Caputo derivative is the same as that of the integer differential equations: this derivative has more applications in physical pro- cesses and engineering problems (Gorenflo & Mainardi, 2008). In this paper, we just employ the Caputo fractional-order derivative to evaluate the BP training algorithm for fractional FNNs. For con- venience, we use the notion aD t to denote Caputo fractional deriva- tive operator instead of Caputo aD t . 3. Algorithm description Let us begin with an introduction of a network with three layers. The numbers of neurons for the input, hidden and output layers are p, n and 1, respectively. Suppose that the training sample set is {xj, Oj} J j=1 Rp R, where xj and Oj are the input and the corresponding ideal output of the jth sample. Let g, f : R R be given activation functions for the hidden and the output layers, separately. Let V = vij n p be the weight matrix connecting the input and the hidden layers, and write vi = vi1, vi2, . . . , vip T Rp for i = 1, 2, . . . , n. The weight vector connecting the hidden and output layers is denoted by u = (u1, u2, . . . , un)T Rn. To simplify the presentation, we combine the weight matrix V with the weight vector u, and write the total weight vector as follows w = (u, V) = (uT, vT 1, . . . , vT n)T Rn(p+1). (5) For convenience, we introduce the following vector valued function G(z) = (g(z1), g(z2), . . . , g(zn))T, z Rn. (6) 3.1. BP algorithm based on gradient descent method For any given jth input xj Rp, the final actual output is y = f u G Vxj . (7) For any fixed weights w, the error of the neural networks is defined as E (w) = 1 2 J j=1 Oj f u G Vxj 2 = J j=1 fj(u G(Vxj)), (8) where fj(t) = 1 2(Oj f (t))2, j = 1, 2, . . . , J, t R. We note that the constructed function fj( ) is a composite function of f and G in terms of the jth sample. Given an initial weight w0 = u0, V0 , the kth weight vector, wk = uk, Vk , is iteratively updated according to the classical BP training algorithm (Rumelhart et al., 1986). To be specific, we show the updating formula with the following forms, uk+1 i = uk i Euk i (w), (9) vk+1 im = vk im Evk im(w), (10) where k N, i = 1, 2, . . . , n, m = 1, 2, . . . , p, uk i and vk im are the corresponding component weights of the weight vector uk and vk i , respectively; > 0 is the learning rate; and Euk i (w) = J j=1 f j (u G(Vxj))g(vk i xj), (11) Evk im(w) = J j=1 f j (u G(Vxj))uig (vk i xj)xj m, (12) where k N, i = 1, 2, . . . , n, j = 1, 2, . . . , J. 3.2. Fractional-order BP algorithm based on Caputo fractional-order derivative For the given jth input sample xj Rp, i,j = vi1x1 j + vi2x2 j + + vipxp j = vi xj (i = 1, 2, . . . , n) is the jth input value of ith hidden neuron. The input of output layer is represented by j = u1g(v1 xj)+ u2g(v2 xj)+ + ung(vn xj) = u G(Vxj) and the actual output is given by yj = f (u G Vxj ) after activation. For any fixed weights w, a conventional square error function is given as E (w) = 1 2 J j=1 Oj f u G Vxj 2 = J j=1 fj(u G(Vxj)), (13) where fj(t) = 1 2(Oj f (t))2, j = 1, 2, . . . , J, t R. For any initial weight w0 = (u0, V0), the general kth weight vector wk = uk, Vk of weight sequence is adjusted in terms of Caputo -order steepest descent method. To be specific, the updating formulas of the components of wk are given as follows uk+1 i = uk i cD uk i E(w), (14) vk+1 im = vk im cD vk im E(w), (15) 22 J. Wang et al. / Neural Networks 89 (2017) 19 30 where > 0 is the learning rate, 0 < < 1 is the fractional order and c = min{ui k, vk im}(k N, i = 1, . . . , n, m = 1, . . . , p). According to the definition of Caputo fractional derivative, the fractional-order differential of a composite function may be represented as the product of an integer-order differential and a fractional order differential. Suppose h(s(t)) to be a composite function, its -order derivative with respect to t is aD t h(s) = s(h(s)) aD t s(t). (16) Next, we will divide it into two parts: Part 1. cD uk i E(w). According to (16), we have cD uk i E(w) = E(w) j c D uk i ( j), (17) where E(w) j = J j=1 f j (u G(Vxj)). (18) By the definition of Caputo -order derivative, 0 < < 1, we see that cD uk i ( j) = 1 (1 ) uk i c (uk i ) g(vk i xj)d = 1 (1 ) g(vk i xj) uk i c (uk i ) d = 1 (1 ) 1 1 g(vk i xj)(uk i c)1 , (19) where k N, i = 1, 2, . . . , n and j = 1, 2, . . . , J. Therefore, it follows from (17) (19) that cD uk i E(w) = 1 (1 ) (1 ) J j=1 f j (u G(Vxj))g(vk i xj)(uk i c)1 . (20) Part 2. cD vk im E(w). Similar to (17), we deduce that cD vk im E(w) = E(w) i,j cD vk im( i,j), (21) where E(w) i,j = J j=1 f j (u G(Vxj))uig (vi xj). (22) Employing (4), we find that cD vk im( i,j) = 1 (1 ) vk im c (vk im ) xj md = 1 (1 ) xj m 1 1 (vk im )1 | vk im c = 1 (1 ) (1 ) xj m(vk im c)1 (23) where k N, i = 1, 2, . . . , n, m = 1, 2, . . . , p and j = 1, 2, . . . , J. Combining (21) (23), we have that cD vk im E(w) = 1 (1 ) (1 ) J j=1 f j (u G(Vxj))uk i g (vi xj)xj m(vk im c)1 , (24) where k N, i = 1, 2, . . . , n, m = 1, 2, . . . , p, j represents the order of the sample and the parameter is the fractional order of Caputo derivative. To be simple, we write the vector form of the above fractional- order Caputo derivative cD wE(w) = (cD u1E(w), . . . , cD unE(w), cD v11E(w), . . . , cD vnpE(w)). (25) 4. Main results In this section, the convergent behavior of the proposed Caputo fractional-order BP training algorithm is described. For any x = (x1, x2, . . . , xp) Rp, we write x = p m=1 x2 m, where stands for the Euclidean norm in Rp. Let = {w : cD wE(w) = 0} be the -order stationary point set of the error function E(w), where Rn(p+1) is a bounded open set and 0 < < 1. Actually, we assume that the activation functions g and f are bounded and infinitely differentiable on R, furthermore, all of its corresponding derivatives are also continuous and bounded on R. For the convenience of theoretical analysis, we assume that the uniform upper bound of |g(t)|, |g (t)|, |g (t)|, |f j (t)| and |f j (t)| (j = 1, . . . , J) is C1. In terms of the assumption (A2) and the finite samples, we denote that C2 max 1 j J xj , (26) C3 sup k N wk , (27) where j = 1, . . . , J, k N. An upper bounded assumption of learning rate is an essential part to guarantee the deterministic convergence of the proposed algorithm. It is shown as below = (1 ) (1 )(C3 c) 1 1 2nJC2 1 C3C2 2 + C1J 2 (1 + C2 1 C2 2) + nJC3 1 + JC3 1 C2 2 C2 3 , (28) where the parameters , c, n, J, C1, C2, C3 are already listed above. To be clear, we list all of the following assumptions which are employed to analyze the theoretical results of the proposed algorithm. (A1) The activations g and f are the common Sigmoid functions; (A2) The boundedness of the weight sequence wk (k N) is valid during training procedure; (A3) The learning rate is a positive number with an upper bound ; (A4) The set contains a finite number of points. Theorem 4.1. Suppose that the error function E(w) is defined by (13), w0 Rn(p+1) be an arbitrary initial value, the learning sequence {wk} be generated by (14) and (15). Assume that conditions (A1) (A3) are valid, then we have (i) E(wk+1) E(wk), k N; (ii) There exists E 0 such that limk E(wk) = E ; (iii) limk cD wE(wk) = 0; In addition, if assumption (A4) is also valid, then we have following strong convergence: (iv) There exists w such that limk wk = w . J. Wang et al. / Neural Networks 89 (2017) 19 30 23 Table 1 Datasets summary for classification. Data set Data size Input features Classes 1. Iris 150 4 3 2. Liver 345 6 2 3. Sonar 208 60 2 Table 2 Network structures and learning parameters for different datasets. Data sets Architecture Initial interval Max iteration Learning rate 1. Iris 4-20-3 [ 0.5,0.5] 600 0.5 2. Liver 6-22-2 [ 0.5,0.5] 200 0.1 3. Sonar 60-20-2 [ 0.8,0.8] 500 0.05 5. Experiments To demonstrate the performance and the theoretical results of the presented algorithm, we carried out the following two simulations. The first simulation focuses on the comparison of the three different fractional-order BP algorithms: Caputo, RL and GL in Section 2. Specifically, the second one displays the observations of the proposed algorithm which based on the Caputo derivatives. In addition, the two curves of error function and the norm of gradient illustrate the theoretical conclusions of this paper. These two simulations have been carried out in MATLAB 2015 environment running an Intel i5, 2.67G CPU. 5.1. Three benchmark datasets Due to the three classical definitions of fractional-order deriva- tives, it is necessary to compare their performance on benchmark datasets. The classification datasets listed in Table 1 for this simula- tion are three simple problems. Each dataset is randomly split into two subsets with a fixed proportion, training and testing samples are separately set to be 60% and 40%. To avoid the large inputs dom- inating the calculation, all of the datasets have been transformed into the interval [ 1, 1] in terms of preprocessing procedure. For the above benchmark datasets, we construct the corre- sponding networks for the different learning algorithms (cf. Ta- ble 2). They differ by network structures, initial intervals of weights, maximum epochs and learning rates. For the sake of com- paring the performance sufficiently, the simulations were repeated 5 times for each dataset. The 60% training samples of each dataset are first randomly chosen and then fixed during training for the different algorithms. To analyze the numerical performance based on different fractional-order derivatives, three performance metrics have been conducted: training accuracy, testing accuracy and training time. Fig. 1 demonstrates the classification ability on training samples and the generalization on testing samples, separately. According to the three factional-order derivatives, the training and testing accuracies on Iris are respectively listed in Fig. 1 (a1) and (a2). It shows that the BP algorithm based on Caputo derivative performs much better than the other two algorithms. For Liver dataset, however, Fig. 1 (b1) and (b2) observe that the RL and GL based BP algorithms perform similar (except for the case of 1 9-order) and better than Caputo based BP algorithm. While, it displays that Caputo based BP algorithm performs slightly better than GL and RL based BP algorithms on Sonar training samples in Fig. 1 (c1). Fig. 1 (c2) demonstrates that Caputo and RL based BP algorithms beat the GL based BP algorithm for the smaller fractional-order derivatives ( 1 9 and 2 9 orders), and perform the similar generalization ability for the other fraction-orders. We note that all of these three algorithms perform very similar with each other when the fraction-order value approaches 8 9 on both training and testing samples. The training times listed in Table 3 represent the average run- ning time for the given fractional-order and fractional derivatives. It clearly shows that the training procedure consumes pretty sim- ilar time for the different fractional orders in the same fractional- order derivative. Interestingly, for given fractional orders, the Caputo and GL based BP algorithms are more time consuming (ap- proximately twice) than RL based BP algorithm. This would be an important topic to study the computation burden and reveal the inherent mechanism of these three different algorithms in the fu- ture. 5.2. MNIST To verify the convergence of the proposed FAL-BP, simulations have been done on the MNIST handwritten digital dataset. The results have been compared with the common BP algorithm. The MNIST handwritten digital dataset is collected from the NIST database of the National Institute of Standards and Tech- nology. Digital images are normalized to an image 28 28, and represented as 784 1 vectors. Each element in the vector is a number between 0 and 255, representing the gray levels of each pixel. The MNIST database has a total number of 60,000 training samples and 10,000 testing samples. In this simulation, we employ different fractional -order derivatives to compute the gradient of error function, where = 1 9, 2 9, 3 9, 4 9, 1 2, 5 9, 6 9, 7 9, 8 9 and 9 9 = 1, sep- arately ( = 1 corresponds to standard integer-order derivative for the common BP). The networks have one hidden layer, and the architecture is set to be 784 {100, 200, 300, 500} 10. For com- parison, each network was trained 5 times to calculate the follow- ing average metrics: training accuracy, testing accuracy, training errors and norms of gradient of error function. For each training, the initial weights, stop criteria, learning rate and hidden nodes are chosen to be identical for the 10 different values of , and the maximum training iteration is set to be 100 epochs. The detailed codes are from (Nielsen, 2016). To speed up the training process, the codes were translated from Python to Matlab program language. Our programming algorithms are identical to those in Nielsen (2016). To display the performances of differences of different fractional-order BP neural networks, we employed different learn- ing parameters: learning rates and hidden nodes are set to be {0.5, 1, 2, 3} and {100, 200, 300, 500}. For convenience, we graphed the training and testing accuracies with various learn- ing rates for fixed number of hidden nodes in Fig. 2. Each row of the figure shows the training and testing accuracies based on vari- ous learning rates and fixed hidden nodes, that is, one can observe the tendencies of accuracy for each learning parameter. Generally speaking, the accuracies with larger learning rates are higher than those with smaller learning rates in each sub-figure. In addition, we observe that training and testing accuracies are gradually in- creasing with increasing fractional-orders and then reach the peak around 7 9-order. One exception is in the sub-figure b(2), the testing accuracy is slightly higher than that in 8 9-order case (learning rate is 3). Another one is that the training accuracies of 6 9-order network with 500 hidden nodes are equal to or slightly higher than those with 7 9-order network. Fig. 3 shows the performances of different fractional-order BP algorithms with different hidden nodes for each fixed learning rate. In this figure, each row displays the training and testing accuracies based on various hidden nodes and fixed learning rate. It helps to find variations of accuracy under different fractional- order derivatives. It is clear to see that the networks with 100 hidden nodes show lowest performance among various network architectures. In addition, we note that the other three networks with 200, 300 and 500 hidden nodes perform similarly. This 24 J. Wang et al. / Neural Networks 89 (2017) 19 30 Fig. 1. The performance comparison of different fractional-order BP algorithms in terms of various fractional definitions on datasets: Iris, Liver and Sonar. Table 3 The comparison of training time on different fractional-order BP algorithms in terms of Iris, Liver and Sonar datasets. Data sets Frac defs 1/9 2/9 3/9 4/9 1/2 5/9 6/9 7/9 8/9 1. Iris Caputo 1.220 1.276 1.257 1.281 1.188 1.245 1.236 1.214 1.279 GL 1.311 1.276 1.229 1.244 1.149 1.257 1.263 1.296 1.240 RL 0.628 0.660 0.609 0.606 0.565 0.613 0.659 0.636 0.654 2. Liver Caputo 6.008 5.883 5.879 5.866 5.362 5.860 5.854 5.866 5.864 GL 5.853 5.876 5.871 5.860 5.375 5.868 5.887 5.887 5.831 RL 2.919 2.932 2.934 2.930 2.688 2.936 2.935 2.921 2.928 3. Sonar Caputo 4.916 4.893 4.893 4.916 4.697 5.021 4.940 4.984 4.893 GL 4.927 4.909 4.904 4.929 4.708 4.949 5.007 4.936 4.887 RL 2.449 2.460 2.480 2.475 2.330 2.475 2.491 2.503 2.463 shows us the number of hidden nodes between 200 and 300 is a better choice, since more hidden nodes are more time-consuming computationally. More importantly, we reach an interesting conclusion that 7 9-order BP algorithms have been observed to have better performances in most cases. Under these specific parameter settings: learning rate {0.5, 1, 2, 3} and hidden nodes {100, 200, 300, 500}, Figs. 2 and 3 demon- strate that the best fractional-order in training MNIST database should be near 7 9-order BP algorithm. To verify the theoretical results of this work, we have redone the simulation (learning rate is 0.5, and 100 hidden nodes) with 1000 training epochs and compare the 7 9-order and integer-order BP algorithms in terms of the above two performance figures. Fig. 4 shows the training accuracies for up to 1000 training epochs. It J. Wang et al. / Neural Networks 89 (2017) 19 30 25 Fig. 2. The comparison of different fractional and integer order BP algorithms for various learning rates with fixed numbers of hidden nodes. clearly shows that 7 9-order BP algorithm performs much better than the integer-order BP algorithm. In addition, it performs sta- ble after approximately 400 training epochs. Fig. 5 demonstrates the errors of 7 9-order BP networks. It is clear to see that the errors are monotonically decreasing. It also shows the norms of gradient of error function with respect to the total weights (25) for 7 9-order network in Fig. 5. We observe that the 7 9-order BP algorithms per- form stably and tend to converge to zero with increasing iterations. These observations effectively verify the theoretical results of the presented algorithm in this paper, such as monotonicity and con- vergence. 26 J. Wang et al. / Neural Networks 89 (2017) 19 30 Fig. 3. The comparison of different fractional and integer order BP algorithms for various numbers of hidden nodes at fixed learning rates. 6. Conclusions In this paper, we have extended the fractional steepest descent approach to BP training of FNNs. The proposed method and its the- oretical analyses distinct from the existing results. We have an- alyzed the convergence of a Caputo fractional-order two-layers neural network trained with BP algorithm. From theoretical point of view, we have proven the monotonicity of error function and weak (strong) convergence of the proposed Caputo fractional- order BP algorithm. The numerical results support the theo- retical conclusions very well. Boundedness of weight sequence during training plays an important role in convergent behavior on J. Wang et al. / Neural Networks 89 (2017) 19 30 27 Fig. 4. The training accuracies of 7 9 -order and common integer BP algorithms. Fig. 5. The training error and the norm of gradient with respect to weights. both numerical simulations and theoretical analysis in this paper. Our latest numerical simulations demonstrate that the weight se- quence by introducing l2 regularization method approaches to be stable during training. In addition, the trained network improves the generalization much better. Then, how to rigorously prove the boundedness of weight sequence is one attracted topic in our fu- ture work. Acknowledgments The authors would like to express their gratitude to the editors and anonymous reviewers for their valuable comments and sug- gestions which improve the quality of this paper. Appendix For the sake of description, we introduce the following notations: 1uk i = uk+1 i uk i = cD uk i E(w); (29) 1vk im = vk+1 im vk im = cD vk im E(w); (30) 1wk = wk+1 wk = cD wkE(w); (31) Gk = G Vxk ; (32) 9k = Gk+1 Gk (33) where k N; i = 1, 2, . . . , n; m = 1, 2, . . . , p. The proof of Theorem 4.1 is divided into four parts dealing with statements (i) (iv), respectively. Proof to (i) of Theorem 4.1. E(wk+1) E(wk), k N. By the error function (13) we have E(wk+1) = J j=1 fj(uk+1 Gk+1,j), (34) E(wk) = J j=1 fj(uk Gk,j). (35) By using the Taylor mean value theorem with Lagrange remain- der, we have the following estimation E(wk+1) E(wk) = J j=1 fj(uk+1 Gk+1,j) fj(uk Gk,j) = J j=1 f j (uk Gk,j)(1uk Gk,j + uk 9k,j + 1uk 9k,j) + 1 2 J j=1 f j (s k,j 1 )(uk+1 Gk+1,j uk Gk,j)2 1 + 2 + 3 + 4, (36) where s k,j 1 R is a constant between uk Gk,j and uk+1 Gk+1,j, 1 = J j=1 f j (uk Gk,j)1uk Gk,j, 2 = J j=1 f j (uk Gk,j)uk 9k,j, 3 = J j=1 f j (uk Gk,j)1uk 9k,j and 4 = 1 2 J j=1 f j (s k,j 1 )(uk+1 Gk+1,j uk Gk,j)2. To study the monotonic property of the given error function, we sequentially analyze the above four items, 1, 2, 3 and 4. For 1, we have that 1 = J j=1 f j (uk Gk,j)1uk Gk,j = J j=1 f j (uk Gk,j)1uk 1g(vk 1 xj) + J j=1 f j (uk Gk,j)1uk 2g(vk 2 xj) + + J j=1 f j (uk Gk,j)1uk ng(vk n xj). (37) It is clear to see that this equation is tightly related to Eq. (20). In order to substitute the same parts, the following two cases are considered Case 1. If (uk i c)1 = 0. By (27) and substituting (20) and (29) into (37), we have 1 = 1 (1uk 1)2(1 ) (1 )(uk 1 c) 1 1 (1uk n)2(1 ) (1 )(uk n c) 1 1 (1 ) (1 )(C3 c) 1 n i=1 (1uk i )2 = 1 (1 ) (1 )(C3 c) 1 1uk 2. (38) Case 2. If (uk i c)1 = 0. Without loss of generality, we as- sume that (uk 1 c)1 = 0. By (20) and (29), it is easy to prove that 28 J. Wang et al. / Neural Networks 89 (2017) 19 30 cD uk 1 E(w) = 0 and 1uk 1 = 0 are valid. Hence, (37) can be repre- sented as follows 1 = 0 1 (1uk 2)2(1 ) (1 )(uk 2 c) 1 1 (1uk n)2(1 ) (1 )(uk n c) 1 1 (1 ) (1 )(C3 c) 1 n i=1 (1uk i )2 = 1 (1 ) (1 )(C3 c) 1 1uk 2. (39) In terms of Case 1 and Case 2, we have the following estimation 1 1 (1 ) (1 )(C3 c) 1 1uk 2. (40) By using the Taylor mean value theorem with Lagrange remain- der, we get 2 = J j=1 f j (uk Gk,j)uk 9k,j = J j=1 f j (uk Gk,j) n i=1 uk i g (vk i xj)1vk i xj + J j=1 f j (uk Gk,j) 1 2 n i=1 uk i g (s k,j 2 )(1vk i xj)2 = J j=1 f j (uk Gk,j) n i=1 uk i g (Vk i xj)(1vk i1x j 1 + + 1vk ipxj p) + J j=1 f j (uk Gk,j) 1 2 n i=1 uk i g (s k,j 2 )(1vk i xj)2, (41) where s k,j 2 R is a constant between vk i xj and vk+1 i xj. It is remarkable to see that the result of above equation is re- lated to Eq. (24). By virtue of the above results, two cases also should be considered in order to substitute the same parts. Case 3. If (vk im c)1 = 0. Substituting (24) and (30) into (41) and (27), we have 2 = 1 (1 ) (1 ) n i=1 (1vk i1)2(vk i1 c) 1 1 (1 ) (1 ) n i=1 (1vk ip)2(vk ip c) 1 + 1 2 J j=1 f j (uk Gk,j) n i=1 uk i g (s k,j 2 )(1vk i xj)2 1 (1 ) (1 )(C3 c) 1 n i=1 p m=1 (1vk im)2 + 1 2 nJC2 1 C3C2 2 n i=1 1vk i 2 1 (1 ) (1 )(C3 c) 1 n i=1 1vk i 2 + A1 1wk 2, (42) where A1 = 1 2nJC2 1 C3C2 2 . Case 4. If (vk im c)1 = 0. Without loss of generality, we as- sume that (vk i1 c)1 = 0. Then, by (24) and (30), cD vi1k E(wk) = 0 and 1vk i1 = 0 are valid. In addition, it holds that 2 = 0 1 (1 ) (1 ) n i=1 (1vk i2)2(vk i2 c) 1 1 (1 ) (1 ) n i=1 (1vk ip)2(vk ip c) 1 + 1 2 J j=1 f j (uk Gk,j) n i=1 uk i g (s k,j 2 )(1vk i xj)2 1 (1 ) (1 )(C3 c) 1 n i=1 p m=2 (1vk im)2 + 1 2 nJC2 1 C3C2 2 n i=1 1vk i 2 1 (1 ) (1 )(C3 c) 1 n i=1 1vk i 2 + A1 1wk 2. (43) According to Case 3 and Case 4, we can obtain that 2 1 (1 ) (1 )(C3 c) 1 n i=1 1vk i 2 + A1 1wk 2, (44) where A1 = 1 2nJC2 1 C3C2 2 . Based on Cauchy Schwarz and the fundamental inequalities, it holds the following evaluation | 3| = J j=1 f j (uk Gk,j)1uk 9k,j J j=1 f j (uk Gk,j) 1uk 9k,j C1 2 J j=1 1uk 2 + 9k,j 2 . (45) By Lagrange mean-value theorem, we have 9k,j = Gk+1,j Gk,j = n i=1 (g (s k,j 3 ))2(1vk i xj)2 n i=1 (g (s k,j 3 ))2 1vk i 2 xj 2 C1C2 n i=1 1vk i 2, (46) and Gk,j = G(Vkxj) nC1 C4, (47) where s k,j 3 R is a constant between vk i xj and vk+1 i xj. Substituting (46) into (45), we have | 3| C1 2 J j=1 1uk 2 + C2 1 C2 2 n i=1 1vk i 2 J. Wang et al. / Neural Networks 89 (2017) 19 30 29 C1J 2 (1 + C2 1 C2 2) 1wk 2 = A2 1wk 2, (48) where A2 = C1J 2 (1 + C2 1 C2 2). Similarly, we have | 4| = 1 2 J j=1 f j (s k,j 1 )(uk+1 Gk+1,j uk Gk,j)2 C1 2 J j=1 (1uk Gk,j + uk 9k,j)2 C1 2 J j=1 2( 1uk 2 Gk,j 2 + uk 2 9k,j 2). (49) Substituting (46) and (47) into (49), we can obtain | 4| C1 2 J j=1 2 C2 4 1uk 2 + C2 3 C2 1 C2 2 n i=1 1vk i 2 (JC1C2 4 + JC3 1 C2 2 C2 3) 1wk 2 = A3 1wk 2, (50) where A3 = JC1C2 4 + JC3 1 C2 2 C2 3 . By assumption (A3), the estimation of (36) can be given as fol- lows E(wk+1) E(wk) = 1 + 2 + 3 + 4 1 + 2 + | 3| + | 4| 1 (1 ) (1 )(C3 c) 1 + A1 + A2 + A3 1wk 2 = 1 A4 + A5 1wk 2 0, (51) where A4 = (1 ) (1 )(C3 c) 1, A5 = A1 + A2 + A3. (52) We note that the upper bound of learning rate, , can be deter- mined by Eq. (52), that is, = A4 A5 . This then completes the proof of the statement (i) of Theo- rem 4.1: There exists E 0 such that limk E(wk) = E . Proof to (ii) of Theorem 4.1. From the conclusion of (i), we know that E(wk+1) E(wk), E(wk) 0 and E(wk) is also bounded below. Hence there exists E 0 such that lim k E(wk) = E . (53) The proof of (ii) is thus completed. Proof to (iii) of Theorem 4.1. limk c D wE(wk) = 0. Based on the result of (51), we can obtain that E(wk+1) E(wk) 1 A4 + A5 1wk 2 0. (54) Let = 1 A4 A5, we have the following formula E(wk+1) E(wk) 1wk 2 E(wk 1) ( 1wk 1 2 + 1wk 2) E(w0) k l=0 1wl 2. (55) Since E(wk+1) 0, we then get that k l=0 1wl 2 E(w0). (56) Let k , it holds that l=0 1wl 2 1 E(w0) < . (57) By the rule of series convergence, the general term has the following result lim k 1wk = 0. (58) According to (31), it is easy to obtain that lim k D wE(wk) = 0. (59) The proof is thus completed. Proof to (iv) of Theorem 4.1. We note that the error function E(w) is continuous and differentiable on the compact set Rn(p+1), furthermore, the weight sequence satisfies that lim k D wE(wk) = 0, lim k 1wk = 0. (60) Due to the fact that the fractional derivatives (20) and (24) are both continuous on , thus the corresponding gradient D wE(wk) is also continuous. The weight sequence {wk} (k N) has a subsequence {wki} (i N) which is convergent to, say, w . Then, limi wki = w . According to the continuity of D wE(wk), we can obtain that lim i D wE(wki) = D wE(w ) = 0. (61) By the assumption (A4), is the -order stationary point set of the error function E(w). Obviously, the limit point w . We can induce that any limit point is a stationary point of E. Let be the set of limit points {wk}. Then, it holds that based on the above discussion. Suppose that the set of limit points is = {z1, z2, . . . , zm}(m > 1), = min{ zi zj |i = j, i, j = 1, . . . , m} > 0, and that N(zi, ) = {s| w zi }. We can choose an integer k0 0 such that wk m i=1 N(zi, 4) and wk wk+1 4 for all k k0. There exists k1 k0 such that wk1 N(z1, 4). In addition, we have zi wk1+1 zi z1 ( z1 wk1 + wk1 wk1+1 ) 2 4 = 2, (i 2). (62) It shows that wk1+1 is not in the neighborhood N(zi, 4) for i 2. Then, we have that wk1+1 N(z1, 4). By mathematical induction, it can be obtained that wk N(z1, 4) for all k k1. However, this is contrast with the condition (A4) which states that there are a finite different number stationary points. Then, we have m = 1 and this completes the proof of (iv) of Theorem 4.1. References Chen, X. (2013). Application of fractional calculus in bp neural networks. (Ph.D. thesis), Nanjing, Jiangsu: Nanjing Forestry University. Chen, B., & Chen, J. (2016). Global o(t ) stability and global asymptotical periodicity for a non-autonomous fractional-order neural networks with time-varying delays. Neural Networks, 73, 47 57. Delavari, H., Baleanu, D., & Sadati, J. (2012). Stability analysis of caputo fractional- order nonlinear systems revisited. Nonlinear Dynamics, 67(4), 2433 2439. 30 J. Wang et al. / Neural Networks 89 (2017) 19 30 Deng, W., & Li, C. (2005). Chaos synchronization of the fractional l system. Physica A. Statistical Mechanics and its Applications, 353, 61 72. Gorenflo, R., & Mainardi, F. (2008). Fractional calculus: integral and differential equations of fractional order. Mathematics, 49(2), 277 290. Kvitsinskii, A. A. (1993). Fractional integrals and derivatives: theory and applications. Theoretical and Mathematical Physics, 3, 397 414. Love, E. R. (1971). Fractional derivatives of imaginary order. Journal of the London Mathematical Society, 3(2), 241 259. McBride, A. C. (1986). Fractional calculus. USA: Halsted. Miller, K. S. (1995). Derivatives of noninteger order. Mathematics Magazine, 68, 183 192. Nielsen, M. 2016. Neural networks and deep learning. http://neuralnetworksanddeeplearning.com/chap1.html. Nishimoto, K. (1989). Fractional calculus: integrations and differentiations of arbitrary order. New Haven, CT, USA: New Haven Univ. Press. Oldham, K. B., & Spanier, J. (1974). The fractional calculus: theory and applications of differentiation and integration to arbitrary order. USA: Academic. Pu, Y., Zhou, J., Zhang, Y., Zhang, N., Huang, G., & Siarry, P. (2015). Fractional extreme value adaptive training method: fractional steepest descent approach. IEEE Transactions on Neural Networks and Learning Systems, 26(4), 653 662. Rakkiyappan, R., Cao, J., & Velmurugan, G. (2015). Existence and uniform stability analysis of fractional-order complex-valued neural networks with time delays. IEEE Transaction on Neural Networks and Learning Systems, 26(1), 84 97. Rakkiyappan, R., Sivaranjani, R., Velmurugan, G., & Cao, J. (2016). Analysis of global o(t ) stability and global asymptotical periodicity for a class of fractional- order complex-valued neural networks with time varying delays. Neural Networks, 77, 51 69. Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Learning representations by back propagating errors. Nature, 323(9), 533 536. Shao, H., Wu, W., & Liu, L. (2010). Convergence of online gradient method with penalty for bp neural networks. Communications in Mathematical Research, 26(1), 67 75. Wang, H., Yu, Y., & Wen, G. (2014). Stability analysis of fractional-order hopfield neural networks with time delays. Neural Networks, 55, 98 109. Wang, H., Yu, Y., Wen, G., Zhang, S., & Yu, J. (2015). Global stability analysis of fractional-order hopfield neural networks with time delay. Neurocomputing, 154, 15 23. Wu, X., Li, J., & Chen, G. (2008). Chaos in the fractional order unified system and its synchronization. Journal of the Franklin Institute, 345(4), 392 401. Wu, W., Shao, H., & Li, Z. (2006). Convergence of batch bp algorithm with penalty for fnn training. In Neural information processing, Vol. 4232 (pp. 562 569). Wu, W., Wang, J., Cheng, M., & Li, Z. (2011b). Convergence analysis of online gradient method for bp neural networks. Neural Networks, 24, 91 98. Wu, A., & Zen, Z. (2013). Anti-synchronization control of a class of memristive recurrent neural networks. Commnications in Nonlinear Science and Numerical Simulation, 18(2), 373 385. Wu, A., Zhang, J., & Zen, Z. (2011a). Dynamic behaviors of a class of memristor-based hopfield networks. Physics Letters A, 375(15), 1661 1665. Xiao, M., Zheng, W., Jiang, G., & Cao, J. (2015). Undamped oscillations generated by hopf bifurcations in fractional-order recurrent neural networks with caputo derivative. IEEE Transaction on Neural Networks and Learning Systems, 26(12), 3201 3214. Xu, Z., Zhang, R., & Jin, W. (2009). When does online bp training converge? IEEE Transactions on Neural Networks, 20(10), 1529 1539. Zhang, S., Yu, Y., & Wang, H. (2015). Mittag-leffler stability of fractional-order hopfield neural networks. Nonlinear Analysis. Hybrid Systems, 16, 104 121.