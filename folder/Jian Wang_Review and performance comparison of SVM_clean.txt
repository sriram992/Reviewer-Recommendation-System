Review and performance comparison of SVM- and ELM- based classi ers$ Jan Chorowskia, Jian Wangb,a,c, Jacek M. Zuradaa aComputational Intelligence Laboratory, Department of Electrical and Computer Engineering, University of Louisville, Louisville, KY 40292, USA bSchool of Mathematical Sciences, Dalian University of Technology, Dalian 116024, China cCollege of Science, China University of Petroleum, Qingdao, 266580, China Abstract This paper presents how commonly used machine learning classi ers can be analyzed using a common framework of convex optimization. Four clas- si er models, the Support Vector Machine (SVM), the Least-Squares SVM (LSSVM), the Extreme Learning Machine (ELM), and the Margin Loss ELM (MLELM) are discussed to demonstrate how speci c parametrizations of a general problem statement a ect the classi er design and performance, and how ideas from the four di erent classi ers can be mixed and used together. Furthermore, twenty one public domain benchmark datasets are used to ex- perimentally evaluate ve performance metrics of each model and corrobo- rate the theoretical analysis. Comparison of classi cation accuracies under a nested cross-validation evaluation shows that with on exception all four mod- els perform similarly on the evaluated datasets. However, the four classi ers command di erent amounts of computational resources for both testing and training. These requirements are directly linked to their formulations as di erent convex optimization problems. Keywords: Classi ers, convex quadratic programming, SVM, LSSVM, ELM, MLELM, randomization. $J.W. and J.C. contributed equally to this paper. This work was supported by the China Scholarship Council (CSC) and the China Post- doctoral Science Foundation (No. 2012M520624). Email addresses: jan.chorowski@louisville.edu (Jan Chorowski), wangjiannl@mail.dlut.edu.cn (Jian Wang), jacek.zurada@louisville.edu (Jacek M. Zurada) Preprint submitted to Neurocomputing December 7, 2012 1. Introduction The discovery of Support Vector Machines in 1995 [1, 2] marks the be- ginning of development of a family of very e cient and unique classi ers. They operate by rst mapping input samples into a highly dimensional fea- ture space using a xed nonlinear transformation. The samples are then classi ed in this feature space with a linear decision function chosen in a way that maximizes the margin separating samples from di erent classes. This contribution discusses how similarly operating classi ers can be analyzed in a common framework of convex optimization problems. This is done by por- traying classi ers similarities and by highlighting how the di erences in their de nition a ect the properties of the resulting models. The SVM embodies many important principles. It solves the problem of classi cation directly without trying to solve the much harder problem of estimating the distribution of data samples [3]. It provides e cient means of trading the training error for generalization error. Furthermore, even in the nonlinear case, the very central minimization task is stated as a convex optimization problem for which e cient numerical methods of nding the globally optimum solution exist. The SVM uses two main ideas. First, kernel functions are used to trans- form the problem from the original input space into a highly dimensional one, called the feature space, where linear separation of training samples belonging to di erent classes is possible. Second, to nd the best separat- ing hyperplane, the concept of maximum margin is introduced. Finally, the optimization problem which de nes the SVM is convex and quadratic, and therefore it can be solved e ciently. We begin by formulating a convex optimization problem general enough to encompass all classi ers of interest [4]. Let there be a training set {(xi, yi)} Rn R consisting of N training samples xi and corresponding labels yi. The following analysis is restricted to the case of two classes, which are encoded as { 1, +1}. In section 2.6 we discuss extensions to multi-class problems. Let : Rn Rm denote a nonlinear transformation which projects data samples x into a m dimensional feature space (m can be in nite). Consider the following optimization problem: minimize w,b A||w||p p + C N X i=0 L(wT (xi) + b, yi) (1) 2 where w is a vector of tunable weights, b is the bias term, || ||p denotes a convex norm (p 1), L(o, y) is a loss function penalizing the classi er output o when the desired class is y, and A and C are nonnegative constants. Assume that L( , y) is convex in the rst argument for all possible y. Then L(wt (xi) + b, yi) is a convex function because convexity is invariant under a ne mappings. The problem (1) is then convex because it is a weighted nonnegative sum of convex terms [4]. Furthermore, when the transformation has an explicit formula small instances can be readily solved using convex programming tools, such as CVX [5]. In the special case of the identity feature transformation (x) = x, and when the L2 (p = 2) norm is used, the problem can be reduced to one of: 1. regularized least-squares regression (ridge regression) when L(o, y) = (o y)2, 2. regularized logistic regression when L(o, y) = log(1 + exp( y o)), 3. linear SVM when L(o, y) = max(0, 1 y o) (margin loss or hinge loss). Those three classi ers di er only by the loss function whose minimum is sought. The three di erent loss functions computed when y = 1 have been compared in Fig. 1 along with the (not convex) misclassi cation count which is 1 for a misclassi ed sample and 0 otherwise. The least-squares loss func- tion can be derived by nding a maximum likelihood estimation of weights under the assumption of Gaussian noise, while the logistic regression loss function stems from the assumption of Bernoulli binomial noise [6]. The loss function used in the SVM was introduced as a convex approximation to the number of misclassi cations [1]. It can be seen that it is closely linked to the loss function used in the logistic regression. Probabilistic interpretations of outputs of the SVM have been studied in [7, 8]. We have included in Fig. 1 the plot of the loss function L(o, y) = (tanh(o) y)2 which commonly arises when an arti cial neural network uses the hyperbolic tangent activa- tion function in the output layer. We see that it is not convex and can be interpreted as a smooth scaled approximation of the misclassi cation count. The addition of the hyperbolic tangent function may seem insigni cant, but it has important consequences the optimization problem no longer has a single global optimum and training algorithms converge only to local optima. To solve nonlinear problems, a transformation needs to be performed to project the data samples from the input space into a highly dimensional 3 2 1.5 1 0.5 0 0.5 1 1.5 2 2.5 3 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5 misclassification count least squares (ridge regression) logistic regression SVM (hinge loss) ANN tanh activation with quadratic loss Figure 1: Comparison between loss functions: misclassi cation count, least squares (ridge regression), logistic regression, SVM (margin loss), and arti cial neural network with tanh output activation function and quadratic loss. The curve of logistic regression has been scaled by 1/ ln 2 such that all curves contain the point (0, 1) (modi ed after [6]). feature space in which linear separation of the data is possible. In the case of an explicitly known transformation (x), the training of the classi er can be performed by directly solving the problem (1). For example, the Extreme Learning [9] approach advocates the use of randomly chosen transformations. In its original formulation, the Extreme Learning Machine (ELM) does not only use random transformations, but also chooses to minimize the least squares loss, for which a closed-form solution given by the Moore-Penrose pseudo-inverse exists to facilitate very fast training. In many cases it is bene cial to de ne the transformation indirectly with the use of a kernel function K(x1, x2) [1, 10 12]. If K satis es the conditions of Mercer s theorem, then there exist a space in which K de nes an inner product operation K(x1, x2) = (x1)T (x2). The kernel function can be used when the optimization problem (1) can be transformed into a form which depends only on the inner products between projections of samples. This is possible when the weights are regularized using the L2 norm. Without the loss of generality, we can substitute A = 1/2 into (1). Furthermore, dummy variables oi are introduced to represent classi er output: minimize w,b,o wTw 2 + C N X i=1 L(oi, yi) subject to: oi = wT (xi) + b (2) When L is di erentiable with respect to oi, (2) can be solved using the 4 method of Lagrange multipliers [12, 13]. Construct the Lagrangian = wTw 2 + C N X i=1 L(oi, yi) N X i=1 i(wT (xi) + b oi), (3) where i are Lagrange multipliers there is one for each constraint or, equiv- alently, one multiplier for each training sample. A stationary point is com- puted by solving the following system of equations [12, 13]: w = 0 wT = N X i=1 i (xi)T (4a) b = 0 N X i=1 i = 0 (4b) o = 0 C L(oi, yi) oi = i i (4c) = 0 wT (xi) + b = oi i. (4d) Furthermore, since the problem is convex the stationary point is the global minimizer of (2). From (4a) we can see that the weight vector w can be expressed as a weighted sum of projections of training samples [11, 12]. Furthermore, it can be plugged into the last equation (4d). Substituting the kernel function in place of (xi)T (xj) leads to a solution for . The relation wT (x) = PN i=1 i (xi)T (x) = PN i=1 iK(xi, x) can be used to classify new samples. Observe that unless special care is taken, most of the coe cients i can be nonzero and classi cation of new samples will be computationally expen- sive. This technique has been used to derive kernelized versions of many commonly used classi ers, that include kernelized logistic regression [14] and Least Squares SVM [15, 16] (LSSVM, or kernelized least squares) classi er. It is informative to compare four exemplary architectures of classi ers (presented in more detail in Sections 2 and 3) with the intuitive Fig. 2. The gure illustrates classi ers optimization goals for the objective function as columns, and the feature transformation methods from input space to a highly dimensional feature space as rows. Since the choice of the loss function is independent from the choice of the feature transformation we can form four di erent classi er architectures. 5 Optimization goal Margin Loss Least Squares Feature Transformation Kernel wTw 2 + C N X i=1 max(0, 1 oiyi) wTw 2 + C N X i=1 (oi yi)2 SVM (1995) LSSVM (1999) Randomization wTw 2 + C N X i=1 max(0, 1 oiyi) wTw 2 + C N X i=1 (oi yi)2 MLELM (2010) ELM (2006) where oi = wT (xi) + b and b may be 0 for ELM Figure 2: Relationship between SVM and ELM algorithms. 2. Algorithms and related basic concepts This section discusses in more detail how the four categories of classi ers highlighted in Fig. 2 operate. 2.1. Support Vector Machine (SVM) SVM was rst introduced by Vapnik [1, 2, 10, 17] for solving pattern classi cation and nonlinear function approximation problems [18 20]. For pattern classi cation, the main paradigm is to nd the optimal separating hyperplane as the decision boundary located in such a way that the margin of separation between classes is maximized. The derivation of the margin loss L(o, y) = max(0, 1 o y) used in the SVM and leading to a large margin separating classi er is presented in [1, 6, 12, 21]. For non-linear classi cation, training patterns are rst mapped into a highly dimensional space, for which kernel functions are used. The optimization problem solved by the SVM [1, 12, 20, 21] is: minimize w,b 1 2wTw + C N X i=1 max(0, 1 yi(wT (xi) + b)) (5) Because the loss function is not smooth, it is not possible to solve the La- grangian (3). We can, however, transform (5) into the following equivalent minimization task [1, 12, 20, 21]: minimize w,b, 1 2wTw + C N X i=1 i subject to: i 1 yi(wT (xi) + b) i i 0 i, (6) 6 where i are arti cial slack variables representing classi er errors and C is a constant. We can solve (6) using KKT optimality conditions [13]. Unlike the case of a di erentiable loss function which can be reduced to the system of equa- tions (4), the solution now requires maximizing an auxiliary problem, which is also called the dual [1, 12, 20, 21]: maximize 1 2 N X i=1 N X j=1 yiyj i j (xi) (xj) + N X i=1 i subject to: N X i=1 yi i = 0 i 0 i C i (7) The optimum weights are again recovered as a linear combination of Lagrange multipliers i: wT = PN i=1 i (xi)T. An important property of the SVM, linked directly to its non-smooth loss function and the resulting problem formulation with inequality constraints, is that the vector is sparse [1]. This means that classifying new data using the SVM requires the iteration over a small fraction of the training set. This provides important performance bene ts. 2.2. Least Squares Support Vector Machine (LSSVM) The original solution of the kernelized least squares classi er, called LSSVM has been presented in [15, 16]. The essential di erence between SVM and LSSVM is the use of the least squares loss function L(o, y) = 1 2(o y)2. Substituting this loss function into the equality constrained problem formu- lation (2) de nes LSSVM problem as1 minimize w,b,o 1 2wTw + C 2 N X i=1 (oi yi)2 subject to: oi = wT (xi) + b (8) 1In [15, 16] the authors de ned LSSVM using a slightly di erent, yet equivalent for- mulation. 7 The system of equations obtained by constructing the Lagrangian becomes (compare with (4) and [15, 16]): wT = N X i=1 i (xi)T 0 = N X i=1 i i = C(yi oi) i oi = wT (xi) + b i (9) A closed form solution for the Lagrange multipliers i can readily be ob- tained. The training of the LSSVM is much simpler than the training of the SVM, which requires solving (5). In case of large-scale problems many e cient it- erative methods exist to solve systems of linear equations [22]. Conjugate gradient method, one of the common iterative optimization methods, has been applied in [23] to solve (9). Benchmarking classi cation tests with UCI Machine Learning Repository data have been reported in 2004 for SVM and LSSVM classi ers, and the results demonstrate consistently good per- formance of these two methods [24]. However, the main shortcoming of LSSVM is that of the loss of sparseness of the multipliers i due to the choice of a smooth loss function. With large majority of multipliers i usually nonzero, the LSSVM classi er commands a lot of resources to classify new samples. Several heuristic remediations [25 29] have therefore been proposed to improve their sparseness and robustness. In [25], a weight pruning procedure is used based on removing training sam- ples by the sorted support value spectrum. An improvement method for achieving sparsity in LSSVM is presented in [26] by considering the residu- als for all training samples instead of only those incorporated in the sparse kernel expansion. 2.3. Extreme Learning Machine (ELM) ELM architecture was originally proposed in 2006 as a single hidden layer feedforward network in [9, 30]. It didn t use the regularization term used in (11). The most important contribution of the ELM is the proposition of using random independent nonlinear feature transformations . An input vector x 8 is transformed into an M dimensional vector whose jth component is de ned by one of: ( A(x))j = ( T j x + j) (10a) ( M(x))j = (|| j x||2/ j) (10b) where ( ) is a nonlinear function, such as the logistic sigmoid, exponent of the negated square, or the sign function, j is a vector of i.i.d. ran- dom weights sampled from the normal or uniform distribution, and j is a randomly sampled bias term. The transformations de ned by (10a) are sometimes called random additive nodes, while those de ned by (10b) are called multiplicative nodes. The ELM is similar to an arti cial neural network, whose weights and biases in the rst layer are randomly initialized and kept constant, while the weights (and optionally biases) of the second layer are selected by minimizing the least squares error. There are two important characteristics of feedfor- ward neural networks [31 39]: interpolation capability and universal approx- imation capability. The interpolation ability of ELM is rigorously proved in [9] and any N arbitrary distinct samples can be learned precisely with at most N hidden nodes and under the condition that the activation functions of hidden layer are in nitely di erentiable in any interval. As an important theoretical contribution, the universal approximation ability of ELM for any continuous target function is presented in detail for the activation functions that are nonconstant, piecewise continuous and the feature space of hidden layer is dense in L2 [30, 40, 41]. In terms of the training performance and the above theoretical guarantee, ELM and its variants such as fully complex ELM [42 44], online sequential ELM [44 46] and ensemble ELM [46 48] have been extensively studied. De- tailed survey for interested readers is in [49] and a comparison of ELM, SVM, LSSVM is in [50]. Regularized extreme learning machine (RELM) is based on structural risk minimization principle and weighted least squares. The RELM has led to a better generalization performance than the original ELM algorithm [51, 52]. The ELM classi er is usually designed without the bias term b, since theoretical analysis shows that it is not needed. Also, the regularization term was rst used in [51 53]. The ELM solves the following problem: minimize w wTw + C 2 N X i=1 (wT (xi) yi)2 (11) 9 where the transformation is one of (10) with and sampled randomly. This problem can be readily solved via the Moore-Penrose matrix pseudo- inverse [49, 50]: w =  I C + (X) (X)T  1 (X)Y (12a) = (X)  I C + (X)T (X)  1 Y, (12b) where X is a matrix whose ith column is the ith training sample xi, (X) is a matrix whose ith column is (xi), Y is a vector of class labels, and I is the identity matrix. The solutions (12a) and (12b) exist when the matrix (X) is full column rank or full row rank, respectively. They can be readily derived by solving (9) with setting b = 0. When one directly solves for the weight vector, the solution (12a) is obtained, while solving rst for and then plugging into the formula for w yields (12b). 2.4. Margin Loss Extreme Learning Machine (MLELM) Perhaps the most striking property of the ELM method is that the ran- domly chosen input transformations are su cient to approximate any func- tion [30] and moreover yield good classi cation accuracy. One can consider to use a kernel K(x1, x2) = (x1)T (x2) in which the transformation (x) is given by (10) and is generated prior to classi er training. Using this ELM- inspired kernel for SVM training was analyzed in [54, 55]. We observe that since the formula for the feature transformation is known, training can be accomplished either by supplying transformed inputs to a linear SVM solver, or by setting the kernel K(x1, x2) = (x1)T (x2) in a nonlinear one. One can consider the case of using in nitely many randomly chosen hid- den neurons. Under a Gaussian prior on weights, it is possible to obtain an analytical formula of the ELM kernel. We direct the interested reader to [56 58]. 2.5. L1 weight regularization When the transformation (x) is explicitly known, as it is the case with e.g. the ELM method, one may wish to minimize the L1 norm of the weight vector w. This enforces sparsity of the weights and can be used for feature selection as in the LASSO method [59]. In fact, the LASSO method has been used to minimize the number of hidden neurons used by the ELM [60]. 10 In [61] an extension of LASSO is used to rank hidden neurons of an ELM for multivariate regression. Finally, when (x) is known and one wishes to minimize the L1 norm on weights and use the margin loss penalty, then the 1-norm Support Vector Machine is obtained [62 64]. The optimization problem (1) can be solved e ciently using linear programming. Substituting the L1 norm and margin loss function yields the problem: minimize w,b ||w||1 + C N X i=1 max(0, 1 yi(wT (xi) + b)). (13) Introducing slack variables i and decomposing w into its positive and neg- ative parts w = w+ w results in: minimize w+,w ,b, N X i=1 (w+ i + w i ) + C N X i=1 i subject to: i yi((w+ w )T (xi) + b) 1 i 0 w+, w 0 (14) which is indeed a linear programming problem. An algorithm recovering the full solution path obtained while varying the parameter C is presented in [62]. 2.6. Multi-class operation The presented classi ers are essentially binary ones. Multi-class operation is possible by training many binary classi ers. Suppose there are K classes. One possibility is to train K one-versus-rest classi ers, each distinguishing between class k and all remaining classes. This scheme was proposed for the ELM as multioutput multiclass [50]. Usually, the class with the highest classi er output is selected. Another option is to train K(K 1)/2 one- versus-one classi ers, which is the default in [65]. A voting scheme is the used to select the class of new samples. A full discussion of this topic is outside of the scope of this paper and we refer to reviews [66, 67]. 3. Relationship between the discussed classi ers All four above described classi er models share many common properties. Considering the optimization goal, they all belong to the same mathematical 11 category of convex quadratic programming, which guarantees that the train- ing will converge to a single global optimum. Furthermore, they all consist of a chosen a priori and xed nonlinear transformation followed by a trained linear classi er. As demonstrated by the experiments presented in the next section, the accuracies of di erent classi er models in this family tested on popular datasets from the UCI repository are comparable. Despite these similarities, the classi er models di er in many important ways with respect to their distinct inspirations. We summarize those di er- ences and their consequences in the next sections. 3.1. Di erences Due to Loss Function The quadratic loss function adopted in LSSVM and ELM may seem to be unsuitable for classi cation because it not only penalizes wrong answers, but also penalizes correct answers which are far from the decision boundary, which can be seen in Fig. 1. In contrast, the margin loss used in SVM and MLELM penalizes only answers that are incorrect or that are correct but lie close to the decision boundary. However, the results of the experiments do not indicate that classi cation accuracy is much in uenced by the adopted loss function. The choice of the loss function determines how the classi er can be trained and how expensive is the classi cation of new data. The quadratic loss function is smooth and the resulting KKT system has a closed form solu- tion [15, 55]. Thus the LSSVM and ELM can be easily trained. However, all Lagrange multipliers are nonzero and the decision boundary depends on all training samples. On the other hand, the margin loss function adopted in SVM and MLELM is not smooth and the resulting KKT dual system needs to be solved in an iterative way by maximizing an auxiliary problem. Due to the singularity of the loss function only a fraction of the Lagrange multipli- ers is nonzero [1]. The decision boundary depends on the training samples (support vectors) corresponding to the nonzero multipliers. 3.2. Di erences Due to the Feature Transformation During nonlinear operation, SVM and LSSVM use a feature transfor- mation given implicitly by the kernel function [1, 15]. Therefore, they are always trained in the dual space and the decision boundary is given by the Lagrange multipliers. In contrast, ELM classi ers randomly choose a feature transformation according to (10) [32]. Since the transformation is known, 12 ELM-based classi ers can be trained using both the primal and dual formu- lation. Furthermore, the weights determining the decision boundary can be computed and no training samples must be stored to classify new data. 3.3. Meta-parameters Used by the Classi ers While all classi ers discussed in this paper are de ned by a convex op- timization problem, their operation requires the selection of a number of meta-parameters.For linear operation, the SVM and LSSVM require the se- lection of the cost parameter C which balances the penalty for errors and the regularization terms in (1) [1]. For nonlinear operation, a kernel function needs to be chosen, possibly requiring the selection of a parameter [12]. Pop- ular choices are the Gaussian kernel K(x, y) = exp ( ||x y||2/(2 2)) which requires the parameter or the polynomial kernel K(x, y) = (x y+1)p which requires the parameter p. ELM-based classi ers also require the selection of the regularization pa- rameter C. Furthermore, parameters of the feature transformations (10) must be speci ed. These are the number of hidden units which determines the dimensionality of the transformation, the nonlinear activation function , and the probability distribution of the weights and biases of the random hidden neurons. However, the original introduction of the ELM method suggested that the only important parameter is the number of hidden units [30]. The regularization constant C was originally not used. The proba- bility distributions of the weights and biases of the hidden units were only required to match mild requirements of the universal approximation the- orems [30] and were not speci ed. The o cial ELM toolkit always uses ij Uniform( 1, 1), j Uniform(0, 1) [68]. The regularization con- stant was added subsequently in [49] and there is evidence that variance of the chosen weight distribution a ects the generalization performance [57]. 3.4. Discussion of Training and Testing Times The ELM and LSSVM have a closed-form solution that can be used to compute the weights and it is easy to determine their training times [15, 55]. In contrast, training of the SVM and MLELM is iterative and the determi- nation of the training times is more di cult. We will use N to denote the number of training samples, n to denote the dimensionality of the input data, and m to denote the dimensionality of the feature space, which corresponds to the number of hidden neurons in ELM training. 13 To ELM requires the computations of the matrix (X) which takes O(Nnm) operations. The weights w are then computed using the for- mula (12a) which requires O(Nm2 + m3) operations or (12b) which requires O(N 2m + N 3) operations [50]. When the number of training samples is much greater than the number of hidden neurons and the dimensionality of the input, training an ELM requires O(Nm2) operations and O(N(m + n)) memory if formula (12a) is used. A sample can be classi ed using O(mn) operations. An implementation of the nonlinear LSSVM that solves the system of equations (9) by matrix inversion requires to rst compute the kernel matrix in O(N 2n) operations when the Gaussian or polynomial kernels are used. The inversion of the kernel matrix requires O(N 3) operations using O(N 2) memory. More e cient algorithms have been experimentally found to scale quadratically with the training set size [69]. Since the majority of Lagrange multipliers is non-zero [15], classi cation of a sample requires the iteration over the whole training set which takes O(Nn) operations. Due to the iterative nature of SVM training, determining the required number of operations is di cult. In general, training time of the SVM de- pends on the the number of support vectors NS [10]. It has been found exper- imentally that the SMO algorithm scales between linearly and quadratically with the number of training examples [70]. Classi cation of new samples re- quires O(NSn) operations [10] since we need to only account for the nonzero Lagrange multipliers which correspond to support vectors. MLELM can be trained in either the primal or dual form. To be com- patible with most solvers [13] the non-di erentiable loss function needs to be transformed into a di erentiable equivalent form in the primal problem formulation. This requires the introduction of arti cial variables, similar to the transformation of the L1-norm SVM as shown in (13) and (14) [62]. In the dual form any solver used to train the SVM can be used. We have experimentally observed, however, that the MLELM commands much more computations than an SVM to train, even for the same solver used. 4. Experimental analysis The four discussed classi er models have been compared using 21 clas- si cation datasets, which include 11 binary classi cation and 10 multi-class classi cation problems. Due to the di erences of scale, the datasets have been split into two categories based on data volumes as in Fig. 3: small size 14 Data Set Training Set Size Testing Set Size Input Features Classes 5 fold Cross Validation for small size data 1. Promoter Gene 106 -- 57 2 2. Iris 150 -- 4 3 3. Sonar 208 -- 60 2 4. Glass Identification 214 -- 9 7 5. Breast Caner 286 -- 9 2 6. Ecoli 336 -- 7 8 7. Liver Disorders 345 -- 6 2 8. Ionosphere 351 -- 34 2 9. 1st Monk s Problems 432 -- 6 2 10. Congressional Voting d 435 -- 16 2 11. Soybean 683 -- 36 19 12. Pima Indians diabetes b 768 -- 8 2 13. Vehicle 846 -- 19 4 14. Vowel 990 -- 14 11 15. German Credit Data 1,000 -- 20 2 16. Contraceptive Method h 1,473 -- 9 3 Fixed size for large data 17. Splice-junction 2,000 1,190 61 3 18. Waveform Version 2 3,000 2,000 40 3 19. Mushroom 4,000 4,124 22 2 20. Letter Recognition 10,000 10,000 16 26 21. Adult 10,000 38,842 14 2 Summary: Total data: 21; Small size data: 16; Large size data: 5. The training set size was reduced to 5000 samples for MLELM on the Adult and Letter due to its long running times. Figure 3: Data set summary for classi cation performance comparison. (No. 1 16) and large size (No. 17 21). For small size datasets, 5-fold cross-validation has been performed, i.e., each dataset is randomly split into 5 subsets, then each subset is used as test sets for a classi er built on the remaining four. The large size datasets have been split into xed size train- ing set and test set separately as indicated in Fig. 3. For all the classi ers we have normalized all input variables to the ( 1, 1) range. Furthermore, we have used the 1-of-N encoding of nominal inputs, i.e. each nominal input was expanded into N dummy inputs, one for each possible value, with only one input equal to 1. In some datasets a few nominal attributes contained missing values. We encoded them as vectors of all zeros. For datasets with many target labels, we have used the one-vs-one strategy for the SVM, and one-vs-all for the other classi ers. In all the cases a 5-fold cross-validation was performed to select meta-parameters on each training set. Furthermore, with the exception of MLELM on the Letter dataset, each experiment was repeated three times. All benchmark datasets are from the UCI Machine Learning Repository [71]. To compare the computational performance of di erent classi er models, SVM results have been obtained by directly using the LIBSVM library [65]; 15 the current version of ELM [68] has been employed for the performance anal- ysis. For LSSVM classi er, the classic algorithm [15, 16] has been rewritten to verify its performance in terms of di erent parameters which are consistent with other classi ers. We have implemented the O(N 3) matrix inversion al- gorithm because it is the only algorithm implemented in the LS-SVMlab 1.8 o cial toolbox [72]. To implement the MLELM we have experimented with di erent quadratic programming solvers working both in primal and dual space. Considered solvers included the Matlab s Optimization Toolbox, Lib- SVM con gured to use a precomputed kernel matrix, CVX [5, 73], and SVM and Kernel Methods Matlab Toolbox [74], which experimentally proved to be the fastest. The randomization of the weights of hidden layer is the same as in the ELM implementation [68]. The detailed simulation parameters for the experiments are speci ed in the Fig. 4. All simulations have been carried out in MATLAB 2010 environment run- ning in an Intel Core i4, 2.67GHZ CPU. The samples of each training dataset are rst randomized and then xed for training of the four classi ers. To ana- lyze numerical results, four performance metrics have been graphed: training accuracy, testing accuracy, training time and testing time. Three learning session trials were executed for each experiment. The average performance across three trials was gathered in Table 1 and plotted in Fig. 5 and Fig. 6. The con dence interval based on testing accuracy has also been computed using the methodology outlined in [12]. As indicated in Eqs. (15)-(18), accu- racies for small datasets represent the average accuracy rate on the training sets with 5-fold cross validation averaged over three runs, while accuracies for large datasets are for the xed training/testing set split. Training times incorporate 5-fold cross-validation parameter tuning. Testing time is the running time on the test sets after the completed training procedure. The de nitions of the above metrics are speci ed as following: trAcc CV = 1 5 5 X i=1  1 ei Ntr CV  , (15) teAcc CV = 1 5 5 X 1  1 i Nte CV  , (16) trAcc F = 1 e Ntr F , (17) teAcc F = 1 Nte F , (18) 16 Objective Func- tion 1 2||w||2 2 + C N X i=0 max(0, 1 oiyi) Kernel Function Gaussian Reg. constant C 2 5, 2 2, . . . 216 Kernel Param. 2 15, 2 12, . . . 23 (a) SVM Objective Func- tion 1 2||w||2 2 + C N X i=0 (oi yi)2 Kernel Function Gaussian Reg. constant C 2 5, 2 2, . . . 216 Kernel Param. 2 15, 2 12, . . . 23 (b) LSSVM Objective Func- tion 1 2||w||2 2 + C N X i=0 (oi yi)2 Activation Func- tion Sigmoid Hidden weights and biases distri- bution weights: U( 1, 1) biases: U(0, 1) Reg. Constant C 2 5, 2 2, . . . 216 Number of Hid- den Neurons m 10, 30, 100, 300, 1000, 2000, 5000 (c) ELM Objective Func- tion 1 2||w||2 2 + C N X i=0 max(0, 1 oiyi) Activation Func- tion Sigmoid Hidden weights and biases distri- bution weights: U( 1, 1) biases: U(0, 1) Reg. constant C 2 5, 2 2, . . . 216 or 2 5, 2 1, . . . 215 on Letter and Adult Number of Hid- den Neurons 10, 30, 100, 300, 1000, 2000, 5000 or 10, 30, 100, 300, 1000, 2000 on Letter and Adult (d) MLELM where oi = wT (xi) + b and b may be 0 for ELM Figure 4: Experiment designs for classi ers with detailed parameters. 17 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Accuracy SVM LSSVM ELM MLELM Figure 5: Testing Accuracy for di erent classi ers with indicated 95% con dence intervals. where trAcc CV and teAcc CV represent the training and testing accuracy with 5 fold cross-validation tests for small size datasets separately, while trAcc F and teAcc F stand for the training and testing accuracy with the xed training and test samples for large size datasets as in Fig. 3, Ntr CV and Nte CV are the number of training and test samples of each of the 5- fold cross validation runs, ei and i (i = 1, , 5) represent the number of misclassi cations in the ith cross validation run on Ntr CV and Nte CV , respectively. Similarly, Ntr F and Nte F mean the number of the xed training and test samples for large size datasets, then e and stand for the number of misclassi cations on Ntr F and Nte F, respectively. From Table 1 and Fig. 5, we can conclude that each of the four di er- ent classi ers has a very similar accuracy performance for most classi cation problems. The biggest di erence is seen on the Adult dataset on which the ELM-based classi ers fail to choose proper parameters under the employed cross-validation framework. We have established that the maximum cross- validation accuracy on the training set was usually obtained for 2000 hidden neurons and C = 0.03, while the best testing accuracy requires fewer number of hidden neuron (limiting the search to 1000 gave better result). Other dif- ferences are seen on datasets Glass and Sonar . The similar performance of SVM and LSSVM shows that on the benchmark problems that we used 18 Table 1: Average performance of the tested classi ers. SVM LSSVM ELM MLELM Train Acc. Test Acc. Train Time [s] Test Time [s] Train Acc. Test Acc. Train Time [s] Test Time [s] Train Acc. Test Acc. Train Time [s] Test Time [s] Train Acc. Test Acc. Train Time [s] Test Time [s] Promoter Gene 1.00 0.92 6.05 0.01 1.00 0.92 4.91 0.02 1.00 0.90 41.40 0.03 1.00 0.90 77.54 0.03 Iris 0.98 0.95 1.84 0.01 0.98 0.96 3.26 0.01 0.97 0.95 16.45 0.00 0.98 0.96 107.07 0.00 Sonar 1.00 0.88 7.45 0.01 1.00 0.87 6.63 0.02 1.00 0.83 32.49 0.02 0.99 0.82 105.80 0.02 Glass Identification 0.89 0.68 7.51 0.00 0.90 0.69 5.02 0.02 0.86 0.67 24.57 0.01 0.87 0.64 359.21 0.01 Breast Cancer 0.86 0.73 15.57 0.01 0.82 0.73 12.73 0.04 0.79 0.71 39.21 0.01 0.80 0.72 318.78 0.01 Ecoli 0.90 0.87 11.11 0.01 0.90 0.87 9.09 0.02 0.90 0.88 37.63 0.01 0.90 0.86 464.37 0.01 Liver Disorders 0.77 0.72 61.54 0.01 0.77 0.72 9.48 0.02 0.77 0.72 36.48 0.01 0.76 0.71 248.41 0.01 Ionosphere 0.98 0.95 12.51 0.01 0.99 0.95 13.85 0.04 0.99 0.92 39.90 0.03 0.98 0.93 117.60 0.02 1st Monk problems 1.00 1.00 212.49 0.01 1.00 1.00 17.62 0.04 1.00 1.00 56.02 0.02 1.00 1.00 280.45 0.02 Congressional Voting 0.98 0.96 11.52 0.01 0.99 0.95 23.09 0.06 0.97 0.96 56.41 0.05 0.98 0.95 119.44 0.01 Soybean 0.98 0.93 179.97 0.11 0.98 0.94 119.03 0.23 1.00 0.93 113.30 0.07 0.99 0.93 2274 0.04 Pima Indians Diabetes 0.79 0.76 244.70 0.02 0.80 0.76 52.01 0.07 0.79 0.76 102.98 0.03 0.78 0.77 904.25 0.02 Vehicle 0.95 0.84 95.15 0.03 0.95 0.85 67.32 0.13 0.96 0.85 116.74 0.06 0.97 0.84 1807 0.07 Vovel 1.00 0.99 131.60 0.08 1.00 0.99 100.04 0.19 1.00 0.98 146.32 0.16 1.00 0.98 7406 0.10 German Credit Data 0.85 0.76 199.44 0.09 0.89 0.77 293.49 0.39 0.91 0.76 141.73 0.04 0.89 0.74 2337 0.06 Contraceptive Method 0.62 0.49 1089 0.15 0.61 0.52 251.94 0.38 0.60 0.50 266.37 0.05 0.61 0.48 37108 0.04 Splice-junction 0.98 0.96 17063 16.23 1.00 0.96 29996 153.88 1.00 0.91 1007 3.05 1.00 0.94 13254 6.25 Waveform version 2 0.88 0.86 294.90 0.46 0.89 0.86 476.91 2.93 0.92 0.86 227.88 0.36 0.89 0.86 13151 0.06 Mushroom 1.00 1.00 1112 2.57 1.00 1.00 3588 32.83 1.00 1.00 407.66 0.25 1.00 1.00 3792 0.10 Letter Recognition 1.00 0.97 2543 7.58 1.00 0.97 8109 57.46 0.99 0.96 1826 5.20 1.00 0.92 132636 3.12 Adult 0.86 0.85 12823 45.29 0.87 0.84 23470 766.19 0.86 0.54 1894 2.02 0.85 0.44 152793 1.17 19 1.0E+00 1.0E+01 1.0E+02 1.0E+03 1.0E+04 1.0E+05 1.0E+06 Training Time [s] SVM LSSVM ELM MLELM Figure 6: Training Time for di erent classi ers. the choice of the loss function doesn t have much impact on classi cation ac- curacy. Also, with the exception of the Adult data, ELM-based methods demonstrate that the random choice of the hidden layer results in similar performance to SVM-based methods. We have included the time necessary for tuning parameters with the grid search into the reported training times. While all the classi ers solve op- timization problems with unique global optima, they are non-convex with regard to their parameters that we discuss in section 3.3. Tuning those pa- rameters is often necessary for good performance and we believe that the time consumed by this search should be included into the total training time. Comparing the training times on Fig. 6, it can be seen that MLELM is a signi cantly more time-consuming algorithm than the remaining ones. All of the QP solvers tested yielded similar unsatisfactory running times. This may indicate that the optimization problem is inherently hard due to corre- lations between hidden neuron activations obtained from the ELM random projection of training samples. Running times on small datasets show that LibSVM is the fastest method. Our analysis in section 3.4 has shown that the ELM requires a time proportional to the number of hidden neurons, which was larger that the size of small datasets. Also, the highly tuned solver used in LibSVM may have a lower overhead. Running times on larger datasets for 20 which the number of training samples is larger than the maximum number of ELM hidden nodes shows that the ELM is the most computationally e cient of the tested classi ers. The SVM is the second fastest, which corresponds with experimental results of the performance of the SMO solver. Inspection of the testing time shown in Table 1 indicates that LSSVM is the most time-consuming method to test for all datasets. This is consistent with the theoretical analysis of its computational complexity and demon- strates that the sparseness of the support vectors is lost due to the choice of L2 norm in the objective function of LSSVM. Similar as with training time analysis, SVM s testing time varies from data to data However, it is generally more time-consuming than ELM and MLELM, whose evaluations require only a feedforward pass through the network. Fig. 5 shows the con dence intervals for testing accuracy at a 95% con - dence level. We noticed that the con dence intervals on the same dataset are generally identical due to the similar accuracies for the four di erent clas- si ers. Another characteristic is that the con dence intervals for large size datasets have been typically much tighter than those for small size datasets. The main reason is that the con dence interval is dependent on the number of test samples besides the testing accuracy. Finally, it can be seen that there are several low testing accuracies such as for cases Breast Cancer , Ecoli and Ionosphere for small datasets. This indicates that the performance of the classi ers really depends on the nonlinear degree of datasets themselves rather than the number of samples. Conclusions This paper discusses and compares four closely related classi cation mod- els based on margin loss and least squares objective functions. A uniform framework of convex optimization is introduced to highlight the similarities and di erences between the models. Implications of di erent parametriza- tions of the convex problem (1) are stated and justi ed in the experimental section. But for the Adult dataset all four investigated models o er comparable classi cation accuracies. The choice of the one to use is, therefore, problem dependent. The classical ELM algorithm yields usually good testing accu- racy while being fast to train and fast to apply to new data. However, the results on the Adult dataset demonstrate that the parameters of the ELM must be carefully chosen and validated. The MLELM fusion of SVM s loss 21 function and ELM s feature transformation performs poorly, because it is often unacceptably slow to train and yields similar accuracy to the classical ELM algorithm. The SVM o ers state-of-the art accuracy, it is however more expensive than the ELM to both train and apply to new data. The LSSVM often matches or surpasses SVM s accuracy, an has a simpler than the SVM training algorithm. However, its application to new data requires processing of the whole training set, which can be unacceptable for certain applications. Acknowledgments The authors wish to thank the anonymous reviewers for many insightful comments and suggestions which greatly improved this work. The authors thank Dr. Tolga Ensari for his advice in preparation of this manuscript. [1] C. Cortes, V. Vapnik, Support-vector networks, Machine learning 20 (3) (1995) 273 297. [2] V. Vapnik, The nature of statistical learning theory, Springer-Verlag New York Inc, 1995. [3] V. Cherkassky, F. Mulier, Learning from data: concepts, theory, and methods, Wiley-IEEE Press, 2007. [4] S. Boyd, L. Vandenberghe, Convex optimization, Cambridge Univ Pr, 2004. [5] M. Grant, S. Boyd, Cvx: Matlab software for disciplined convex pro- gramming, version 1.21 (2011). URL http://cvxr.com/cvx [6] C. Bishop, Pattern recognition and machine learning, springer New York, 2006. [7] Y. Grandvalet, J. Mari ethoz, S. Bengio, A probabilistic interpretation of svms with an application to unbalanced classi cation, in: Advances in Neural Information Processing Systems, Vol. 18, 2006, pp. 467 474. [8] P. Sollich, Probabilistic methods for support vector machines, Advances in neural information processing systems 12 (2000) 349 355. 22 [9] G. Huang, Q. Zhu, C. Siew, Extreme learning machine: theory and applications, Neurocomputing 70 (1) (2006) 489 501. [10] B. Boser, I. Guyon, V. Vapnik, A training algorithm for optimal margin classi ers, in: Proceedings of the fth annual workshop on Computa- tional learning theory, 1992, pp. 144 152. [11] G. Wahba, et al., Support vector machines, reproducing kernel hilbert spaces and the randomized gacv, Advances in Kernel Methods-Support Vector Learning 6 (1999) 69 87. [12] P. Tan, M. Steinbach, V. Kumar, et al., Introduction to data mining, Pearson Addison Wesley Boston, 2006. [13] J. Nocedal, S. Wright, Numerical optimization, Springer verlag, 1999. [14] J. Zhu, T. Hastie, Kernel logistic regression and the import vector ma- chine, Journal of Computational and Graphical Statistics 14 (1) (2005) 185 205. [15] J. Suykens, J. Vandewalle, Least squares support vector machine classi- ers, Neural processing letters 9 (3) (1999) 293 300. [16] J. Suykens, J. Vandewalle, Multiclass least squares support vector ma- chines, in: Neural Networks, 1999. IJCNN 99. International Joint Con- ference on, Vol. 2, IEEE, 1999, pp. 900 903. [17] V. Vapnik, Statistical learning theory, Wiley, New York, 1998. [18] A. Smola, B. Sch olkopf, K. M uller, The connection between regular- ization operators and support vector kernels, Neural Networks 11 (4) (1998) 637 649. [19] B. Sch olkopf, C. Burges, A. E. Smola, Advances in kernel methods: support vector learning, The MIT press, 1999. [20] N. Cristianini, J. Shawe-Taylor, An introduction to support Vector Ma- chines: and other kernel-based learning methods, Cambridge Univ Pr, 2000. [21] C. Burges, A tutorial on support vector machines for pattern recognition, Data mining and knowledge discovery 2 (2) (1998) 121 167. 23 [22] G. Golub, C. Van Loan, Matrix computations, Vol. 3, Johns Hopkins Univ Pr, 1996. [23] J. Suykens, L. Lukas, P. V. Dooren, B. D. Moor, J. Vandewalle, Least squares support vector machine classi ers: a large scale algorithm, in: European Conference on Circuit Theory and Design, 1999, 1999, pp. 839 842. [24] T. Van Gestel, J. Suykens, B. Baesens, S. Viaene, J. Vanthienen, G. De- dene, B. De Moor, J. Vandewalle, Benchmarking least squares support vector machine classi ers, Machine Learning 54 (1) (2004) 5 32. [25] J. Suykens, J. De Brabanter, L. Lukas, J. Vandewalle, Weighted least squares support vector machines: robustness and sparse approximation, Neurocomputing 48 (1-4) (2002) 85 105. [26] G. Cawley, N. Talbot, Improved sparse least-squares support vector ma- chines, Neurocomputing 48 (1-4) (2002) 1025 1031. [27] L. Wei, Z. Chen, J. Li, W. Xu, Sparse and robust least squares support vector machine: a linear programming formulation, in: Grey Systems and Intelligent Services, 2007. GSIS 2007. IEEE International Confer- ence on, IEEE, 2007, pp. 1134 1138. [28] J. Liu, J. Li, W. Xu, Y. Shi, A weighted lq adaptive least squares sup- port vector machine classi ers-robust and sparse approximation, Expert Systems with Applications 38 (3) (2011) 2253 2259. [29] L. Wei, Z. Chen, J. Li, Evolution strategies based adaptive lp ls-svm, Information Sciences 181 (2011) 3000 3016. [30] G. Huang, L. Chen, C. Siew, Universal approximation using incremental constructive feedforward networks with random hidden nodes, Neural Networks, IEEE Transactions on 17 (4) (2006) 879 892. [31] S. O. Haykin, Neural Networks: A Comprehensive Foundation, 2E, Prentice Hall, 1999. [32] G. Huang, H. Babri, Upper bounds on the number of hidden neurons in feedforward networks with arbitrary bounded nonlinear activation functions, Neural Networks, IEEE Transactions on 9 (1) (1998) 224 229. 24 [33] M. Sartori, P. Antsaklis, A simple method to derive bounds on the size and to train multilayer neural networks, Neural Networks, IEEE Transactions on 2 (4) (1991) 467 471. [34] S. Huang, Y. Huang, Bounds on the number of hidden neurons in multi- layer perceptrons, Neural Networks, IEEE Transactions on 2 (1) (1991) 47 55. [35] M. Leshno, V. Lin, A. Pinkus, S. Schocken, Multilayer feedforward net- works with a nonpolynomial activation function can approximate any function, Neural networks 6 (6) (1993) 861 867. [36] B. Gao, Y. Xu, Univariant approximation by superpositions of a sig- moidal function, Journal of mathematical analysis and applications 178 (1) (1993) 221 226. [37] K. Hornik, Approximation capabilities of multilayer feedforward net- works, Neural networks 4 (2) (1991) 251 257. [38] K. Hornik, M. Stinchcombe, H. White, Multilayer feedforward networks are universal approximators, Neural networks 2 (5) (1989) 359 366. [39] K. Funahashi, On the approximate realization of continuous mappings by neural networks, Neural networks 2 (3) (1989) 183 192. [40] G. Huang, L. Chen, Convex incremental extreme learning machine, Neu- rocomputing 70 (16-18) (2007) 3056 3062. [41] G. Huang, L. Chen, Enhanced random search based incremental extreme learning machine, Neurocomputing 71 (16) (2008) 3460 3468. [42] M. Li, G. Huang, P. Saratchandran, N. Sundararajan, Fully complex extreme learning machine, Neurocomputing 68 (2005) 306 314. [43] G. Huang, M. Li, L. Chen, C. Siew, Incremental extreme learning ma- chine with fully complex hidden nodes, Neurocomputing 71 (4) (2008) 576 583. [44] H. Rong, G. Huang, N. Sundararajan, P. Saratchandran, Online se- quential fuzzy extreme learning machine for function approximation and classi cation problems, Systems, Man, and Cybernetics, Part B: Cyber- netics, IEEE Transactions on 39 (4) (2009) 1067 1072. 25 [45] N. Liang, G. Huang, P. Saratchandran, N. Sundararajan, A fast and accurate online sequential learning algorithm for feedforward networks, Neural Networks, IEEE Transactions on 17 (6) (2006) 1411 1423. [46] Y. Lan, Y. Soh, G. Huang, Ensemble of online sequential extreme learn- ing machine, Neurocomputing 72 (13-15) (2009) 3391 3395. [47] Z. Sun, T. Choi, K. Au, Y. Yu, Sales forecasting using extreme learning machine with applications in fashion retailing, Decision Support Systems 46 (1) (2008) 411 419. [48] M. Van Heeswijk, Y. Miche, T. Lindh-Knuutila, P. Hilbers, T. Honkela, E. Oja, A. Lendasse, Adaptive ensemble models of extreme learning machines for time series prediction, Lecture Notes in Computer Science 5769 (2009) 305 314. [49] G. Huang, D. Wang, Y. Lan, Extreme learning machines: a survey, International Journal of Machine Learning and Cybernetics 2 (2011) 107 122. [50] G.-B. Huang, H. Zhou, X. Ding, R. Zhang, Extreme learning machine for regression and multiclass classi cation, Systems, Man, and Cybernetics, Part B: Cybernetics, IEEE Transactions on 42 (2) (2012) 513 529. [51] W. Deng, Q. Zheng, L. Chen, Regularized extreme learning machine, in: Computational Intelligence and Data Mining, 2009. CIDM 09. IEEE Symposium on, IEEE, 2009, pp. 389 395. [52] W. Deng, L. Chen, Color image watermarking using regularized extreme learning machine, Neural Network World 20 (3, Sp. Iss.{SI}) (2010) 317 330. [53] Q. Liu, Q. He, Z. Shi, Extreme support vector machine classi er, in: Proceedings of the 12th Paci c-Asia conference on Advances in knowl- edge discovery and data mining, Springer-Verlag, 2008, pp. 222 233. [54] B. Fr enay, M. Verleysen, Using svms with randomised feature spaces: an extreme learning approach, in: Proceedings of the 18th European symposium on arti cial neural networks (ESANN), Bruges, Belgium, Vol. 28, 2010, p. 30. 26 [55] G. Huang, X. Ding, H. Zhou, Optimization method based extreme learn- ing machine for classi cation, Neurocomputing 74 (1) (2010) 155 163. [56] C. Williams, Computation with in nite neural networks, Neural Com- putation 10 (5) (1998) 1203 1216. [57] E. Parviainen, J. Riihim aki, Y. Miche, A. Lendasse, Interpreting ex- treme learning machine as an approximation to an in nite neural net- work, in: KDIR 2010: Proceedings of the International Conference on Knowledge Discovery and Information Retrieval, Valencia, Spain, 2010. [58] B. Fr enay, M. Verleysen, Parameter-insensitive kernel in extreme learn- ing for non-linear support vector regression, Neurocomputing 74 (2011) 2526 2531. [59] R. Tibshirani, Regression shrinkage and selection via the lasso, Journal of the Royal Statistical Society. Series B (Methodological) 58 (1996) 267 288. [60] J. Mart nez-Mart nez, P. Escandell-Montero, E. Soria-Olivas, J. Mart n- Guerrero, R. Magdalena-Benedito, J. G omez-Sanchis, Regularized extreme learning machine for regression problems, Neurocomputing 74 (17) (2011) 3716 3721. [61] Y. Miche, A. Sorjamaa, A. Lendasse, Op-elm: Theory, experiments and a toolbox, in: Proceedings of the 18th international conference on Ar- ti cial Neural Networks, Part I, ICANN 08, Springer-Verlag, Berlin, Heidelberg, 2008, pp. 145 154. [62] J. Zhu, S. Rosset, T. Hastie, R. Tibshirani, 1-norm support vector ma- chines, Advances in neural information processing systems 16 (1) (2004) 49 56. [63] O. Mangasarian, Exact 1-norm support vector machines via uncon- strained convex di erentiable minimization, The Journal of Machine Learning Research 7 (2006) 1517 1530. [64] L. Wang, X. Shen, On l 1-norm multiclass support vector machines, Journal of the American Statistical Association 102 (478) (2007) 583 594. 27 [65] C. Chang, C. Lin, Libsvm: a library for support vector machines, ACM Transactions on Intelligent Systems and Technology (TIST) 2 (3) (2011) 27. [66] C. Hsu, C. Lin, A comparison of methods for multiclass support vector machines, Neural Networks, IEEE Transactions on 13 (2) (2002) 415 425. [67] K. Duan, S. Keerthi, Which is the best multiclass svm method? an empirical study, Multiple Classi er Systems (2005) 732 760. [68] O cial elm toolbox, accessed 6/9/2012. URL http://www.ntu.edu.sg/home/egbhuang/ELM_Codes.htm [69] S. Keerthi, S. Shevade, Smo algorithm for least-squares svm formula- tions, Neural computation 15 (2003) 487 507. [70] J. C. Platt, Fast training of support vector machines using sequential minimal optimization, in: B. Sch olkopf, C. J. C. Burges, A. J. Smola (Eds.), Advances in kernel methods, MIT Press, Cambridge, MA, USA, 1999, pp. 185 208. [71] D. N. A. Asuncion, UCI machine learning repository (2007). URL http://www.ics.uci.edu/$\sim$mlearn/{MLR}epository. html [72] K. D. Brabanter, P. Karsmakers, C. A. F. Ojeda, J. D. Brabanter, K. Pel- ckmans, B. D. Moor, J. Vandewalle, J. Suykens, Ls-svmlab toolbox, ac- cessed 6/9/2012. URL http://www.esat.kuleuven.be/sista/lssvmlab/ [73] M. Grant, S. Boyd, Graph implementations for nonsmooth convex pro- grams, in: Recent Advances in Learning and Control (tribute to M. Vidyasagar), V. Blondel, S. Boyd, and H. Kimura, editors, Lecture Notes in Control and Information Sciences, Springer, 2008, pp. 95 110. [74] S. Canu, Y. Grandvalet, V. Guigue, A. Rakotomamonjy, Svm and kernel methods matlab toolbox, Perception Systemes et Information, INSA de Rouen, Rouen, France 2 (2005) 2. 28