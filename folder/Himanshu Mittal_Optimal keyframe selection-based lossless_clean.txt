Complex & Intelligent Systems (2022) 8:1047 1070 https://doi.org/10.1007/s40747-021-00569-6 ORIGINAL ARTICLE Optimal keyframe selection-based lossless video-watermarking technique using IGSA in LWT domain for copyright protection Roop Singh1 Himanshu Mittal2 Raju Pal2 Received: 19 May 2021 / Accepted: 19 October 2021 / Published online: 13 November 2021 The Author(s) 2021 Abstract Video piracy is a challenging issue in the modern world. Approximately 90% of newly released lms were illegally distributed around the world via the Internet. To overcome this issue, video watermarking is an effective process that integrates a logo in video frames as a watermark. Therefore, this paper presents an ef cient lossless video-watermarking scheme based on optimal keyframe selection using an intelligent gravitational search algorithm in linear wavelet transform. This technique obtains color motion and motionless frames from the cover video by the histogram difference method. One-level linear wavelet transform is performed on the chrominance channel of motion frames and a low-frequency sub-band LL opts for watermark embedding. The performance of the proposed technique has been evaluated against 12 video processing attacks in terms of imperceptibility and robustness. Experiments demonstrate that the proposed technique outperforms ve state-of-the-art schemes on the considered attacks. Keywords Video watermarking Linear wavelet transform Intelligent gravitational search algorithm Introduction In recent years, the unauthorized users can easily access the digital media content (image, audio, and video). These content are illegally copied, manipulated, and distributed [1,2] across the globe over the internet. Around 90% newly released movies are illegally recorded by the camcorder device and distributed via the internet globally [3]. The Dark Knight movie is one example of video piracy whose 7 million copies were delivered illegally in 6 months after its release [4]. As a result, copyright and content security are the most prevalent issues in the modern world. To address the same, video watermarking is one of the promising solutions which B Himanshu Mittal himanshu.mittal224@gmail.com Roop Singh roopsolanki@gmail.com Raju Pal raju3131.pal@gmail.com 1 Department of Electronics and Communication Engineering, Uttarakhand Technical University, Dehradun, Uttarakhand, India 2 Department of Computer Science and Engineering, Jaypee Institute of Information Technology, Noida 201304, India embeds information into video frames for copyright protec- tion [5]. Many watermarking algorithms have been proposed over the last decade and have been classi ed based on spe- ci c characteristics. In general, video-watermarking meth- ods operate in two domains, namely spatial and frequency. In the spatial domain, a watermark is directly embedded by modifying the pixels. Approaches of this domain have low computational complexity. However, these methods have poor data-hiding ability and low robustness [6,7]. On the con- trary, frequency domain methods are pretty much ef cient in this respect. In the frequency domain, the watermark is inte- grated with the wavelet coef cients of video frame. Some of the popular frequency domain methods are discrete Fourier transform (DFT), singular value decomposition (SVD), dis- crete cosine transform (DCT), and discrete wavelet transform (DWT). However, DWT is often employed due to its multi- resolution capabilities. This domain methods accomplish high payload capacity, imperceptibility, robustness, and more secure [6]. Further, This domain methods are computation- ally expensive as compared to spatial domain methods. Generally, researchers focus on the three parameters of a video watermarking method, i.e., imperceptibility, robust- ness, and payload capacity [8]. In video watermarking, imperceptibility corresponds to the effectiveness of a water- 123 1048 Complex & Intelligent Systems (2022) 8:1047 1070 marking method in concealing a watermark into the cover video frames, while robustness measures the ef ciency of a watermarking process in recovering the watermark from video frames after video attacks. Moreover, payload capacity refers to the number of bits contained within each frame. The complexity of a watermarking method often increases with the increased payload capacity. In the literature, it has been witnessed that these three parameters are contradictory and con ned with each other [6]. Therefore, it is a dif cult task to develop an effective watermarking system that is ef cient in terms of these parameters along with low computational complexity. In the literature, many video-watermarking schemes are presented to achieve the objectives mentioned above. Table 1 lists some of the popular video-watermarking schemes. It can be observed that most schemes perform watermark embedding in DWT, DCT, and SVD domains [9 15]. Gen- erally, such schemes are computationally complex [28] and unable to recover the lossless watermark image due to shift- variant property [29]. Moreover, few schemes are based on linear wavelet transform (LWT) [17 19]. The LWT-based video-watermarking schemes are resistant to both image pro- cessing attacks and temporal video attacks such as impulse noise,Gaussiannoise, ltering,compressionattacks,butfails against geometric attacks [25 28,30]. Further, it can be envi- sioned in Table 1 that existing video-watermarking schemes incorporate watermark in the non-motion frame of luminance components which results in poor imperceptibility [31]. Moreover, most video watermarking schemes employ single scaling factors (SEF) approach that signi cantly affects the balance between imperceptibility and robustness. To attain a better equilibrium between imperceptibility and robustness, the integration of video-watermarking schemes with multiple scaling factors (MSF) is a promising solution [20 22,24,32]. However, the selection of the optimum value of MSF is an NP-complete problem [33] which can be solved by employ- ing meta-heuristic algorithms. Meta-heuristic algorithms are optimization algorithms that imitate the optimization behavior of natural phenom- ena [34,35]. Gravitational search algorithm (GSA) [36] is one of such meta-heuristic algorithms inspired by the New- tonianlawofgravity.InGSA,theoptimalsolutionisobtained through a collection of objects which co-ordinates with each other according to the law of gravity and law of motion [37]. In comparison to different meta-heuristic algorithms, GSA has a low computational cost and high convergence rate [38]. In addition, GSA has been broadly acknowledged in the literature for multimodal challenges, notably for clustering applications [39]. Moreover, GSA advantages in nding the best solution using the current positions only, and therefore, it is considered as a memory-less algorithm [36,40]. However, due to the lack of demographic diversity and an inappro- priate balance between exploration and exploitation, it often stagnates into local optima [41]. In the literature, researchers have proposed several variants of GSA. Liu et al. [42] sug- gested dynamically adapting inertia factors for improving position updation. Further, Olivas et al. [43] introduced an interval type-2 fuzzy system-based modi ed variant of GSA whichimprovestheexploitationandexplorationofthesearch space. The adaptive GSA (AGSA) presented by Mirjalili et al. [44] allows the exploitation of the GSA to be modi ed based on the current situation. A variation of GSA called an exponential kbest gravitation search algorithm (eKGSA) had been introduced by Mittal et al. [45] for optimal thresholds for multi-level image segmentation. The hierarchical gravi- tation search algorithm proposed by Wang et al. [46] deals with premature convergence and low search capacity. Rawal et al. [47] presented fast convergent GSA which utilizes a sigmoidal function and an exponential step size to accelerate convergence and exploitation. Recently, Mittal et al. [48] pre- sented a new variant of GSA, intelligent gravitational search algorithm (IGSA), which outperformed GSA regarding con- vergence rate and solution precision. Therefore, the key contributions of the paper are twofold, (1) a new video-watermarking technique has been proposed, termed as a lossless video-watermarking technique using intelligent gravitational search algorithm and Hessenberg transform in linear wavelet transform (IGSA-LH) and (2) to attain equilibrium between imperceptibility and robustness, intelligent gravitational search algorithm (IGSA) has been leveraged to acquire an optimal set of multiple scaling fac- tors. For experimental analysis, the proposed technique has been evaluated on four standard benchmark videos against 12 image and video attacks in terms of imperceptibility param- eters, namely mean peak signal to noise ratio (MPSNR) and mean structural similarity index (MSSIM) and robustness parameter, i.e., mean normalized correlation (MNC). Fur- ther, the obtained results are compared with ve existing video watermarking techniques, namely Karmakar et al. [2], Bhardwaj et al. [49], Farri et al. [17], Kuraparthi et al. [24], and Agilandeeswari et al. [50]. The remaining paper is arranged in the following order. The preliminaries for the proposed technique are presented in the next section. The third section describes the proposed technique followed by the experimental ndings presented in the fourth section. Finally, the last section draws the con- clusion. Preliminaries Color space conversion from RGB to YUV The RGB color space s red (R), green (G), and blue (B) com- ponents are transformed to the YUV color space s luminance (Y) and chrominance (U, V) components. The luminance 123 Complex & Intelligent Systems (2022) 8:1047 1070 1049 Table 1 Comparative description of video-watermarking schemes in frequency domain Author/article Methodology Author objectives Performance evaluation parameter Application Choi et al. [9] (1) DCT (1) High robustness (1) PSNR Copyright protection (2) High video quality (2) BER Preda et al. [10] (1) DWT (1) Transparency (1) PSNR Copyright protection (2) PRN (2) Robustness (2) BER (3) Secret key Yassin et al. [11] (1) DWT (1) High imperceptibility (1) PSNR (2) PCA (2) Robustness (2) NC Faragallah et al. [12] (1) DWT + SVD (1) An ef cient (1) PSNR Copyright protection (2) Robust (2) NC (3) Imperceptible (3) BER Masoumi et al. [13] (1) 3D-DWT (1) Transparency (1) PSNR Copyright protection (2) Spread spectrum (2) Robustness (2) NC Thanh et al. [14] (1) DCT (1) High robustness (1) PSNR (2) KAZE feature (2) SSIM (3) NCC Masoumi et al. [15] (1) 3D-DWT (1) High transparency (1) MSE Ownership veri cation (2) CDMA (2) Robustness (2) PSNR (3) PRN (3) SSIM (4) MS-SSIM Rast et al. [16] (1) DWT (1) Imperceptibility (1) PSNR Copyright protection (2) SVD (2) Robustness (2) CC (3) QR decomposition Farri et al. [17] (1) LWT (1) Imperceptibility (1) PSNR Copyright protection (2) SVD (2) Robustness (2) SSIM (3) Arnold transform (3) NCC (4) BER Suresh et al. [18] (1) LWT (1) Imperceptibility (1) PSNR Copyright protection (2) GWO (2) Robustness (2) SSIM (3) Chaotic key (3) NC 123 1050 Complex & Intelligent Systems (2022) 8:1047 1070 Table 1 continued Author/article Methodology Author objectives Performance evaluation parameter Application Ayubi et al. [19] (1) DWT (1) Imperceptibility (1) PSNR (2) LWT (2) Robustness (2) SSIM (3) SVD (3) NC Aditya et al. [20] (1) DCT (1) Imperceptibility (1) PSNR (2) DWT (2) Robustness (2) NC (3) Cuckoo search Agrawal et al. [21] (1) DWT (1) Imperceptibility (1) PSNR (2) GA-PSO (2) Robustness (2) NC Gupta et al. [22] (1) DWT (1) Imperceptibility (1) PSNR (2) GSO (2) Robustness (2) NC (3) BER Kumar et al. [23] (1) RDWT-SVD (1) Imperceptibility (1) PSNR (2) MOO (2) Robustness (2)SSIM (3) NC Kuraparthi et al. [24] (1) DWT-SVD (1) Imperceptibility (1) PSNR Copyright protection (2) ABC (2) Robustness (2) NC Sahu et al. [25] (1) SIFT (1) Robustness (1) PSNR (2) DCT (3) Feature points Dutta et al. [26] (1) SIFT (1) Robustness (1) PSNR (2) DCT (3) Feature points Singh et al. [27] (1) SIFT (1) Strong imperceptibility (1) MPSNR Copyright protection (2) DWT (2) Strong robustness (2) MSSIM (3) Arnold transform (3) NC (4). BER 123 Complex & Intelligent Systems (2022) 8:1047 1070 1051 components (Y) re ect the majority of the frame informa- tion and overall strength, while the chrominance components (U, V) indicate the color information of the frame [31]. The mathematical color conversion formula is given by Eq. (1): Y U V = 0 127 127 0.2989 0.5866 0.1145 0.1688 0.3312 0.5000 0.5000 0.4184 0.0816 R G B (1) Linear wavelet transform DWT is a rst-generation wavelet that yields oating-point coef cients. At any time, these coef cients may alter during subsequent processing. As a result, information is lost during watermark embedding due to the truncation of oating-point pixel values. LWT replaces the up and downsampling of DWT with split and merges into each level, resulting in a nonlinear wavelet transform. Because of the split and merge procedure, the computational complexity is nearly half of DWT [51]. To alleviate the same, the linear wavelet transform (LWT) is an extended version of DWT based on second-generation wavelets [52]. LWT splits an image into low-frequency (LL) and high-frequency (LH, HL, HH) sub- band. In contrast to DWT, LWT transforms the value of an image pixel from integers to integers, resulting in loss- less, computationally faster, and reliable execution. There are three key steps, namely split, predict, and update which are discussed below. The complete procedure of LWT is illus- trated in Fig. 1. 1. Splitting: Consider an image F(m, n) that is split into two sections, i.e, even (Fe(m, n)) and odd (Fo(m, n)) which are de ned by Eqs. (2) and (3), respectively: Fe(m, n) = F(m, 2n) (2) Fo(m, n) = F(m, 2n + 1) (3) 2. Dual lifting (Predict): Generally, the prediction operator (P ) merges many even parts and applies the resulting value to the actual odd part. The odd part is evaluated using a prediction operator from the local neighborhood s even coef cients. The error in odd part prediction de ned by Eq. (4) yields high-frequency coef cients (h(m, n)) and updated value of odd part is given by Eq. (5): h(m, n) = Fo(m, n) P [Fe(m, n)] (4) Fo(m, n) = h(m, n) + P [Fe(m, n)] (5) 3. Primal lifting (update): Similarly, the low-frequency coef cients (h (m, n)) are produced by modifying the even part along with the updating value (Uh(m, n)) which is given by Eq. (6). Further, Uh(m, n) is updated through F(m, n) using Eq. (7): h (m, n) = Fe(m, n) + Uh(m, n) (6) Uh(m, n) = U[F(m, n)] (7) Arnold transform Arnold transform (AT) is a technique for scrambling images that improve protection and identi es their true owner [52]. The AT is used to scramble a watermark logo before embed- ding. It shuf es the pixel positions to create the new chaotic image iteratively, resulting in a scrambled image. An unau- thorized user cannot retrieve the watermark logo from the watermarked image without knowing the security key. Fig- ure 2 depicts the watermark logo and its scrambled version at different iterations. The gure shows that as the number of iterations increase, the watermark logo scrambles ef ciently. The AT is equated to a square watermark logo of dimension R2 R2 as depicted in Eq. (8): a0 b0  = 1 1 1 1  a b  mod R2, (8) where (a, b) represents the watermark pixel values and (a0, b0) represents the scrambled image pixel values at ith iteration. The inverse Arnold transform (IAT) recovers the watermark logo through Eq. (9): a b  =  2 1 1 1  x0 y0  + R2 R2  modR2 (9) Fig. 1 Linear wavelet transform 123 1052 Complex & Intelligent Systems (2022) 8:1047 1070 Fig. 2 Arnold transform at different iterations: a Watermark logo, b itr = 50, c itr = 75, d itr = 100 Hessenberg transform The Hessenberg transform (HT) is a procedure for fac- torizing a general matrix (S) using orthogonal similarity transformations [53,54]. The HT of a matrix (S) is given by Eq. (10): HT[S] = Q H QT, (10) where Q and H are orthogonal and Hessenberg matrix, respectively, such that hi j = 0 and i > j + 1. Usually, Hessenberg transform is performed on household matrices. The Householder matrix (P) is an orthogonal matrix de ned by Eq. (11): P = (Ir 2uuT)/uTu, (11) where Ir and u are m m identity matrix and nonzero vector in Rm, respectively. The overall procedure consists of m 2 steps for a matrix (S) of size m m. Therefore, H is computed by Eq. (12): H = (P1P2 Pm 3Pm 2)T S(P1P2 Pm 3Pm 2) (12)  H = QT S Q (13)  S = Q H QT, (14) where Q = P1P2 Pm 3Pm 3. For example, HT of a matrix (S) of size 4 4 is computed as follows: S = 228 39 208 51 245 66 63 65 140 215 237 158 36 65 90 121 (15) After performing HT on a matrix (S), matrix (Q) and matrix (H) are given as follows: Q = 1.0000 0 0 0 0 0.8613 0.5039 0.0656 0 0.4921 0.7950 0.3546 0 0.1266 0.3377 0.9327 (16) H = 228.0000 142.4106 162.9320 23.6301 284.4662 255.7505 109.1450 85.5923 0 246.4589 113.4365 68.8549 0 0 29.5251 54.8130 (17) Intelligent gravitational search algorithm (IGSA) Mittal et al. [48] proposed a variant of the gravitational search algorithm, intelligent gravitational search algorithm (IGSA), to improve the solution precision. IGSA focuses on enhanc- ing the exploitation ability among the objects. To do so, the position equation is modi ed by including the global best solution(gBest)andglobalworstsolution(gWorst).InIGSA, theattractionofobjectstowards gBest isproportionatedwith gWorst. Mathematically, the modi ed position equation of IGSA for an ith object in dth dimension at tth iteration is depicted in Eq. (18): xd i (t + 1) = xd i (t) + vd i (t + 1) + r(t) (gBestd(t) xd i (t)) |( gWorstd(t)) xd i (t))|   Intelligent component , (18) where r(t) [0, 1] and is a constant whose value is taken as 0.9. Proposed technique Theproposedtechnique,termedasalosslessvideo-watermarking techniqueusingintelligentgravitationalsearchalgorithmand Hessenberg transform in linear wavelet transform (IGSA- LH),isexplainedinthefollowingfoursections:identi cation 123 Complex & Intelligent Systems (2022) 8:1047 1070 1053 of motion frames and keyframes ( Identi cation of motion frames and keyframes ), embedding process ( Embedding process ), extraction process ( Extraction process ), and nding optimal scaling factors through IGSA ( Selection of multiple scaling factors using IGSA algorithm ). The overall procedure of the proposed technique is illustrated in Fig. 3. Identification of motion frames and keyframes Motion frames are identi ed by performing the histogram difference method on cover video [55]. The complete keyframe selection procedure is illustrated in Fig. 4. In this method, the absolute histogram difference of two video frames back to back is calculated. If the absolute histogram difference between consecutive video frames is greater than a prede ned threshold, a frame is identi ed as a motion frame. Similarly, keyframes are detected from the selected motion frames followed by entropy. Suppose, the single motion frame entropy is greater than the average entropy of motion framesthenthatmotionframeisidenti edasakeyframe.Fig- ure 5 depicts the identi ed keyframes corresponding to each cover video. A detailed work ow for identifying keyframes are referred to motion frames which is given below: 1. The cover video (V) is divided into k frames of size R1 R1, namely f1, f2, . . . , fk. 2. Compute the sum of the absolute histogram differences (h1 h2) between two adjacent frames. 3. The sum of the absolute differences is compared to a pre- de ned threshold [55]. If the sum exceeds the threshold, the frame is referred as the motion frame (MF). 4. Continue until last frame of video. 5. Calculate the entropy of all the selected motion frames. 6. If the single frame entropy is greater than the compared average entropy of motion frames then that frame is selected as keyframe (k f ). Embedding process In this section, a scrambled watermark is incorporated into the keyframes. The following steps are considered while embedding watermark: 1. Convert keyframe (K f ) into YUV color space and select chrominance component (Kv) by Eq. (19): [Ky Ku Kv] = rgb2yuv(K f ) (19) Fig. 3 The proposed video-watermark embedding and extraction process 123 1054 Complex & Intelligent Systems (2022) 8:1047 1070 Fig. 4 The work ow of identi cation of motion frames and keyframes 2. The chrominance component (Kv) is decomposed into four sub-bands (LLv, LHv, HLv, HHv) by performing 1- level LWT de ned by Eq. (20) and consider sub-band (LLv) for embedding purpose: [LLv, HLv, HLv, HHv] = LWT(Mv) (20) 3. Perform HT on sub-band LLv using Eq. (21): HT[LLv] = Qv Hv QT v (21) 4. Select watermark logo (w) of size (R2 R2) and per- form AT to obtain scrambled watermark (sw) for security enhancement purpose. 5. Scrambled watermark (sw) is decomposed into four sub- bands by performing 1-level LWT using Eq. (22) and sub-band LLw is selected for further processing: [LLw, HLw, HLw, HHw] = LWT(sw) (22) 6. Perform HT on sub-band LLw using Eq. (23): HT[LLw] = Qw Hw QT w (23) 7. Apply IGSA to obtain the set of MSF factors ( ) accord- ing to the procedure detailed in Selection of multiple scaling factors using IGSA algorithm . 8. Embed the component Hw into Hv using Eq. (24): Hv = Hv + Hw (24) 8. Apply inverse HT (IHT) on modi ed matrix ( Hv) to obtain a Hessenberg embedded matrix ( LLv) by Eq. (25): LLv = Qv Hv QT v (25) 9. Apply inverse LWT (ILWT) on matrix ( LLv) to obtain the watermarked video frames (Wv). Wv = ILWT[ LLv, LHv, HLv, HHv] (26) 10. Finally, convert all YUV watermarked video keyframes into RGB watermarked video keyframes and merges with rest video frames to obtain watermarked video (V ). Extraction process In extraction, the reverse operation of the embedding process is performed to recover the watermark logo and is presented below: 1. Divide the RGB watermarked video (V ) into k frames, namely f 1 , f 2 , . . . , f k , and extract keyframes (k f ). 2. Convertthewatermarkedvideokeyframes(k f )intoYUV color space and select chrominance component (k v). 3. The chrominance component (K v) is decomposed into four sub-bands (LL v, LH v, HL v, HH v)byperforming1-levelLWTde ned by Eq. (27) and consider sub-band (LL v) for extraction purpose [LL v, LH v, HL v, HH v] = LWT(k v) (27) 4. Apply HT on sub-band LL v of watermarked video keyframes by Eq. (28): HT [LL v] = Q v H v Q T v (28) 5. Extract watermark logo using Eq. (29): H w = (H v Hv) (29) 123 Complex & Intelligent Systems (2022) 8:1047 1070 1055 Fig. 5 The extracted keyframes corresponding to a Silent, b Foreman, c Mobile, d Hall monitor 6. Perform IHT on matrix (H w) using Eq. (30): LL w = Qw H w QT w (30) 7. Apply ILWT on matrix (LL w) to obtain the extracted and scrambled watermark (LLew) using Eq. (31): LLew = I LWT [LL w, LHw, HLw, HHw] (31) 8. Again, Apply IAT to recover the watermark logo (w ). Selection of multiple scaling factors using IGSA algorithm Thescalingfactorisacriticalparameterinwatermarkembed- ding. It regulates imperceptibility and robustness simultane- ously. However, to maintain the trade-off between the above two parameters, a single scaling factor completely fails. The IGSA algorithm is employed to identify the optimal set of MSF through an objective function given in Eq. (32). The 123 1056 Complex & Intelligent Systems (2022) 8:1047 1070 Fig. 6 Watermarked video frames and their corresponding extracted watermark logos a Silent, b Foreman, c Mobile, d Hall monitor (a) MP- SNR=48.97 MSSIM=0.9995 (b) MNC=0.9998 (c) MP- SNR=47.98 MSSIM=0.9997 (d) MNC=0.9997 (e) MP- SNR=48.07 MSSIM=0.9993 (f) MNC=0.9998 (g) MP- SNR=48.52 MSSIM=0.9998 (h) MNC=0.9997 Table 2 The performance of the proposed technique under various attacks for considered videos in terms of robustness Attack index Video attack Silent Foreman Mobile Hall monitor MF Median ltering (3 3) 0.9998 0.9993 0.9994 0.9999 WF Wiener ltering (3 3) 0.9997 0.9998 0.9992 0.9995 GF Gaussian ltering (3 3) 0.9997 0.9994 0.9996 0.9992 RO Rotation (45 ) 0.9998 0.9986 0.9994 0.9997 TR Translation (30, 30) 0.9999 0.9993 0.9997 0.9998 CR Cropping CR (center) 0.9995 0.9996 0.9992 0.9997 SH Sharpening 0.9996 0.9997 0.9996 0.9995 GC Gamma correction ( = 0.6) 0.9993 0.9997 0.9997 0.9994 HI Histogram equalization 0.9998 0.9993 0.9994 0.9996 GN Gaussian noise (0, 10%) 0.9880 0.9878 0.9872 0.9877 SP Salt and pepper noise (10%) 0.9882 0.9885 0.9884 0.9889 PN Poisson noise 0.9953 0.9942 0.9921 0.9947 complete procedure for determining optimal scaling factors by IGSA is given below: 1. IGSA initializes a random population where, each indi- vidual consists of d scaling factors. 2. The objective function described by Eq. (32) is used to calculate the tness of each individual: OF(i, j)max =  MSSIM( f , f ) + MNC( f , f ) 2 + K x=1 MNC(w, w) K  (32) where MSSIM ( f , f ) and MNC ( f , f ) evaluate mean structural similarity index and mean normalized correla- tion between cover video frames ( f ) and watermarked video frames ( f ). MNC (w, w) measures the mean nor- malized correlation between the watermark logo (w) and extracted watermark logo ( w), and K denotes the total number of performed attacks. 3. Each individual of IGSA is updated according to Eq. (18). 4. This process is continued until the maximum iterations are reached. Results and discussions Experiments aresimulatedonMATLAB2020aona2.5GHz, I5 processor, and 8 GB RAM system. The ef cacy of the proposed technique (IGSA-LH) has been evaluated on four standard benchmark videos: Silent, Foreman, Mobile, and Hall monitor in terms of imperceptibility and robustness. Imperceptibility is examined between cover video and water- 123 Complex & Intelligent Systems (2022) 8:1047 1070 1057 Attack type Attacked video frames Extracted watermark Silent Foreman Mobile Hall monitor Silent Foreman Mobile Hall monitor 1. MF 2. WF 3. GF 4. RO 5. TR 6. CR 7. SH 8. GC 9. HI 10. GN 11. SN 12. PN Fig. 7 The quality of watermarked video frames and extracted watermark logos after various attacks marked video using MPSNR and MSSIM, while robustness is evaluated between watermark logo and extracted water- mark logo using MNC. The cover videos and watermark logo are taken from the online available database [56,57]. The parameter settings of the considered techniques are taken from the respective literature. The experimental results are studied in the following sections: Imperceptibility analy- sis of the proposed technique analyzes the imperceptibility while robustness is examined against supposed attacks in Robustness analysis of the proposed technique . Perfor- mance analysis against existing techniques presents the comparative analysis of the proposed technique (IGSA-LH) with ve recent video-watermarking schemes under consid- ered attacks in terms of MPSNR, MSSIM, and MNC values. 123 1058 Complex & Intelligent Systems (2022) 8:1047 1070 In addition, statistical validation of the proposed technique is discussed in Statistical Analysis of the proposed tech- nique . Finally, Comparative analysis of time complexity provides the comparative analysis of the considered tech- niques in terms of time complexity. Imperceptibility analysis of the proposed technique The imperceptibility evaluates the quality of watermarked video frames. The imperceptibility is assessed between cover video frames ( f ) and watermarked video frames ( f ). The performance of the proposed technique (IGSA-HT) has been examined on all considered four videos in terms of MPSNR and MSSIM which are de ned by Eqs. (33) and (34), respec- tively. Figure 6 shows the quality of watermarked video frames which are quite similar to keyframes depicted in Fig. 5). The proposed technique attains higher MPSNR values as 47.65, 48.97, 47.98, 48.07, 48.52 and MSSIM values as 0.9997, 0.9998, 0.9997, 0.9993, 0.9998 against each consid- ered video. The gure depicts that the proposed technique performs effectively in the embedding phase. MPSNR( f , f ) = 1 F F  i=1 PSN Ri (33) MSSIM( f , f ) = 1 F F  i=1 SSI Mi, (34) where F denotes the total number of video frames. Robustness analysis of the proposed technique The quality of extracted watermark logo under the various image and video attacks is measured by robustness. The robustness is assessed between the original watermark logo and extracted watermark logo in terms of MNC which is de ned by Eq. 35. A total of 12 attacks are confronted on the watermarked video to evaluate the robustness of the pro- posed technique. These attacks includes (i) median ltering (3 3), (ii) wiener ltering (3 3), (iii) Gaussian lter- ing (3 3), (iv) rotation (45 ), (v) translation (30, 30), (vi) cropping (center), (vii) sharpening, (viii) gamma correction ( = 0.6), (ix) histogram equalization, (x) Gaussian noise (0, 10%), (xi) salt and pepper noise (10%), and (xii) Poisson noise. Table 2 tabulates the MNC values for all considered four videos against each attack. It can be visualized from the table that the proposed technique attains superior MNC val- ues. The quality of extracted watermark logos corresponding to each attack for all videos are illustrated in Fig. 7. It can be observed from the gure that the proposed technique recov- ers a superior quality watermark against each attack except Table 3 The imperceptibility (MPSNR, MSSIM) of the considered techniques under no attack Cover video IGSA-LH Karmakar et al. [2] Bhardwaj et al. [49] Farri et al. [17] Kuraparthi et al. [24] Agilandeeswari et al. [50] MPSNR MSSIM MPSNR MSSIM MPSNR MSSIM MPSNR MSSIM MPSNR MSSIM MPSNR MSSIM Silent 48.97 0.9995 42.65 0.9897 46.63 0.9989 42.67 0.9397 46.78 0.9876 45.82 0.9892 Foreman 47.98 0.9997 42.57 0.9889 44.07 0.9875 46.71 0.9373 46.72 0.9874 45.76 0.9890 Mobile 48.07 0.9993 41.98 0.9843 44.13 0.9821 41.23 0.9844 46.83 0.9879 45.90 0.9895 Hall monitor 48.52 0.9999 42.37 0.9863 43.75 0.9983 42.31 0.9973 46.70 0.9881 45.67 0.9897 Average 48.39 0.9996 42.39 0.9873 44.65 0.9917 43.23 0.9647 46.76 0.9878 45.79 0.9894 123 Complex & Intelligent Systems (2022) 8:1047 1070 1059 Gaussian noise, salt and pepper noise, and Poisson noise: MNC(w, w) = 1 F F  i=1 NCi (35) Performance analysis against existing techniques The proposed technique (IGSA-LH) has been compared with ve recent video-watermarking techniques, namely Kar- makar et al. [2], Bhardwaj et al. [49], Farri et al. [17], Kuraparthi et al. [24], and Agilandeeswari et al. [50]. Com- parative analysis of imperceptibility discusses the imper- ceptibility performance of the proposed technique against existing techniques in terms of MPSNR and MSSIM. While Comparative analysis of robustness studies the robustness of the considered techniques in terms of MNC parameters on 12 video attacks. Comparative analysis of imperceptibility Table 3 highlights the MPSNR and MSSIM values cor- responding to each video against compared schemes. It can be noticed from the table that the proposed technique attains an average value of MPSNR as 48.39 and MSSIM value as 0.9996. These superior values con rm that the pro- posed technique outperforms the compared schemes. For the quantitative analysis, the MPSNR values are plotted against considered methods under all considered four videos which are shown in Fig. 8. Comparative analysis of robustness Tables 4, 5, 6, 7, 8, 9, 10, 11, 12 and 15 illustrate the com- parative analysis of robustness of the considered techniques against 12 different video attacks. (i) Median ltering attack: The median attack with kernel size 3 3 is performed on watermarked video frames. Table 4 compiles the individual and average values of four videos for all considered schemes. The proposed technique achieves individual MNC values as (0.9990, 0.9984, 0.9991, 0.9986) and average MNC value as 0.9988 against this attack for Silent, Foreman, Mobile, and Hall monitor videos. These results con rm that the IGSA-LH technique outperforms compared schemes against median ltering attack. (ii) Wiener ltering attack: To evaluate the robustness, the watermarked video frames are attacked by wiener ltering. Table 5 compiles the MNC individual val- ues and averaged MNC values for the considered four videos against each considered scheme. The averaged MNC values attained by the considered schemes are as (0.9993, 0.9830, 0.9956, 0.9986, 0.9880, 0.9869). These results con rm that the average MNC value of the IGSA-LH technique is superior to the compared schemes that show the IGSA-LH technique s robust- ness under wiener ltering attack. (iii) Gaussian ltering attack: Gaussian ltering with ker- nel size 3 3 is encountered on watermarked video frames. Table 6 presents the average MNC values as (0.9992, 0.9837, 0.9965, 0.9990, 0.9910, 0.9876) corresponding to each considered schemes for all con- sidered four videos. From the table, it can be con rmed that the proposed technique attains a higher average MNC value as compared schemes. Therefore, the pro- posed technique performs effectively under Gaussian ltering attacks. (iv) Rotation attack: A watermarked video is rotated clock- wise by 45 angle to prove the robustness. For this attack, the average MNC values are as (0.9997, 0.9835, 0.9951, 0.9960, 0.9921, 0.9884) which are tabulated in Table 7. This table visualizes that the proposed tech- nique s average MNC value is superior to the compared schemes. Hence, the proposed technique outperforms under this attack. (v) Translation attack: To evaluate the effectiveness of the proposed technique, a translation attack (30, 30) shifts the pixels value of the watermarked video. Under this attack, Table 8 highlights the average MNC values corresponds to each considered scheme. These val- ues are as (0.9996, 0.9834, 0.9950, 0.9958, 0.9920, 0.9890) which con rm that the proposed technique has the highest average MNC value against compared schemes. Therefore, the proposed technique perfor- mance is outstanding under this attack. (vi) Cropping attack: In cropping attack, a watermarked video frame is cropped 25% from the center. The aver- age MNC values obtained by each scheme under this attack are as (0.9993, 0.9833, 0.9947, 0.9957, 0.9904, 0.9868) which are visualized in Table 9. It can be observed from the table that the proposed technique attains a higher average MNC value. This table con- cludes that the IGSA-LH technique is quite effective under cropping attack. (vii) Sharpening attack: A sharpening attack is performed on watermarked video frames. The average MNC values correspond to sharpening attack against consid- ered schemes are as (0.9988, 0.9828, 0.9951, 0.9956, 0.9932, 0.9934) and visualized in Table 10. It can be envisioned from the table that the proposed technique attains a higher average MNC value which is re ected in bold. Hence, the proposed technique is quite resilient to sharpening attack. 123 1060 Complex & Intelligent Systems (2022) 8:1047 1070 Fig. 8 Comparative analysis of IGSA-LH technique against considered techniques in terms of imperceptibility (viii) Histogram equalization attack: Histogram attack is confronted on watermarked video frames to compare the robustness. In histogram attack, the considered schemes attains average MNC values as (0.9979, 0.9824, 0.9948, 0.9954, 0.9926, 0.9909), respectively, and depicted in Table 12. Histogram attack gets max- imum average MNC value (0.9979) corresponding to the proposed technique. This value con rms that the performance of the proposed technique is quite effec- tive under this attack. (ix) Gamma correction attack: The contrast of the water- marked video frames is enhanced by gamma correction (0.6) and average MNC values under this attack are pre- sented in Table 11. Table 11 tabulates the average MNC values as (0.9982, 0.9825, 0.9951, 0.9955, 0.9912, 0.9888) corresponding to each considered schemes. these results con rm that the average MNC value is higher for proposed technique. (x) Salt and pepper noise attack: Watermarked video frames are attacked by salt and pepper noise (1%) and average MNC values corresponding to each scheme are depicted in Table 14. Farri et al. [17] gets maximum average MNC value as 0.9954. Therefore, the proposed technique is performing better than Karmakar et al. [2]. However, Farri et al. [17] outperforms. xi. Gaussian noise attack: In Gaussian noise, the water- marked video frames are destroyed by 10% noise density. Table 13 depicts the average MNC values as ( 0.9903, 0.9818, 0.9919, 0.9953, 0.9915, 0.9911). It can be visualized from the table that the IGSA-LH technique performs better than Karmakar et al. [2]. However, Farri et al. [17] attains a higher average MNC value 0.9953 which conforms that this scheme outper- forms under the Gaussian noise attack. xii. Poissonnoiseattack:Poissonnoiseattackisperformed on video-watermarked frames to compare the robust- ness. The average NC values against each scheme are envisioned in Table 15. The average MNC values are as (0.9910, 0.9823, 0.9922, 0.9960, 0.9925, 0.9930) for considered schemes, respectively. It can be observed from the table that Farri et al. [17] is also outperform- ing under this attack. Moreover, the average MNC values of all the consid- ered videos corresponding to each attack against considered schemes are also presented graphically for better visualiza- tion, shown in Fig. 9. It can be illustrated from the gure that the proposed technique outperforms all considered attacks except noise attacks. Therefore, the proposed technique is resilient to considered attacks. 123 Complex & Intelligent Systems (2022) 8:1047 1070 1061 Fig. 9 Comparative analysis of IGSA-LH technique against considered techniques in terms of robustness Statistical analysis of the proposed technique To statistically validate the performance of the proposed technique (IGSA-LH), a non-parametric Friedman s test is performed for each considered performance parameters, i.e., MPSNR, MSSIM, and MNC. The test contains two hypothesis, i.e., the null hypothesis (H0) and the alternative hypothesis (H1). As per the H0, all parameter values gener- ated by comparative methods are signi cantly equal, while H1 says that comparative methods are signi cantly different. The p value return by test is 0.01 for 30 different executions which is less than considered signi cant level ( = 0.05). Therefore, it can be stated that H0 fails and the obtained results are signi cantly different. Further, Table 16 depicts the ranking of the considered techniques for each parame- ter, respectively, where the proposed technique ranked rst. It can be con rmed from the tables that the proposed tech- nique is statistically better than the considered techniques in terms of each parameter. Comparative analysis of time complexity The proposed technique has also been compared with ve recent state-of-the-art schemes in terms of embedding and extraction time for 300 video frames of each considered video. Table 17 depicts the embedding and extraction time in seconds. It can be con rmed from the table that the proposed technique spans few seconds only as compared techniques. These results show that the proposed technique has low time complexity, making the proposed technique suitable for video watermarking in real-time. Hence, the proposed tech- nique is computationally ef cient. Conclusion This paper presents a lossless ef cient video-watermarking technique based on an optimal keyframes selection using IGSA and HT in the LWT domain. In this scheme, a scram- bled watermark logo is incorporated into the keyframes followed by a one-level LWT. IGSA algorithm acquires a set of MSF which maintains the equilibrium between imperceptibility and robustness. The security of the IGSA- LH technique has been improved by performing Arnold transform on the watermark logo prior to embedding. The experimental results were validated against 12 video attacks and compared ve recent state-of-the-art schemes. The com- parative analysis of imperceptibility and robustness validate that the IGSA-LH technique is quite resilient to attacks. In future work, the performance of the proposed technique can be evaluated on color watermark logos over different video attacks along with various parameters. Further, the elliptic curve cryptography (ECC) technique can be applied to a proposed technique for security enhancement. Moreover, the applicability of the proposed technique can be extended to big-data applications. 123 1062 Complex & Intelligent Systems (2022) 8:1047 1070 Table 4 The robustness (MNC) of the considered techniques under median ltering attack for considered videos Cover video IGSA-LH Karmakar et al. [2] Bhardwaj et al. [49] Farri et al. [17] Kuraparthi et al. [24] Agilandeeswari et al. [50] Silent 0.9990 0.9827 0.9987 0.9968 0.9889 0.9795 Foreman 0.9984 0.9783 0.9914 0.9976 0.9829 0.9807 Mobile 0.9991 0.9897 0.9956 0.9982 0.9827 0.9801 Hall monitor 0.9986 0.9783 0.9972 0.9980 0.9855 0.9842 Average 0.9988 0.9823 0.9957 0.9977 0.9850 0.9811 Table 5 The robustness (MNC) of the considered techniques under wiener ltering attack for considered videos Cover video IGSA-LH Karmakar et al. [2] Bhardwaj et al. [49] Farri et al. [17] Kuraparthi et al. [24] Agilandeeswari et al. [50] Silent 0.9998 0.9834 0.9973 0.9990 0.9825 0.9875 Foreman 0.9993 0.9789 0.9934 0.9995 0.9901 0.9865 Mobile 0.9990 0.9893 0.9951 0.9969 0.9890 0.9862 Hall monitor 0.9992 0.9805 0.9966 0.9989 0.9902 0.9872 Average 0.9993 0.9830 0.9956 0.9986 0.9880 0.9869 123 Complex & Intelligent Systems (2022) 8:1047 1070 1063 Table 6 The robustness (MNC) of the considered techniques under Gaussian ltering attack for considered videos Cover video IGSA-LH Karmakar et al. [2] Bhardwaj et al. [49] Farri et al. [17] Kuraparthi et al. [24] Agilandeeswari et al. [50] Silent 0.9994 0.9840 0.9975 0.9992 0.9901 0.9868 Foreman 0.9992 0.9801 0.9968 0.9997 0.9905 0.9873 Mobile 0.9990 0.9890 0.9954 0.9980 0.9912 0.9879 Hall monitor 0.9991 0.9817 0.9962 0.9991 0.9923 0.9885 Average 0.9992 0.9837 0.9965 0.9990 0.9910 0.9876 Table 7 The robustness (MNC) of the considered techniques under rotation attack for considered videos Cover video IGSA-LH Karmakar et al. [2] Bhardwaj et al. [49] Farri et al. [17] Kuraparthi et al. [24] Agilandeeswari et al. [50] Silent 0.9997 0.9897 0.9979 0.9951 0.9909 0.9856 Foreman 0.9998 0.9780 0.9904 0.9972 0.9916 0.9890 Mobile 0.9997 0.9898 0.9954 0.9962 0.9923 0.9893 Hall monitor 0.9996 0.9763 0.9968 0.9956 0.9937 0.9896 Average 0.9997 0.9835 0.9951 0.9960 0.9921 0.9884 123 1064 Complex & Intelligent Systems (2022) 8:1047 1070 Table 8 The robustness (MNC) of the considered techniques under translation attack for considered videos Cover video IGSA-LH Karmakar et al. [2] Bhardwaj et al. [49] Farri et al. [17] Kuraparthi et al. [24] Agilandeeswari et al. [50] Silent 0.9997 0.9894 0.9981 0.9955 0.9914 0.9871 Foreman 0.9995 0.9782 0.9905 0.9967 0.9922 0.9901 Mobile 0.9998 0.9893 0.9950 0.9960 0.9925 0.9897 Hall monitor 0.9994 0.9766 0.9963 0.9952 0.9919 0.9890 Average 0.9996 0.9834 0.9950 0.9958 0.9920 0.9890 Table 9 The robustness (MNC) of the considered techniques under cropping attack for considered videos Cover video IGSA-LH Karmakar et al. [2] Bhardwaj et al. [49] Farri et al. [17] Kuraparthi et al. [24] Agilandeeswari et al. [50] Silent 0.9990 0.9899 0.9978 0.9952 0.9906 0.9867 Foreman 0.9992 0.9780 0.9900 0.9970 0.9896 0.9872 Mobile 0.9994 0.9890 0.9951 0.9958 0.9902 0.9854 Hall monitor 0.9995 0.9765 0.9960 0.9949 0.9911 0.9878 Average 0.9993 0.9833 0.9947 0.9957 0.9904 0.9868 123 Complex & Intelligent Systems (2022) 8:1047 1070 1065 Table 10 The robustness (MNC) of the considered techniques under sharpening attack for considered videos Cover video IGSA-LH Karmakar et al. [2] Bhardwaj et al. [49] Farri et al. [17] Kuraparthi et al. [24] Agilandeeswari et al. [50] Silent 0.9989 0.9879 0.9981 0.9953 0.9917 0.9925 Foreman 0.9982 0.9781 0.9903 0.9970 0.9927 0.9932 Mobile 0.9989 0.9894 0.9956 0.9942 0.9936 0.9929 Hall monitor 0.9990 0.9756 0.9964 0.9958 0.9947 0.9949 Average 0.9988 0.9828 0.9951 0.9956 0.9932 0.9934 Table 11 The robustness (MNC) of the considered techniques under gamma correction attack for considered videos Cover video IGSA-LH Karmakar et al. [2] Bhardwaj et al. [49] Farri et al. [17] Kuraparthi et al. [24] Agilandeeswari et al. [50] Silent 0.9983 0.9875 0.9980 0.9950 0.9905 0.9913 Foreman 0.9980 0.9778 0.9901 0.9971 0.9897 0.9817 Mobile 0.9985 0.9890 0.9955 0.9940 0.9915 0.9924 Hall monitor 0.9981 0.9757 0.9967 0.9957 0.9929 0.9899 Average 0.9982 0.9825 0.9951 0.9955 0.9912 0.9888 123 1066 Complex & Intelligent Systems (2022) 8:1047 1070 Table 12 The robustness (MNC) of the considered techniques under histogram equalization attack for considered videos Cover video IGSA-LH Karmakar et al. [2] Bhardwaj et al. [49] Farri et al. [17] Kuraparthi et al. [24] Agilandeeswari et al. [50] Silent 0.9976 0.9873 0.9978 0.9951 0.9919 0.9893 Foreman 0.9981 0.9780 0.9900 0.9970 0.9933 0.9902 Mobile 0.9982 0.9891 0.9952 0.9941 0.9937 0.9943 Hall monitor 0.9977 0.9752 0.9963 0.9953 0.9916 0.9897 Average 0.9979 0.9824 0.9948 0.9954 0.9926 0.9909 Table 13 The robustness (MNC) of the considered techniques under Gaussian noise attack for considered videos Cover video IGSA-LH Karmakar et al. [2] Bhardwaj et al. [49] Farri et al. [17] Kuraparthi et al. [24] Agilandeeswari et al. [50] Silent 0.9881 0.9863 0.9970 0.9943 0.9889 0.9903 Foreman 0.9871 0.9776 0.9886 0.9980 0.9943 0.9912 Mobile 0.9980 0.9883 0.9856 0.9938 0.9895 0.9916 Hall monitor 0.9879 0.9750 0.9964 0.9949 0.9932 0.9913 Average 0.9903 0.9818 0.9919 0.9953 0.9915 0.9911 123 Complex & Intelligent Systems (2022) 8:1047 1070 1067 Table 14 The robustness (MNC) of the considered techniques under salt and pepper noise attack for considered videos Cover video IGSA-LH Karmakar et al. [2] Bhardwaj et al. [49] Farri et al. [17] Kuraparthi et al. [24] Agilandeeswari et al. [50] Silent 0.9878 0.9865 0.9968 0.9940 0.9951 0.9911 Foreman 0.9873 0.9773 0.9884 0.9981 0.9934 0.9918 Mobile 0.9975 0.9843 0.9858 0.9943 0.9930 0.9952 Hall monitor 0.9880 0.9751 0.9968 0.9951 0.9917 0.9935 Average 0.9902 0.9808 0.9920 0.9954 0.9933 0.9929 Table 15 The robustness (MNC) of the considered techniques under Poisson noise attack for considered videos Cover video IGSA-LH Karmakar et al. [2] Bhardwaj et al. [49] Farri et al. [17] Kuraparthi et al. [24] Agilandeeswari et al. [50] Silent 0.9885 0.9867 0.9968 0.9953 0.9914 0.9917 Foreman 0.9891 0.9786 0.9876 0.9986 0.9929 0.9919 Mobile 0.9982 0.9887 0.9865 0.9941 0.9922 0.9948 Hall monitor 0.9880 0.9753 0.9978 0.9959 0.9951 0.9937 Average 0.9910 0.9823 0.9922 0.9960 0.9925 0.9930 123 1068 Complex & Intelligent Systems (2022) 8:1047 1070 Table 16 Friedman s test of the proposed technique and considered techniques over MPSNR, MSSIM, and MNC Rank Mean PSNR (MPSNR) Mean SSIM (MSSIM) Mean PSNR (MNC) Method Mean rank value Method Mean rank value Method Mean rank value 1 IGSA-LH 2.67 IGSA-LH 2.04 IGSA-LH 1.89 2 Kuraparthi et al. [24] 3.10 Farri et al. [17] 2.79 Farri et al. [17] 2.56 3 Agilandeeswari et al. [50] 3.56 Kuraparthi et al. [24] 3.17 Bhardwaj et al. [49] 3.27 4 Farri et al. [17] 4.01 Bhardwaj et al. [49] 3.96 Kuraparthi et al. [24] 3.51 5 Bhardwaj et al. [49] 4.32 Karmakar et al. [2] 4.12 Agilandeeswari et al. [50] 4.11 6 Karmakar et al. [2] 4.56 Agilandeeswari et al. [50] 4.63 Karmakar et al. [2] 4.24 Table 17 Time complexity with considered video-watermarking techniques (in s) Video Proposed Existing technique IGSA-LH Karmakar et al. [2] Bhardwaj et al. [49] Farri et al. [17] Kuraparthi et al. [24] Agilandeeswari et al. [50] EmT ExT EmT ExT EmT ExT EmT ExT EmT ExT EmT ExT Silent 23.6 7.43 65.05 8.10 43.56 9.93 45.06 11.63 30.12 8.27 38.14 9.11 Foreman 19.12 6.28 64.21 7.90 41.21 7.61 42.71 9.31 29.56 8.15 37.87 9.17 Mobile 14.89 5.15 60.19 7.01 39.51 7.17 41.01 8.87 29.43 8.21 37.89 8.78 Hall monitor 20.41 5.89 59.18 6.13 41.39 8.57 42.89 10.27 29.74 8.25 37.52 9.53 EmT embedding time, ExT extraction time 123 Complex & Intelligent Systems (2022) 8:1047 1070 1069 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adap- tation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indi- cate if changes were made. The images or other third party material in this article are included in the article s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitteduse,youwillneedtoobtainpermissiondirectlyfromthecopy- right holder. To view a copy of this licence, visit http://creativecomm ons.org/licenses/by/4.0/. References 1. Asghar MN, Ghanbari M (2012) Mikey for keys management of h. 264 scalable video coded layers. J King Saud Univ Comput Inf Sci 24(2):107 116 2. Karmakar A, Phadikar A, Phadikar BS, Maity GK (2016) A blind video watermarking scheme resistant to rotation and collusion attacks. J King Saud Univ Comput Inf Sci 28(2):199 210 3. Balamurugan N, Adimoolam M, John A, et al (2021) A novel ef cient algorithm for duplicate video comparison in surveillance video storage systems. J Ambient Intell Humaniz Comput 1 12 4. Stelter B, Stone B (2009) Digital pirates winning battle with stu- dios. New York Times, New York, p 5 5. Asikuzzaman M, Pickering MR (2017) An overview of digital video watermarking. IEEE Trans Circuits Syst Video Technol 28(9):2131 2153 6. Singh R, Ashok A (2021) An optimized robust watermarking technique using CKGSA in frequency domain. J Inf Secur Appl 58(102):734 7. Singh R, Ashok A, Saraswat M (2020) Optimised robust water- markingtechnique usingCKGSAinDCT-SVDdomain.IETImage Process 14(10):2052 2063 8. Phadikar A (2013) Multibit quantization index modulation: a high- rate robust data-hiding method. J King Saud Univ Comput Inf Sci 25(2):163 171 9. Choi D, Do H, Choi H, Kim T (2010) A blind mpeg-2 video watermarking robust to camcorder recording. Signal Process 90(4):1327 1332 10. Preda R, Vizireanu DN (2011) Robust wavelet-based video water- marking scheme for copyright protection using the human visual system. J Electron Imaging 20(1):013022 11. Yassin NI, Salem NM, El Adawy MI (2012) Block based video watermarking scheme using wavelet transform and principle com- ponent analysis. Int J Comput Sci Issues (IJCSI) 9(1):296 12. FaragallahOS(2013)Ef cientvideowatermarkingbasedonsingu- lar value decomposition in the discrete wavelet transform domain. AEU-Int J Electron Commun 67(3):189 196 13. Masoumi M, Amiri S (2013) A blind scene-based watermark- ing for video copyright protection. AEU-Int J Electron Commun 67(6):528 535 14. Thanh TM, Hiep PT, Tam TM, Tanaka K (2014) Robust semi-blind video watermarking based on frame-patch matching. AEU-Int J Electron Commun 68(10):1007 1015 15. Masoumi M, Rezaei M, Hamza AB (2015) A blind spatio-temporal data hiding for video ownership veri cation in frequency domain. AEU-Int J Electron Commun 69(12):1868 1879 16. Rasti P, Samiei S, Agoyi M, Escalera S, Anbarjafari G (2016) Robust non-blind color video watermarking using qr decompo- sition and entropy analysis. J Vis Commun Image Represent 38:838 847 17. Farri E, Ayubi P (2018) A blind and robust video watermarking based on IWT and new 3D generalized chaotic sine map. Nonlinear Dyn 93(4):1875 1897 18. Suresh M, Sam IS (2020) Optimized interesting region iden- ti cation for video steganography using fractional grey wolf optimization along with multi-objective cost function. J King Saud Univ Comput Inf Sci :1 8 19. Ayubi P, Barani MJ, Valandar MY, Irani BY, Sadigh RSM (2021) A new chaotic complex map for robust video watermarking. Artif Intell Rev 54(2):1237 1280 20. Aditya K, Choudhary A, Raj A, Sing M (2017) Video watermarking based on cuckoo search and scene change detection. In: 2017 inter- national conference on intelligent sustainable systems (ICISS). IEEE, pp 420 424 21. Agrawal P, Khurshid A (2014) Dwt and ga-pso based novel watermarking for videos using audio watermark. In: International conference in swarm intelligence. Springer, pp 212 220 22. Gupta G, Gupta V, Chandra M (2018) An ef cient video water- marking based security model. Microsyst Technol 24(6):2539 2548 23. Kumar PM, Srinivas K, Prabhakar B (2021) A new video watermarking using redundant discrete wavelet in singular value decomposition domain with multi-objective optimization. Concurr Comput Pract Exp 24:e6217 24. Kuraparthi S, Kollati M, Kora P (2019) Robust optimized dis- crete wavelet transform-singular value decomposition based video watermarking. Traitement du Signal 36(6):565 573 25. Sahu N, Sur A (2017) Sift based video watermarking resistant to temporal scaling. J Vis Commun Image Represent 45:77 86 26. Dutta T, Gupta HP (2017) An ef cient framework for compressed domain watermarking in p frames of high-ef ciency video cod- ing (hevc)-encoded video. ACM Tran Multimed Comput Commun Appl (TOMM) 13(1):1 24 27. Singh KM (2018) A robust rotation resilient video watermarking scheme based on the sift. Multimed Tools Appl 77(13):16419 16444 28. Rajab L, Al-Khatib T, Al-Haj A (2015) A blind DWT-SCHUR based digital video watermarking technique. J Softw Eng Appl 8(04):224 29. Sharma S, Sharma H, Sharma JB (2019) An adaptive color image watermarking using RDWT-SVD and arti cial bee colony based quality metric strength factor optimization. Appl Soft Comput 84(105):696 30. Lin YT, Huang CY, Lee GC (2011) Rotation, scaling, and trans- lation resilient watermarking for images. IET Image Process 5(4):328 340 31. Asikuzzaman M, Alam MJ, Lambert AJ, Pickering MR (2014) Imperceptible and robust blind video watermarking using chromi- nance embedding: a set of approaches in the DT CWT domain. IEEE Trans Inf Forensics Secur 9(9):1502 1517 32. Mittal H, Saraswat M (2020) A new fuzzy cluster validity index for hyper-ellipsoid or hyper-spherical shape close clusters with distant centroids. IEEE Trans Fuzzy Syst 29:3249 3258 33. Pal R, Saraswat M (2017) Data clustering using enhanced biogeography-based optimization. In: 2017 tenth international con- ference on contemporary computing (IC3). IEEE 34. Raju P, Subash Y, Rishabh K et al (2020) EEWC: energy- ef cient weighted clustering method based on genetic algorithm for HWSNS. Complex Intell Syst 6(2):391 400 35. Tripathi AK, Mittal H, Saxena P, Gupta S (2021) A new recommen- dation system using map-reduce-based tournament empowered whale optimization algorithm. Complex Intell Syst 7(1):297 309 123 1070 Complex & Intelligent Systems (2022) 8:1047 1070 36. Rashedi E, Nezamabadi-Pour H, Saryazdi S (2009) GSA: a gravi- tational search algorithm. Inf Sci 179(13):2232 2248 37. Mittal H, Pandey AC, Saraswat M, Kumar S, Pal R, Modwel G (2021) A comprehensive survey of image segmentation: clustering methods, performance parameters, and benchmark datasets. Mul- timed Tools Appl 1 26 38. Mittal H, Pandey AC, Pal R, Tripathi A (2021) A new clustering method for the diagnosis of covid19 using medical images. Appl Intell 51(5):2988 3011 39. Anari B, Torkestani JA, Rahmani AM (2017) Automatic data clustering using continuous action-set learning automata and its application in segmentation of images. Appl Soft Comput 51:253 265 40. Mittal H, Saraswat M (2018) cKGSA based fuzzy clustering method for image segmentation of RGB-D images. In: 2018 eleventh international conference on contemporary computing (IC3). IEEE, pp 1 6 41. Mittal H, Pal R, Kulhari A, Saraswat M (2016) Chaotic kbest gravitational search algorithm (ckgsa). In: 2016 ninth international conference on contemporary computing (IC3). IEEE, pp 1 6 42. Liu J, Xing Y, Ma Y, Li Y (2020) Gravitational search algo- rithm based on multiple adaptive constraint strategy. Computing 102(10):2117 2157 43. Olivas F, Valdez F, Melin P, Sombra A, Castillo O (2019) Interval type-2 fuzzy logic for dynamic parameter adaptation in a modi ed gravitational search algorithm. Inf Sci 476:159 175 44. Mirjalili S, Lewis A (2014) Adaptive gbest-guided gravitational search algorithm. Neural Comput Appl 25(7):1569 1584 45. MittalH,SaraswatM(2018)Anoptimummulti-levelimagethresh- olding segmentation using non-local means 2d histogram and exponential kbest gravitational search algorithm. Eng Appl Artif Intell 71:226 235 46. Wang Y, Yu Y, Gao S, Pan H, Yang G (2019) A hierarchical grav- itational search algorithm with an effective gravitational constant. Swarm Evolut Comput 46:118 139 47. Rawal P, Sharma H, Sharma N (2020) Fast convergent gravita- tional search algorithm. In: Recent trends in communication and intelligent systems. Springer, pp 1 12 48. Mittal H, Saraswat M (2019) An automatic nuclei segmenta- tion method using intelligent gravitational search algorithm based superpixel clustering. Swarm Evolut Comput 45:15 32 49. BhardwajA,VermaVS,JhaRK(2018)Robustvideowatermarking using signi cant frame selection based on coef cient difference of lifting wavelet transform. Multimed Tools Appl 77(15):19659 19678 50. Agilandeeswari L, Ganesan K (2018) RST invariant robust video watermarking algorithm using quaternion curvelet transform. Mul- timed Tools Appl 77(19):25431 25474 51. Zear A, Singh PK (2021) Secure and robust color image dual water- marking based on LWT-DCT-SVD. Multimed Tools Appl 1 18 52. Koley S (2019) A feature adaptive image watermarking framework based on phase congruency and symmetric key cryptography. J King Saud Univ Comput Inf Sci :1 10 53. Bhatnagar G, Wu QJ (2013) Biometrics inspired watermarking based on a fractional dual tree complex wavelet transform. Future Gener Comput Syst 29(1):182 195 54. Wang Z, Bovik AC, Sheikh HR, Simoncelli EP (2004) Image qual- ity assessment: from error visibility to structural similarity. IEEE Trans Image Process 13(4):600 612 55. Thounaojam DM, Khelchandra T, Singh K, Roy S et al (2016) A genetic algorithm and fuzzy logic approach for video shot boundary detection. Comput Intell Neurosci 2016:1 12 56. Sipi (2020) http://www.sipi.usc.edu/database/. Accessed 2 Mar 2020 57. Xiph (2020) https://media.xiph.org/video/derf/. (Accessed on 01/31/2020) Publisher s Note Springer Nature remains neutral with regard to juris- dictional claims in published maps and institutional af liations. 123