IEEE Proof IEEE TRANSACTIONS ON CYBERNETICS 1 A Multiobjective Genetic Programming-Based Ensemble for Simultaneous Feature Selection and Classi cation Kaustuv Nag and Nikhil R. Pal, Fellow, IEEE Abstract We present an integrated algorithm for simultane- 1 ous feature selection (FS) and designing of diverse classi ers 2 using a steady state multiobjective genetic programming (GP), 3 which minimizes three objectives: 1) false positives (FPs); 2) false 4 negatives (FNs); and 3) the number of leaf nodes in the tree. 5 Our method divides a c-class problem into c binary classi ca- 6 tion problems. It evolves c sets of genetic programs to create c 7 ensembles. During mutation operation, our method exploits the 8 tness as well as un tness of features, which dynamically change 9 with generations with a view to using a set of highly relevant 10 features with low redundancy. The classi ers of ith class deter- 11 mine the net belongingness of an unknown data point to the ith 12 class using a weighted voting scheme, which makes use of the 13 FP and FN mistakes made on the training data. We test our 14 method on eight microarray and 11 text data sets with diverse 15 number of classes (from 2 to 44), large number of features 16 (from 2000 to 49 151), and high feature-to-sample ratio (from 17 1.03 to 273.1). We compare our method with a bi-objective GP 18 scheme that does not use any FS and rule size reduction strat- 19 egy. It depicts the effectiveness of the proposed FS and rule size 20 reduction schemes. Furthermore, we compare our method with 21 four classi cation methods in conjunction with six features selec- 22 tion algorithms and full feature set. Our scheme performs the 23 best for 380 out of 474 combinations of data sets, algorithm and 24 FS method. 25 Index Terms Classi cation, ensemble, feature selection (FS), 26 genetic programming (GP). 27 I. INTRODUCTION 28 C LASSIFICATION is one of the most important and 29 frequently encountered problems in data mining and 30 machine learning. A wide range of real world problems of 31 different domains can be restated as classi cation problems. 32 This includes diagnosis from microarray data, text categoriza- 33 tion, medical diagnosis, software quality assurance, and many 34 more. The objective of classi cation is to take an input vector 35 x = (x1, . . . , xd)T and to assign it to one of the K classes Ck, 36 Manuscript received August 7, 2014; revised November 25, 2014 and January 27, 2015; accepted February 5, 2015. This work was supported in part by the Networked Communications Program of Chunghwa Telecom Company, Taiwan, and in part by the INSPIRE Fellowship through the Department of Science and Technology, India, under Grant IF120686. AQ1 This paper was recommended by Associate Editor S. Mostaghim. K. Nag is with the Department of Instrumentation and Electronics Engineering, Jadavpur University, Kolkata-700098, India (e-mail: kaustuv.nag@gmail.com). N. R. Pal is with the Electronics and Communication Sciences Unit, Indian Statistical Institute, Kolkata 700108, India. Color versions of one or more of the gures in this paper are available online at http://ieeexplore.ieee.org. Digital Object Identi er 10.1109/TCYB.2015.2404806 where k = 1, . . . , K. A model, called classi er, is used to 37 solve this problem. The classi er encodes a set of criteria. 38 Depending upon some features of the data point, these crite- 39 ria assign the data point to a particular class [1]. Sometimes, 40 ensembles of weak classi ers are used to obtain better classi- 41 cation accuracy. High dimensionality, high feature-to-sample 42 ratio, and redundant and/or noisy features are some common 43 sources of dif culties associated with classi cation. 44 Feature selection (FS) is a process to select a small but use- 45 ful subset of features from the set of available features which 46 is adequate for solving the problem in an ef cient manner. 47 Some available features may be redundant, not useful, and may 48 cause confusion during the learning phase. These unwanted 49 features needlessly increase the size and complexity of the fea- 50 ture space. It increases the computation cost for learning, and 51 may sometimes be responsible for nding suboptimal solu- 52 tions of the problem. This makes FS techniques important for 53 the analysis of high dimensional data sets, especially when 54 feature-to-sample ratio is extremely high. 55 The microarray technology has made it possible to diagnose 56 different types of cancers directly using the microarray data 57 sets. One of the main dif culties we face to do this is the high 58 feature-to-sample ratio of microarray data sets, which makes 59 FS an important step. Finding keywords as well as contexts 60 from text data is essential to detect (without human interven- 61 tion) the context of web pages, emails, or questions/answers, 62 etc. A large set of distinct words in large texts (high num- 63 ber of features) and many categories of texts (high number of 64 classes) are the two complicated challenges that we face in 65 this task. 66 Genetic programming (GP) [2] [5] is a biological-evolution 67 inspired methodology where each solution is a program or 68 an equation that evolves with one or more objective func- 69 tions to perform speci c tasks. References [6] [12] used GP 70 to design classi ers or to generate rules for binary classi- 71 cation problems. Some researchers have also attempted to 72 solve multiclass problems [13] [17]. GP has also been used 73 for simultaneous FS and classi cation [18]. Liu and Xu [19] 74 used GP to create ensembles of classi ers (genetic programs). 75 They have used these ensembles to classify microarray data 76 sets. Though GP is a powerful tool, it has a drawback: without 77 special care each genetic program (equation) becomes huge. 78 As an effect, they do not learn the patterns in the training 79 data. Rather, memorizes them. It also makes genetic programs 80 to be dif cult to comprehend. Besides, though ensembles can 81 2168-2267 c 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. IEEE Proof 2 IEEE TRANSACTIONS ON CYBERNETICS perform better than individual classi ers [20], to obtain better 82 performance, each ensemble should be diverse and each mem- 83 ber of the ensemble should be accurate [20] [22]. Without 84 special care, due to lack of explicit diversity preservation 85 mechanism, the solutions of single objective GP may lose AQ2 86 diversity. 87 The objective of this paper is to nd an embedded method- 88 ology of simultaneous FS and classi cation employing GP as a 89 tool. Some of the novel contributions of our schemes are as fol- 90 lows. We have introduced a new multiobjective GP (MOGP), 91 called archive-based steady state micro GP (ASMiGP). It 92 is enriched by several new operators. The mating selec- 93 tion judicially uses Roulette wheel selection, instead of two 94 tire multiobjective selection scheme (where domination is 95 preferred over diversity). The crossover is new and uses male 96 female differentiation so that the off-spring is more likely to be AQ3 97 close to the female parent in the genotypic space. The mutation 98 is restrictive and performs less exploration in the hypothesis 99 space. Thus, it reduces disruption. For feature nodes, instead 100 of tness, mutation uses un tness to select the mutation point. 101 Altering the tness and un tness of the features in different 102 stages of learning, we change the objective of searching in the 103 corresponding stages. We use ASMiGP to learn c diverse sets 104 of classi ers (equations) minimizing three objectives: 1) false 105 positive (FP); 2) false negative (FN); and 3) number of leaf 106 nodes of a tree to restrict the rule size. Throughout the learning 107 process, implicit FS is performed using MOGP, whereas sev- 108 eral lter-based approaches are used in different stages of the 109 procedure. In this way, we obtain concise rules that involve 110 only simple arithmetic operations. A weighted negative vot- 111 ing is then used among the rules of each ensemble. These 112 weights are determined on the basis of the performance of 113 the binary classi ers on the training data set. A new measure 114 of weighted negative voting, called net belongingness, is also 115 introduced. 116 The proposed method has been tested on eight multiclass 117 microarray data sets having large number of features, varying 118 from 2000 to 49 151, and high feature-to-sample ratio, vary- 119 ing from 32.26 to 273.06. It has also been tested on eleven 120 high dimensional (varying from 3182 to 26 832) text data sets, 121 where the number of classes (categories) vary from 6 to 44. 122 Experimental results reveal that our method can generate 123 ensembles of classi ers with concise rules that can do a good 124 job of classi cation with a small subset of features. 125 II. BACKGROUND AND RELATED WORKS 126 This section provides a background to GP, GP-based online 127 FS and classi cation, ensembles, voting schemes, and concise 128 rule nding. This section also includes some related works on 129 these topics. 130 A. Concise Introduction to GP 131 GP [2] [5] is an approach to nd programs that can solve 132 a given problem. GP uses Darwinian principle of survival of 133 the ttest. It evolves the programs using biologically inspired 134 genetic operations like reproduction, crossover, and mutation 135 in the search space to nd such programs which would be able 136 to solve the given problem [15], [23], [24]. The overall search 137 process of traditional single objective GP is quite similar to 138 traditional population-based genetic algorithm. The main dif- 139 ference among them is in the representation of solutions. The 140 usual steps of traditional single objective GP can be found 141 in [2], [15], and [24]. 142 There exist several ways to encode a program. Probably, the 143 most popular one is to use a tree structure [25] [27] which 144 we use. In tree-based GP, a program is represented by a 145 tree where the internal nodes are from a set of functions F, 146 and the leaf nodes are from a set of terminals T . The sub- 147 trees of a function node are the arguments of that function. 148 The sets F and T must satisfy the closure and suf ciency 149 properties [2], [15]. To satisfy the closure property, F must 150 be well de ned and closed for any combination of probable 151 arguments that it may encounter [15]. Again, to satisfy the 152 suf ciency property, F and T must be able to represent any 153 possible valid solution of the problem. 154 B. FS: Why Embedded? 155 FS methods are generally divided into two main groups: 156 1) lter method and 2) wrapper method [28]. A lter method 157 does not use any feedback from the classi er or any mining 158 algorithm. It relies on the general characteristics of data. On 159 the contrary, to measure the goodness of features, a wrapper 160 method uses a predetermined classi er or mining algorithm, 161 which will nally use the selected features. Consequently, a 162 wrapper method exploits interactions among the subset of fea- 163 tures on which it is tested. But, to nd an optimal set of 164 features, a wrapper method needs to measure performances 165 on all possible subsets of features. This becomes infeasible 166 for high dimensional data sets. To overcome this problem, 167 wrapper methods typically use a heuristic-based forward or 168 backward selection mechanism [28], which does not evaluate 169 all possible subsets. So, in this paper, we consider embed- 170 ded methods, where FS and designing of the classi cation 171 system are done together. Embedded methods do not need 172 to evaluate all possible subsets. Moreover, they can account 173 for interaction between features and the classi er that is used 174 to solve the problem [29]. Usually, embedded methods attain 175 comparable accuracy to wrapper methods as well as compara- 176 ble ef ciency to lter methods. Though for every classi cation 177 tool it may not be easy to de ne such an integrated mecha- 178 nism, several attempts of FS using embedded methods have 179 already been made. These attempts include using single objec- 180 tive GP [18], neural networks [30], [31], and support vector 181 machines [32]. 182 C. GP in Classi cation and FS 183 Many researchers have used GP as a tool for classi - 184 cation and FS. Some literature on this topic can be found 185 in [1] and [33]. It has been used in both lter [34] [37] 186 and wrapper [38] [41] approaches. GP has also been 187 used for extracting decision trees [42] [47]. Among these 188 in [45] and [46], MOGP has been used. GP has also been 189 adopted for learning rule-based systems [7] [9], [48] [51]. 190 Both binary classi cation [6], [7], [10] [12], [48] and 191 IEEE Proof NAG AND PAL: MOGP-BASED ENSEMBLE FOR SIMULTANEOUS FS AND CLASSIFICATION 3 multiclass [8], [9], [13] [17], [49] [51] classi cation prob- 192 lems have been addressed by GP-based (rule-based) 193 systems. Even, researchers have applied GP-based 194 (rule-based) system for binary classi cation of imbal- 195 anced data [52], [53]. Discriminant functions (DFs)-based 196 GP for online FS and classi cation has been adopted 197 in [18] to solve multiclass problems. It is noteworthy 198 that GP has also been used in feature extraction for edge 199 detection [54]. 200 Since, in this paper, we have used MOGP for learning DFs, 201 we discuss some relevant GP-based systems. In DFs-based GP, 202 each program is a mathematical equation, where the variables 203 are features of the data. Usually every program is associated 204 with a class. For every input data point, the program converts 205 it to a single output value. If this output value is more than 206 a prede ned threshold (usually zero), the point belongs to the 207 class to which the program is associated with. Thus, a sin- 208 gle equation is enough for binary classi cation problems. For 209 c-class problems, there are two common ways. The rst and 210 more frequently used approach is to consider a c-class problem 211 as c binary classi cation problems [1]. Thus, c number of DFs 212 are used to discriminate c classes. The second and less popular 213 approach is to use a single DF with (c 1) threshold values to 214 create c intervals. Each of these intervals is then assigned to a 215 particular class. For both categories, a common practice is to 216 evolve a population where each individual encodes either one 217 (for binary classi cation problems having single threshold, or 218 for multiclass problems having multiple thresholds) or multi- 219 ple DFs each of which uses a single threshold. Such encoding 220 schemes have been used in [15] and [18]. In another practice, 221 each solution encodes multiple DFs having a single threshold. 222 The nal output value in this case may be obtained using a 223 (weighted) voting scheme among the output values of each 224 function of the same individual. An example of this scheme 225 can be found in [19]. 226 In the recent years, researchers have made some notable 227 attempts to solve classi cation problems using MOGP. 228 Wang et al. [55] proposed a MOGP to obtain a group of 229 nondominated classi ers, with which the maximum receiver 230 operating characteristic convex hull (ROCCH) can be obtained. 231 To achieve this, they have adopted four different multiobjec- 232 tive frameworks into GP. For further improvement of each 233 individual GPs performance, they have de ned two local 234 search strategies that have been especially designed for clas- 235 si cation problems. The experimental results in [55] have 236 demonstrated the ef cacy of the proposed memetic scheme in 237 their MOGP framework. Another convex hull-based MOGP, 238 called CH-MOGP, can be found in [56]. Wang et al. [56] 239 showed the differences between the conventional multiobjec- 240 tive optimization problem and ROCCH maximization problem. 241 Wang et al. introduced a convex hull-based sorting without 242 redundancy and a new selection procedure which are suitable 243 for ROCCH maximization problem. Again, Bhowan et al. [22] 244 proposed a MOGP-based approach that is especially designed 245 for unbalanced data sets. This approach [22] evolves diverse 246 and accurate ensembles of GP classi ers with good per- 247 formance on both the majority and the minority classes. 248 The individual members of the evolved ensembles, that 249 are composed of nondominated solutions, vote on class 250 membership. 251 D. GP, Bloating, and Concise Rules 252 During evolution of GP, variable length genomes gradually 253 start to increase its length without signi cant improvement in 254 tness. This incident, called bloating [57], is a well known 255 phenomenon in GP. Bloating causes genetic programs: 1) to 256 keep reducible genome structures and 2) to memorize training 257 data points, rather than recognizing patterns hidden in them. 258 To nd rules which are to some extent human interpretable and 259 can be analyzed, each of the genetic programs must be concise. 260 A plausible way to achieve this target is to control bloat- 261 ing. A popular way to handle bloating is to take into account 262 the program size [2], [58] [61]. Some other methods include 263 spatially-structured populations [62], [63], island-based mod- 264 els to introduce spatial structure to the population [62], [63], 265 intron deletion [64], and dynamic population sizing [65], [66]. 266 Song et al. [67] attempted to understand the GP evolved solu- 267 tions; while Luna et al. [68] attempted to nd comprehensible 268 rules in subgroup discovery. 269 E. Ensemble Classi er 270 In classi cation, the basic task is to search through a 271 hypothesis space to nd a suitable hypothesis that will 272 be able to classify the data points in a better way. An 273 ensemble combines multiple hypotheses to form a better 274 one [19], [21], [22], [53], [69], [70]. Empirically, an ensem- 275 ble performs better when each of the classi ers is highly 276 accurate (strong learner) and the members of the ensemble are 277 signi cantly diverse. The explanation behind the better perfor- 278 mance of ensemble classi ers than a single classi er has been 279 described in [20]. Normally, to decide the class label of a data 280 point, the member classi ers of an ensemble use (weighted) 281 voting. Ensembles are often used in bio-informatics [71]. 282 III. PROPOSED WORK 283 A. Representation of Classi ers or Solutions 284 In this paper, we evolve c-populations of genetic programs. 285 Each individual of these populations is a binary classi er. Each 286 binary classi er is represented by a single tree. When a data 287 point is passed through an individual of the ith population, 288 if the output value is positive, the individual says that the 289 passed data point belongs to the ith class; otherwise it says that 290 the point does not belong to that class. The internal (nonleaf) 291 nodes of these trees are functions F. The terminal nodes must 292 be either a feature or a constant from the set C. We have 293 imposed some constraint on the minimum size (architecture) 294 of the trees. In each tree there must be at least one function 295 node and two terminal nodes (with at least one feature node). 296 Though, a single feature (terminal) node might be enough to 297 determine the class label, this would rarely happen in practice. 298 The above restrictions make the trees more useful without any 299 loss of generalization capability. Again, after generation of any 300 tree throughout the learning phase (using mutation, crossover, 301 or random initialization), the largest subtree consisting of only 302 IEEE Proof 4 IEEE TRANSACTIONS ON CYBERNETICS constants as its leaf nodes is replaced by a constant leaf node 303 having the equivalent constant value. 304 B. MOGP for Learning 305 Larger genetic programs may memorize the training patterns 306 which, in turn, may increase the training accuracy and reduce 307 the understandability of the rules. Therefore, we aim to nd 308 smaller but accurate classi ers. Again, when c is high enough, 309 even a balanced c-class data set may get converted to c num- 310 ber of highly imbalanced bi-classi cation data sets. Instead of 311 minimization of classi cation error, simultaneous minimiza- 312 tion of FP and FN would be more appropriate in this regard. 313 Suppose a classi er makes some mistakes m = mp + mn on a 314 given training data set, where mp and mn are, respectively, the 315 number of FPs and FNs. Consider two classi ers, each making 316 the same number of mistakes m on a given data set. Let for the 317 rst classi er mp = mn and for the second classi er mp mn 318 or mp mn. If the cost of an FP is the same as that of a FN, 319 we would prefer the rst classi er. Consequently, minimization 320 of both FP and FN would be a better strategy than minimiza- 321 tion of the total number of misclassi cation, particularly for 322 imbalanced data sets. Thus, we have three objectives, i.e., min- 323 imizations of: 1) FP; 2) FN; and 3) rule size. Moreover, when 324 different classes are overlapped, minimization of FP is usu- 325 ally in con ict with the minimization of FN and vice versa. 326 Multiobjective optimization is more suitable when we need to 327 optimize more than one con icting objective. Therefore, dur- 328 ing the learning of each binary classi er, we minimize three 329 objectives using an MOGP: 1) FP; 2) FN; and 3) number of 330 leaf nodes in the tree. The third objective is used to reduce 331 the size of the tree which enhances the understandability and 332 reduces the pattern memorization capability. The algorithm, 333 proposed in this paper, is called ASMiGP, which is presented 334 in Algorithm 1. 335 In an evolutionary search, it is desired to have as many gen- 336 erations as possible and steady state nature of an algorithm 337 maximizes the number of generations when the number of 338 function evaluations is xed [72]. Due to maximization of gen- 339 erations, a steady state evolutionary search is more exploitative 340 to enhance the searching in a region which is more likely to 341 have or closer to the Pareto front and avoiding exploration in 342 regions that are less likely to improve the solutions. It causes 343 faster convergence. In other words, independent of the tness 344 evaluation process, steady state selection is more performant 345 than discrete generational selection [73]. Therefore, we have 346 used a steady state algorithm instead of a generational one. 347 C. FS 348 In this paper, we have used the embedded model of FS. 349 Explicit FS is performed during population initialization and 350 mutation. Implicit FS is performed during crossover. Filtering 351 is also performed at three different stages of the learning pro- 352 cess; in particular, at the beginning and after 50% and after 353 75% of evaluations as described next in this subsection. 354 To facilitate the FS, following [74], we de ne an index that 355 assesses the discriminating ability of a feature. Consider a 356 two-class problem. Note that for a multiclass problem, the 357 Algorithm 1: ASMiGP 1 Initialize population using ramped-half-and-half method. 2 Initialize the archive solutions using initial solutions. 3 while EvaluationsCurrent EvaluationsMaximum do 4 repeat 5 operator = Select crossover or mutation. 6 if operator = = crossover then 7 Select male and female parents (mating selection). 8 Perform crossover. 9 end 10 else 11 Select an individual (mating selection). 12 Mutate the individual. 13 end 14 until the in x equation of off-spring is distinct from in x equation of any individual of the archive 15 Evaluate the new off-spring. 16 EvaluationsCurrent = EvaluationsCurrent + 1 17 Update the archive using new offspring (multi-objective environmental selection). 18 end 19 Fronts = Perform fast-non-dominated-sort. 20 return rst front of Fronts. one-versus-all case can also be viewed as a two-class prob- 358 lem. Let there be np number of training points and the class 359 label for the jth data point be +1 if it belongs to class 1 and 1 360 if it belongs to class 2. Let the value of the fth feature for the 361 ith data point be fi; i = 1, . . . , np. If the fth feature is a good 362 discriminatory feature then for all data points from class 1, 363 it should have high values and for all points from class 2, it 364 should have low values or vice-versa. Hence for a good dis- 365 criminatory feature, we can de ne an ideal feature vector with 366 values 1, if the data point is from class 1 and 0 otherwise; or 367 the feature value is 0, if it is from class 1, otherwise, it is 0. 368 Let Sf be the vector containing the ideal feature values for fea- 369 ture f and s be the ideal feature value of the fth feature for 370 the ith sample. Note that, there could other important features 371 that are not linearly related to the class structure. We are not 372 considering them in this preliminary ltering step. As in [74], 373 we compute the Pearson s correlation (or any other measure of 374 similarity) between f and s as a measure of feature relevance 375 Cf = np  j=1  sji s  fj f      np  j=1  sji s 2 np  j=1  fj f 2. 376 (1) 377 A higher value of |Cf | indicates a stronger discriminative 378 power of feature f. For a multiclass problem, using the one- 379 versus-all strategy, for the ith binary classi cation problem 380 (class i versus the rest), the correlation for the fth feature is 381 denoted by Ci f . 382 Now, we describe the FS procedure. Let the set of all fea- 383 tures be Fall. We intend to incorporate only those features 384 from Fall which are more likely to help the classi ers to 385 IEEE Proof NAG AND PAL: MOGP-BASED ENSEMBLE FOR SIMULTANEOUS FS AND CLASSIFICATION 5 decide the class label. We assign different tness and un t- 386 ness measures to the features during the learning phase. To 387 remove a feature node from any tree (during mutation), we 388 select it using Roulette wheel selection on the un tness val- 389 ues of the features which are present in that tree. Similarly, 390 when a new feature is inserted in the tree, it is selected using 391 Roulette wheel selection on the tness values. During the 392 rst 50% evaluations, the tness and un tness features are 393 de ned as 394 F0%,i tness( f, i) = Ci f Cimax 2 , if Ci f  Cimax > 0.3 0, otherwise (2) 395 F0%,i un tness( f, i) = 1.0 F0%,i tness (3) 396 where Ci max = max f Fall |Ci f |. Equation (2) sets the tness of 397 very poor (poor with respect to its discriminating power) to 398 zero to eliminate their impact during the initial evolution. Let 399 Feval=0%,i Fall be the features with nonzero tness values. 400 Basically, at this stage we are using a lter on the feature set 401 Fall to obtain a smaller feature set. 402 After completion of 50% evaluations for each population, 403 we nd the features used in that population. Let the feature 404 set be Feval=50%,i. Then we make the tness of all features in 405 Fall Feval=50%,i to zero. This is done with the assumption 406 that after 50% evaluations useful features have been used by 407 the collection of decision trees. Now, the tness and un tness 408 values of all features in Feval=50%,i are modi ed according 409 to (4) and (5), respectively 410 F50%,i tness( f, i) = |Ci f |  f =g g F50%,i | fg|, if f Feval=50%,i 0, otherwise (4) 411 F50%,i un tness( f, i) = e F50%,i tness( f,i) minf  F50%,i tness  maxf  F50%,i tness  minf  F50%,i tness  (5) 412 where fg is the Pearson s correlation between fth and gth 413 features. Here, we try to select features with higher relevance 414 but reducing the redundancy in the set of selected features. 415 The tness, de ned in (4), increases when the feature is highly 416 correlated with class label. Similarly, it reduces when it is more 417 correlated with other existent features. This is done to achieve 418 maximum relevance minimum redundancy. 419 After 75% function evaluations, again we take another 420 snapshot of the population. Let the existent features for the 421 population be Feval=75%,i Feval=50%,i. Then, the tness 422 and un tness values of the features in Feval=75%,i are de ned 423 in (6) and (7), respectively 424 F75%,i tness( f, i) =  F0% tness( f, i), if f Feval=75%,i 0, otherwise (6) 425 F75%,i un tness( f, i) = 1.0 F75%,i tness( f, i). (7) 426 D. Population and Archive Initialization 427 We initialize each population using ramped-half-and-half 428 method. While constructing the random trees, selection of ter- 429 minal nodes has been made with a probability pvar. To insert a 430 terminal node in a tree, a random number rn is drawn in [0, 1]. 431 If rn < pvar then a feature node is added, otherwise a con- 432 stant node is added. The function nodes are chosen from the 433 set F, with equal probability of inclusion for all functions. The 434 feature nodes are selected using Roulette wheel selection on 435 tness F0% tness, de ned in (2). 436 To initialize the archive from the initial population, we have 437 used the multiobjective archive initialization scheme present 438 in [72] and [75]. It requires two parameters: 1) maximum 439 archive size (Nmax) and 2) minimum archive size (Nmin). 440 E. Selection of Crossover or Mutation 441 Since ASMiGP is a steady state MOGP, in each generation 442 we generate only one offspring. We use either crossover or 443 mutation to do that. A random number rc is drawn in [0, 1]. 444 if rc < pc then crossover operator is selected otherwise the 445 mutation operator is selected for that generation. 446 F. Mating Selection 447 ASMiGP uses crossover with male and female differenti- 448 ation which needs one male and female parent. We perform 449 Roulette wheel selection using classi cation accuracy of the 450 binary classi ers as tness to select the female parent. Then 451 the male parent is selected randomly from the remaining 452 archive. The only condition to be satis ed is that the male 453 and female parents must be distinct. For mutation, we need 454 only one individual and it is selected in the same way as done 455 for the female parent in case of crossover operator. 456 A choice of mating selection could have been the use 457 of some bi-level selection operator, where Pareto dominance 458 is preferred over diversity. In that case, solutions along the 459 whole Pareto optimal solution set with good diversity would 460 have been selected as the primary (female) parents. Note that, 461 we have used crossover with male female differentiation that 462 tries to generate an off-spring near the primary parent in the 463 hypothesis space. Consequently, this might cause generation of 464 Pareto optimal binary classi ers along the whole Pareto opti- 465 mal solution set. Though they are Pareto optimal, these binary 466 classi ers may have poor accuracies. This is because of the 467 implicit imbalance nature of the binary classi cation problems 468 (due to conversion from multiclass classi cation problems) 469 and different classi er sizes. An ensemble classi er, however, 470 performs better when individual members of the ensemble 471 are more accurate. Therefore, we use classi cation accuracy- 472 based mating selection. It guides the search to generate more 473 accurate binary classi ers. 474 G. Crossover 475 In this paper, we have used crossover operation with male 476 and female differentiation. We want the off-spring to be near 477 the female (acceptor) parent in the hypothesis space. The 478 male (donor) parent is used to make the off-spring diverse 479 IEEE Proof 6 IEEE TRANSACTIONS ON CYBERNETICS from its mother. To do this, two random points (nodes) are 480 selected from each of the parents. The probabilities of selec- 481 tion of terminal nodes and function nodes as a crossover point 482 (node) are, respectively, pc t and (1 pc t ). Then, the subtree 483 rooted at the selected node of the mother tree is replaced with 484 a similarly selected subtree from the father tree. If the off- 485 spring is identical to any of its parents, the whole procedure 486 is repeated (before evaluation/learning of the off-spring). 487 H. Mutation 488 In most of the GP-based systems, for mutation a subtree 489 rooted at a randomly selected node is replaced by a new 490 randomly generated subtree. Though this kind of mutation 491 explores more in the hypothesis space, it may be too disrup- 492 tive in nature. Therefore, we intend to use less exploration 493 by keeping the tree structure unaltered. During mutation we 494 perform the following operations on a tree. 495 1) Each constant of the tree is replaced by another random 496 constant with probability pm c . 497 2) Each function node of the tree is replaced by anther 498 random function node with probability pm f . 499 3) Only one feature node of the tree is replaced by another 500 feature node. 501 For feature nodes, to select the mutation point Roulette 502 wheel selection is performed on the un tness of the features 503 which are present in the tree. Similarly, to insert a new feature 504 at the mutation point, we select a feature using Roulette wheel 505 selection based on the tness values (probability proportional 506 to tness) of the features. 507 This restricted mutation scheme ensures that the tree struc- 508 ture of an equation remains the same. It also ensures that the 509 variables in an equation do not change drastically changing 510 more than one variable would shift the solution (equation) in 511 the hypothesis space by a larger amount. 512 I. Environmental Selection 513 ASMiGP uses the archive truncation strategy used 514 in [72] and [75]. This multiobjective archive truncation strat- 515 egy uses Pareto-based tness function, i.e., tness is Pareto 516 dominance rank. This scheme maintains an archive (ensemble) 517 which has an adaptive and dynamic size. It does not allow the 518 archive to fall below a minimum size ensuring diversity in the 519 genotypic space. Moreover, the environmental selection dimin- 520 ishes the exploration in areas of objective space that are less 521 likely to yield improved solutions [72], ensuring diversity in 522 phenotypic space. Furthermore, we have made the following 523 difference in the environmental selection. ASMiGP ensures 524 that the in x expression of every off-spring (after mutation or 525 crossover) is distinct from every member of the archive. To 526 achieve this, before evaluating the offspring, ASMiGP con- 527 verts it to its in x expression and then compares the expression 528 with that of each individual of the archive. Only if the expres- 529 sion is unique, the off-spring is evaluated and added to the 530 archive. Otherwise it is discarded and a new off-spring is gen- 531 erated. Another noticeable difference is that here the number of 532 objectives is three. Note that the diversity maintenance in each 533 ensemble both in phenotypic space and in genotypic space 534 nds a diverse set of trees (bi-classi ers). Trees, being diverse, 535 enhance the performance of the corresponding ensemble. In 536 this context, it is worth mentioning that the archive, along 537 with the archive truncation strategy, helps to realize a good 538 Pareto front by explicitly maintaining diversity among the t 539 (according to rank) solutions. However, the archive alone is not 540 suf cient to evolve a good Pareto front along all the objectives. 541 J. Decision Making 542 To determine the class label, we nd the net belongingness 543 of a point to each class. The net belongingness lies in [0, 1]. 544 A higher value indicates more net belongingness to that class. 545 A data point is assigned to that class for which it has the 546 highest net belongingness. 547 After the learning, we obtain a set of c archives, A = 548 {A1, A2, . . . , Ac}; i, 1 |Ai| Nmax, where c is the num- 549 ber of classes, and Ai is the set of all binary classi ers for the 550 ith class. To determine the net belongingness of a data point p 551 to class m, Bnet m (p), it is passed through all genetic programs 552 of set Am. The net belongingness, Bnet m (p), of the point p for 553 class m is computed using 554 Bnet m (p) = 1 2 1 |Am| |Am|  i=1 Bi m(p) + 1.0 (8) 555 where Bi m is de ned as 556 Bi m(p) = +  1.0 FPi m FPmax m  , if Ai m(p) > 0  1.0 FNi m FNmax m  , otherwise. (9) 557 In (9), FPi m and FNi m, respectively, represent the number 558 of FPs and FNs made by the ith individual in set Am on the 559 training data. FPmax m and FNmax m are, respectively, the maximum 560 possible FP and the maximum possible FN for the mth class; 561 and Ai m(p) is the output from ith individual of Am for input 562 data point p. Finally, p is assigned the class k, when Bnet k = 563 c max m=1{Bnet m }. Note that FPmax m and FPmin m are determined by the 564 training data. 565 The concept of net belongingness is inspired by the con- 566 cept of negative voting. Negative voting has been widely used 567 in diverse applications [76] [78]. It is more effective when 568 circumstances unfavorable to the preferences invoke stronger 569 electoral responses than the similar favorable responses, as 570 well as the behaviors of the voters are well de ned [79]. In 571 our scheme, the learners for the ith class learn to vote yes 572 for the points of the ith class and no for the points which 573 do not belong to the ith class. For multiclass problems, it 574 is more likely that a binary classi er learns to say no than 575 to say yes for a much higher number of points. Therefore, 576 we found negative voting to be more suitable in this context. 577 However, we have used a weighted negative voting scheme. 578 The accuracies for the responses yes and no of the ith binary 579 classi er of mth class are, respectively,  1.0 FPi m/FPmax m  580 and  1.0 FNi m/FNmax m  . These values have been used as cor- 581 responding weights for the responses yes and no of the ith 582 binary classi er of the mth class. We have used the positive 583 IEEE Proof NAG AND PAL: MOGP-BASED ENSEMBLE FOR SIMULTANEOUS FS AND CLASSIFICATION 7 TABLE I PARAMETER SETTINGS FOR THE PROPOSED METHOD TABLE II SUMMARY OF MICROARRAY DATA SETS and the negative signs to indicate acceptance and rejection of 584 the data point for the mth class, respectively. 585 IV. EXPERIMENTATION 586 A. Experimental Settings 587 We have repeated tenfold cross validation of the proposed 588 method for ten times. Table I shows the parameter settings 589 that we have used for this purpose. The training data is 590 Z-score normalized. Furthermore, based on the means and the 591 standard deviations of the features of the training data, the 592 test data was Z-score normalized. Note that except Nmax and 593 Nmin, all parameters are standard parameters used in any GP 594 simulations, while Nmax and Nmin are needed for archive main- 595 tenance. Although all the parameters can be chosen using a 596 cross validation mechanism, because of huge computational 597 overhead, we could not do that. Based on a few experi- 598 ments we selected these parameters. These choices are not 599 optimal. However, since the same set of values are used for 600 widely different types of data sets, it demonstrates the effec- 601 tiveness of the proposed scheme. The proposed method has 602 been implemented in Java with the help of jMetal [80], [81]. 603 We have used eight microarray and eleven text data sets 604 which are summarized in Tables II and III, respectively. 605 B. Importance of FS and Rule Size Reduction 606 To demonstrate the importance of the proposed FS and rule 607 size reduction scheme, we have compared our method with 608 GP without FS and no restriction to equation size. To do that, 609 we have made the following changes in the proposed method. 610 1) No explicit FS as described in Section III-C is done. 611 2) There are only two objectives, FP and FN. 612 TABLE III SUMMARY OF TEXT DATA SETS All other parts of the algorithm and the parameter values 613 remain unchanged. Similar to the proposed scheme, we have 614 also executed tenfold cross validation for ten times for all the 615 data sets. Along with results (mean values of the correspond- 616 ing ten runs) obtained using the proposed method, the results 617 obtained using this scheme for microarray and for text data 618 sets are, respectively, summarized in Tables IV and V. From 619 these two tables, we observe the following. 620 1) In all cases, the test accuracy is much higher for the pro- 621 posed method. Especially, for GCM having 14 classes, 622 the difference between the test accuracies are remarkably 623 high. 624 2) The tree size increases signi cantly when the third 625 objective, i.e., the restriction on rule size, is not used. 626 3) For most of the data sets the proposed method selects 627 smaller number of distinct features per tree as well as 628 for per classi er. 629 These observations clearly demonstrate the importance of 630 the FS as well as constraining the rule size. Since our FS 631 scheme discards features with poor relevance and uses features 632 with good discriminating power yet avoiding use of redundant 633 features, it not only makes the discovery of useful rules eas- 634 ier but also implicitly constrains the rule length. Thus, FS 635 plays a very important role having two positive impacts: it 636 makes identi cation of useful rules easier and it promotes the 637 minimization of the third objective. 638 C. Comparing With Other Methods 639 To compare the performance of the proposed method 640 we have used the experimental results reported in [82] (see 641 [82, Tables IV VII]). Song et al. [82] used four different 642 types of classi cation algorithms: 1) probability-based naive 643 Bayes (NB); 2) tree based C4.5; 3) instance-based lazy learn- 644 ing algorithm IB1; and 4) rule-based RIPPER both before and 645 after FS. Along with the full feature set, they have used six 646 FS algorithms in their experiment. 647 1) FAST [82]. 648 2) FCBF [83], [84]. 649 3) CFS [85]. 650 4) ReliefF [86]. 651 5) Consist [87]. 652 6) FOCUS-SF [88]. 653 IEEE Proof 8 IEEE TRANSACTIONS ON CYBERNETICS TABLE IV EXPERIMENTAL RESULTS ON MICROARRAY DATA SETS (MEAN VALUES OF TEN RUNS OF TENFOLD CROSS VALIDATION) TABLE V EXPERIMENTAL RESULTS ON TEXT DATA SETS (MEAN VALUES OF TEN RUNS OF TENFOLD CROSS VALIDATION) Use of all features can be viewed as the seventh FS algo- 654 rithm. To make this paper comprehensive, we are not dis- 655 cussing the experimental settings used in [82]. Note that 656 accuracies for few data sets for few pairs of FS schemes are 657 not available in [82]. 658 1) Results With Microarray Data Sets: Table IV presents 659 the results of the proposed method on microarray data sets. 660 We have already stated that for each classi er, Song et al. [82] 661 used seven FS schemes (six FS method as well as the set of 662 all features). For each FS method ve repetitions of the ten- 663 fold cross validation experiment were done in [82]. And then, 664 for each FS method, the average accuracy over the ve rep- 665 etitions is reported. We compare this average accuracy with 666 the average accuracy that we have obtained by our method 667 over the ten repetitions of the tenfold cross validation experi- 668 ments. In particular, we count the number of cases (each case 669 refers to one FS scheme) in which our algorithm outperforms. 670 Note that, for some combination of data set and classi er, the 671 total number of cases is less than seven as for those combi- 672 nations some results are not available. Table VI reports these 673 counts. To elaborate Table VI, consider the entry correspond- 674 ing to column IB1 and row CLL-SUB-111. For the data set 675 CLL-SUB-111, in [82, Table VI], Song et al. reported the per- 676 formance of the algorithm IB1 for six different FS algorithms. 677 Our algorithm is found to perform better than ve of the six 678 FS algorithms and hence the entry for row CLL-SUB-111 and 679 column IB1 is 5/6. Note that, for this data set, Song et al. [82] 680 did not report performance of IB1 using all features. All but 681 the entries in the last column of Table VI are generated in 682 TABLE VI COMPARISON WITH NB, C4.5, IB1, AND RIPPER ON MICROARRAY DATA SETS the same manner. The last column of Table VI, which reports 683 the row total, reveals that our method performs the best for 684 61.40% (132 out of 215) test cases. 685 If we compute the percentage of features selected by the 686 ensembles, it may not be very small for some data sets, like 687 Colon. But, if we consider the number of features selected per 688 binary classi er (tree), we nd that this number is quite small, 689 e.g., the maximum value is 0.16% for Colon. We assume that 690 binary classi ers (binary trees) having less than twenty nodes 691 are concise enough. We need to remember that we are talking 692 about raw rules (equations) directly obtained from GP which 693 are most likely affected by bloating. Simpli cation of these 694 rules may lead to reduction in their sizes. Based on this, in 695 all but one data set we could nd easy to analyze rules. For 696 SMK-CAN-187 the extracted rules are more complex possibly 697 because of complex structure of the data. 698 IEEE Proof NAG AND PAL: MOGP-BASED ENSEMBLE FOR SIMULTANEOUS FS AND CLASSIFICATION 9 Fig. 1. Showing the number of distinct features present in the archive with respect to function evaluations for GLA-BRA-180 data set. In Table S-I (see supplementary materials), we present the 699 best rules (or binary classi ers or GPs) we found for each class 700 of each microarray data set. These GPs have the highest train- 701 ing accuracy among the GPs obtained from the rst fold of the 702 rst ten fold cross validation. If there are more than one rule 703 having the same maximum training accuracy, the rule with the 704 smallest length has been reported. The features in the equa- 705 tions are indexed starting from 0. From Table S-I, we can 706 observe that for several classes the proposed method could nd 707 substantially small rules. Noticeably, for four, six, eight, and 708 three classes, the proposed method could nd the best binary 709 classi er having only 1 4 distinct variables, respectively. So, 710 21 out of 33 (i.e., 63.64%) cases, we could nd considerably 711 small (and hence easy to interpret) rules. For some classes, 712 the rules having the highest training accuracy were not very 713 comprehensible because the length of the rule was not very 714 small. If we think that (2.0 xi) is more easy to understand 715 than (xi + xi), though both equations have the same size, we 716 observe that for 21 out of 33 (i.e., 63.64%) cases the proposed 717 method could nd rules having highest accuracy, which are 718 not at all affected by bloating and are comprehensible without 719 simpli cation. 720 To investigate how the number of distinct features changes 721 in the archive, we have executed our algorithm with 400 000 722 function evaluations using the entire GLA-BRA-180 data set. 723 In Fig. 1, we have plotted the number of distinct features 724 after initial and after each 12.5% of maximum function eval- 725 uations. For all four classes of GLA-BRA-180 data set, the 726 number of distinct features in the archive initially falls dras- 727 tically depicting the impact of FS. After some generations, it 728 becomes almost constant. Moreover, the number of nal dis- 729 tinct features for each class is quite small. This indeed reveals 730 a desirable behavior of the our scheme. 731 To show how the individual binary classi ers accuracies 732 change with function evolutions, we have plotted each indi- 733 viduals FPs and FNs for all four classes of GLA-BRA-180 734 data set in Figs. S-1 and S-2 (see supplementary materials). For 735 this part of the experiment also, we have used the entire data 736 TABLE VII COMPARISON WITH NB, C4.5, IB1, AND RIPPER ON TEXT DATA SETS set for training. From the gures, it is observed that with the 737 increase in number of evolutions, the average accuracy of the 738 solutions tends to increase. However, for class four, some solu- 739 tions having lower level of accuracies (having higher FPs and 740 FNs) are there even after 400 000 function evaluations. This is 741 because, even after such high number of function evaluations, 742 the algorithm was still searching for more concise solutions, 743 and could nd some trees of comparatively smaller size. 744 2) Results With Text Data Sets: Similar to Table VI with 745 microarray data sets, we present the same result (number of 746 test cases for which our algorithm provides the best results 747 for each classi er and data set pair) with text data sets in 748 Table VII. Table VII reveals that for text data sets the proposed 749 scheme performs the best for 95.75% (248 out of 259) cases. 750 If we consider the same criteria that binary classi ers (trees) 751 having less than twenty nodes are concise enough, then our 752 method could not nd interpretable rules for 5 (oh10.wc, 753 ohoscal.wc, la2s.wc, la1s.wc, and new3s.wc) out of the 11 754 text data sets. The number of selected features is not at all 755 small for most of the text data sets. Some reasons behind 756 this may be that the number of classes is high for the text 757 data sets and the existence of more complex class structure, 758 which is de ned by the keywords and relation of keywords to 759 the imposed classes is usually not as straightforward as genes 760 have to cancers. However, the percentage of features selected 761 per binary classi ers (trees) is quite small. Noticeably, the 762 accuracy obtained for new3s.wc (having 44 classes), is much 763 higher than that of the other methods (for accuracies of other 764 methods, see [82]). We can also observe that for data sets hav- 765 ing more than or equal to ten classes, our methods performs 766 comparatively better than other methods. 767 D. Statistical Signi cance Testing 768 To compare the proposed method with existing FS and clas- 769 si cation methods, we consider four classi cation and six 770 FS methods as well as with the full feature set. Thus we 771 compare our algorithm with 28 FS and classi cation algo- 772 rithm pairs. We have performed Wilcoxon signed ranks test 773 to show that the performance of the proposed algorithm is 774 signi cantly different over 28 pairs of FS and classi cation 775 algorithm. For this, we have considered both the microarray 776 and the text data sets together and have used the average test 777 IEEE Proof 10 IEEE TRANSACTIONS ON CYBERNETICS TABLE VIII WILCOXON SIGNED RANKS TEST (ONE-TAILED) FOR PAIRWISE COMPARISONS WITH THE PROPOSED METHOD AT = 0.05 accuracies achieved with different algorithms on these data 778 sets. Moreover, we have removed the cases for which accu- 779 racies are not known (see [82, Tables IV VII]). Table VIII 780 shows that out of the 28 cases, only in one case the null 781 hypothesis H0 was accepted. Here the null hypothesis is that 782 there is no signi cant difference in performance between our 783 algorithm and a comparing algorithm. We have also used 784 Friedman test [89], [90] to check if all the 29 (28 existing 785 and our proposed) algorithms perform similarly over seven 786 data sets: Colon, TOX-171, Leukemia1, Leukemia2, GCM, 787 tr12.wc, and tr23.wc (see [82, Tables IV VII], some accu- 788 racies for other data sets are not available). The obtained 789 Friedman statistics is 60.13, which is signi cant at = 0.001, 790 as the corresponding statistic 2 with 28 degrees of freedom 791 is 56.90. Thus, Friedman test suggests existence of signi cant 792 difference among the algorithms. 793 V. CONCLUSION 794 In this paper, we have used a MOGP, called ASMiGP, to 795 evolve diverse sets of binary classi ers to solve multiclass clas- 796 si cation problems. Simultaneous FS and rule extraction are 797 performed during the genetic evolution. An important objec- 798 tive of this paper is to nd simple classi cation rules that may 799 be human understandable, which in the given context trans- 800 lates to rules with simple operations and short length. The 801 proposed method is found to achieve its goal. 802 Ensembles perform better when weighted voting is used, the 803 members of the ensembles are diverse enough, and the classi- 804 ers are accurate [21]. Here, we have created c sets of diverse 805 ensembles. Unlike other ensemble-based method, here each 806 ensemble represents diverse classi ers for just one class. The 807 number of features used by the ensembles is high with respect 808 to the number of features selected per (binary classi er) tree. 809 This suggests that the binary classi ers are diverse enough. In 810 our algorithm, we evolve c distinct species in parallel, which 811 try to learn distinct patterns and no interspecies gene exchange 812 is ever allowed. This property makes each species different 813 from the other species and the system becomes less vulner- 814 able, especially when each species has successfully learnt its 815 designated patterns. 816 Our method has been tested on nineteen (eight microar- 817 ray and eleven text) data sets. The experimental results 818 show that we could nd easy-to-understand rules for overall 819 63.2% (87.5% for microarray and 54.5% for text) data sets. 820 We compared our method with four classi cation methods 821 in conjunction with seven FS methods including use of 822 the all-feature set. For 80.17% (61.40% for microarray and 823 95.75% for text) cases our method outperforms others. The 824 improvement in performance is shown statistically signi cant 825 compared to the others. 826 The overall performance of the proposed method is better 827 on text data sets compared to that of microarray data sets. Two 828 important differences between these two groups of data sets are 829 that: 1) microarray data sets have comparatively large feature- 830 to-sample ratios and may not have enough number of points 831 in each class for MOGP to learn the structure of the data sets 832 and 2) the text data sets have comparatively large number of 833 classes as well as instances. Our limited experiments suggest 834 that when there are enough points in each class so that each 835 species can successfully learn its target pattern, the proposed 836 method works better. We found that for text data our method 837 performs noticeably better than the other methods particularly 838 when number of classes is high (ten or more than that). 839 We have shown that our method is very effective when the 840 dimension of the data sets are as high as 49 151. But in all our 841 data sets, number of samples were not very big. If we apply 842 the proposed method on a data set with a really large num- 843 ber samples, it may require a signi cant amount of time. The 844 method may also take a substantial amount of time when the 845 number of classes is high and we have limited parallel process- 846 ing capability. With the today s high performance computing 847 technologies, however, these are not really crucial shortcom- 848 ings. Yet, we have not applied it to truly big data. This paper 849 can be adapted to deal with big data, especially using Hadoop. 850 Our future research interest is to use MOGP to classify 851 multiclass text data, where each text may belong to more than 852 one category (class) and to use net belongingness as a fuzzy 853 membership of the documents to the corresponding category. 854 We also intend to modify the proposed learning method to 855 stepwise learning, so that we can use it for big text data. 856 REFERENCES 857 [1] P. G. Espejo, S. Ventura, and F. Herrera, A survey on the application of 858 genetic programming to classi cation, IEEE Trans. Syst., Man, Cybern. 859 C, Appl. Rev., vol. 40, no. 2, pp. 121 144, Mar. 2010. 860 [2] J. R. Koza, Genetic Programming: On the Programming of Computers 861 by Means of Natural Selection. Cambridge, MA, USA: MIT Press, 1992. 862 [3] J. R. Koza, Genetic Programming II: Automatic Discovery of Reusable 863 Programs. Cambridge, MA, USA: MIT Press, 1994. 864 [4] J. R. Koza, Genetic programming as a means for programming com- 865 puters by natural selection, Stat. Comput., vol. 4, no. 2, pp. 87 112, 866 1994. 867 [5] J. R. Koza, F. H. Bennett, III, and O. Stiffelman, Genetic Programming 868 as a Darwinian Invention Machine. Berlin, Germany: Springer, 1999. 869 [6] P. J. Rauss, J. M. Daida, and S. Chaudhary, Classi cation of spectral 870 imagery using genetic programming, in Proc. Genet. Evol. Comput. 871 Conf. (GECCO), Las Vegas, NV, USA, 2000, Art ID. 48109. AQ4 872 [7] S. A. Stanhope and J. M. Daida, Genetic programming for automatic 873 target classi cation and recognition in synthetic aperture radar imagery, 874 in Evolutionary Programming VII. Berlin, Germany: Springer, 1998, 875 pp. 735 744. 876 [8] I. De Falco, A. D. Cioppa, and E. Tarantino, Discovering interest- 877 ing classi cation rules with genetic programming, Appl. Soft Comput., 878 vol. 1, no. 4, pp. 257 269, 2002. 879 [9] C. C. Bojarczuk, H. S. Lopes, and A. A. Freitas, Genetic programming 880 for knowledge discovery in chest-pain diagnosis, IEEE Eng. Med. Biol. 881 Mag., vol. 19, no. 4, pp. 38 44, Jul./Aug. 2000. 882 IEEE Proof NAG AND PAL: MOGP-BASED ENSEMBLE FOR SIMULTANEOUS FS AND CLASSIFICATION 11 [10] H. Gray, R. Maxwell, I. Martinez-Perez, C. Arus, and S. Cerdan, 883 Genetic programming for classi cation of brain tumours from nuclear 884 magnetic resonance biopsy spectra, in Proc. 1st Annu. Conf. Genet. 885 Program., 1996, p. 28 31. 886 [11] D. Hope, E. Munday, and S. Smith, Evolutionary algorithms in the 887 classi cation of mammograms, in Proc. IEEE Symp. Comput. Intell. 888 Image Signal Process. (CIISP), Honolulu, HI, USA, 2007, pp. 258 265. 889 [12] G. Wilson and M. Heywood, Introducing probabilistic adaptive map- 890 ping developmental genetic programming with redundant mappings, 891 Genet. Program. Evol. Mach., vol. 8, no. 2, pp. 187 220, 2007. 892 [13] J. Kishore, L. M. Patnaik, V. Mani, and V. Agrawal, Application of 893 genetic programming for multicategory pattern classi cation, IEEE 894 Trans. Evol. Comput., vol. 4, no. 3, pp. 242 258, Sep. 2000. 895 [14] J.-Y. Lin, H.-R. Ke, B.-C. Chien, and W.-P. Yang, Designing a classi er 896 by a layered multi-population genetic programming approach, Pattern 897 Recognit., vol. 40, no. 8, pp. 2211 2225, 2007. 898 [15] D. P. Muni, N. R. Pal, and J. Das, A novel approach to design classi ers 899 using genetic programming, IEEE Trans. Evol. Comput., vol. 8, no. 2, 900 pp. 183 196, Apr. 2004. 901 [16] M. Zhang and P. Wong, Genetic programming for medical classi ca- 902 tion: A program simpli cation approach, Genet. Program. Evol. Mach., 903 vol. 9, no. 3, pp. 229 255, 2008. 904 [17] M. Zhang and W. Smart, Using Gaussian distribution to construct 905 tness functions in genetic programming for multiclass object clas- 906 si cation, Pattern Recognit. Lett., vol. 27, no. 11, pp. 1266 1274, 907 2006. 908 [18] D. P. Muni, N. R. Pal, and J. Das, Genetic programming for simulta- 909 neous feature selection and classi er design, IEEE Trans. Syst., Man, 910 Cybern. B, Cybern., vol. 36, no. 1, pp. 106 117, Feb. 2006. 911 [19] K.-H. Liu and C.-G. Xu, A genetic programming-based approach to the 912 classi cation of multiclass microarray datasets, Bioinformatics, vol. 25, 913 no. 3, pp. 331 337, 2009. 914 [20] T. G. Dietterich, Ensemble methods in machine learning, in Multiple 915 Classi er Systems. Berlin, Germany: Springer, 2000, pp. 1 15. 916 [21] L. K. Hansen and P. Salamon, Neural network ensembles, IEEE Trans. 917 Pattern Anal. Mach. Intell., vol. 12, no. 10, pp. 993 1001, Oct. 1990. 918 [22] U. Bhowan, M. Johnston, M. Zhang, and X. Yao, Evolving diverse 919 ensembles using genetic programming for classi cation with unbal- 920 anced data, IEEE Trans. Evol. Comput., vol. 17, no. 3, pp. 368 386, 921 Jun. 2013. 922 [23] W. B. Langdon and R. Poli, Foundations of Genetic Programming. 923 Berlin, Germany: Springer, 2002. 924 [24] K. Neshatian, M. Zhang, and P. Andreae, A lter approach to multi- 925 ple feature construction for symbolic learning classi ers using genetic 926 programming, IEEE Trans. Evol. Comput., vol. 16, no. 5, pp. 645 661, 927 Oct. 2012. 928 [25] P. Nordin, A compiling genetic programming system that directly 929 manipulates the machine code, Advances in Genetic Programming, 930 vol. 1. Cambridge, MA, USA: MIT Press, 1994, pp. 311 331. 931 [26] M. O Neill and C. Ryan, Grammatical evolution, IEEE Trans. Evol. 932 Comput., vol. 5, no. 4, pp. 349 358, Aug. 2001. 933 [27] J. F. Miller and P. Thomson, Cartesian genetic programming, in 934 Genetic Programming. Berlin, Germany: Springer, 2000, pp. 121 132. 935 [28] R. Kohavi and G. H. John, Wrappers for feature subset selection, Artif. 936 Intell., vol. 97, no. 1, pp. 273 324, 1997. 937 [29] Y.-C. Chen, N. R. Pal, and I.-F. Chung, An integrated mechanism 938 for feature selection and fuzzy rule extraction for classi cation, IEEE 939 Trans. Fuzzy Syst., vol. 20, no. 4, pp. 683 698, Aug. 2012. 940 [30] N. Pal and K. Chintalapudi, A connectionist system for feature 941 selection, Neural Parallel Sci. Comput., vol. 5, no. 3, pp. 359 382, 942 1997. 943 [31] D. Chakraborty and N. R. Pal, Selecting useful groups of features in 944 a connectionist framework, IEEE Trans. Neural Netw., vol. 19, no. 3, 945 pp. 381 396, Mar. 2008. 946 [32] I. Guyon, J. Weston, S. Barnhill, and V. Vapnik, Gene selection for can- 947 cer classi cation using support vector machines, Mach. Learn., vol. 46, 948 nos. 1 3, pp. 389 422, 2002. 949 [33] H. Jabeen and A. R. Baig, Review of classi cation using genetic 950 programming, Int. J. Eng. Sci. Technol., vol. 2, no. 2, pp. 94 103, 2010. 951 [34] M. Muharram and G. D. Smith, Evolutionary constructive induc- 952 tion, IEEE Trans. Knowl. Data Eng., vol. 17, no. 11, pp. 1518 1528, 953 Nov. 2005. 954 [35] K. Neshatian and M. Zhang, Genetic programming and class-wise 955 orthogonal transformation for dimension reduction in classi cation prob- 956 lems, in Genetic Programming. Berlin, Germany: Springer, 2008, 957 pp. 242 253. 958 [36] H. Guo and A. K. Nandi, Breast cancer diagnosis using genetic 959 programming generated feature, Pattern Recognit., vol. 39, no. 5, 960 pp. 980 987, 2006. 961 [37] H. Guo, L. B. Jack, and A. K. Nandi, Feature generation using genetic 962 programming with application to fault classi cation, IEEE Trans. Syst., 963 Man, Cybern. B, Cybern., vol. 35, no. 1, pp. 89 99, Feb. 2005. 964 [38] K. Krawiec, Genetic programming-based construction of features for 965 machine learning and knowledge discovery tasks, Genet. Program. 966 Evol. Mach., vol. 3, no. 4, pp. 329 343, 2002. 967 [39] X. Tan, B. Bhanu, and Y. Lin, Fingerprint classi cation based on 968 learned features, IEEE Trans. Syst., Man, Cybern. C, Appl. Rev., vol. 35, 969 no. 3, pp. 287 300, Aug. 2005. 970 [40] M. G. Smith and L. Bull, Genetic programming with a genetic algo- 971 rithm for feature construction and selection, Genet. Program. Evol. 972 Mach., vol. 6, no. 3, pp. 265 281, 2005. 973 [41] Y. Lin and B. Bhanu, Evolutionary feature synthesis for object recog- 974 nition, IEEE Trans. Syst., Man, Cybern. C, Appl. Rev., vol. 35, no. 2, 975 pp. 156 171, May 2005. 976 [42] J. R. Koza, Concept formation and decision tree induction using 977 the genetic programming paradigm, in Parallel Problem Solving from 978 Nature. Berlin, Germany: Springer, 1991, pp. 124 128. 979 [43] G. Folino, C. Pizzuti, and G. Spezzano, Genetic programming and sim- 980 ulated annealing: A hybrid method to evolve decision trees, in Genetic 981 Programming. Berlin, Germany: Springer, 2000, pp. 294 303. 982 [44] J. Eggermont, Evolving fuzzy decision trees with genetic programming 983 and clustering, in Genetic Programming. Berlin, Germany: Springer, 984 2002, pp. 71 82. 985 [45] H. Zhao, A multi-objective genetic programming approach to develop- 986 ing pareto optimal decision trees, Decis. Support Syst., vol. 43, no. 3, 987 pp. 809 826, 2007. 988 [46] T. M. Khoshgoftaar and Y. Liu, A multi-objective software quality clas- 989 si cation model using genetic programming, IEEE Trans. Rel., vol. 56, 990 no. 2, pp. 237 245, Jun. 2007. 991 [47] A. Tsakonas, A comparison of classi cation accuracy of four genetic 992 programming-evolved intelligent structures, Inf. Sci., vol. 176, no. 6, 993 pp. 691 724, 2006. 994 [48] S. Sakprasat and M. C. Sinclair, Classi cation rule mining for automatic 995 credit approval using genetic programming, in Proc. IEEE Congr. Evol. 996 Comput. (CEC), Singapore, 2007, pp. 548 555. 997 [49] C. Qing-Shan, Z. De-Fu, W. Li-Jun, and C. Huo-Wang, A modi ed 998 genetic programming for behavior scoring problem, in Proc. IEEE 999 Symp. Comput. Intell. Data Min. (CIDM), Honolulu, HI, USA, 2007, 1000 pp. 535 539. 1001 [50] E. Carreno, G. Leguizam n, and N. Wagner, Evolution of classi cation 1002 rules for comprehensible knowledge discovery, in Proc. IEEE Congr. 1003 Evol. Comput. (CEC), Singapore, 2007, pp. 1261 1268. 1004 [51] R. R. Mendes, F. B. de Voznika, A. A. Freitas, and J. C. Nievola, 1005 Discovering fuzzy classi cation rules with genetic programming and 1006 co-evolution, in Principles of Data Mining and Knowledge Discovery. 1007 Berlin, Germany: Springer, 2001, pp. 314 325. 1008 [52] A. L. Garcia-Almanza and E. P. Tsang, Evolving decision rules to 1009 predict investment opportunities, Int. J. Autom. Comput., vol. 5, no. 1, 1010 pp. 22 31, 2008. 1011 [53] U. Bhowan, M. Johnston, and M. Zhang, Developing new tness func- 1012 tions in genetic programming for classi cation with unbalanced data, 1013 IEEE Trans. Syst., Man, Cybern. B, Cybern., vol. 42, no. 2, pp. 406 421, 1014 Apr. 2012. 1015 [54] W. Fu, M. Johnston, and M. Zhang, Low-level feature extraction 1016 for edge detection using genetic programming, IEEE Trans. Cybern., 1017 vol. 44, no. 8, pp. 1459 1472, Aug. 2014. 1018 [55] P. Wang, K. Tang, T. Weise, E. Tsang, and X. Yao, Multiobjective 1019 genetic programming for maximizing ROC performance, 1020 Neurocomputing, vol. 125, pp. 102 118, Feb. 2014. 1021 [56] P. Wang et al., Convex hull-based multi-objective genetic programming 1022 for maximizing receiver operating characteristic performance, IEEE 1023 Trans. Evol. Comput., to be published. AQ5 1024 [57] P. A. Whigham and G. Dick, Implicitly controlling bloat in genetic 1025 programming, IEEE Trans. Evol. Comput., vol. 14, no. 2, pp. 173 190, 1026 Apr. 2010. 1027 [58] S. Luke and L. Panait, A comparison of bloat control methods for 1028 genetic programming, Evol. Comput., vol. 14, no. 3, pp. 309 344, 2006. 1029 [59] R. Poli, A simple but theoretically-motivated method to control bloat 1030 in genetic programming, in Genetic Programming. Berlin, Germany: 1031 Springer, 2003, pp. 204 217. 1032 [60] M. J. Streeter, The root causes of code growth in genetic program- 1033 ming, in Genetic Programming. Berlin, Germany: Springer, 2003, 1034 pp. 443 454. 1035 IEEE Proof 12 IEEE TRANSACTIONS ON CYBERNETICS [61] S. Luke and L. Panait, Fighting bloat with nonparametric parsimony 1036 pressure, in Parallel Problem Solving from NaturePPSN VII. Berlin, 1037 Germany: Springer, 2002, pp. 411 421. 1038 [62] F. Fern ndez-de Vega, G. G. Gil, J. A. G. Pulido, and J. L. Guisado, 1039 Control of bloat in genetic programming by means of the island model, 1040 in Parallel Problem Solving from Nature-PPSN VIII. Berlin, Germany: 1041 Springer, 2004, pp. 263 271. 1042 [63] F. Fernandez, G. Galeano, and J. Gomez, Comparing synchronous and 1043 asynchronous parallel and distributed genetic programming models, in 1044 Genetic Programming. Berlin, Germany: Springer, 2002, pp. 326 335. 1045 [64] M. Castelli, L. Vanneschi, and S. Silva, Semantic search-based genetic 1046 programming and the effect of intron deletion, IEEE Trans. Cybern., 1047 vol. 44, no. 1, pp. 103 113, Jan. 2014. 1048 [65] D. Rochat, M. Tomassini, and L. Vanneschi, Dynamic size populations 1049 in distributed genetic programming, in Genetic Programming. Berlin, 1050 Germany: Springer, 2005, pp. 50 61. 1051 [66] F. Fernandez, L. Vanneschi, and M. Tomassini, The effect of plagues in 1052 genetic programming: A study of variable-size populations, in Genetic 1053 Programming. Berlin, Germany: Springer, 2003, pp. 317 326. 1054 [67] A. Song, Q. Shi, and W. Yin, Understanding of GP-evolved motion 1055 detectors, IEEE Comput. Intell. Mag., vol. 8, no. 1, pp. 46 55, 1056 Feb. 2013. 1057 [68] J. Luna, J. Romero, C. Romero, and S. Ventura, On the use of genetic 1058 programming for mining comprehensible rules in subgroup discovery, 1059 IEEE Trans. Cybern., vol. 44, no. 12, pp. 2329 2341, Dec. 2014. 1060 [69] S. Pang, D. Kim, and S. Y. Bang, Membership authentication in the 1061 dynamic group by face classi cation using SVM ensemble, Pattern 1062 Recognit. Lett., vol. 24, no. 1, pp. 215 225, 2003. 1063 [70] H. Ishibuchi and T. Yamamoto, Evolutionary multiobjective optimiza- 1064 tion for generating an ensemble of fuzzy rule-based classi ers, in 1065 Genetic and Evolutionary Computation GECCO. Berlin, Germany: 1066 Springer, 2003, pp. 1077 1088. 1067 [71] P. Yang, Y. H. Yang, B. B. Zhou, and A. Y. Zomaya, A review of 1068 ensemble methods in bioinformatics, Current Bioinformat., vol. 5, no. 4, 1069 pp. 296 308, 2010. 1070 [72] K. Nag, T. Pal, and N. Pal, ASMiGA: An archive-based steady- 1071 state micro genetic algorithm, IEEE Trans. Cybern., vol. 45, no. 1, 1072 pp. 40 52, Jan. 2015. 1073 [73] R. Enache, Steady state evolutionary algorithms, Honda Res. 1074 Inst. Europe GmbH, Offenbach am Main, Germany, Tech. 1075 Rep. HRI-EU 04-02, Jan. 2004. AQ6 1076 [74] J.-H. Hong and S.-B. Cho, Gene boosting for cancer classi cation 1077 based on gene expression pro les, Pattern Recognit., vol. 42, no. 9, 1078 pp. 1761 1767, 2009. 1079 [75] K. Nag and T. Pal, A new archive based steady state genetic algorithm, 1080 in Proc. IEEE Congr. Evol. Comput. (CEC), Brisbane, QLD, Australia, 1081 2012, pp. 1 7. 1082 [76] S. Kernell, Presidential popularity and negative voting: An alterna- 1083 tive explanation of the midterm congressional decline of the president s 1084 party, Amer. Polit. Sci. Rev., vol. 71, no. 1, pp. 44 66, 1977. 1085 [77] M. Fang, H. Takauj, S. Kaneko, and H. Watanabe, Robust optical ow 1086 estimation for underwater image, in Proc. Int. Symp. Optomechatronic 1087 Technol. (ISOT), Istanbul, Turkey, 2009, pp. 185 190. 1088 [78] M. Fang, H. Takauji, and S. Kaneko, Rapid computation of robust 1089 optical ow by ef cient complementary voting, in Proc. World Autom. 1090 Congr. (WAC), Kobe, Japan, 2010, pp. 1 6. 1091 [79] M. P. Fiorina and K. A. Shepsle, Is negative voting an artifact? Amer. 1092 J. Polit. Sci., vol. 33, pp. 423 439, May 1989. 1093 [80] J. Durillo, A. Nebro, and E. Alba, The jMetal framework for multi- 1094 objective optimization: Design and architecture, in Proc. IEEE Congr. 1095 Evol. Comput. (CEC), Barcelona, Spain, Jul. 2010, pp. 4138 4325. 1096 [81] J. J. Durillo and A. J. Nebro, jMetal: A java framework for 1097 multi-objective optimization, Adv. Eng. Softw., vol. 42, pp. 760 771, 1098 2011. [Online]. Available: http://www.sciencedirect.com/science/article/ 1099 pii/S0965997811001219 1100 [82] Q. Song, J. Ni, and G. Wang, A fast clustering-based feature sub- 1101 set selection algorithm for high-dimensional data, IEEE Trans. Knowl. 1102 Data Eng., vol. 25, no. 1, pp. 1 14, Jan. 2013. 1103 [83] L. Yu and H. Liu, Feature selection for high-dimensional data: A fast 1104 correlation-based lter solution, in Proc. 20th Int. Conf. Mach. Learn., 1105 vol. 20. Washington, DC, USA, 2003, pp. 856 863. 1106 [84] L. Yu and H. Liu, Ef cient feature selection via analysis of rele- 1107 vance and redundancy, J. Mach. Learn. Res., vol. 5, pp. 1205 1224, 1108 Dec. 2004. 1109 [85] M. A. Hall, Correlation-based feature selection for machine learn- 1110 ing, Ph.D. dissertation, Dept. Comput. Sci., Univ. Waikato, Hamilton, 1111 New Zealand, 1999. 1112 [86] M. Robnik- ikonja and I. Kononenko, Theoretical and empirical analy- 1113 sis of ReliefF and RreliefF, Mach. Learn., vol. 53, nos. 1 2, pp. 23 69, 1114 2003. 1115 [87] M. Dash, H. Liu, and H. Motoda, Consistency based feature selection, 1116 in Knowledge Discovery and Data Mining. Current Issues and New 1117 Applications. Berlin, Germany: Springer, 2000, pp. 98 109. 1118 [88] H. Almuallim and T. G. Dietterich, Learning boolean concepts in 1119 the presence of many irrelevant features, Artif. Intell., vol. 69, no. 1, 1120 pp. 279 305, 1994. 1121 [89] M. Friedman, The use of ranks to avoid the assumption of normal- 1122 ity implicit in the analysis of variance, J. Amer. Stat. Assoc., vol. 32, 1123 no. 200, pp. 675 701, 1937. 1124 [90] M. Friedman, A comparison of alternative tests of signi cance for the 1125 problem of m rankings, Ann. Math. Stat., vol. 11, no. 1, pp. 86 92, 1126 1940. 1127 Kaustuv Nag received the B.Tech. degree in com- 1128 puter science and engineering from the West Bengal 1129 University of Technology, Kolkata, India, and the 1130 M.Tech. degree in computer science and engineering 1131 from the National Institute of Technology, Durgapur, 1132 Durgapur, India, in 2010 and 2012, respectively. He 1133 is currently pursuing the Ph.D. degree with Jadavpur 1134 University, Kolkata. 1135 He was a Visiting Researcher at the Indian 1136 Statistical Institute, Kolkata. His current research 1137 interests include genetic algorithm, genetic program- 1138 ming, and arti cial neural networks. 1139 Mr. Nag was the recipient of INSPIRE Fellowship. 1140 Nikhil R. Pal (M 91 SM 00 F 05) AQ7 is a Professor 1141 with the Electronics and Communication Sciences 1142 Unit, Indian Statistical Institute, Kolkata, India. His 1143 current research interest includes bioinformatics, 1144 brain science, fuzzy logic, pattern analysis, neural 1145 networks, and evolutionary computation. 1146 Dr. Pal AQ8 was the Editor-in-Chief of the IEEE 1147 TRANSACTIONS ON FUZZY SYSTEMS from 2005 1148 to 2010. He has served/been serving on the 1149 Editorial/Advisory Board/Steering Committee of 1150 several journals, including the International Journal 1151 of Approximate Reasoning, Applied Soft Computing, the International Journal 1152 of Knowledge-Based Intelligent Engineering Systems, the International 1153 Journal of Neural Systems, Fuzzy Sets and Systems, the International Journal 1154 of Intelligent Computing in Medical Sciences and Image Processing, the 1155 Fuzzy Information and Engineering: An International Journal, the IEEE 1156 TRANSACTIONS ON FUZZY SYSTEMS, and the IEEE TRANSACTIONS ON 1157 CYBERNETICS. He has given several plenary/keynote speeches in different 1158 premier international conferences in computational intelligence. He has served 1159 as the General Chair, a Program Chair, and a Co-Program Chair of several 1160 conferences. He was a Distinguished Lecturer of the IEEE Computational 1161 Intelligence Society (CIS). He is currently the Vice President of the IEEE 1162 CIS. He was a member of the Administrative Committee of the IEEE CIS. 1163 He is a fellow of the National Academy of Sciences, India, the Indian National 1164 Academy of Engineering, the Indian National Science Academy, and the 1165 International Fuzzy Systems Association. 1166 IEEE Proof AUTHOR QUERIES AQ1: Please con rm the funding information is correct as set. AQ2: Please verify and con rm whether the edits made to the line nos. 86 87 retains your intended meaning. AQ3: Note that both offspring and off-spring are used throughout the paper. Please indicate if changes are needed. AQ4: Please con rm that the edits made to Reference [6] is correct as set. AQ5: Please provide the volume number, issue number, page range, month, and the publication year for Reference [56]. AQ6: Please provide the department name for Reference [73]. AQ7: Please provide the educational background for the author N. R. Pal. AQ8: Please verify and con rm that the edits made to N. R. Pal biography retain the intended meaning. IEEE Proof IEEE TRANSACTIONS ON CYBERNETICS 1 A Multiobjective Genetic Programming-Based Ensemble for Simultaneous Feature Selection and Classi cation Kaustuv Nag and Nikhil R. Pal, Fellow, IEEE Abstract We present an integrated algorithm for simultane- 1 ous feature selection (FS) and designing of diverse classi ers 2 using a steady state multiobjective genetic programming (GP), 3 which minimizes three objectives: 1) false positives (FPs); 2) false 4 negatives (FNs); and 3) the number of leaf nodes in the tree. 5 Our method divides a c-class problem into c binary classi ca- 6 tion problems. It evolves c sets of genetic programs to create c 7 ensembles. During mutation operation, our method exploits the 8 tness as well as un tness of features, which dynamically change 9 with generations with a view to using a set of highly relevant 10 features with low redundancy. The classi ers of ith class deter- 11 mine the net belongingness of an unknown data point to the ith 12 class using a weighted voting scheme, which makes use of the 13 FP and FN mistakes made on the training data. We test our 14 method on eight microarray and 11 text data sets with diverse 15 number of classes (from 2 to 44), large number of features 16 (from 2000 to 49 151), and high feature-to-sample ratio (from 17 1.03 to 273.1). We compare our method with a bi-objective GP 18 scheme that does not use any FS and rule size reduction strat- 19 egy. It depicts the effectiveness of the proposed FS and rule size 20 reduction schemes. Furthermore, we compare our method with 21 four classi cation methods in conjunction with six features selec- 22 tion algorithms and full feature set. Our scheme performs the 23 best for 380 out of 474 combinations of data sets, algorithm and 24 FS method. 25 Index Terms Classi cation, ensemble, feature selection (FS), 26 genetic programming (GP). 27 I. INTRODUCTION 28 C LASSIFICATION is one of the most important and 29 frequently encountered problems in data mining and 30 machine learning. A wide range of real world problems of 31 different domains can be restated as classi cation problems. 32 This includes diagnosis from microarray data, text categoriza- 33 tion, medical diagnosis, software quality assurance, and many 34 more. The objective of classi cation is to take an input vector 35 x = (x1, . . . , xd)T and to assign it to one of the K classes Ck, 36 Manuscript received August 7, 2014; revised November 25, 2014 and January 27, 2015; accepted February 5, 2015. This work was supported in part by the Networked Communications Program of Chunghwa Telecom Company, Taiwan, and in part by the INSPIRE Fellowship through the Department of Science and Technology, India, under Grant IF120686. AQ1 This paper was recommended by Associate Editor S. Mostaghim. K. Nag is with the Department of Instrumentation and Electronics Engineering, Jadavpur University, Kolkata-700098, India (e-mail: kaustuv.nag@gmail.com). N. R. Pal is with the Electronics and Communication Sciences Unit, Indian Statistical Institute, Kolkata 700108, India. Color versions of one or more of the gures in this paper are available online at http://ieeexplore.ieee.org. Digital Object Identi er 10.1109/TCYB.2015.2404806 where k = 1, . . . , K. A model, called classi er, is used to 37 solve this problem. The classi er encodes a set of criteria. 38 Depending upon some features of the data point, these crite- 39 ria assign the data point to a particular class [1]. Sometimes, 40 ensembles of weak classi ers are used to obtain better classi- 41 cation accuracy. High dimensionality, high feature-to-sample 42 ratio, and redundant and/or noisy features are some common 43 sources of dif culties associated with classi cation. 44 Feature selection (FS) is a process to select a small but use- 45 ful subset of features from the set of available features which 46 is adequate for solving the problem in an ef cient manner. 47 Some available features may be redundant, not useful, and may 48 cause confusion during the learning phase. These unwanted 49 features needlessly increase the size and complexity of the fea- 50 ture space. It increases the computation cost for learning, and 51 may sometimes be responsible for nding suboptimal solu- 52 tions of the problem. This makes FS techniques important for 53 the analysis of high dimensional data sets, especially when 54 feature-to-sample ratio is extremely high. 55 The microarray technology has made it possible to diagnose 56 different types of cancers directly using the microarray data 57 sets. One of the main dif culties we face to do this is the high 58 feature-to-sample ratio of microarray data sets, which makes 59 FS an important step. Finding keywords as well as contexts 60 from text data is essential to detect (without human interven- 61 tion) the context of web pages, emails, or questions/answers, 62 etc. A large set of distinct words in large texts (high num- 63 ber of features) and many categories of texts (high number of 64 classes) are the two complicated challenges that we face in 65 this task. 66 Genetic programming (GP) [2] [5] is a biological-evolution 67 inspired methodology where each solution is a program or 68 an equation that evolves with one or more objective func- 69 tions to perform speci c tasks. References [6] [12] used GP 70 to design classi ers or to generate rules for binary classi- 71 cation problems. Some researchers have also attempted to 72 solve multiclass problems [13] [17]. GP has also been used 73 for simultaneous FS and classi cation [18]. Liu and Xu [19] 74 used GP to create ensembles of classi ers (genetic programs). 75 They have used these ensembles to classify microarray data 76 sets. Though GP is a powerful tool, it has a drawback: without 77 special care each genetic program (equation) becomes huge. 78 As an effect, they do not learn the patterns in the training 79 data. Rather, memorizes them. It also makes genetic programs 80 to be dif cult to comprehend. Besides, though ensembles can 81 2168-2267 c 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. IEEE Proof 2 IEEE TRANSACTIONS ON CYBERNETICS perform better than individual classi ers [20], to obtain better 82 performance, each ensemble should be diverse and each mem- 83 ber of the ensemble should be accurate [20] [22]. Without 84 special care, due to lack of explicit diversity preservation 85 mechanism, the solutions of single objective GP may lose AQ2 86 diversity. 87 The objective of this paper is to nd an embedded method- 88 ology of simultaneous FS and classi cation employing GP as a 89 tool. Some of the novel contributions of our schemes are as fol- 90 lows. We have introduced a new multiobjective GP (MOGP), 91 called archive-based steady state micro GP (ASMiGP). It 92 is enriched by several new operators. The mating selec- 93 tion judicially uses Roulette wheel selection, instead of two 94 tire multiobjective selection scheme (where domination is 95 preferred over diversity). The crossover is new and uses male 96 female differentiation so that the off-spring is more likely to be AQ3 97 close to the female parent in the genotypic space. The mutation 98 is restrictive and performs less exploration in the hypothesis 99 space. Thus, it reduces disruption. For feature nodes, instead 100 of tness, mutation uses un tness to select the mutation point. 101 Altering the tness and un tness of the features in different 102 stages of learning, we change the objective of searching in the 103 corresponding stages. We use ASMiGP to learn c diverse sets 104 of classi ers (equations) minimizing three objectives: 1) false 105 positive (FP); 2) false negative (FN); and 3) number of leaf 106 nodes of a tree to restrict the rule size. Throughout the learning 107 process, implicit FS is performed using MOGP, whereas sev- 108 eral lter-based approaches are used in different stages of the 109 procedure. In this way, we obtain concise rules that involve 110 only simple arithmetic operations. A weighted negative vot- 111 ing is then used among the rules of each ensemble. These 112 weights are determined on the basis of the performance of 113 the binary classi ers on the training data set. A new measure 114 of weighted negative voting, called net belongingness, is also 115 introduced. 116 The proposed method has been tested on eight multiclass 117 microarray data sets having large number of features, varying 118 from 2000 to 49 151, and high feature-to-sample ratio, vary- 119 ing from 32.26 to 273.06. It has also been tested on eleven 120 high dimensional (varying from 3182 to 26 832) text data sets, 121 where the number of classes (categories) vary from 6 to 44. 122 Experimental results reveal that our method can generate 123 ensembles of classi ers with concise rules that can do a good 124 job of classi cation with a small subset of features. 125 II. BACKGROUND AND RELATED WORKS 126 This section provides a background to GP, GP-based online 127 FS and classi cation, ensembles, voting schemes, and concise 128 rule nding. This section also includes some related works on 129 these topics. 130 A. Concise Introduction to GP 131 GP [2] [5] is an approach to nd programs that can solve 132 a given problem. GP uses Darwinian principle of survival of 133 the ttest. It evolves the programs using biologically inspired 134 genetic operations like reproduction, crossover, and mutation 135 in the search space to nd such programs which would be able 136 to solve the given problem [15], [23], [24]. The overall search 137 process of traditional single objective GP is quite similar to 138 traditional population-based genetic algorithm. The main dif- 139 ference among them is in the representation of solutions. The 140 usual steps of traditional single objective GP can be found 141 in [2], [15], and [24]. 142 There exist several ways to encode a program. Probably, the 143 most popular one is to use a tree structure [25] [27] which 144 we use. In tree-based GP, a program is represented by a 145 tree where the internal nodes are from a set of functions F, 146 and the leaf nodes are from a set of terminals T . The sub- 147 trees of a function node are the arguments of that function. 148 The sets F and T must satisfy the closure and suf ciency 149 properties [2], [15]. To satisfy the closure property, F must 150 be well de ned and closed for any combination of probable 151 arguments that it may encounter [15]. Again, to satisfy the 152 suf ciency property, F and T must be able to represent any 153 possible valid solution of the problem. 154 B. FS: Why Embedded? 155 FS methods are generally divided into two main groups: 156 1) lter method and 2) wrapper method [28]. A lter method 157 does not use any feedback from the classi er or any mining 158 algorithm. It relies on the general characteristics of data. On 159 the contrary, to measure the goodness of features, a wrapper 160 method uses a predetermined classi er or mining algorithm, 161 which will nally use the selected features. Consequently, a 162 wrapper method exploits interactions among the subset of fea- 163 tures on which it is tested. But, to nd an optimal set of 164 features, a wrapper method needs to measure performances 165 on all possible subsets of features. This becomes infeasible 166 for high dimensional data sets. To overcome this problem, 167 wrapper methods typically use a heuristic-based forward or 168 backward selection mechanism [28], which does not evaluate 169 all possible subsets. So, in this paper, we consider embed- 170 ded methods, where FS and designing of the classi cation 171 system are done together. Embedded methods do not need 172 to evaluate all possible subsets. Moreover, they can account 173 for interaction between features and the classi er that is used 174 to solve the problem [29]. Usually, embedded methods attain 175 comparable accuracy to wrapper methods as well as compara- 176 ble ef ciency to lter methods. Though for every classi cation 177 tool it may not be easy to de ne such an integrated mecha- 178 nism, several attempts of FS using embedded methods have 179 already been made. These attempts include using single objec- 180 tive GP [18], neural networks [30], [31], and support vector 181 machines [32]. 182 C. GP in Classi cation and FS 183 Many researchers have used GP as a tool for classi - 184 cation and FS. Some literature on this topic can be found 185 in [1] and [33]. It has been used in both lter [34] [37] 186 and wrapper [38] [41] approaches. GP has also been 187 used for extracting decision trees [42] [47]. Among these 188 in [45] and [46], MOGP has been used. GP has also been 189 adopted for learning rule-based systems [7] [9], [48] [51]. 190 Both binary classi cation [6], [7], [10] [12], [48] and 191 IEEE Proof NAG AND PAL: MOGP-BASED ENSEMBLE FOR SIMULTANEOUS FS AND CLASSIFICATION 3 multiclass [8], [9], [13] [17], [49] [51] classi cation prob- 192 lems have been addressed by GP-based (rule-based) 193 systems. Even, researchers have applied GP-based 194 (rule-based) system for binary classi cation of imbal- 195 anced data [52], [53]. Discriminant functions (DFs)-based 196 GP for online FS and classi cation has been adopted 197 in [18] to solve multiclass problems. It is noteworthy 198 that GP has also been used in feature extraction for edge 199 detection [54]. 200 Since, in this paper, we have used MOGP for learning DFs, 201 we discuss some relevant GP-based systems. In DFs-based GP, 202 each program is a mathematical equation, where the variables 203 are features of the data. Usually every program is associated 204 with a class. For every input data point, the program converts 205 it to a single output value. If this output value is more than 206 a prede ned threshold (usually zero), the point belongs to the 207 class to which the program is associated with. Thus, a sin- 208 gle equation is enough for binary classi cation problems. For 209 c-class problems, there are two common ways. The rst and 210 more frequently used approach is to consider a c-class problem 211 as c binary classi cation problems [1]. Thus, c number of DFs 212 are used to discriminate c classes. The second and less popular 213 approach is to use a single DF with (c 1) threshold values to 214 create c intervals. Each of these intervals is then assigned to a 215 particular class. For both categories, a common practice is to 216 evolve a population where each individual encodes either one 217 (for binary classi cation problems having single threshold, or 218 for multiclass problems having multiple thresholds) or multi- 219 ple DFs each of which uses a single threshold. Such encoding 220 schemes have been used in [15] and [18]. In another practice, 221 each solution encodes multiple DFs having a single threshold. 222 The nal output value in this case may be obtained using a 223 (weighted) voting scheme among the output values of each 224 function of the same individual. An example of this scheme 225 can be found in [19]. 226 In the recent years, researchers have made some notable 227 attempts to solve classi cation problems using MOGP. 228 Wang et al. [55] proposed a MOGP to obtain a group of 229 nondominated classi ers, with which the maximum receiver 230 operating characteristic convex hull (ROCCH) can be obtained. 231 To achieve this, they have adopted four different multiobjec- 232 tive frameworks into GP. For further improvement of each 233 individual GPs performance, they have de ned two local 234 search strategies that have been especially designed for clas- 235 si cation problems. The experimental results in [55] have 236 demonstrated the ef cacy of the proposed memetic scheme in 237 their MOGP framework. Another convex hull-based MOGP, 238 called CH-MOGP, can be found in [56]. Wang et al. [56] 239 showed the differences between the conventional multiobjec- 240 tive optimization problem and ROCCH maximization problem. 241 Wang et al. introduced a convex hull-based sorting without 242 redundancy and a new selection procedure which are suitable 243 for ROCCH maximization problem. Again, Bhowan et al. [22] 244 proposed a MOGP-based approach that is especially designed 245 for unbalanced data sets. This approach [22] evolves diverse 246 and accurate ensembles of GP classi ers with good per- 247 formance on both the majority and the minority classes. 248 The individual members of the evolved ensembles, that 249 are composed of nondominated solutions, vote on class 250 membership. 251 D. GP, Bloating, and Concise Rules 252 During evolution of GP, variable length genomes gradually 253 start to increase its length without signi cant improvement in 254 tness. This incident, called bloating [57], is a well known 255 phenomenon in GP. Bloating causes genetic programs: 1) to 256 keep reducible genome structures and 2) to memorize training 257 data points, rather than recognizing patterns hidden in them. 258 To nd rules which are to some extent human interpretable and 259 can be analyzed, each of the genetic programs must be concise. 260 A plausible way to achieve this target is to control bloat- 261 ing. A popular way to handle bloating is to take into account 262 the program size [2], [58] [61]. Some other methods include 263 spatially-structured populations [62], [63], island-based mod- 264 els to introduce spatial structure to the population [62], [63], 265 intron deletion [64], and dynamic population sizing [65], [66]. 266 Song et al. [67] attempted to understand the GP evolved solu- 267 tions; while Luna et al. [68] attempted to nd comprehensible 268 rules in subgroup discovery. 269 E. Ensemble Classi er 270 In classi cation, the basic task is to search through a 271 hypothesis space to nd a suitable hypothesis that will 272 be able to classify the data points in a better way. An 273 ensemble combines multiple hypotheses to form a better 274 one [19], [21], [22], [53], [69], [70]. Empirically, an ensem- 275 ble performs better when each of the classi ers is highly 276 accurate (strong learner) and the members of the ensemble are 277 signi cantly diverse. The explanation behind the better perfor- 278 mance of ensemble classi ers than a single classi er has been 279 described in [20]. Normally, to decide the class label of a data 280 point, the member classi ers of an ensemble use (weighted) 281 voting. Ensembles are often used in bio-informatics [71]. 282 III. PROPOSED WORK 283 A. Representation of Classi ers or Solutions 284 In this paper, we evolve c-populations of genetic programs. 285 Each individual of these populations is a binary classi er. Each 286 binary classi er is represented by a single tree. When a data 287 point is passed through an individual of the ith population, 288 if the output value is positive, the individual says that the 289 passed data point belongs to the ith class; otherwise it says that 290 the point does not belong to that class. The internal (nonleaf) 291 nodes of these trees are functions F. The terminal nodes must 292 be either a feature or a constant from the set C. We have 293 imposed some constraint on the minimum size (architecture) 294 of the trees. In each tree there must be at least one function 295 node and two terminal nodes (with at least one feature node). 296 Though, a single feature (terminal) node might be enough to 297 determine the class label, this would rarely happen in practice. 298 The above restrictions make the trees more useful without any 299 loss of generalization capability. Again, after generation of any 300 tree throughout the learning phase (using mutation, crossover, 301 or random initialization), the largest subtree consisting of only 302 IEEE Proof 4 IEEE TRANSACTIONS ON CYBERNETICS constants as its leaf nodes is replaced by a constant leaf node 303 having the equivalent constant value. 304 B. MOGP for Learning 305 Larger genetic programs may memorize the training patterns 306 which, in turn, may increase the training accuracy and reduce 307 the understandability of the rules. Therefore, we aim to nd 308 smaller but accurate classi ers. Again, when c is high enough, 309 even a balanced c-class data set may get converted to c num- 310 ber of highly imbalanced bi-classi cation data sets. Instead of 311 minimization of classi cation error, simultaneous minimiza- 312 tion of FP and FN would be more appropriate in this regard. 313 Suppose a classi er makes some mistakes m = mp + mn on a 314 given training data set, where mp and mn are, respectively, the 315 number of FPs and FNs. Consider two classi ers, each making 316 the same number of mistakes m on a given data set. Let for the 317 rst classi er mp = mn and for the second classi er mp mn 318 or mp mn. If the cost of an FP is the same as that of a FN, 319 we would prefer the rst classi er. Consequently, minimization 320 of both FP and FN would be a better strategy than minimiza- 321 tion of the total number of misclassi cation, particularly for 322 imbalanced data sets. Thus, we have three objectives, i.e., min- 323 imizations of: 1) FP; 2) FN; and 3) rule size. Moreover, when 324 different classes are overlapped, minimization of FP is usu- 325 ally in con ict with the minimization of FN and vice versa. 326 Multiobjective optimization is more suitable when we need to 327 optimize more than one con icting objective. Therefore, dur- 328 ing the learning of each binary classi er, we minimize three 329 objectives using an MOGP: 1) FP; 2) FN; and 3) number of 330 leaf nodes in the tree. The third objective is used to reduce 331 the size of the tree which enhances the understandability and 332 reduces the pattern memorization capability. The algorithm, 333 proposed in this paper, is called ASMiGP, which is presented 334 in Algorithm 1. 335 In an evolutionary search, it is desired to have as many gen- 336 erations as possible and steady state nature of an algorithm 337 maximizes the number of generations when the number of 338 function evaluations is xed [72]. Due to maximization of gen- 339 erations, a steady state evolutionary search is more exploitative 340 to enhance the searching in a region which is more likely to 341 have or closer to the Pareto front and avoiding exploration in 342 regions that are less likely to improve the solutions. It causes 343 faster convergence. In other words, independent of the tness 344 evaluation process, steady state selection is more performant 345 than discrete generational selection [73]. Therefore, we have 346 used a steady state algorithm instead of a generational one. 347 C. FS 348 In this paper, we have used the embedded model of FS. 349 Explicit FS is performed during population initialization and 350 mutation. Implicit FS is performed during crossover. Filtering 351 is also performed at three different stages of the learning pro- 352 cess; in particular, at the beginning and after 50% and after 353 75% of evaluations as described next in this subsection. 354 To facilitate the FS, following [74], we de ne an index that 355 assesses the discriminating ability of a feature. Consider a 356 two-class problem. Note that for a multiclass problem, the 357 Algorithm 1: ASMiGP 1 Initialize population using ramped-half-and-half method. 2 Initialize the archive solutions using initial solutions. 3 while EvaluationsCurrent EvaluationsMaximum do 4 repeat 5 operator = Select crossover or mutation. 6 if operator = = crossover then 7 Select male and female parents (mating selection). 8 Perform crossover. 9 end 10 else 11 Select an individual (mating selection). 12 Mutate the individual. 13 end 14 until the in x equation of off-spring is distinct from in x equation of any individual of the archive 15 Evaluate the new off-spring. 16 EvaluationsCurrent = EvaluationsCurrent + 1 17 Update the archive using new offspring (multi-objective environmental selection). 18 end 19 Fronts = Perform fast-non-dominated-sort. 20 return rst front of Fronts. one-versus-all case can also be viewed as a two-class prob- 358 lem. Let there be np number of training points and the class 359 label for the jth data point be +1 if it belongs to class 1 and 1 360 if it belongs to class 2. Let the value of the fth feature for the 361 ith data point be fi; i = 1, . . . , np. If the fth feature is a good 362 discriminatory feature then for all data points from class 1, 363 it should have high values and for all points from class 2, it 364 should have low values or vice-versa. Hence for a good dis- 365 criminatory feature, we can de ne an ideal feature vector with 366 values 1, if the data point is from class 1 and 0 otherwise; or 367 the feature value is 0, if it is from class 1, otherwise, it is 0. 368 Let Sf be the vector containing the ideal feature values for fea- 369 ture f and s be the ideal feature value of the fth feature for 370 the ith sample. Note that, there could other important features 371 that are not linearly related to the class structure. We are not 372 considering them in this preliminary ltering step. As in [74], 373 we compute the Pearson s correlation (or any other measure of 374 similarity) between f and s as a measure of feature relevance 375 Cf = np  j=1  sji s  fj f      np  j=1  sji s 2 np  j=1  fj f 2. 376 (1) 377 A higher value of |Cf | indicates a stronger discriminative 378 power of feature f. For a multiclass problem, using the one- 379 versus-all strategy, for the ith binary classi cation problem 380 (class i versus the rest), the correlation for the fth feature is 381 denoted by Ci f . 382 Now, we describe the FS procedure. Let the set of all fea- 383 tures be Fall. We intend to incorporate only those features 384 from Fall which are more likely to help the classi ers to 385 IEEE Proof NAG AND PAL: MOGP-BASED ENSEMBLE FOR SIMULTANEOUS FS AND CLASSIFICATION 5 decide the class label. We assign different tness and un t- 386 ness measures to the features during the learning phase. To 387 remove a feature node from any tree (during mutation), we 388 select it using Roulette wheel selection on the un tness val- 389 ues of the features which are present in that tree. Similarly, 390 when a new feature is inserted in the tree, it is selected using 391 Roulette wheel selection on the tness values. During the 392 rst 50% evaluations, the tness and un tness features are 393 de ned as 394 F0%,i tness( f, i) = Ci f Cimax 2 , if Ci f  Cimax > 0.3 0, otherwise (2) 395 F0%,i un tness( f, i) = 1.0 F0%,i tness (3) 396 where Ci max = max f Fall |Ci f |. Equation (2) sets the tness of 397 very poor (poor with respect to its discriminating power) to 398 zero to eliminate their impact during the initial evolution. Let 399 Feval=0%,i Fall be the features with nonzero tness values. 400 Basically, at this stage we are using a lter on the feature set 401 Fall to obtain a smaller feature set. 402 After completion of 50% evaluations for each population, 403 we nd the features used in that population. Let the feature 404 set be Feval=50%,i. Then we make the tness of all features in 405 Fall Feval=50%,i to zero. This is done with the assumption 406 that after 50% evaluations useful features have been used by 407 the collection of decision trees. Now, the tness and un tness 408 values of all features in Feval=50%,i are modi ed according 409 to (4) and (5), respectively 410 F50%,i tness( f, i) = |Ci f |  f =g g F50%,i | fg|, if f Feval=50%,i 0, otherwise (4) 411 F50%,i un tness( f, i) = e F50%,i tness( f,i) minf  F50%,i tness  maxf  F50%,i tness  minf  F50%,i tness  (5) 412 where fg is the Pearson s correlation between fth and gth 413 features. Here, we try to select features with higher relevance 414 but reducing the redundancy in the set of selected features. 415 The tness, de ned in (4), increases when the feature is highly 416 correlated with class label. Similarly, it reduces when it is more 417 correlated with other existent features. This is done to achieve 418 maximum relevance minimum redundancy. 419 After 75% function evaluations, again we take another 420 snapshot of the population. Let the existent features for the 421 population be Feval=75%,i Feval=50%,i. Then, the tness 422 and un tness values of the features in Feval=75%,i are de ned 423 in (6) and (7), respectively 424 F75%,i tness( f, i) =  F0% tness( f, i), if f Feval=75%,i 0, otherwise (6) 425 F75%,i un tness( f, i) = 1.0 F75%,i tness( f, i). (7) 426 D. Population and Archive Initialization 427 We initialize each population using ramped-half-and-half 428 method. While constructing the random trees, selection of ter- 429 minal nodes has been made with a probability pvar. To insert a 430 terminal node in a tree, a random number rn is drawn in [0, 1]. 431 If rn < pvar then a feature node is added, otherwise a con- 432 stant node is added. The function nodes are chosen from the 433 set F, with equal probability of inclusion for all functions. The 434 feature nodes are selected using Roulette wheel selection on 435 tness F0% tness, de ned in (2). 436 To initialize the archive from the initial population, we have 437 used the multiobjective archive initialization scheme present 438 in [72] and [75]. It requires two parameters: 1) maximum 439 archive size (Nmax) and 2) minimum archive size (Nmin). 440 E. Selection of Crossover or Mutation 441 Since ASMiGP is a steady state MOGP, in each generation 442 we generate only one offspring. We use either crossover or 443 mutation to do that. A random number rc is drawn in [0, 1]. 444 if rc < pc then crossover operator is selected otherwise the 445 mutation operator is selected for that generation. 446 F. Mating Selection 447 ASMiGP uses crossover with male and female differenti- 448 ation which needs one male and female parent. We perform 449 Roulette wheel selection using classi cation accuracy of the 450 binary classi ers as tness to select the female parent. Then 451 the male parent is selected randomly from the remaining 452 archive. The only condition to be satis ed is that the male 453 and female parents must be distinct. For mutation, we need 454 only one individual and it is selected in the same way as done 455 for the female parent in case of crossover operator. 456 A choice of mating selection could have been the use 457 of some bi-level selection operator, where Pareto dominance 458 is preferred over diversity. In that case, solutions along the 459 whole Pareto optimal solution set with good diversity would 460 have been selected as the primary (female) parents. Note that, 461 we have used crossover with male female differentiation that 462 tries to generate an off-spring near the primary parent in the 463 hypothesis space. Consequently, this might cause generation of 464 Pareto optimal binary classi ers along the whole Pareto opti- 465 mal solution set. Though they are Pareto optimal, these binary 466 classi ers may have poor accuracies. This is because of the 467 implicit imbalance nature of the binary classi cation problems 468 (due to conversion from multiclass classi cation problems) 469 and different classi er sizes. An ensemble classi er, however, 470 performs better when individual members of the ensemble 471 are more accurate. Therefore, we use classi cation accuracy- 472 based mating selection. It guides the search to generate more 473 accurate binary classi ers. 474 G. Crossover 475 In this paper, we have used crossover operation with male 476 and female differentiation. We want the off-spring to be near 477 the female (acceptor) parent in the hypothesis space. The 478 male (donor) parent is used to make the off-spring diverse 479 IEEE Proof 6 IEEE TRANSACTIONS ON CYBERNETICS from its mother. To do this, two random points (nodes) are 480 selected from each of the parents. The probabilities of selec- 481 tion of terminal nodes and function nodes as a crossover point 482 (node) are, respectively, pc t and (1 pc t ). Then, the subtree 483 rooted at the selected node of the mother tree is replaced with 484 a similarly selected subtree from the father tree. If the off- 485 spring is identical to any of its parents, the whole procedure 486 is repeated (before evaluation/learning of the off-spring). 487 H. Mutation 488 In most of the GP-based systems, for mutation a subtree 489 rooted at a randomly selected node is replaced by a new 490 randomly generated subtree. Though this kind of mutation 491 explores more in the hypothesis space, it may be too disrup- 492 tive in nature. Therefore, we intend to use less exploration 493 by keeping the tree structure unaltered. During mutation we 494 perform the following operations on a tree. 495 1) Each constant of the tree is replaced by another random 496 constant with probability pm c . 497 2) Each function node of the tree is replaced by anther 498 random function node with probability pm f . 499 3) Only one feature node of the tree is replaced by another 500 feature node. 501 For feature nodes, to select the mutation point Roulette 502 wheel selection is performed on the un tness of the features 503 which are present in the tree. Similarly, to insert a new feature 504 at the mutation point, we select a feature using Roulette wheel 505 selection based on the tness values (probability proportional 506 to tness) of the features. 507 This restricted mutation scheme ensures that the tree struc- 508 ture of an equation remains the same. It also ensures that the 509 variables in an equation do not change drastically changing 510 more than one variable would shift the solution (equation) in 511 the hypothesis space by a larger amount. 512 I. Environmental Selection 513 ASMiGP uses the archive truncation strategy used 514 in [72] and [75]. This multiobjective archive truncation strat- 515 egy uses Pareto-based tness function, i.e., tness is Pareto 516 dominance rank. This scheme maintains an archive (ensemble) 517 which has an adaptive and dynamic size. It does not allow the 518 archive to fall below a minimum size ensuring diversity in the 519 genotypic space. Moreover, the environmental selection dimin- 520 ishes the exploration in areas of objective space that are less 521 likely to yield improved solutions [72], ensuring diversity in 522 phenotypic space. Furthermore, we have made the following 523 difference in the environmental selection. ASMiGP ensures 524 that the in x expression of every off-spring (after mutation or 525 crossover) is distinct from every member of the archive. To 526 achieve this, before evaluating the offspring, ASMiGP con- 527 verts it to its in x expression and then compares the expression 528 with that of each individual of the archive. Only if the expres- 529 sion is unique, the off-spring is evaluated and added to the 530 archive. Otherwise it is discarded and a new off-spring is gen- 531 erated. Another noticeable difference is that here the number of 532 objectives is three. Note that the diversity maintenance in each 533 ensemble both in phenotypic space and in genotypic space 534 nds a diverse set of trees (bi-classi ers). Trees, being diverse, 535 enhance the performance of the corresponding ensemble. In 536 this context, it is worth mentioning that the archive, along 537 with the archive truncation strategy, helps to realize a good 538 Pareto front by explicitly maintaining diversity among the t 539 (according to rank) solutions. However, the archive alone is not 540 suf cient to evolve a good Pareto front along all the objectives. 541 J. Decision Making 542 To determine the class label, we nd the net belongingness 543 of a point to each class. The net belongingness lies in [0, 1]. 544 A higher value indicates more net belongingness to that class. 545 A data point is assigned to that class for which it has the 546 highest net belongingness. 547 After the learning, we obtain a set of c archives, A = 548 {A1, A2, . . . , Ac}; i, 1 |Ai| Nmax, where c is the num- 549 ber of classes, and Ai is the set of all binary classi ers for the 550 ith class. To determine the net belongingness of a data point p 551 to class m, Bnet m (p), it is passed through all genetic programs 552 of set Am. The net belongingness, Bnet m (p), of the point p for 553 class m is computed using 554 Bnet m (p) = 1 2 1 |Am| |Am|  i=1 Bi m(p) + 1.0 (8) 555 where Bi m is de ned as 556 Bi m(p) = +  1.0 FPi m FPmax m  , if Ai m(p) > 0  1.0 FNi m FNmax m  , otherwise. (9) 557 In (9), FPi m and FNi m, respectively, represent the number 558 of FPs and FNs made by the ith individual in set Am on the 559 training data. FPmax m and FNmax m are, respectively, the maximum 560 possible FP and the maximum possible FN for the mth class; 561 and Ai m(p) is the output from ith individual of Am for input 562 data point p. Finally, p is assigned the class k, when Bnet k = 563 c max m=1{Bnet m }. Note that FPmax m and FPmin m are determined by the 564 training data. 565 The concept of net belongingness is inspired by the con- 566 cept of negative voting. Negative voting has been widely used 567 in diverse applications [76] [78]. It is more effective when 568 circumstances unfavorable to the preferences invoke stronger 569 electoral responses than the similar favorable responses, as 570 well as the behaviors of the voters are well de ned [79]. In 571 our scheme, the learners for the ith class learn to vote yes 572 for the points of the ith class and no for the points which 573 do not belong to the ith class. For multiclass problems, it 574 is more likely that a binary classi er learns to say no than 575 to say yes for a much higher number of points. Therefore, 576 we found negative voting to be more suitable in this context. 577 However, we have used a weighted negative voting scheme. 578 The accuracies for the responses yes and no of the ith binary 579 classi er of mth class are, respectively,  1.0 FPi m/FPmax m  580 and  1.0 FNi m/FNmax m  . These values have been used as cor- 581 responding weights for the responses yes and no of the ith 582 binary classi er of the mth class. We have used the positive 583 IEEE Proof NAG AND PAL: MOGP-BASED ENSEMBLE FOR SIMULTANEOUS FS AND CLASSIFICATION 7 TABLE I PARAMETER SETTINGS FOR THE PROPOSED METHOD TABLE II SUMMARY OF MICROARRAY DATA SETS and the negative signs to indicate acceptance and rejection of 584 the data point for the mth class, respectively. 585 IV. EXPERIMENTATION 586 A. Experimental Settings 587 We have repeated tenfold cross validation of the proposed 588 method for ten times. Table I shows the parameter settings 589 that we have used for this purpose. The training data is 590 Z-score normalized. Furthermore, based on the means and the 591 standard deviations of the features of the training data, the 592 test data was Z-score normalized. Note that except Nmax and 593 Nmin, all parameters are standard parameters used in any GP 594 simulations, while Nmax and Nmin are needed for archive main- 595 tenance. Although all the parameters can be chosen using a 596 cross validation mechanism, because of huge computational 597 overhead, we could not do that. Based on a few experi- 598 ments we selected these parameters. These choices are not 599 optimal. However, since the same set of values are used for 600 widely different types of data sets, it demonstrates the effec- 601 tiveness of the proposed scheme. The proposed method has 602 been implemented in Java with the help of jMetal [80], [81]. 603 We have used eight microarray and eleven text data sets 604 which are summarized in Tables II and III, respectively. 605 B. Importance of FS and Rule Size Reduction 606 To demonstrate the importance of the proposed FS and rule 607 size reduction scheme, we have compared our method with 608 GP without FS and no restriction to equation size. To do that, 609 we have made the following changes in the proposed method. 610 1) No explicit FS as described in Section III-C is done. 611 2) There are only two objectives, FP and FN. 612 TABLE III SUMMARY OF TEXT DATA SETS All other parts of the algorithm and the parameter values 613 remain unchanged. Similar to the proposed scheme, we have 614 also executed tenfold cross validation for ten times for all the 615 data sets. Along with results (mean values of the correspond- 616 ing ten runs) obtained using the proposed method, the results 617 obtained using this scheme for microarray and for text data 618 sets are, respectively, summarized in Tables IV and V. From 619 these two tables, we observe the following. 620 1) In all cases, the test accuracy is much higher for the pro- 621 posed method. Especially, for GCM having 14 classes, 622 the difference between the test accuracies are remarkably 623 high. 624 2) The tree size increases signi cantly when the third 625 objective, i.e., the restriction on rule size, is not used. 626 3) For most of the data sets the proposed method selects 627 smaller number of distinct features per tree as well as 628 for per classi er. 629 These observations clearly demonstrate the importance of 630 the FS as well as constraining the rule size. Since our FS 631 scheme discards features with poor relevance and uses features 632 with good discriminating power yet avoiding use of redundant 633 features, it not only makes the discovery of useful rules eas- 634 ier but also implicitly constrains the rule length. Thus, FS 635 plays a very important role having two positive impacts: it 636 makes identi cation of useful rules easier and it promotes the 637 minimization of the third objective. 638 C. Comparing With Other Methods 639 To compare the performance of the proposed method 640 we have used the experimental results reported in [82] (see 641 [82, Tables IV VII]). Song et al. [82] used four different 642 types of classi cation algorithms: 1) probability-based naive 643 Bayes (NB); 2) tree based C4.5; 3) instance-based lazy learn- 644 ing algorithm IB1; and 4) rule-based RIPPER both before and 645 after FS. Along with the full feature set, they have used six 646 FS algorithms in their experiment. 647 1) FAST [82]. 648 2) FCBF [83], [84]. 649 3) CFS [85]. 650 4) ReliefF [86]. 651 5) Consist [87]. 652 6) FOCUS-SF [88]. 653 IEEE Proof 8 IEEE TRANSACTIONS ON CYBERNETICS TABLE IV EXPERIMENTAL RESULTS ON MICROARRAY DATA SETS (MEAN VALUES OF TEN RUNS OF TENFOLD CROSS VALIDATION) TABLE V EXPERIMENTAL RESULTS ON TEXT DATA SETS (MEAN VALUES OF TEN RUNS OF TENFOLD CROSS VALIDATION) Use of all features can be viewed as the seventh FS algo- 654 rithm. To make this paper comprehensive, we are not dis- 655 cussing the experimental settings used in [82]. Note that 656 accuracies for few data sets for few pairs of FS schemes are 657 not available in [82]. 658 1) Results With Microarray Data Sets: Table IV presents 659 the results of the proposed method on microarray data sets. 660 We have already stated that for each classi er, Song et al. [82] 661 used seven FS schemes (six FS method as well as the set of 662 all features). For each FS method ve repetitions of the ten- 663 fold cross validation experiment were done in [82]. And then, 664 for each FS method, the average accuracy over the ve rep- 665 etitions is reported. We compare this average accuracy with 666 the average accuracy that we have obtained by our method 667 over the ten repetitions of the tenfold cross validation experi- 668 ments. In particular, we count the number of cases (each case 669 refers to one FS scheme) in which our algorithm outperforms. 670 Note that, for some combination of data set and classi er, the 671 total number of cases is less than seven as for those combi- 672 nations some results are not available. Table VI reports these 673 counts. To elaborate Table VI, consider the entry correspond- 674 ing to column IB1 and row CLL-SUB-111. For the data set 675 CLL-SUB-111, in [82, Table VI], Song et al. reported the per- 676 formance of the algorithm IB1 for six different FS algorithms. 677 Our algorithm is found to perform better than ve of the six 678 FS algorithms and hence the entry for row CLL-SUB-111 and 679 column IB1 is 5/6. Note that, for this data set, Song et al. [82] 680 did not report performance of IB1 using all features. All but 681 the entries in the last column of Table VI are generated in 682 TABLE VI COMPARISON WITH NB, C4.5, IB1, AND RIPPER ON MICROARRAY DATA SETS the same manner. The last column of Table VI, which reports 683 the row total, reveals that our method performs the best for 684 61.40% (132 out of 215) test cases. 685 If we compute the percentage of features selected by the 686 ensembles, it may not be very small for some data sets, like 687 Colon. But, if we consider the number of features selected per 688 binary classi er (tree), we nd that this number is quite small, 689 e.g., the maximum value is 0.16% for Colon. We assume that 690 binary classi ers (binary trees) having less than twenty nodes 691 are concise enough. We need to remember that we are talking 692 about raw rules (equations) directly obtained from GP which 693 are most likely affected by bloating. Simpli cation of these 694 rules may lead to reduction in their sizes. Based on this, in 695 all but one data set we could nd easy to analyze rules. For 696 SMK-CAN-187 the extracted rules are more complex possibly 697 because of complex structure of the data. 698 IEEE Proof NAG AND PAL: MOGP-BASED ENSEMBLE FOR SIMULTANEOUS FS AND CLASSIFICATION 9 Fig. 1. Showing the number of distinct features present in the archive with respect to function evaluations for GLA-BRA-180 data set. In Table S-I (see supplementary materials), we present the 699 best rules (or binary classi ers or GPs) we found for each class 700 of each microarray data set. These GPs have the highest train- 701 ing accuracy among the GPs obtained from the rst fold of the 702 rst ten fold cross validation. If there are more than one rule 703 having the same maximum training accuracy, the rule with the 704 smallest length has been reported. The features in the equa- 705 tions are indexed starting from 0. From Table S-I, we can 706 observe that for several classes the proposed method could nd 707 substantially small rules. Noticeably, for four, six, eight, and 708 three classes, the proposed method could nd the best binary 709 classi er having only 1 4 distinct variables, respectively. So, 710 21 out of 33 (i.e., 63.64%) cases, we could nd considerably 711 small (and hence easy to interpret) rules. For some classes, 712 the rules having the highest training accuracy were not very 713 comprehensible because the length of the rule was not very 714 small. If we think that (2.0 xi) is more easy to understand 715 than (xi + xi), though both equations have the same size, we 716 observe that for 21 out of 33 (i.e., 63.64%) cases the proposed 717 method could nd rules having highest accuracy, which are 718 not at all affected by bloating and are comprehensible without 719 simpli cation. 720 To investigate how the number of distinct features changes 721 in the archive, we have executed our algorithm with 400 000 722 function evaluations using the entire GLA-BRA-180 data set. 723 In Fig. 1, we have plotted the number of distinct features 724 after initial and after each 12.5% of maximum function eval- 725 uations. For all four classes of GLA-BRA-180 data set, the 726 number of distinct features in the archive initially falls dras- 727 tically depicting the impact of FS. After some generations, it 728 becomes almost constant. Moreover, the number of nal dis- 729 tinct features for each class is quite small. This indeed reveals 730 a desirable behavior of the our scheme. 731 To show how the individual binary classi ers accuracies 732 change with function evolutions, we have plotted each indi- 733 viduals FPs and FNs for all four classes of GLA-BRA-180 734 data set in Figs. S-1 and S-2 (see supplementary materials). For 735 this part of the experiment also, we have used the entire data 736 TABLE VII COMPARISON WITH NB, C4.5, IB1, AND RIPPER ON TEXT DATA SETS set for training. From the gures, it is observed that with the 737 increase in number of evolutions, the average accuracy of the 738 solutions tends to increase. However, for class four, some solu- 739 tions having lower level of accuracies (having higher FPs and 740 FNs) are there even after 400 000 function evaluations. This is 741 because, even after such high number of function evaluations, 742 the algorithm was still searching for more concise solutions, 743 and could nd some trees of comparatively smaller size. 744 2) Results With Text Data Sets: Similar to Table VI with 745 microarray data sets, we present the same result (number of 746 test cases for which our algorithm provides the best results 747 for each classi er and data set pair) with text data sets in 748 Table VII. Table VII reveals that for text data sets the proposed 749 scheme performs the best for 95.75% (248 out of 259) cases. 750 If we consider the same criteria that binary classi ers (trees) 751 having less than twenty nodes are concise enough, then our 752 method could not nd interpretable rules for 5 (oh10.wc, 753 ohoscal.wc, la2s.wc, la1s.wc, and new3s.wc) out of the 11 754 text data sets. The number of selected features is not at all 755 small for most of the text data sets. Some reasons behind 756 this may be that the number of classes is high for the text 757 data sets and the existence of more complex class structure, 758 which is de ned by the keywords and relation of keywords to 759 the imposed classes is usually not as straightforward as genes 760 have to cancers. However, the percentage of features selected 761 per binary classi ers (trees) is quite small. Noticeably, the 762 accuracy obtained for new3s.wc (having 44 classes), is much 763 higher than that of the other methods (for accuracies of other 764 methods, see [82]). We can also observe that for data sets hav- 765 ing more than or equal to ten classes, our methods performs 766 comparatively better than other methods. 767 D. Statistical Signi cance Testing 768 To compare the proposed method with existing FS and clas- 769 si cation methods, we consider four classi cation and six 770 FS methods as well as with the full feature set. Thus we 771 compare our algorithm with 28 FS and classi cation algo- 772 rithm pairs. We have performed Wilcoxon signed ranks test 773 to show that the performance of the proposed algorithm is 774 signi cantly different over 28 pairs of FS and classi cation 775 algorithm. For this, we have considered both the microarray 776 and the text data sets together and have used the average test 777 IEEE Proof 10 IEEE TRANSACTIONS ON CYBERNETICS TABLE VIII WILCOXON SIGNED RANKS TEST (ONE-TAILED) FOR PAIRWISE COMPARISONS WITH THE PROPOSED METHOD AT = 0.05 accuracies achieved with different algorithms on these data 778 sets. Moreover, we have removed the cases for which accu- 779 racies are not known (see [82, Tables IV VII]). Table VIII 780 shows that out of the 28 cases, only in one case the null 781 hypothesis H0 was accepted. Here the null hypothesis is that 782 there is no signi cant difference in performance between our 783 algorithm and a comparing algorithm. We have also used 784 Friedman test [89], [90] to check if all the 29 (28 existing 785 and our proposed) algorithms perform similarly over seven 786 data sets: Colon, TOX-171, Leukemia1, Leukemia2, GCM, 787 tr12.wc, and tr23.wc (see [82, Tables IV VII], some accu- 788 racies for other data sets are not available). The obtained 789 Friedman statistics is 60.13, which is signi cant at = 0.001, 790 as the corresponding statistic 2 with 28 degrees of freedom 791 is 56.90. Thus, Friedman test suggests existence of signi cant 792 difference among the algorithms. 793 V. CONCLUSION 794 In this paper, we have used a MOGP, called ASMiGP, to 795 evolve diverse sets of binary classi ers to solve multiclass clas- 796 si cation problems. Simultaneous FS and rule extraction are 797 performed during the genetic evolution. An important objec- 798 tive of this paper is to nd simple classi cation rules that may 799 be human understandable, which in the given context trans- 800 lates to rules with simple operations and short length. The 801 proposed method is found to achieve its goal. 802 Ensembles perform better when weighted voting is used, the 803 members of the ensembles are diverse enough, and the classi- 804 ers are accurate [21]. Here, we have created c sets of diverse 805 ensembles. Unlike other ensemble-based method, here each 806 ensemble represents diverse classi ers for just one class. The 807 number of features used by the ensembles is high with respect 808 to the number of features selected per (binary classi er) tree. 809 This suggests that the binary classi ers are diverse enough. In 810 our algorithm, we evolve c distinct species in parallel, which 811 try to learn distinct patterns and no interspecies gene exchange 812 is ever allowed. This property makes each species different 813 from the other species and the system becomes less vulner- 814 able, especially when each species has successfully learnt its 815 designated patterns. 816 Our method has been tested on nineteen (eight microar- 817 ray and eleven text) data sets. The experimental results 818 show that we could nd easy-to-understand rules for overall 819 63.2% (87.5% for microarray and 54.5% for text) data sets. 820 We compared our method with four classi cation methods 821 in conjunction with seven FS methods including use of 822 the all-feature set. For 80.17% (61.40% for microarray and 823 95.75% for text) cases our method outperforms others. The 824 improvement in performance is shown statistically signi cant 825 compared to the others. 826 The overall performance of the proposed method is better 827 on text data sets compared to that of microarray data sets. Two 828 important differences between these two groups of data sets are 829 that: 1) microarray data sets have comparatively large feature- 830 to-sample ratios and may not have enough number of points 831 in each class for MOGP to learn the structure of the data sets 832 and 2) the text data sets have comparatively large number of 833 classes as well as instances. Our limited experiments suggest 834 that when there are enough points in each class so that each 835 species can successfully learn its target pattern, the proposed 836 method works better. We found that for text data our method 837 performs noticeably better than the other methods particularly 838 when number of classes is high (ten or more than that). 839 We have shown that our method is very effective when the 840 dimension of the data sets are as high as 49 151. But in all our 841 data sets, number of samples were not very big. If we apply 842 the proposed method on a data set with a really large num- 843 ber samples, it may require a signi cant amount of time. The 844 method may also take a substantial amount of time when the 845 number of classes is high and we have limited parallel process- 846 ing capability. With the today s high performance computing 847 technologies, however, these are not really crucial shortcom- 848 ings. Yet, we have not applied it to truly big data. This paper 849 can be adapted to deal with big data, especially using Hadoop. 850 Our future research interest is to use MOGP to classify 851 multiclass text data, where each text may belong to more than 852 one category (class) and to use net belongingness as a fuzzy 853 membership of the documents to the corresponding category. 854 We also intend to modify the proposed learning method to 855 stepwise learning, so that we can use it for big text data. 856 REFERENCES 857 [1] P. G. Espejo, S. Ventura, and F. Herrera, A survey on the application of 858 genetic programming to classi cation, IEEE Trans. Syst., Man, Cybern. 859 C, Appl. Rev., vol. 40, no. 2, pp. 121 144, Mar. 2010. 860 [2] J. R. Koza, Genetic Programming: On the Programming of Computers 861 by Means of Natural Selection. Cambridge, MA, USA: MIT Press, 1992. 862 [3] J. R. Koza, Genetic Programming II: Automatic Discovery of Reusable 863 Programs. Cambridge, MA, USA: MIT Press, 1994. 864 [4] J. R. Koza, Genetic programming as a means for programming com- 865 puters by natural selection, Stat. Comput., vol. 4, no. 2, pp. 87 112, 866 1994. 867 [5] J. R. Koza, F. H. Bennett, III, and O. Stiffelman, Genetic Programming 868 as a Darwinian Invention Machine. Berlin, Germany: Springer, 1999. 869 [6] P. J. Rauss, J. M. Daida, and S. Chaudhary, Classi cation of spectral 870 imagery using genetic programming, in Proc. Genet. Evol. Comput. 871 Conf. (GECCO), Las Vegas, NV, USA, 2000, Art ID. 48109. AQ4 872 [7] S. A. Stanhope and J. M. Daida, Genetic programming for automatic 873 target classi cation and recognition in synthetic aperture radar imagery, 874 in Evolutionary Programming VII. Berlin, Germany: Springer, 1998, 875 pp. 735 744. 876 [8] I. De Falco, A. D. Cioppa, and E. Tarantino, Discovering interest- 877 ing classi cation rules with genetic programming, Appl. Soft Comput., 878 vol. 1, no. 4, pp. 257 269, 2002. 879 [9] C. C. Bojarczuk, H. S. Lopes, and A. A. Freitas, Genetic programming 880 for knowledge discovery in chest-pain diagnosis, IEEE Eng. Med. Biol. 881 Mag., vol. 19, no. 4, pp. 38 44, Jul./Aug. 2000. 882 IEEE Proof NAG AND PAL: MOGP-BASED ENSEMBLE FOR SIMULTANEOUS FS AND CLASSIFICATION 11 [10] H. Gray, R. Maxwell, I. Martinez-Perez, C. Arus, and S. Cerdan, 883 Genetic programming for classi cation of brain tumours from nuclear 884 magnetic resonance biopsy spectra, in Proc. 1st Annu. Conf. Genet. 885 Program., 1996, p. 28 31. 886 [11] D. Hope, E. Munday, and S. Smith, Evolutionary algorithms in the 887 classi cation of mammograms, in Proc. IEEE Symp. Comput. Intell. 888 Image Signal Process. (CIISP), Honolulu, HI, USA, 2007, pp. 258 265. 889 [12] G. Wilson and M. Heywood, Introducing probabilistic adaptive map- 890 ping developmental genetic programming with redundant mappings, 891 Genet. Program. Evol. Mach., vol. 8, no. 2, pp. 187 220, 2007. 892 [13] J. Kishore, L. M. Patnaik, V. Mani, and V. Agrawal, Application of 893 genetic programming for multicategory pattern classi cation, IEEE 894 Trans. Evol. Comput., vol. 4, no. 3, pp. 242 258, Sep. 2000. 895 [14] J.-Y. Lin, H.-R. Ke, B.-C. Chien, and W.-P. Yang, Designing a classi er 896 by a layered multi-population genetic programming approach, Pattern 897 Recognit., vol. 40, no. 8, pp. 2211 2225, 2007. 898 [15] D. P. Muni, N. R. Pal, and J. Das, A novel approach to design classi ers 899 using genetic programming, IEEE Trans. Evol. Comput., vol. 8, no. 2, 900 pp. 183 196, Apr. 2004. 901 [16] M. Zhang and P. Wong, Genetic programming for medical classi ca- 902 tion: A program simpli cation approach, Genet. Program. Evol. Mach., 903 vol. 9, no. 3, pp. 229 255, 2008. 904 [17] M. Zhang and W. Smart, Using Gaussian distribution to construct 905 tness functions in genetic programming for multiclass object clas- 906 si cation, Pattern Recognit. Lett., vol. 27, no. 11, pp. 1266 1274, 907 2006. 908 [18] D. P. Muni, N. R. Pal, and J. Das, Genetic programming for simulta- 909 neous feature selection and classi er design, IEEE Trans. Syst., Man, 910 Cybern. B, Cybern., vol. 36, no. 1, pp. 106 117, Feb. 2006. 911 [19] K.-H. Liu and C.-G. Xu, A genetic programming-based approach to the 912 classi cation of multiclass microarray datasets, Bioinformatics, vol. 25, 913 no. 3, pp. 331 337, 2009. 914 [20] T. G. Dietterich, Ensemble methods in machine learning, in Multiple 915 Classi er Systems. Berlin, Germany: Springer, 2000, pp. 1 15. 916 [21] L. K. Hansen and P. Salamon, Neural network ensembles, IEEE Trans. 917 Pattern Anal. Mach. Intell., vol. 12, no. 10, pp. 993 1001, Oct. 1990. 918 [22] U. Bhowan, M. Johnston, M. Zhang, and X. Yao, Evolving diverse 919 ensembles using genetic programming for classi cation with unbal- 920 anced data, IEEE Trans. Evol. Comput., vol. 17, no. 3, pp. 368 386, 921 Jun. 2013. 922 [23] W. B. Langdon and R. Poli, Foundations of Genetic Programming. 923 Berlin, Germany: Springer, 2002. 924 [24] K. Neshatian, M. Zhang, and P. Andreae, A lter approach to multi- 925 ple feature construction for symbolic learning classi ers using genetic 926 programming, IEEE Trans. Evol. Comput., vol. 16, no. 5, pp. 645 661, 927 Oct. 2012. 928 [25] P. Nordin, A compiling genetic programming system that directly 929 manipulates the machine code, Advances in Genetic Programming, 930 vol. 1. Cambridge, MA, USA: MIT Press, 1994, pp. 311 331. 931 [26] M. O Neill and C. Ryan, Grammatical evolution, IEEE Trans. Evol. 932 Comput., vol. 5, no. 4, pp. 349 358, Aug. 2001. 933 [27] J. F. Miller and P. Thomson, Cartesian genetic programming, in 934 Genetic Programming. Berlin, Germany: Springer, 2000, pp. 121 132. 935 [28] R. Kohavi and G. H. John, Wrappers for feature subset selection, Artif. 936 Intell., vol. 97, no. 1, pp. 273 324, 1997. 937 [29] Y.-C. Chen, N. R. Pal, and I.-F. Chung, An integrated mechanism 938 for feature selection and fuzzy rule extraction for classi cation, IEEE 939 Trans. Fuzzy Syst., vol. 20, no. 4, pp. 683 698, Aug. 2012. 940 [30] N. Pal and K. Chintalapudi, A connectionist system for feature 941 selection, Neural Parallel Sci. Comput., vol. 5, no. 3, pp. 359 382, 942 1997. 943 [31] D. Chakraborty and N. R. Pal, Selecting useful groups of features in 944 a connectionist framework, IEEE Trans. Neural Netw., vol. 19, no. 3, 945 pp. 381 396, Mar. 2008. 946 [32] I. Guyon, J. Weston, S. Barnhill, and V. Vapnik, Gene selection for can- 947 cer classi cation using support vector machines, Mach. Learn., vol. 46, 948 nos. 1 3, pp. 389 422, 2002. 949 [33] H. Jabeen and A. R. Baig, Review of classi cation using genetic 950 programming, Int. J. Eng. Sci. Technol., vol. 2, no. 2, pp. 94 103, 2010. 951 [34] M. Muharram and G. D. Smith, Evolutionary constructive induc- 952 tion, IEEE Trans. Knowl. Data Eng., vol. 17, no. 11, pp. 1518 1528, 953 Nov. 2005. 954 [35] K. Neshatian and M. Zhang, Genetic programming and class-wise 955 orthogonal transformation for dimension reduction in classi cation prob- 956 lems, in Genetic Programming. Berlin, Germany: Springer, 2008, 957 pp. 242 253. 958 [36] H. Guo and A. K. Nandi, Breast cancer diagnosis using genetic 959 programming generated feature, Pattern Recognit., vol. 39, no. 5, 960 pp. 980 987, 2006. 961 [37] H. Guo, L. B. Jack, and A. K. Nandi, Feature generation using genetic 962 programming with application to fault classi cation, IEEE Trans. Syst., 963 Man, Cybern. B, Cybern., vol. 35, no. 1, pp. 89 99, Feb. 2005. 964 [38] K. Krawiec, Genetic programming-based construction of features for 965 machine learning and knowledge discovery tasks, Genet. Program. 966 Evol. Mach., vol. 3, no. 4, pp. 329 343, 2002. 967 [39] X. Tan, B. Bhanu, and Y. Lin, Fingerprint classi cation based on 968 learned features, IEEE Trans. Syst., Man, Cybern. C, Appl. Rev., vol. 35, 969 no. 3, pp. 287 300, Aug. 2005. 970 [40] M. G. Smith and L. Bull, Genetic programming with a genetic algo- 971 rithm for feature construction and selection, Genet. Program. Evol. 972 Mach., vol. 6, no. 3, pp. 265 281, 2005. 973 [41] Y. Lin and B. Bhanu, Evolutionary feature synthesis for object recog- 974 nition, IEEE Trans. Syst., Man, Cybern. C, Appl. Rev., vol. 35, no. 2, 975 pp. 156 171, May 2005. 976 [42] J. R. Koza, Concept formation and decision tree induction using 977 the genetic programming paradigm, in Parallel Problem Solving from 978 Nature. Berlin, Germany: Springer, 1991, pp. 124 128. 979 [43] G. Folino, C. Pizzuti, and G. Spezzano, Genetic programming and sim- 980 ulated annealing: A hybrid method to evolve decision trees, in Genetic 981 Programming. Berlin, Germany: Springer, 2000, pp. 294 303. 982 [44] J. Eggermont, Evolving fuzzy decision trees with genetic programming 983 and clustering, in Genetic Programming. Berlin, Germany: Springer, 984 2002, pp. 71 82. 985 [45] H. Zhao, A multi-objective genetic programming approach to develop- 986 ing pareto optimal decision trees, Decis. Support Syst., vol. 43, no. 3, 987 pp. 809 826, 2007. 988 [46] T. M. Khoshgoftaar and Y. Liu, A multi-objective software quality clas- 989 si cation model using genetic programming, IEEE Trans. Rel., vol. 56, 990 no. 2, pp. 237 245, Jun. 2007. 991 [47] A. Tsakonas, A comparison of classi cation accuracy of four genetic 992 programming-evolved intelligent structures, Inf. Sci., vol. 176, no. 6, 993 pp. 691 724, 2006. 994 [48] S. Sakprasat and M. C. Sinclair, Classi cation rule mining for automatic 995 credit approval using genetic programming, in Proc. IEEE Congr. Evol. 996 Comput. (CEC), Singapore, 2007, pp. 548 555. 997 [49] C. Qing-Shan, Z. De-Fu, W. Li-Jun, and C. Huo-Wang, A modi ed 998 genetic programming for behavior scoring problem, in Proc. IEEE 999 Symp. Comput. Intell. Data Min. (CIDM), Honolulu, HI, USA, 2007, 1000 pp. 535 539. 1001 [50] E. Carreno, G. Leguizam n, and N. Wagner, Evolution of classi cation 1002 rules for comprehensible knowledge discovery, in Proc. IEEE Congr. 1003 Evol. Comput. (CEC), Singapore, 2007, pp. 1261 1268. 1004 [51] R. R. Mendes, F. B. de Voznika, A. A. Freitas, and J. C. Nievola, 1005 Discovering fuzzy classi cation rules with genetic programming and 1006 co-evolution, in Principles of Data Mining and Knowledge Discovery. 1007 Berlin, Germany: Springer, 2001, pp. 314 325. 1008 [52] A. L. Garcia-Almanza and E. P. Tsang, Evolving decision rules to 1009 predict investment opportunities, Int. J. Autom. Comput., vol. 5, no. 1, 1010 pp. 22 31, 2008. 1011 [53] U. Bhowan, M. Johnston, and M. Zhang, Developing new tness func- 1012 tions in genetic programming for classi cation with unbalanced data, 1013 IEEE Trans. Syst., Man, Cybern. B, Cybern., vol. 42, no. 2, pp. 406 421, 1014 Apr. 2012. 1015 [54] W. Fu, M. Johnston, and M. Zhang, Low-level feature extraction 1016 for edge detection using genetic programming, IEEE Trans. Cybern., 1017 vol. 44, no. 8, pp. 1459 1472, Aug. 2014. 1018 [55] P. Wang, K. Tang, T. Weise, E. Tsang, and X. Yao, Multiobjective 1019 genetic programming for maximizing ROC performance, 1020 Neurocomputing, vol. 125, pp. 102 118, Feb. 2014. 1021 [56] P. Wang et al., Convex hull-based multi-objective genetic programming 1022 for maximizing receiver operating characteristic performance, IEEE 1023 Trans. Evol. Comput., to be published. AQ5 1024 [57] P. A. Whigham and G. Dick, Implicitly controlling bloat in genetic 1025 programming, IEEE Trans. Evol. Comput., vol. 14, no. 2, pp. 173 190, 1026 Apr. 2010. 1027 [58] S. Luke and L. Panait, A comparison of bloat control methods for 1028 genetic programming, Evol. Comput., vol. 14, no. 3, pp. 309 344, 2006. 1029 [59] R. Poli, A simple but theoretically-motivated method to control bloat 1030 in genetic programming, in Genetic Programming. Berlin, Germany: 1031 Springer, 2003, pp. 204 217. 1032 [60] M. J. Streeter, The root causes of code growth in genetic program- 1033 ming, in Genetic Programming. Berlin, Germany: Springer, 2003, 1034 pp. 443 454. 1035 IEEE Proof 12 IEEE TRANSACTIONS ON CYBERNETICS [61] S. Luke and L. Panait, Fighting bloat with nonparametric parsimony 1036 pressure, in Parallel Problem Solving from NaturePPSN VII. Berlin, 1037 Germany: Springer, 2002, pp. 411 421. 1038 [62] F. Fern ndez-de Vega, G. G. Gil, J. A. G. Pulido, and J. L. Guisado, 1039 Control of bloat in genetic programming by means of the island model, 1040 in Parallel Problem Solving from Nature-PPSN VIII. Berlin, Germany: 1041 Springer, 2004, pp. 263 271. 1042 [63] F. Fernandez, G. Galeano, and J. Gomez, Comparing synchronous and 1043 asynchronous parallel and distributed genetic programming models, in 1044 Genetic Programming. Berlin, Germany: Springer, 2002, pp. 326 335. 1045 [64] M. Castelli, L. Vanneschi, and S. Silva, Semantic search-based genetic 1046 programming and the effect of intron deletion, IEEE Trans. Cybern., 1047 vol. 44, no. 1, pp. 103 113, Jan. 2014. 1048 [65] D. Rochat, M. Tomassini, and L. Vanneschi, Dynamic size populations 1049 in distributed genetic programming, in Genetic Programming. Berlin, 1050 Germany: Springer, 2005, pp. 50 61. 1051 [66] F. Fernandez, L. Vanneschi, and M. Tomassini, The effect of plagues in 1052 genetic programming: A study of variable-size populations, in Genetic 1053 Programming. Berlin, Germany: Springer, 2003, pp. 317 326. 1054 [67] A. Song, Q. Shi, and W. Yin, Understanding of GP-evolved motion 1055 detectors, IEEE Comput. Intell. Mag., vol. 8, no. 1, pp. 46 55, 1056 Feb. 2013. 1057 [68] J. Luna, J. Romero, C. Romero, and S. Ventura, On the use of genetic 1058 programming for mining comprehensible rules in subgroup discovery, 1059 IEEE Trans. Cybern., vol. 44, no. 12, pp. 2329 2341, Dec. 2014. 1060 [69] S. Pang, D. Kim, and S. Y. Bang, Membership authentication in the 1061 dynamic group by face classi cation using SVM ensemble, Pattern 1062 Recognit. Lett., vol. 24, no. 1, pp. 215 225, 2003. 1063 [70] H. Ishibuchi and T. Yamamoto, Evolutionary multiobjective optimiza- 1064 tion for generating an ensemble of fuzzy rule-based classi ers, in 1065 Genetic and Evolutionary Computation GECCO. Berlin, Germany: 1066 Springer, 2003, pp. 1077 1088. 1067 [71] P. Yang, Y. H. Yang, B. B. Zhou, and A. Y. Zomaya, A review of 1068 ensemble methods in bioinformatics, Current Bioinformat., vol. 5, no. 4, 1069 pp. 296 308, 2010. 1070 [72] K. Nag, T. Pal, and N. Pal, ASMiGA: An archive-based steady- 1071 state micro genetic algorithm, IEEE Trans. Cybern., vol. 45, no. 1, 1072 pp. 40 52, Jan. 2015. 1073 [73] R. Enache, Steady state evolutionary algorithms, Honda Res. 1074 Inst. Europe GmbH, Offenbach am Main, Germany, Tech. 1075 Rep. HRI-EU 04-02, Jan. 2004. AQ6 1076 [74] J.-H. Hong and S.-B. Cho, Gene boosting for cancer classi cation 1077 based on gene expression pro les, Pattern Recognit., vol. 42, no. 9, 1078 pp. 1761 1767, 2009. 1079 [75] K. Nag and T. Pal, A new archive based steady state genetic algorithm, 1080 in Proc. IEEE Congr. Evol. Comput. (CEC), Brisbane, QLD, Australia, 1081 2012, pp. 1 7. 1082 [76] S. Kernell, Presidential popularity and negative voting: An alterna- 1083 tive explanation of the midterm congressional decline of the president s 1084 party, Amer. Polit. Sci. Rev., vol. 71, no. 1, pp. 44 66, 1977. 1085 [77] M. Fang, H. Takauj, S. Kaneko, and H. Watanabe, Robust optical ow 1086 estimation for underwater image, in Proc. Int. Symp. Optomechatronic 1087 Technol. (ISOT), Istanbul, Turkey, 2009, pp. 185 190. 1088 [78] M. Fang, H. Takauji, and S. Kaneko, Rapid computation of robust 1089 optical ow by ef cient complementary voting, in Proc. World Autom. 1090 Congr. (WAC), Kobe, Japan, 2010, pp. 1 6. 1091 [79] M. P. Fiorina and K. A. Shepsle, Is negative voting an artifact? Amer. 1092 J. Polit. Sci., vol. 33, pp. 423 439, May 1989. 1093 [80] J. Durillo, A. Nebro, and E. Alba, The jMetal framework for multi- 1094 objective optimization: Design and architecture, in Proc. IEEE Congr. 1095 Evol. Comput. (CEC), Barcelona, Spain, Jul. 2010, pp. 4138 4325. 1096 [81] J. J. Durillo and A. J. Nebro, jMetal: A java framework for 1097 multi-objective optimization, Adv. Eng. Softw., vol. 42, pp. 760 771, 1098 2011. [Online]. Available: http://www.sciencedirect.com/science/article/ 1099 pii/S0965997811001219 1100 [82] Q. Song, J. Ni, and G. Wang, A fast clustering-based feature sub- 1101 set selection algorithm for high-dimensional data, IEEE Trans. Knowl. 1102 Data Eng., vol. 25, no. 1, pp. 1 14, Jan. 2013. 1103 [83] L. Yu and H. Liu, Feature selection for high-dimensional data: A fast 1104 correlation-based lter solution, in Proc. 20th Int. Conf. Mach. Learn., 1105 vol. 20. Washington, DC, USA, 2003, pp. 856 863. 1106 [84] L. Yu and H. Liu, Ef cient feature selection via analysis of rele- 1107 vance and redundancy, J. Mach. Learn. Res., vol. 5, pp. 1205 1224, 1108 Dec. 2004. 1109 [85] M. A. Hall, Correlation-based feature selection for machine learn- 1110 ing, Ph.D. dissertation, Dept. Comput. Sci., Univ. Waikato, Hamilton, 1111 New Zealand, 1999. 1112 [86] M. Robnik- ikonja and I. Kononenko, Theoretical and empirical analy- 1113 sis of ReliefF and RreliefF, Mach. Learn., vol. 53, nos. 1 2, pp. 23 69, 1114 2003. 1115 [87] M. Dash, H. Liu, and H. Motoda, Consistency based feature selection, 1116 in Knowledge Discovery and Data Mining. Current Issues and New 1117 Applications. Berlin, Germany: Springer, 2000, pp. 98 109. 1118 [88] H. Almuallim and T. G. Dietterich, Learning boolean concepts in 1119 the presence of many irrelevant features, Artif. Intell., vol. 69, no. 1, 1120 pp. 279 305, 1994. 1121 [89] M. Friedman, The use of ranks to avoid the assumption of normal- 1122 ity implicit in the analysis of variance, J. Amer. Stat. Assoc., vol. 32, 1123 no. 200, pp. 675 701, 1937. 1124 [90] M. Friedman, A comparison of alternative tests of signi cance for the 1125 problem of m rankings, Ann. Math. Stat., vol. 11, no. 1, pp. 86 92, 1126 1940. 1127 Kaustuv Nag received the B.Tech. degree in com- 1128 puter science and engineering from the West Bengal 1129 University of Technology, Kolkata, India, and the 1130 M.Tech. degree in computer science and engineering 1131 from the National Institute of Technology, Durgapur, 1132 Durgapur, India, in 2010 and 2012, respectively. He 1133 is currently pursuing the Ph.D. degree with Jadavpur 1134 University, Kolkata. 1135 He was a Visiting Researcher at the Indian 1136 Statistical Institute, Kolkata. His current research 1137 interests include genetic algorithm, genetic program- 1138 ming, and arti cial neural networks. 1139 Mr. Nag was the recipient of INSPIRE Fellowship. 1140 Nikhil R. Pal (M 91 SM 00 F 05) AQ7 is a Professor 1141 with the Electronics and Communication Sciences 1142 Unit, Indian Statistical Institute, Kolkata, India. His 1143 current research interest includes bioinformatics, 1144 brain science, fuzzy logic, pattern analysis, neural 1145 networks, and evolutionary computation. 1146 Dr. Pal AQ8 was the Editor-in-Chief of the IEEE 1147 TRANSACTIONS ON FUZZY SYSTEMS from 2005 1148 to 2010. He has served/been serving on the 1149 Editorial/Advisory Board/Steering Committee of 1150 several journals, including the International Journal 1151 of Approximate Reasoning, Applied Soft Computing, the International Journal 1152 of Knowledge-Based Intelligent Engineering Systems, the International 1153 Journal of Neural Systems, Fuzzy Sets and Systems, the International Journal 1154 of Intelligent Computing in Medical Sciences and Image Processing, the 1155 Fuzzy Information and Engineering: An International Journal, the IEEE 1156 TRANSACTIONS ON FUZZY SYSTEMS, and the IEEE TRANSACTIONS ON 1157 CYBERNETICS. He has given several plenary/keynote speeches in different 1158 premier international conferences in computational intelligence. He has served 1159 as the General Chair, a Program Chair, and a Co-Program Chair of several 1160 conferences. He was a Distinguished Lecturer of the IEEE Computational 1161 Intelligence Society (CIS). He is currently the Vice President of the IEEE 1162 CIS. He was a member of the Administrative Committee of the IEEE CIS. 1163 He is a fellow of the National Academy of Sciences, India, the Indian National 1164 Academy of Engineering, the Indian National Science Academy, and the 1165 International Fuzzy Systems Association. 1166 IEEE Proof AUTHOR QUERIES AQ1: Please con rm the funding information is correct as set. AQ2: Please verify and con rm whether the edits made to the line nos. 86 87 retains your intended meaning. AQ3: Note that both offspring and off-spring are used throughout the paper. Please indicate if changes are needed. AQ4: Please con rm that the edits made to Reference [6] is correct as set. AQ5: Please provide the volume number, issue number, page range, month, and the publication year for Reference [56]. AQ6: Please provide the department name for Reference [73]. AQ7: Please provide the educational background for the author N. R. Pal. AQ8: Please verify and con rm that the edits made to N. R. Pal biography retain the intended meaning.