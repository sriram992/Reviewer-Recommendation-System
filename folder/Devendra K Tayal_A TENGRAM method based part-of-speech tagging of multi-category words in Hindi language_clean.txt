A TENGRAM method based part-of-speech tagging of multi-category words in Hindi language J.P. Gupta a, Devendra K. Tayal b, Arti Gupta c, a Jaypee Institute of Information Technology, Noida, Uttar Pradesh, India b Department of Computer Engineering, GGSIP University, New Delhi, India c Department of Computer Science & IT, Jaypee Institute of Information Technology, Noida, Uttar Pradesh, India a r t i c l e i n f o Keywords: Part-of-speech Multi-category Devanagari TENGRAM Discernibility matrix Discernibility function Reducts Decision table Rough sets Decision rules Condition attributes Decision attribute a b s t r a c t In this paper, we have dealt on the problem of part-of-speech tagging of multi-category words which appear within the sentences of Hindi language. Firstly, a Hindi tagger is proposed which provides part- of-speech tags developed using grammar of Hindi language. For this purpose, Hindi Devanagari alphabets are used and their Hindi transliteration is done within the proposed tagger. Thereafter, a Rules based TENGRAM method is described with an illustrative example, which guides to disambiguate multi-cate- gory words within sentences of Hindi corpus. The rules generated in TENGRAM are the result of compu- tation of discernibility matrices, discernibility functions and reducts. These computations have been generated from decision tables which are based on theory of Rough sets. Basically, a discernibility matrix helps in cutting down indiscernible condition attributes; a discernibility function has rows corresponding to each column in the discernibility matrix which develops reducts; and the reducts provide a minimal subset of attributes which preserve indiscernibility relation of decision tables and hence they generate the decision rules.  2011 Elsevier Ltd. All rights reserved. 1. Introduction Part-of-speech tagging (POS tagging or POST) (Araujo, 2002; Tapanainen & Voutilainen, 1994) is the process of marking up the words in a corpus corresponding to a particular part of speech, based on both its de nition as well as its context (relationship with adjacent and related words in a sentence). POS provides signi cant information about a word and its adjacent members, clues on how a word is pronounced, word sense disambiguation, information extraction and question answering system. POS tagging is harder than just having a list of words and their parts-of-speech, because some words can represent more than one part-of-speech at differ- ent times. Such words are called multi-category words (MCW) and are ambiguous in nature. Our aim is to identify such ambiguous MCWs occurring in Hindi corpus and to disambiguate them using Rules based TENGRAM method which is proposed by us in the present paper. In 2006, statistical algorithms such as Maximum Entropy Mar- kov Model (MEMM) (Dalal, Nagraj, Sawant, & Shelke, 2006) was proposed which tagged previously unseen Hindi text using fea- tures based on the context, word, dictionary and corpus. These fea- tures captured the known information and then MEMM was applied to obtain the label sequence. Another, a comparative study of Unigram, Bigram, statistical Hidden Markov Model (HMM) and rule-based Brill s approaches for POS tagging was done in (Fahim, Naushad, & Mumit, 2007; Konchady, 2009). All these algorithms consider POS taggers for Hindi, but the tags used are con ned to NN: noun, PRP: pronoun, VRB: verb, PREP: postposition etc., which are focused on English grammar. In this paper, we propose a Hindi tagger, having Hindi grammar which is based on Devanagari alpha- bets and their transliteration (elaborated in the next section). Devanagari is the script used for writing Hindi and the characters of Devanagari can be transcribed into English by replacing individ- ual Devanagari letters with corresponding letters from the English language alphabets (Appendix A). Also, a Hindi professional can read text which is displayed with our fonts without much dif - culty, which in turn helps in revitalization of Hindi, based on com- putational aspects of the language. In the present paper, we use Rough sets (Pawlak, 1982, 1991; Polkowski, 2002; Polkowski & Skowron, 1998; Polkowski, Tsumot- o, & Lin, 2000) to generate decision tables which are used to devel- op rules in the proposed TENGRAM method. The earlier approaches have mainly utilized the decision trees (Quinlan, 1987; Subramani- an, Nosek, Raghunathan, & Kanitkar, 1992) for the same purpose. Decision tree solves simpler problems better but when number of actions is large, many combinations of conditions exist, and there is a risk of ambiguities and omissions. For such complex cases decision tables are preferred over decision trees. Rough sets 0957-4174/$ - see front matter  2011 Elsevier Ltd. All rights reserved. doi:10.1016/j.eswa.2011.05.036 Corresponding author. Mobile: +91 9313519476. E-mail addresses: arti.gupta@jiit.ac.in, frds_4_arti@yahoo.com (A. Gupta). Expert Systems with Applications 38 (2011) 15084 15093 Contents lists available at ScienceDirect Expert Systems with Applications journal homepage: www.elsevier.com/locate/eswa (Demri & Orlowska, 2002; Lin & Cercone, 1997; Orlowska, 1997; Sivanandam & Deepa, 2008) used by us in the proposed model, re- solve imprecision approximately by expressing quantitative con- cepts and does feature selection and reduction through computation of discernibility matrix and discernibility function (Skowron & Rauszer, 1992) which result in the formulation of re- ducts of attributes that are ef cient for rules formulation and cor- rection. Further enhancement in the rule-base is done by eliminating inconsistent rules as discussed in detail in the later sections of this paper. 2. Background and related work This section is organized as follows: Section 2.1 discuuses about literature survey in Hindi. Section 2.1.1.describes existing interme- diate tools and models for initial processing of POST in Hindi, Sec- tion 2.1.2 describes existing jargon for POST in Hindi. The terminology for Hindi that is used by us in the present paper is elaborated in Section 2.1.3. Literature survey of Rough sets (Sec- tion 2.2) describes with the steps to be followed for rules genera- tion using TENGRAM method (Section 3.2, Steps I IV), along with inconsistent rules elimination (Section 3.2, Step V). 2.1. Literature survey of Hindi 2.1.1. Existing tools and models for POST in Hindi Hindi is a morphologically rich Indian language and hence needs analyzingtoolsfor harnessingits structural information. Some inter- mediate tools developed for initial processing of structural informa- tion inHindi are StemmerandMorphological-Analyser (Shrivastava, Agrawal, Mohapatra, Singh, & Bhattacharya, 2005). Stemmer pro- vides the root, suf x and grammatical category of an input word. Morphological-Analyser gives detailed analysis of the word based onits in ectionandcontextof itsusage. Both,StemmerandMorpho- logical-Analyzer tools are unable to produce ef cient results for large number of words, either because words are not present in the word list or due to some spelling errors. Models such as SUFFIX (Rao & Yarowsky, 2007) uses Na ve-Bayes assumption, but for Out- of-Vocabulary (OOV) words. SUFFIX assigns a tag that maximizes the likelihood of that tag, given a xed length suf x of the OOV word.1 2.1.2. Existing jargon for POST in Hindi In their paper (Ray, Harish, Sarkar, & Basu, 2003) proposed an algorithm for local word grouping to untangle xed word order (FWO)dependencies inHindisentences.Hindibeingafreeorderlan- guage, FWO group extraction decreases load on the free order parser and the POST is an essential requirement for local word grouping. The authors denote the set of all relevant and existing POS in Hindi as C, where C = {n,v,a,j,c,pp,q,y}; n: noun or pronoun, v: verb, a: ad- verb, j: adjective, c: conjunction, pp: postposition, q: quali er, y: other parts-of-speech. FOLLOW (Ray et al., 2003), a binary relation- ship and a subset of C C, puts restriction on whether a POS tag can follow another POS tag in a Hindi sentence. FOLLOW is constructed by empirical observation of Hindi and has been hand-tested and found to be correct on sentences of varying complexity. Table 1 depicts FOLLOW (x) = {set of all POS tags that can follow x in a Hindi sentence} or FOLLOW (x) = {y: (Row x, Column y) is 1}. The rows of FOLLOW can be illustrated as- (i) In row1, current word has noun/pronoun (n) as its POS, therefore the words following to (n) can have any of the {n,j,v,a,c,pp,q,y}as POS tags within the context. (ii) In row2, current POS is adjective (j), there- fore words following to (j) within a given context have possibly {n,j,c,q} as POS tag; no other tag can be assigned. The other rows of the Table 1 can be illustrated similarly. 2.1.3. Some more terminology Some taggers already exist for Hindi, but while deciding their POS tags, the English grammar has always been considered as the base. Some of these de ned tags for Hindi are- NN: Noun (e.g. boy, river, thought, hardness), PRP: Pronoun (e.g. who, that, he), QW: Question Words (e.g. what, who), RB: Adverb (e.g. slowly slowly, fast), INIF: Intensi er (e.g. too much, much more), NEG: Negative (e.g. no, not), CC: Conjucts (e.g. and, or), QF: Quanti er (e.g. more, little, all, much), etc. Our focus in this paper is to use Hindi grammar for Hindi words, since Hindi language uses a differ- ent word-order (subject-object-verb) over the English language word-order (subject-verb-object). In other words, in Hindi, verbs are placed at the end of a sentence. Also, Hindi uses postpositions instead of prepositions, which are similar to prepositions except that they are written after the noun. In addition, Hindi translitera- tion done in this paper helps non-Hindi readers to pronounce Hindi alphabets into the known (English language) character format. In this paper we de ne a Hindi tagger with POS tags, available within the Hindi grammar described in (Visvendru, 1993). Hindi Devanagari alphabets are used to de ne POS and their Hindi trans- literation is done using English alphabets (both lower and upper cases) as shown in Figs. 1 and 2. List of Hindi Devnagri alphabets with Hindi transliteration is available in Appendix A. So, C: set of POS tags in Hindi, as stated in the jargon for POST in Hindi (Section 2.1.2) is rede ned as C0 in Fig. 1. C0 is de ned as, C0 = { saMGYA /noun, srvnAma /pronoun, visheSNa /adjective, kriyA /verb, kriyA_visheSNa /adverb, saMbaXa_boXaka /post- position, samucncaya_boXaka /conjunction, visnmyAXi_boX- aka /interjection, virAma_cinha /punctuation}. It is important to note that the words enclosed within are especially Hindi Table 1 FOLLOW relation in Hindi. n j v a c pp q y n 1 1 1 1 1 1 1 1 j 1 1 0 0 1 0 1 0 v 1 1 1 1 1 1 1 1 a 1 1 1 1 1 1 1 1 c 1 1 1 1 1 0 1 1 pp 1 1 1 1 1 1 1 1 q 0 1 0 1 0 0 1 0 y 1 1 1 1 1 0 1 1 Fig. 1. Broad level POS tags in Devanagari and their Hindi transliteration. 1 A Part of Speech Tagger for Indian Languages http://shiva.iiit.ac.in/SPSAL2007/ iiit_tagset_guidelines.pdf. Hindi Pad http://www.softpedia.com/progDownload/ HindiPad-Download-16310.html. Part-of-Speech Tutorial http://alias-i.com/lingpipe/ demos/tutorial/posTags/read-me.html. Rough Sets: A Tutorial http://folli.loria.fr/cds/ 1999/library/pdf/skowron.pdf. J.P. Gupta et al. / Expert Systems with Applications 38 (2011) 15084 15093 15085 transliterated words and are case sensitive (upper and lower case letters of English). Fig. 2 depicts a sample of detailed POS tags de ned in Hindi grammar using Devanagari, along with their Hindi transliteration and acronyms. For an instance, saMGYA / sG is a broad POS tag (Fig. 1) and its detailed tags are vnyaknwi_vAcaka_saMGYA / vv_sG , jAwi_vAcaka_saMGYA / jv_sG , BAv_vAcaka_saMGYA / Bv_sG , samuxAya_vAcaka_saMGYA / sv_sG and xravnya_ vAcaka_saMGYA / xv_sG . On the other hand, some tags like saMbaMXa_boXaka / sM_bX have no further detailing and hence have no detailed view. Similarly, other tags in Fig. 2 can be illustrated. Once, POS tags have been de ned in Hindi syntax along with their acronyms, we propose a modi ed FOLLOW relation as shown in Ta- ble 2. Table 2 is generated by modifying Table 1 of (Ray et al., 2003), using language based heuristic rules of (Tapanainen & Voutilainen, 1994) and Hindi grammar from (Visvendru, 1993). In this paper we describe saMGYA / sG , srvnAma / sr , visheSNa / vN , kriyA / ky , kriyA_visheSNa / kvN , saMbaMxa_boXaka / sM_ bX , samucncaya_boXaka / smu_bX , visnmyAXi_boXaka / vmy_bX , virAma_cinha / vr_cn , as two-dimensional attributes having values as 1 or 0, depending on whether FOLLOW relation is observed or not. Once POS tags in the Hindi grammar and the modi ed FOLLOW relation are de ned in the proposed Hindi tagger, then the Devana- gari alphabets are used to construct Hindi corpus. Sample words from Hindi corpus are shown in Fig. 3 which in- cludes some of the mAtrAeM supported in Hindi like A , E , u etc. 2.2. Rough sets- towards POS tagging Rough sets (Pawlak, 1982, 1991) is a mathematical tool intro- duced by Pawlak in 1980 s. It concerns with the analysis and mod- eling of classi cation and decision problems involving vagueness, impreciseness and uncertainity or partial information. Rough sets offers two kinds of knowledge representations called information system and decision system. An information system (IS) is the most basic kind of knowledge that consists of a set of objects where each object is a collection of attributes values. An IS is an ordered pair, A U; A , where U is a nonempty nite set called universe and A is a nonempty nite set of attributes. Each attribute a 2 A is a total function a: U ? Va, where Va is the set of values of a, called range of a. The element in universe is referred as objects. A decision system (DS), A U; A; fdg is an IS, for which the attributes are separated into disjoint sets of condition attributes A and decision attributes d, (A \ {d} = /). In this paper, our focus is on the decision system which enables rules generation to determine POS tags of MCWs within Hindi corpus. 3. Methodology 3.1. Framework In this section we provide an overall ow of our research work. We use the Tool KrutiPAD 010 http://www.softpedia.com/ Fig. 2. Sample of detailed Hindi POS tags Devanagari, Hindi transliteration and acronyms. Table 2 Modi ed FOLLOW relation for broad POS tags in Hindi. sG sr vN ky kvN sM_bX smu_bX vmy_bX vr_cn sG 1 1 1 1 1 1 1 0 1 sr 1 1 1 1 1 1 1 0 0 vN 1 1 1 0 0 1 0 0 1 ky 1 1 1 1 1 1 1 0 1 kvN 1 1 1 1 1 1 1 0 1 sM_bX 1 1 1 1 1 1 0 0 0 smu_bX 1 1 1 1 1 1 1 0 0 vmy_bX 1 1 1 1 1 1 0 0 0 vr_cn 1 1 1 1 1 1 0 1 1 Fig. 3. Sample words of Hindi transliteration to Devanagari from Hindi training corpus. 15086 J.P. Gupta et al. / Expert Systems with Applications 38 (2011) 15084 15093 progDownload/HindiPad-Download-16310.html, a Hindi editor to enable us to write Hindi alphabets, once Hindi fonts are installed on the computer system. The work- ow starts from preparing a list of Hindi alphabets in Devanagari which once transliterated, enables construction of: (i) POS tagger in Hindi grammar, (ii) Hindi training corpus. The POS tagger proposed here is then applied over the built Hindi corpus, converting that corpus into manual-tagged corpus. From the tagged corpus, we have identi ed multi-category words through implementation in C - language. Further, to disambiguate MCWs , we have applied TENGRAM method (Section 3.2). Firstly, TEN- GRAM prepares decision table for each MCW having condition and decision attributes. After that, TENGRAM reduces the decision table to discernibility matrix (to avoid redundancy of indiscernible objects), discernibility function is then extracted from discernibil- ity matrix (rows of discernibility function corresponds to columns of discernibility matrix). Later on, TENGRAM computes reducts from discernibility function which then generates decision or dependency rules. If decision rules generated above are found to be inconsistent then they must be eliminated from TENGRAM, otherwise the rules are left unchanged. Hence, we obtain processed Hindi corpus with the rules identi- ed for MCWs in the TENGRAM. Thus generation of consistent decision rules can be used as-it-is in any other tested Hindi corpus. Fig. 4 describes the framework of our proposed work applied to Hindi language. 3.2. TENGRAM method TENGRAM method uses rough set features for rules generation (Qian & Zheng, 2004) of POST in Hindi language. TENGRAM con- sists of the following four important steps: (i) Formulation of TENGRAM decision table. (ii) Computation of TENGRAM discernibility matrix, for each decision class. (iii) Computation of TENGRAM discernibility function, for each object of the decision table belonging to a certain decision class. (iv) Generation of TENGRAM dependency rules from the reducts generated. At the end, we have added one more step, i.e. elimination of TENGRAM inconsistent rules for enhancing accuracy of decision rules generated. The above mentioned steps are detailed as follows: Step I. Formulation of TENGRAM decision table First step of TENGRAM method consists of formulation of deci- sion table having POS tags of ve consecutive words (before a par- ticular MCW) and POS tags of ve other consecutive words (after the same MCW) within Hindi sentences. They are called as the con- dition attributes. Also, POS tag of the MCW under consideration is called as a decision attribute. The TENGRAM decision table is as shown in Fig. 5. In other words, TENGRAM decision table is a decision system (DT), represented as DT = (U,CA [ DA), where U = {s1,s2,. . .,sn} is a collection of sentences si,with MCW in Hindi corpus. CA, DA  A, where A = {c5,c4,. . .,c1,c0,c1,c2,. . .,c5} is an attri- bute (column) set, and cj(j 2 [5,5]) is an attribute. CA = {c5,c4,. . .,c1, c1,c2,. . .,c5} is a subset of condition attri- butes, made up of ve POS tags before the considered MCW and ve POS tags after that MCW. DA = {c0} is the subset of decision attributes; c0 is the current POS tag of the MCW. Similarly, the rest of the entries in the Fig. 5 can be explained. It may be possible that some condition attributes (farthest from decision attribute on either side) may not be assigned any of the POS tags since the words corresponding to these attributes donot occur (i.e. null) within Hindi sentences. Such condition attributes are assigned null value (/) and is explained with an example below. Fig. 4. Research framework of POST and TENGRAM method for Hindi language. J.P. Gupta et al. / Expert Systems with Applications 38 (2011) 15084 15093 15087 Example 1. Consider sentences from Hindi corpus having Hindi word iwanI as shown in Fig. 6. Based on TENGRAM, above sentences of Fig. 6 are treated as s1, s2, s3 as given in Fig. 7. Here, we consider only ve consecutive words on either side of the word . If some boundary words donot exist they are left blank (. . .), can be seen in the above sentences s2 and s3 both. Hindi transliteration of s1,s2 and s3 along with their POS tags (based on context information) and acronyms are shown in Table 3. In Table 3, POS tags are computed, based on the context infor- mation. The POS tags which are used here are: Bv_sG , jv_SG , xv_sG , gN_sG as types of saMGYA /noun; njv_sr as type of srvnAma /pronoun; prv_vN , gN_vN , sKv_vN as types of visheSNa /adjective; mu_ky , vN_ky as types of kriyA /verb where mu_ky is the main verb of a sentence and vN_ky is a verb used as an adjective; sM_bX is a saMbaMXa_boXaka /post- position; vB_smu_bX is a type of samucncya_boXaka /conjunc- tion. It can be seen that iwanI is a multi-category word, based on the usage of its context. So, rstly, we construct a decision table for MCW iwanI as shown in Table 4. Table 4 has condition attributes as POS tags (acronyms) of adja- cent words (adjacent to word iwanI ) on both sides of iwanI , within sentences of Hindi corpus (refer Table 3). Also, decision attri- bute is de ned as POS tags (acronyms) of the MCW iwanI . Sen- tence s1 has ve previous POS tags as c5 = Bv_sG , c4 = njv_sr , c3 = jv_sG , c2 = sM_bX , c1 = jv_sG ; ve next tags as c1 = Bv_sG , c2 = sM_bX , c3 = mu_ky , c4 = vB_smu_bX , c5 = jv_sG ; and POS tag of iwanI (within s1) as c0 = prv_vN . Similarly, POST for s2 and s3 are de ned in the Table 4. It is also seen that c5 and c4 in s2 and s3 are /, since words corresponding to these tags donot exist within s2 and s3 respectively. Step II. Computation of TENGRAM discernibility matrix TENGRAM decision table constructed above has so many indis- cernible objects which must be reduced by cutting down its condi- tion attributes. So, we compute TENGRAM discernibility matrix (TDM) which is denoted as: M (DT) = [mij]n n, where n: number of sentences with MCW, mij, the set of attributes which discerns between objects xi and xj and also discerns the decision attributes c0, de ned as: mij = {ck 2 A: ck(xi) ck(xj), for k = 5,4,. . .,1,1,. . .,5 and 1 6 i,j 6 n}, where c0(xi) c0(xj). TDM is symmetric and cii = /, for each i = 1,2,. . .,n,. Hence, there exists only half of elements in TDM in comparison to DT. We now compute TDM for word iwanI (discussed in Example 1) in Table 5. Values in Table 5 are computed from decision table constructed in Table 4. Here we have, discernibility matrix of order 3 3 with cii = /, i = 1, 2, 3 and indiscernible decision attributes for s1 and s2 (since c0(x1) = c0(x2) = prv_vN (Table 4)). So, only discernible ob- jects left are for two cells (column s1 and row s3; column s2 and row s3 respectively). For obtaining values in these cells, we compare ck for k = 3,2,1,1,2,. . .,5 (since c5 and c4 are both / for s2 and s3 respectively (Table 4)). It is observed that when column s1 and row s3 are compared, then discernible objects are c3, c2, c1, c1, c2, c3, c4, c5. Similarly, in the resultant discernibility matrix values for column s2 and row s3 are obtained, as shown in Table 5. Step III. Computation of TENGRAM discernibility function We compute TENGRAM discernibility function (TDF) as f(M(DT)) based on the TDM as a Boolean function of m variables c1 ,. . .,cm (corresponding to c1,. . .,cm) de ned as: f(M(DT))(c1 ,. . .,,cm ) = ^ { _ mij : 1 6 i,j 6 n,mij /}, where mij = {c : c 2 mij}. Each row in the TDF corresponds to one column in the TDM, for each object xi of the decision table belonging to a certain decision class. We now compute TDF for word iwanI using Table 5. In this ta- ble, column 1 has only one value (corresponding to row s3), so the computation of ^ operator becomes trivial, and only the value Fig. 5. TENGRAM decision table for POST of MCW. Fig. 6. Sentences with Hindi word iwanI from Hindi corpus. Fig. 7. Sentences (s1, s2, s3) with Hindi word iwanI . Table 3 Hindi transliteration (H_Trans), POS tagging (P_Tag) and acronymn (Acr) of words in sentences (S#)- s1, s2 and s3 respectively. S# H_Trans P_ Tag Acr s1 bAra BAv_vAcaka_saMGYA Bv_sG apane nija_vAcaka_srvnAma njv_sr KilOne jAwi_vAcaka_saMGYA jv_sG kI saMbaMXa_boXaka sM_bX daMdI jAwi_vAcaka_saMGYA iwanI parimANa_vAcaka_visheSNa prv_vN jZora BAv_vAcaka_saMGYA se saMbaMXa_boXaka PeMkI muKnya_kriyA mu_ky ki viBAjaka_samucncya_boXaka vB_smu_bX xIvAra jAwi_vAcaka_saMGYA s2 gale jAwi_vAcaka_saMGYA se saMbaMXa_boXaka cIMKa BAv_vAcaka_saMGYA iwanI parimANa_vAcaka_visheSNa jZora BAv_vAcaka_saMGYA se saMbaMXa_boXaka nikalawI muKnya_kriyA ki viBAjaka_samucncya_boXaka gala jAwi_vAcaka_saMGYA s3 apanI nija_vAcaka_srvnAma pasaMxa guNa_vAcaka_visheSNa gN_vN kI saMbaMXa_boXaka iwanI saMbaMXa_boXaka sArI saMKnyA_vAcaka_visheSNa sKv_vN cOkaleta xravnya_vAcaka_saMGYA xv_sG ko saMbaMXa_boXaka KarAba guNa_vAcaka_saMGYA gN_sG howe visheSNa_kriyA vN_ky 15088 J.P. Gupta et al. / Expert Systems with Applications 38 (2011) 15084 15093 corresponding to (c3 _ c2 _ c1 _ c1 _ c2 _ c3 _ c4 _ c5) will occur. Similarly, column 2 and column 3 are computed. Step IV. Generation of TENGRAM dependency rules TENGRAM dependency rules are also known as correction or decision rules. These rules are generated directly for each gener- ated reduct, where reducts stand for minimal subsets of condition attributes which preserve indiscernibility relation. A reduct is rep- resented as RED(A), and is obtained from TENGRAM discernibility function f(M(DT)). It can be noted that minimum subset of attributes (i.e. reducts) for word iwanI is (c3 _ c2 _ c1 _ c1 _ c2 _ c3 _ c4 _ c5). Hence, based on this reduct, we can generate rules for word iwanI . The pseudo code for generation of dependency rules for word iwanI is given in Fig. 8. Step V. Elimination of TENGRAM inconsistent rules Once rules are generated through TENGRAM method, then it is essential to check the consistency of these rules. TENGRAM rules are formulated as: if (condition is x) then (re- sult is y). Suppose we have a rule, if (condition is x0) then (result is y0), where x = x0. In this case, it is essential to have y = y0 for rule to be consistent, otherwise rule is inconsistent. Examples of elimination of inconsis- tent rules are stated in the next section. 4. Results 4.1. Some identi ed multi-category words in Hindi In our present work, we have identi ed 19 multi-category words and they appear 116 times in our developed Hindi corpus. List of identi ed MCWs are shown in the Fig. 9. 4.2. Rules generation for each MCW in Hindi For implementation purpose in C -language, we have used a subset of TENGRAM method, which considers three words viz. the MCW and two other words (immediate adjacent neighbours Table 4 TENGRAM decision table for MCW iwanI . U/A Conditional Attributes (CA) Decision Attribute (DA) c5 c4 c3 c2 c1 c1 c2 c3 c4 c5 c0 s1 Bv_sG njv_sr jv_sG sM_bX jv_sG Bv_sG sM_bX mu_ky vB_smu_bX jv_sG prv_vN s2 / / jv_sG sM_bX Bv_sG Bv_sG sM_bX mu_ky vB_smu_bX jv_sG prv_vN s3 / / njv_sr gN_vN sM_bX sKv_vN Xv_sG sM_bX gN_sG vN_ky sM_bX Table 5 TENGRAM discernibility matrix for MCW iwanI . s1 s2 s3 s1 / s2 / / s3 c3, c2, c1, c1, c2, c3, c4, c5 c3, c2, c1, c1, c2, c3, c4, c5 / Fig. 8. Generation of rules for MCW iwanI . Fig. 9. List of the identi ed MCWs within Hindi corpus. J.P. Gupta et al. / Expert Systems with Applications 38 (2011) 15084 15093 15089 Table 6 Decision rules for identi ed MCWs using immediate adjacent neighbours. MCW# Rule# MCW Prev_tag Next_tag Curr_tag 1 WA R1.1 jv_sG mu_ky R1.2 prv_kvN mu_ky R1.3 sM_bX mu_ky R1.4 gN_vN mu_ky R1.5 mu_ky amu_ky R1.6 amu_ky amu_ky 2 vaha R2.1 Bv_sG apv_sr R2.2 nSv_sr apv_sr R2.3 gN_vN apv_sr R2.4 sMv_vN apv_sr R2.5 vN_ky apv_sr R2.6 kvN_ky nSv_sr R2.7 jv_sG sMv_vN 3 bahuwa R3.1 sM_bX sM_bX R3.2 mu_ky prv_kvN R3.3 apv_sr prv_vN R3.4 vv_sG prv_vN R3.5 sy_smu_bX prv_vN R3.6 vB_smu_bX gN_vN prv_vN 4 para R4.1 jv_sG sM_bX R4.2 sG_ky sM_bX R4.3 ap_cn vB_smu_bX R4.4 amu_ky vB_smu_bX 5 use R5.1 Bv_sG sMv_vN R5.2 mu_ky apv_sr 6 wo R6.1 ap_cn vB_smu_bX R6.2 amu_ky vB_smu_bX R6.3 sWv_kvN vB_smu_bX R6.4 jv_sG vB_smu_bX R6.5 vv_sG vB_smu_bX R6.6 mu_ky sWv_kvN Bv_sG R6.7 mu_ky prv_kvN Bv_sG 7 usake R7.1 jv_sG sMv_vN R7.2 mu_ky apv_sr 8 uTAkara R8.1 ap_cn kvN_ky R8.2 jv_sG kvN_ky R8.3 sM_bX sM_bX 9 tUta R9.1 jv_sG gN_vN R9.2 kv_kvN mu_ky 10 WI R10.1 amu_ky amu_ky R10.2 nSv_sr vB_smu_bX mu_ky 11 eka R11.1 jv_sG sKv_vN R11.2 Bv_sG prv_vN 12 gayA R12.1 mu_ky amu_ky R12.2 gN_vN mu_ky R12.3 vB_smu_bX mu_ky 13 alnpa_virAma (-) R13.1 amu_ky nX_cn R13.2 vN_ky nX_cn R13.3 gN_vN gN_vN yk_cn 15090 J.P. Gupta et al. / Expert Systems with Applications 38 (2011) 15084 15093 i.e. on either side of the considered MCW). As explained in Table 6, Curr_tag: POS tagging of current considered MCW, Prev_tag: POS tagging of immediate previous word to the considered MCW and Next_tag: POS tagging of immediate next word to considered MCW, which helps in generating rules for the list of MCWs (refer Fig. 9). The Table 6 consists of six columns, (i) column1 indicates MCW number, numbered as 1,2,3,. . .,19; (ii) column 2 indicates decision rule number, numbered as R1.1,. . .,R13.2,. . .,R19.3; (iii) column 3 indicates MCWs; (iv-v) column 4 and 5 indicate antecedent of the decision rules, which are immediate adjacent (previous and next) POS tags for the corresponding MCW; (vi) column 6 indicates con- sequent of the decision rules for the corresponding MCW. Some instances from Table 6 are discussed below: Consider MCW #1 i.e. WA with Rule#1.1 to 1.6. The rules corresponding to WA are generated in Fig. 10. In this case, we do not require Next_tag to generate rules. Only Prev_tag is utilized for the same. Consider another instance of MCW from Table 6 i.e.MCW#7 usake having Rules# 7.1 and 7.2. The rules corresponding to usake are generated in Fig. 11. In this case, we do not require Prev_tag to generate rules. Only Next_tag is utilized for the same. We consider one more instance of MCW from Table 6 i.e. MCW#13 alnpa_virAma having Rule# 13.3. The rule correspond- ing to alnpa_virAma is generated in Fig. 12. In this case, we require both tags (Prev_tag and Next_tag) to generate rules. Rest of the rules in Table 6 can be explained on the similar lines. 4.3. Elimination of inconsistent rules in Hindi Example of an identi ed inconsistent rules is in Fig. 13. Fig. 13 shows that rules numbered R3.1, R8.3 and R15.3 are inconsistent with the rule numbered R1.3 (Section 3.2, Step V). Making use of Maximum Likelihood Estimate (MLE) (Konchady, 2009), rules R3.1, R8.3 and R15.3 has maximum likelihood over the rule R1.3 (Table 6). So, eliminating the rule R1.3, rest of the rules R3.1, R8.3 and R15.3 are consistent. Similarly, other inconsis- tent rules are eliminated from the corpus in this step. Table 6 (continued) MCW# Rule# MCW Prev_tag Next_tag Curr_tag 14 usakI R14.1 jv_sG sKv_vN R14.2 sWv_kvN apv_sr 15 iwanI R15.1 jv_sG prv_vN R15.2 Bv_sG prv_vN R15.3 sM_bX sM_bX 16 liye R16.1 apv_sr prv_vN mu_ky R16.2 upv_sr mu_ky sM_bX 17 so R17.1 amu_ky vB_smu_bX R17.2 ap_cn vB_smu_bX R17.3 mu_ky sMv_sr 18 WIM R18.1 xv_sG mu_ky R18.2 amu_ky amu_ky 19 hE R19.1 mu_ky amu_ky R19.2 amu_ky amu_ky R19.3 Bv_sG mu_ky Fig. 10. Rules for MCW WA within Hindi corpus. Fig. 11. Rules for MCW usake within Hindi corpus. Fig. 12. Rule for MCW alnpa_virAma within Hindi corpus. Fig. 13. Sample inconsistent rules within Hindi corpus. J.P. Gupta et al. / Expert Systems with Applications 38 (2011) 15084 15093 15091 5. Conclusion In this paper, we have devised a methodology to identify multi- category words in Hindi language based on the context informa- tion and to disambiguate them using rules based TENGRAM meth- od. Once a consistent set of rules is generated, they can be applied analogusly to any other Hindi corpus so as to remove ambiguity of MCWs . We have constructed a Hindi tagger based on Hindi grammar, to deal with different word-order (subject-object-verb) over English grammar order (subject-verb-object). This paper also provides Hin- di transliteration which helps non-Hindi readers to pronounce Hin- di alphabets into their English character format. 6. Future work The current work can be extended by performing phrase level POS tagging which can be applied to a group of words at one go. A hybrid approach i.e. stochastic combined with rules based (rough set-hybrid model) can be developed for better results. Appendix A Hindi phonology consists of snvara and vnyMjana . We have enclosed Hindi transliterated words within and they are case sensitive (upper and lower case letters of English lan- guage). Also, half characters are produced using n . List of Hindi snvara , their mAtrAeM and their Hindi transliteration is in the Table A. List of vnyMjana and their transliteration in Hindi language are in the Table B. References Araujo, L. (2002). Part-of-Speech Tagging with Evolutionary Algorithms. In International Conference on Intelligent Text Processing and Computational Linguistics (pp. 230 239). Springer-Verlag. Dalal, A., Nagraj, K., Sawant, U. & Shelke, S. (2006). Hindi Part-of-Speech Tagging and Chunking: A Maximum Entropy Approach. NLPAI Machine Learning Workshop: Part of Speech Tagging and Chunking for Indian Languages. Demri, S., & Orlowska, E. (2002). Incomplete Information, Structure, Inference, Complexity. Heidelberg: Springer-Verlag. Fahim, H.M., Naushad, U. & Mumit, K.(2007). Comparison of Unigram, Bigram, HMM and Brill s POS Tagging Approaches for Some South Asian Languages. Center for Research on Bangla Language Processing, BARC University. Konchady, M. (2009). Text Mining Application Programming. Cengage Learning, India Edition. Lin, T. Y., & Cercone, N. (1997). Rough Sets and Data Mining- Analysis of Imperfect Data. Boston: Kluwer Academic Publishers. Orlowska, E. (1997). Incomplete Information: Rough Set Analysis. Heidelberg: Physica- Verlag. Pawlak, Z. (1982). Rough Sets. International Journal of Computer and Information Sciences, 11, 341 356. Pawlak, Z. (1991). Rough Sets- Theoretical Aspects of Reasoning About Data. Boston: Klumer Academic Publisher. Polkowski, L. (2002). Rough Sets: Mathematical Foundations. Heidelberg: Physica- Verlag. Polkowski, L., & Skowron, A. (1998). Rough Sets in Knowledge Discovery I and II. Heidelberg: Physica-Verlag. Polkowski, L., Tsumoto, S., & Lin, T. Y. (2000). Rough Set Methods and Applications. In New Developments in Knowledge Discovery in Information Systems. Heidelberg: Physica-Verlag. Qian, Yi-li & Zheng, Jia-heng (2004). An Approach to Improving the Quality of Chinese Text. International conference on Information Technology: Coding and Computing. IEEE Computer Society Las Vegas, Nevada, USA, 2, 183-187. Quinlan, J. R. (1987). Generating Production Rules From Decision Trees. In Tenth International Joint Conference on Arti cial Intelligence (pp. 304 307). Milan, Italy: Morgan Kaufman. Rao, D. & Yarowsky, D. (2007). Part of Speech Tagging and Shallow Parsing of Indian Languages. International Joint Conference on Arti cial Intelligence Workshop on Shallow Parsing in South Indian Languages, Hyderabad, India. Ray, P.R., Harish, V., Sarkar, S. & Basu, A. (2003). Part of Speech Tagging and Local Word Grouping Techniques for Natural Language Parsing in Hindi. International Conference on Natural Language Processing, Mysore, India. Table A List of Hindi snvara , their mAtrAeM and their Hindi transliteration. S.No. Hindi transliteration 1 a 2 A 3 e 4 E 5 u 6 U 7 rh 8 e 9 E 10 o 11 O In Hindi anusnvAra ( ) and visarga ( ) are joined with to form aM and a: respectively. Table B List of vnyMjana and their transliteration in Hindi language. S.No. vnyMjana Detailed Devanagari alphabet Hindi transliteration 1 snparSa kavarga k K g G dz cavarga c C j J tavarga t T d Dh N wavarga w W x X n pavarga p P b B m 2 aMw:snW y r l v 3 USnma sh S s h Some more vnyMjana are: Sra ; kS ; JY and wra respec- tively. Also, DhZ ; dZ are the sounds used in tavarga in Hindi. 15092 J.P. Gupta et al. / Expert Systems with Applications 38 (2011) 15084 15093 Shrivastava, M., Agrawal, N., Mohapatra, B., Singh, S. & Bhattacharya, P. (2005). Morphology Based Natural Language Processing Tools for Indian Languages. Inter Research Institute Student Seminar in Computer Science. India: IIITK. Sivanandam, S. N., & Deepa, S. N. (2008). Principles of Soft Computing. New Delhi, Inida: Wiley India Pvt. Ltd.. Skowron, A., & Rauszer, C. (1992). The Discernibility Matrices and Functions in Information Systems. In Intelligent Decision Support - Handbook of Applications and Advances of the Rough Sets Theory (pp. 331 362). Dordrecht: Kluwer Academic Publishers. Subramanian, G. H., Nosek, J., Raghunathan, S. P., & Kanitkar, S. S. (1992). A Comparison of the Decision Table and Tree. Communications of the ACM, 35(1), 89 94. Tapanainen, P. & Voutilainen, A. (1994). Tagging Accurately Don t Guess if You Know. In Proceedings of ANLP 94. Visvendru, R. P. (1993). Sachitra Hindi Vayakarana Part-I (Class 6). New Delhi, India: Saraswati House Pvt. Ltd. Educational Publisher. J.P. Gupta et al. / Expert Systems with Applications 38 (2011) 15084 15093 15093