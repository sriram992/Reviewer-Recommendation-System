SPECIAL SECTION ON VISUAL SURVEILLANCE AND BIOMETRICS: PRACTICES, CHALLENGES, AND POSSIBILITIES Received December 27, 2017, accepted January 29, 2018, date of publication February 27, 2018, date of current version April 4, 2018. Digital Object Identifier 10.1109/ACCESS.2018.2809552 Robust Feature-Based Automated Multi-View Human Action Recognition System KUANG-PEN CHOU1, MUKESH PRASAD 2, DI WU2, NABIN SHARMA2, (Senior Member, IEEE), DONG-LIN LI3, YU-FENG LIN1, MICHAEL BLUMENSTEIN2, WEN-CHIEH LIN1, AND CHIN-TENG LIN2, (Fellow, IEEE) 1Department of Computer Science, National Chiao Tung University, Hsinchu 300, Taiwan 2Centre for Arti cial Intelligence, School of Software, FEIT, University of Technology Sydney, Sydney, NSW 2007, Australia 3Department of Electrical Engineering, National Chiao Tung University, Hsinchu 300, Taiwan Corresponding author: Mukesh Prasad (mukesh.nctu@gmail.com) This work was supported in part by the Australian Research Council under Grant DP180100670 and Grant DP180100656, in part by the Army Research Laboratory through the Cooperative Agreement under Grant W911NF-10-2-0022 and Grant W911NF-10-D-0002/TO 0023, and in part by the Taiwan Ministry of Science and Technology under Grant MOST 106-2218-E-009-027-MY3 and Grant MOST 106-2221-E-009-016 MY2. ABSTRACT Automated human action recognition has the potential to play an important role in public security, for example, in relation to the multiview surveillance videos taken in public places, such as train stations or airports. This paper compares three practical, reliable, and generic systems for multiview video-based human action recognition, namely, the nearest neighbor classi er, Gaussian mixture model classi er, and the nearest mean classi er. To describe the different actions performed in different views, view-invariant features are proposed to address multiview action recognition. These features are obtained by extracting the holistic features from different temporal scales which are modeled as points of interest which represent the global spatial-temporal distribution. Experiments and cross-data testing are conducted on the KTH, WEIZMANN, and MuHAVi datasets. The system does not need to be retrained when scenarios are changed which means the trained database can be applied in a wide variety of environments, such as view angle or background changes. The experiment results show that the proposed approach outperforms the existing methods on the KTH and WEIZMANN datasets. INDEX TERMS Multi-view video, action recognition, feature extraction, background subtraction, classi cation, machine learning. I. INTRODUCTION Recently, human action recognition research has brought many challenges in the areas of sports, security and per- sonal health care systems. Automatic video analysis systems which can recognize events related to human actions are becoming necessary in different industry areas. Therefore, human action recognition has become a hot research area in computer vision and there have been many papers pub- lished on this and many real-world applications have been developed, such as searching for the structure of large video archives, gesture recognition, video indexing, and video surveillance [1] [7]. Human-computer interaction, in par- ticular, is a crucial application in action recognition research. Visual cues are a signi cant part of human- computer interaction to enable better communication between humans and computers, hence researchers utilize visual cues to recognize gestures and actions. Most of the recent action recognition work samples an action sequence manually before it can be recognized in a lm. However, it is not practical to manually set the beginning and ending of an action sequence of the lm previously. Therefore, a practical recognition system needs to be able to automatically separate many actions in an image sequence. The current published methods for action recognition often sample an action sequence manually before it is recognized in a lm [8] [10]. However, it is not practical that setting the beginning and end of an action sequence of the lm previously. Therefore, a practical recognition system needs to separate many actions at an image sequences automatically. Moreover, actions can be performed as different subjects such as size, posture, motion and clothing, which is still a challenging problem for several reasons, such as illumination, occlusion, shadow, camera movement or other environment changes. In addition, the actions depend on or involve objects which could add another layer of variability. As a conse- quence, action recognition methods often assume that the VOLUME 6, 2018 2169-3536 2018 IEEE. Translations and content mining are permitted for academic research only. Personal use is also permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. 15283 K.-P. Chou et al.: Automated Multi-View Human Action Recognition System action is captured under restricted and simpli ed environ- ments such as static backgrounds, non-complicated action classes and static cameras [11] [15]. In particular, frequently moving the camera to an unknown position is the main cause of view variations. Similar to observing static objects from multi-view points, the actions may appear to be different from different angles. On the other hand, a moving camera could also affect the action appearance by incorporating dynamic view changes. Therefore, an action recognition system should be robust against environment and view-point changes when capturing an action sequence. The current approaches does not require any speci c parameter tuning for data processing and it explicitly exploit spatio-temporal information at multiple temporal scales. Therefore, the proposed approach is able to capture local and global temporal information as well, for interesting points of distribution. The proposed approach labels the beginning and end of the action sequence automatically. In addition, the pro- posed method takes advantage only of the global spatio- temporal information about where and when the points of interest are detected. Therefore, it is able to capture sequence motions and occlusions at a low computational cost. In par- ticular, the proposed approaches use view-invariant features to address multi-view action recognition from a range of perspectives. The key contribution of this paper can be summarized as follows: The proposed approach labels the beginning and end of an action sequence in a video stream automatically. The proposed approach is able to capture sequence motions and occlusions at a low computational cost due the detection of the points of interest. The proposed approach applies view-invariant features to address multi-view action recognition from differ- ent perspectives. Thus, the proposed approach is robust against view changes. The proposed novel action recognition system is more robust against view, scale and subject variance. Fig. 1 shows an overview of the proposed approach for the action recognition system. It can be separated into two parts: of ine training and online testing. In of ine training, feature extraction is the rst stage in extracting interesting information. Secondly, the fea- ture vectors of each image sequence are described. Thirdly, the feature vectors are quantized to reduce their dimension. Finally, these vectors are stored in the database. In online test- ing, the rst two stages are similar to of ine training. Then, using the histogram range of the database, the dimension of the feature vector is reduced. Thus, the results show which action is present in the test data. The proposed approach is evaluated using the KTH dataset [16], the WEIZMAN dataset [17] and the MuHAVi dataset [18]. The rest of the paper is organized as follows. Section II details the related work. Section III describes the datasets. Section IV and V presents the feature extraction and descrip- tion. All the action recognition classi ers applied to different datasets are discussed in Section VI. Section VII introduces FIGURE 1. System Overview. the experiments and the results. Section VIII suggests poten- tial research opportunities and provides a conclusion. II. RELATED WORK In the early stages of action recognition research, the techniques were based on optical ow [19], [20], tracking [21] [24] and a spatio-temporal shape template [17], [25], [26]. The computation of optical ow helps to construct action templates for ow and tracking-based approaches. However, at the boundary of the segmented human body, the features are more sensitive to noise, which are extracted from the ow templates. The action recognition problem is treated as 3D object recognition by spatio-temporal shape template approaches. These approaches require the extraction of highly detailed silhouettes, which may not be possible when there is real-world noisy video input. Further, a recog- nition rate with 100% accuracy has been demonstrated on the WEIZMAN dataset [17], however, these approaches do not work properly on a dataset which contains noise such as the KTH dataset [16]. The KTH dataset contains noises such as low resolution, zooming, and camera move- ment, which makes it impossible to extract a clean silhou- ette. The spatio-temporal interest point-based approaches have become increasingly popular to address this prob- lem. Further, the 2D SIFT descriptors [27] are extended to 3D with the addition of dimension to the histogram orientation by Scovanner et al. [28]. Due to the encoded temporal information, the extended 3D descriptors perform better than the 2D descriptors in action recognition. Fur- thermore, Willems et al. [29] proposed the spatio-temporal domain which is an extension of the SURF descriptor. Schuldt et al. [16] and Dollar et al. [30] described sparse spatio-temporal features to deal with the complexity of human action recognition [18], [31]. Schuldt et al. [16] pro- posed the representation of action using 3D spatio-temporal interest points captured from video frames. Schuldt also produced a histogram of informative words for each action adopting the codebook and bag-of-words (BOW) approach. A dictionary of prototypes or video-words can be formed based on the clustering of the detected points of interest. 15284 VOLUME 6, 2018 K.-P. Chou et al.: Automated Multi-View Human Action Recognition System Similarly, Dollar et al. [30] introduced a multi-dimensional linear lter detector which is able to detect denser points of interest. The BOW approach was applied but it took sparser sampling of the points of interest. Niebles and Fei-Fei [32] introduced a hierarchical model which can be characterized as a constellation of bags-of-features to improve the perfor- mance. The approaches [30], [32] represent BOW features, which are adopted successfully for 2D object categorization and recognition. The BOW features are robust against noises, camera movements and low resolution datasets compared with object tracking and shape-based approaches. Moreover, these approaches mainly focus on individual local space time descriptors rather than global space time descriptors. However, the early work did not consider noise. In recent years, researchers have applied different new methods to tackle the challenges from noise in the human action recog- nition area, such as camera in-variation, camera motion and occlusion. Most of the early work assumes that the action is captured from a static viewpoint without any camera move- ment. However, the patterns of human actions appear to be different from different angles. A person s gestures and their location vary according to each camera angle. Some of the approaches train a single classi er for all viewpoints or a set of classi ers where each classi er deals with one view- point [33], [34]. However, these approaches only extend the system from a single viewpoint to a multi-view dataset. Therefore, the performance only depends on the extracted features and the trained classi ers. Lu et al. [35] introduced motion history and motion energy images to observe the additional action features in the images. This approach may disrupt the background of the image especially if there is more than one person in the image. In order to obtain accu- rate multi-view action representations, researchers proposed some models to generate 3D or 2D body gestures through the multi-view datasets. The human body can be distin- guished into several parts, and action recognition depends on the features extracted from the different body parts. Kumar and Madhavi [11] used an envelope shape to represent the human body and model the action recognition classi er. The aforementioned approaches have dif culty ensuring the performance of the classi er when the viewpoint or envi- ronment changes. However, this paper introduces robust fea- tures to address multi-view action recognition from different perspectives and view changes as well. III. DATASETS The KTH Royal Institute of Technology created a dataset named the KTH Dataset [16] in 2004. It was the largest human sequence action dataset in video with different sce- narios and the most popular dataset at that time, achieving a milestone in the computer vision research area. The KTH dataset includes six action classes, these being boxing, hand clapping, hand waving, walking, jogging and running. Each class is performed by twenty- ve people in four different sce- narios (outdoor actions, outdoor actions with zoom, outdoor actions with different clothing and indoor actions). There are FIGURE 2. Examples of KTH dataset. The four different scenarios are outdoor actions (s1), outdoor actions with zooming (s2), outdoor actions with different clothes (s3) and indoor actions (s4). FIGURE 3. Examples of Weizmann dataset includes extracted silhouettes. a total 25 6 4 = 600 video les in the dataset and each video only contains one person performing a single action as shown in Fig. 2. The resolution and length of each video is 160 120 and ten to fteen seconds respectively captured at twenty- ve FPS. The Weizmann Institute of Science created a dataset named the WEIZMANN Dataset [17] in 2005 comprising 90 low res- olution (180 144) videos involving nine different subjects, each of whom performs 10 basic actions, as shown in Fig. 3. Kingston University collected a large multi-view human action dataset named the MuHAvi (Multicamera Human Action Video) dataset in 2010 [18]. It comprises multi-view videos of 17 different actions performed several times by 14 people in a designated action area and is captured from different angles and distances by eight cameras. The resolu- tion of the dataset is 720 576 pixels and it is captured in complex backgrounds and varying lighting conditions. The eight cameras are positioned on different sides and corners on a rectangular platform, as shown in Fig. 4 and Fig. 5 shows six example frames from this dataset. IV. FEATURE EXTRACTION This section describes feature extraction which includes information on moving object extraction and points of interest extraction. The details of the extraction of moving objects and interest points are shown in Fig. 6. A. MOVING OBJECT LOCALIZATION In action recognition, detecting and segmenting the fore- ground object without the noise produced by camera VOLUME 6, 2018 15285 K.-P. Chou et al.: Automated Multi-View Human Action Recognition System FIGURE 4. Location of cameras at four sides and four corners from MuHAVi dataset. FIGURE 5. Examples of MuHAVi dataset. FIGURE 6. Overview of feature extraction. movements, zoom, shadows etc. is dif cult. To do this, the model can be divided into the following steps. Firstly, the Gaussian mixture model (GMM) is used [36] to construct the background and obtain the silhouette by background sub- traction. Secondly, the Prewitt edge detector [37] can be used to segment the objects from the foreground. The GMM is a common and robust method in background construction. For the purpose of action recognition in a complex scene condition, the GMM is used to build the background image. It is described as follows. The intensity of each pixel varies in a small interval except in the region of foreground objects. It is appropriate to use a Gaussian model to construct the background image. How- ever, in many surveillance videos, if there are waving leaves, sparking light, etc. Some background pixels vary in several speci c intervals. In other words, using two, three or more FIGURE 7. GMM background model construction. Gaussian distributions to model a pixel will obtain better performance. The ow chart of the GMM background con- struction is presented in Fig. 7. Firstly, a low-pass lter is used to reduce the noise. The GMM method models the intensity of each pixel with K Gaussian distributions. The probability that a certain pixel has a value of Xt at time t can be written as: P(Xt) = K X k=1 k, t  Xt, k, t, X k, t  (1) where K is the number of distributions that are used, k,t represents the weight of k-th Gaussian in the mixture at time t, k,t is the mean of k-th Gaussian in the mixture at time t, 6k,t is the covariance matrix of the k-th Gaussian in the mixture at time t, and is a Gaussian probability density function shown in Eq. 2.  Xt, t, X t  = 1 (2 )n/2 | P t|1/2 exp  1 2(Xt t)T X t 1(Xt t)  (2) where n is the dimension of data. In order to simplify the computation, it is assumed that each channel of data is inde- pendent and has the same variance, and it can then be assumed that the covariance matrix is as shown Eq. 3: X k, t = 2 k I (3) Temporal difference is applied to extract the possible back- ground regions, and update the pixels inside these regions. Then, we sort Gaussian distributions by the value of / , and 15286 VOLUME 6, 2018 K.-P. Chou et al.: Automated Multi-View Human Action Recognition System FIGURE 8. Background image construction by GMM. (a) Video Sequence. (b) GMM Background Image. choose the rst B distributions to be the background model, i.e. shown as Eq. 4: B = arg min b  b X k=1 k, t > T  (4) When a new pixel is imported (intensity is Xt+1), it will be checked against the K distributions in turn. If the probability value is within Eq. 5 standard deviations, this pixel is consid- ered as background. Then, weight, mean, variance is updated using Eq. 5, 6, 7: k, t + 1 = (1 ) k, t + (Mk, t + 1) (5) t + 1 = (1 ) t + Xt + 1 (6) 2 t+1 = (1 ) 2 t + (Xt + 1 t + 1)T (Xt + 1 t + 1) (7) where is the learning rate, Mk,t+1 is 1 for the model which matched and 0 for the remaining models. Eq. 8 shows the second learning rate . = (Xt + 1| k, t, k, t) (8) In addition, the remaining Gaussians only update the weight. If no distributions are matched, then the mean, variance and weight of the last distribution are replaced by Xt+1, a high variance and a low weight value, respectively. Fig. 8 shows the background image constructed by GMM. Fig. 9 shows the silhouette obtained by background subtraction. In Fig. 10(a), using the edge detector to detect the location of a moving object from foreground image. In addition, a bounding box is used to indicate the presence of a foreground subject at a particular area in Fig. 10(b). FIGURE 9. Silhouette obtained by background subtraction. (a) Current Image. (b) Silhouette. FIGURE 10. Moving object obtained by Prewitt filter. (a) Location of Moving object. (b) Bounding Box. B. EXTRACTION OF POINTS OF INTEREST The actions performed by the person should be shown in the bounding boxes. For instance, the bounding box must be located around the hands when the person performs the action boxing . Thus, Bregonzio et al. [38] proposed a detector to capture spatio-temporal information from the bounding boxes. More speci cally, the detector works in two steps: rstly, the frame differences are monitored based on the VOLUME 6, 2018 15287 K.-P. Chou et al.: Automated Multi-View Human Action Recognition System focus of the attention and detection of the region of interest. Secondly, 2D Gabor lters of different orientations are used to lter the regions of interest. These two steps give a com- bined lter response based on both the spatial and temporal domains. Points of interest are local spatio-temporal features which can be considered as salient or descriptive of the action in the frames. In Dollar s method [30], the Gabor lter is used to detect intensity variations in the temporal domain. In addition, the detected points of interest correspond to local 3D peaches that represent complex actions. To be more speci c, the response of the Gabor lter is given as: R = (I g hev)2 + (I g hod)2 (9) Where the Gaussian smoothing kernel can be represented as g(x, y : ) and can be applied in the spatial domain. hev and hod are the 1D Gabor lters worked on the temporal domain which can be de ned as: hev (t; , ) = cos(2 t ) e t2 2 (10) hod (t; , ) = sin(2 t )e t2 2 (11) By setting the = 4/ , and are the two free parameters which control the space and time scales of the detector. However, the Dollar detector has four drawbacks: (1) The pure translational motions are ignored by the method; (2) False detection occurs easily because of the noise in the video, which is because the approach uses local informa- tion within a small region; (3) The approach tends to generate a spurious detection background area surrounding object boundary; (4) The detection approach is weakened when there is slow object motion, slight camera movement or zoom. To overcome these four problems of the Dollar detector, the detector proposed by Bregonzio et al. [38] can be uti- lized which proposes different lters for detecting under- going complex motions from salient space-time local areas and capture spatio-temporal information from the bounding boxes. The Gabor lter is a linear lter which is widely used for edge detection in image processing, and the frequency and orientation representations are similar to the human visual system. In addition, it is particularly suitable for the repre- sentation and discrimination of texture. In the spatial domain, a 2D Gabor lter is a Gaussian kernel function modulated by a sinusoidal plane wave. Therefore, the 2D Gabor lter has two parts, the rst part s(x, y) is the carrier, which represents the real part of a complex sinusoid: s (x, y) = cos[2 ( 0x + 0y) + i] (12) where 0 and 0 are the spatial frequencies of the sinusoid controlling the scale of the lter and i de nes the orientation of the lter. In the experiments, the 2D Gabor lters contain 5 different orientations, i = 1, . . . , 5 = {0 , 22.5 , 45 , 67.5 , 90 } which shown in Fig. 11. FIGURE 11. Examples of the 2D Gabor filters oriented along (a) 0o, (b) 22:5 , (c) 45 , (d) 67:5 and (e) 90 . The second part of the lter G(x, y) called the envelope represents a 2D Gaussian-shaped function: G (x, y) = exp(x2 + y2 2 2 ) (13) 15288 VOLUME 6, 2018 K.-P. Chou et al.: Automated Multi-View Human Action Recognition System FIGURE 12. Results of interest point detection. (a) Walk. (b) Wave Arms. (c) Punch. where the width of G(x, y) is controlled by the parameter and 0 = 0 = 1 2 . Therefore, is the only parame- ter control- ling the scale, which is set to 11 pixels in the experiments. By setting the threshold, the points of inter- est can be obtained after convolving the bounding boxes with 2D Gabor lters. Local and distinctive properties of human actions can be represented by using points of interest. Fig. 12 shows the results of the point of interest detection using the MuHAVi dataset. V. FEATURE DESCRIPTION This section introduces the feature vectors described by the location of moving objects and points of interest discussed in the previous chapter. Section IV.A and section IV.B illustrate the box features and cloud features. Moreover, section IV.C, describes the quantization for reducing the dimension of the feature vectors. A. BOX FEATURE The rst set of features is global and holistic and is concerned with the shape and speed of the foreground object. Once the object is segmented from the detected foreground area by the Prewitt edge detector [37], two features are consid- ered: Br t measuring the ratio of the object height and width, BSp t and measuring the absolute speed of the object which is normalized by the height of the object for scale invariance. Each image frame It has one Br t and one BSp t feature. B. CLOUD FEATURE Spatial information, such as human pose information, can be preserved by the detected points of interest. Moreover, the frames have a temporal dependency between each other, and in order to use such information, the points of interest FIGURE 13. Cloud for different temporal scale S. (a) S = 1. (b) S = 2. extracted from a set of consecutive frames are able to accumu- late and form a point cloud [38]. Thus, the points of interest could represent both the spatial and temporal information for human actions. For an action video sequence, A contains T frames, which can be represented as A = [I1, . . . , It, . . . , IT ], where It is the t-th frame of the video. Then, the It is set as the current frame and the Ns as the size of a temporal scale. The sets of the past K cumulative scales can be de ned as [It Ns, . . . , It], [It 2 Ns, . . . It], . . . , [It K Ns, . . . , It]. Thus, for the speci c frame It, there are a set of K interest point clouds where different temporal scales are formed. As shown in Fig. 13, the clouds can be represented as [C1, . . . , CS, . . . , CK]. To be more speci c, by accumulating the detected points of interest over the past S NS frames, the cloud of the s-th scale can be built. Fig. 14 shows examples of the interesting point clouds extracted from the MuHAVi dataset. It shows that different actions represent point clouds of interest which are of a different shape, relative location and distribution. Therefore, the second set of features are called cloud features. The cloud features are scale dependent and are extracted from the point clouds of interest with different scales. Eight features are computed from the s-th scale cloud. The representation of the s-th scale cloud is as follows: [Cr s , CSp s , CVd s , CHd s , CHr s , CWr s ] (14) where Cr s is the height and width ratio of the cloud. CSp s is the absolute speed of the cloud. CVd s and CHd s measure the spatial relationship between the cloud and the detected object area. Speci cally, CVd s is the vertical distance between the geometrical centroid of the object area and the cloud, and CHd s is the horizontal distance between the geometrical centroid of the object area and the cloud. CHr s and CWr s are the height ratio and width ratio between the object area and the cloud respectively. Overall, the six features can be put into two categories: Cr s and CSp s measure the shape and speed of cloud itself; the rest four features capture the relative shape VOLUME 6, 2018 15289 K.-P. Chou et al.: Automated Multi-View Human Action Recognition System FIGURE 14. Examples of the interest points clouds. (a) Walk. (b) Run. (c) Punch. (d) WaveArms. and location information between the object and the cloud areas Since, each video frame includes S temporal scales. For example, for each frame, there are S point clouds of interest. In total, there are 6S features from the point clouds of interest. In addition, two other features emanate from the foreground area. As a result, the representation of each frame is 6S + 2 features, where S is the total number of scales (i.e. 6 fea- tures for each scale along with 2 scale-independent features Br t and BSp t ). An overview of the features of the proposed approach are shown in Fig. 15. C. QUANTIZATION A total (6S + 2)T features are used to represent the whole action sequence, which leads to a very high-dimensional fea- ture space. The high dimension feature space can be caused by over tting and leads to poor recognition performance. If S = 6, we observe one of all the features in all the datasets separately using the empirical cumulative distribution func- tion [39], as shown in Fig. 16. The empirical cumulative distribution function reduces the feature space dimension, and more importantly, makes the system representation less sensitive to feature noises and invariant to duration T for each action sequence. In particular, the proposed system separates the empirical cumulative distribution function into Nb portions. VI. FEATURE REDUCTION AND CLASSIFICATION In of ine training, the proposed system stores quantized fea- ture vectors, as described in Section IV.C. In online testing, FIGURE 15. Overview of the features of the proposed approach. FIGURE 16. Overview of quantization. (a) A histogram range is produced by observing one of all features in all the dataset separately. (b) Each action sequence A is represented as (6S + 2) Nb features. the proposed system uses the histogram range of the training database and transforms the testing data Atest to a feature vector Vtest. Three classi ers are separately used to recognize the testing data for different recognition rates. Fig. 17 shows an overview of the feature reduction and classi cation. The three classi ers are the nearest neighbor classi er (NNC), the Gaussian mixture model classi er (GMMC) and the nearest mean classi er (NMC). This section discusses the different classi ers. A. NEAREST NEIGHBOR CLASSIFIER NNC is used widely for action recognition by computing the absolute distance between the testing vector and all of 15290 VOLUME 6, 2018 K.-P. Chou et al.: Automated Multi-View Human Action Recognition System FIGURE 17. Overview of feature reduction and classification. FIGURE 18. Overview of NNC for the proposed work. the training vectors. Majority voting is used to classify the object, and usually, the object is classi ed to the class which was voted the most common amongst its k nearest neighbors. Fig. 18 shows an overview using NNC to obtain the most similar action to the testing lm. In particular, set K = 5 for WEIZMANN dataset, K = 3 for KTH dataset and K = 6 for MuHAVi dataset. However, it takes a long time at the recognition stage using NNC if there are a large number of training samples because NNC needs to compare whole feature vectors in the database. B. GAUSSIAN MIXTURE MODEL CLASSIFIER To reduce the quantity of the feature vectors another method is to use GMMC to model the training data to speed up the recognition time and to utilize k Gaussian functions to model each feature of the feature vectors in the database. The result is obtained using the maximum probability value which is summed up by the probability values of each feature, as shown in Fig. 19. In particular, three Gaussian functions are set for the KTH dataset, three Gaussian functions for the WEIZMANN dataset and four Gaussian functions for the MuHAvi dataset. C. NEAREST MEAN CLASSIFIER Another method, the NMC, uses minimum distance between the testing vector and training vectors which is the mean value of the feature vectors of the same action and the same view. FIGURE 19. Overview of GMMC for the proposed work. FIGURE 20. Overview of NMC for the proposed work. An absolute distance is chosen for the recognition decision, as shown in Fig. 20. Therefore, NMC is more suitable for the proposed system for real-time recognition and has a better recognition rate. Moreover, the dimension of the subject is reduced to one, which improves performance and results in more ef cient recognition. VII. EXPERIMENT RESULT In this section, several results of action recognition are pre- sented. This section details the recognition rate for sub- ject invariance and for view invariance in section and section VII.B, respectively. The algorithm was implemented on a PC platform with Intel Core i5 3.3GHz and 8GB RAM. The development tool was MATLAB2010 and the operating system was Windows 7. All of the testing inputs are uncom- pressed AVI video les. The resolution of the video frame is based on the testing datasets. In order to construct multi- scale interest point clouds, N s was set to 5 and the total number of scales was 6. This gives 38 features, and a 40-bin histogram can be generated through linear quantization for each feature, for instance, the total features can be represented in 1520 dimensional space. A. SUBJECT INVARIANCE EVALUATION To evaluate subject invariance, the Leave-One-Out Cross- Validation (LOOCV) scheme is adopted to compute the recognition rates. It selects a group of clips from a single subject in a dataset as the testing data, and the rest of the clips are the training data. The repeated progress ensures that each group of clips in the dataset is used once as the VOLUME 6, 2018 15291 K.-P. Chou et al.: Automated Multi-View Human Action Recognition System FIGURE 21. Recognition performance of the proposed approach for KTH dataset measured using confusion matrices: (a) NNC. (b) GMMC. (c) NMC. testing data. For the KTH dataset, the clips of 24 subjects were used for training and the clips of the remaining sub- jects were used for validation. For the WEIZMANN dataset, the training set contains 8 subjects. For the MuHAVi dataset, 5 of the 17 actions (Walk- TurnBack, Run-Stop, Punch, CrawlOnKnees, WaveArms) were chosen as the experimental data and the clips of 6 subjects were used for training and the clips belonging to the remaining subjects were used for vali- dation. The results of using NNC, GMMC and NMC for the KTH dataset, WEIZMANN dataset and MuHAVi dataset are shown in Fig. 21, Fig. 22, Fig. 23 and Table 1. In particular, NMC obtained a recognition rate of 90.5797% for the KTH dataset, 95.5556% for the WEIZMANN dataset and 97.5% for the MuHAVi dataset. Table 2 compares the proposed approaches with the existing approaches, the results showing that GMMC and NMC outperform the existing methods on the WEIZMANN and MuHAVi dataset. B. VIEW INVARIANCE EVALUATION To evaluate the proposed method in relation to view invari- ance, a group of clips from a single view in a dataset is employed as the training data and the remaining clips are the frames, of each action as the testing data. This was repeated so that each group of clips in this dataset is used once as the FIGURE 22. Recognition performance of the proposed approach for WEIZMANN dataset measured using confusion matrices: (a) NNC. (b) GMMC. (c) NMC. TABLE 1. Recognition performance of our approach by using NNC, GMMC, NMC. training data. Five actions out of 17 in the MuHAVi dataset were chosen as the experimental data similar to the subject invariance evaluation. Then, one of the eight views in the MuHAVi dataset is utilized in training and the other view is utilized in testing. This procedure is repeated for all 8 views and the resulting recognition rates are then averaged. The recognition rates are 78.2143% and 81.4286% using GMMC and NMC, respectively (as shown in Fig. 24). Table 3 shows the recognition rate of each view using GMMC and NMC. The recognition rates of training view3, view5, view6 and view8 are better than the others. These views contain more information than the other four views which allows them to be more robust to view change. Table 4 compares the results 15292 VOLUME 6, 2018 K.-P. Chou et al.: Automated Multi-View Human Action Recognition System FIGURE 23. Recognition performance of the proposed approach for MuHAVi dataset measured using confusion matrices: (a) NNC. (b) GMMC. (c) NMC. TABLE 2. Comparative results on the KTH, WEIZMANN, MuHAVi datasets for subject invariance. with the existing approaches. It can be seen that the proposed method is better than the others. We also evaluate the proposed approach in terms of its robustness against different cameras and evaluate it in terms of view invariance using cross dataset testing. There are three similar actions (Walk, Run, Wave) in the three datasets, including different scenes as previously discussed. The results shown in Table 5 and Table 6 indicate that the recognition rate of training the MuHAVi dataset is better than training the KTH and WEIZMANN datasets since the TABLE 3. Recognition performance of each view in MuHAVi dataset using GMMC and NMC. TABLE 4. Comparative results on the MuHAVi dataset for view invariance evaluation. TABLE 5. Recognition rate of the proposed approach for cross dataset testing using GMMC. TABLE 6. Recognition rate of the proposed approach for cross dataset testing using NMC. MuHAVi dataset contains many views. However, the recog- nition rate of the testing MuHAVi dataset is worse than the testing KTH and WEIZMANN datasets since the MuHAVi dataset tests many actions belonging to different views which are not included in the KTH and WEIZMANN datasets. C. AUTO LABELING The proposed method utilizes a mechanism which can watch a person s actions in an image sequence and separate these VOLUME 6, 2018 15293 K.-P. Chou et al.: Automated Multi-View Human Action Recognition System FIGURE 24. Recognition performance of view invariance evaluation using confusion matrices: (a) MuHAvi dataset using GMMC. (b) MuHAvi dataset using NMC. FIGURE 25. Overview of auto labeling. actions automatically. Firstly, it adopts four different tem- poral scales ((1/4)T-frames, (1/2)T-frames and T-frames) of each action to be the training data for the of ine training. Secondly, feature vectors of the four temporal scales belong- ing to each action are produced using the function described in Section IV and Section V. Then, these feature vectors are placed into different temporal scale databases. For the online testing, rst, the system scans the image sequence using a scanning window whose temporal scale is (1/4) T-frames. Second, the (1/4) T-frames window W(1/4)T is transformed to a feature vector using the function detailed in Section IV and Section V. Then, in the classi cation stage, NMC is used to classify the feature vectors from the T-frames database. Actions are classi ed as candidate actions if similarity S is over 70%. Similarity S is de ned as: S = F Nb D F Nb (15) where F is the number of features, Nb is the number of bins and D is the absolute distance between the testing feature vector and the training feature vector. However, if similarity S is below 70% the (1/4) T-frames scanning window W(1/4)T skips I frames to nd other actions from the other images. In the experiment, set F = 38, Nb = 40 and I = 15. As soon as some actions produced by the (1/4)T-frames scanning window are deemed to be candidate actions, the sys- tem uses (1/2)T-frames scanning window W(1/2)T to scan the next (1/4)T-frames and the previous (1/4) T-frames. In the classi cation stage, NMC is utilized to classify the feature vector from (1/2)T-frames database of the candi- date actions. Similar to the (1/4)T frames scanning win- dow W(1/4)T, the candidate actions remain candidate actions if similarity S is over 70%. Then, set beginning of the action from testing image sequences using the rst index of (1/2)T-frames scanning window. (3/4)T-frames, T-frames and (5/4) T-frames scanning windows are used to scan the images and classify the produced feature vectors from the T-frames database. The maximum value similarity S is used to obtain the result. The difference rate R is used to nd the end of the action which occurs when the difference R is over 10%. The difference rate R is de ned as R = Dc Dl Dc (16) where Dc is the absolute distance of the feature vec- tor between the current scanning window and the train- ing database; Dl is the absolute distance of the feature vector between the last scanning window and the training database. Finally, the system labels one of actions in the image sequences and uses (1/4) T-frames scanning window to nd the next action in the video. In the experiment, set T = 100 and Fig. 25 shows an overview of auto labeling. VIII. CONCLUSION This paper presents an approach for real-world applications which automatically labels the beginning and ending of an action sequence. The system uses the proposed view-invariant features to address multi-view action recognition from dif- ferent perspectives for accurate and robust action recogni- tion. The view-invariant features are obtained by extracting holistic features from different temporal scale clouds, which are modeled on the explicit global, spatial and temporal distribution of interest points. The experiments on the KTH and WEIZ- MANN datasets demonstrate that using view- invariant features obtained by extracting holistic features from clouds of interest points is highly discriminative and more robust for recognizing actions under different view changes. The experiments also show the proposed approach performs well with cross-tested datasets using previously trained data, which means there is no need to re-train the system if the scenario changes. REFERENCES [1] K. G. Derpanis, M. Sizintsev, K. J. Cannons, and R. P. Wildes, Action spotting and recognition based on a spatiotemporal orientation analysis, IEEE Trans. Pattern Anal. Mach. Intell., vol. 35, no. 3, pp. 527 540, Mar. 2013. 15294 VOLUME 6, 2018 K.-P. Chou et al.: Automated Multi-View Human Action Recognition System [2] A. Gilbert, J. Illingworth, and R. Bowden, Action recognition using mined hierarchical compound features, IEEE Trans. Pattern Anal. Mach. Intell., vol. 33, no. 5, pp. 883 897, May 2011. [3] L. Liu, L. Shao, X. Zhen, and X. Li, Learning discriminative key poses for action recognition, IEEE Trans. Cybern., vol. 43, no. 6, pp. 1860 1870, Dec. 2013. [4] Y. Yang, I. Saleemi, and M. Shah, Discovering motion primitives for unsupervised grouping and one-shot learning of human actions, gestures, and expressions, IEEE Trans. Pattern Anal. Mach. Intell., vol. 35, no. 7, pp. 1635 1648, Jul. 2013. [5] Z. Jiang, Z. Lin, and L. S. Davis, Recognizing human actions by learning and matching shape-motion prototype trees, IEEE Trans. Pattern Anal. Mach. Intell., vol. 34, no. 3, pp. 533 547, Mar. 2012. [6] K. Guo, P. Ishwar, and J. Konrad, Action recognition from video using feature covariance matrices, IEEE Trans. Image Process., vol. 22, no. 6, pp. 2479 2494, Jun. 2013. [7] Y. Chen, Z. Li, X. Guo, Y. Zhao, and A. Cai, A spatio-temporal interest point detector based on vorticity for action recognition, in Proc. IEEE Int. Conf. Multimedia Expo Workshops (ICMEW), Jul. 2013, pp. 1 6. [8] S. Samanta and B. Chanda, Space-time facet model for human activity classi cation, IEEE Trans. Multimedia, vol. 16, no. 6, pp. 1525 1535, Oct. 2014. [9] Z. Moghaddam and M. Piccardi, Histogram-based training initialisation of hidden Markov models for human action recognition, in Proc. 17th IEEE Int. Nat. Conf. Adv. Video Signal Based Surveill. (AVSS), Sep. 2010, pp. 256 261. [10] Y. Wang, L. Wu, and X. Huang, Action recognition using tri-view constraints, in Proc. 8th IEEE Int. Conf. Adv. Video Signal-Based Surveill. (AVSS), Aug. 2011, pp. 107 112. [11] M. N. Kumar and D. Madhavi, Improved discriminative model for view- invariant human action recognition, Int. J. Comput. Sci. Eng. Technol., vol. 4, no. 3, pp. 1263 1270, 2013. [12] F. Zhang, Y. Wang, and Z. Zhang, View-invariant action recognition in surveillance videos, in Proc. 1st Asian Conf. Pattern Recognit. (ACPR), Nov. 2011, pp. 580 583. [13] T. Guha and R. K. Ward, Learning sparse representations for human action recognition, IEEE Trans. Pattern Anal. Mach. Intell., vol. 34, no. 8, pp. 1576 1588, Aug. 2012. [14] N. Ikizler-Cinbis and S. Sclaroff, Web-based classi ers for human action recognition, IEEE Trans. Multimedia, vol. 14, no. 4, pp. 1031 1045, Aug. 2012. [15] D. Wu and L. Shao, Silhouette analysis-based action recognition via exploiting human poses, IEEE Trans. Circuits Syst. Video Technol., vol. 23, no. 2, pp. 236 243, Feb. 2013. [16] C. Schuldt, I. Laptev, and B. Caputo, Recognizing human actions: A local SVM approach, in Proc. 17th Int. Conf. Pattern Recognit., vol. 3. Aug. 2004, pp. 32 36. [17] L. Gorelick, M. Blank, E. Shechtman, M. Irani, and R. Basri, Actions as space-time shapes, IEEE Trans. Pattern Anal. Mach. Intell., vol. 29, no. 12, pp. 2247 2253, Dec. 2007. [18] S. Singh, S. A. Velastin, and H. Ragheb, MuHAVi: A multicamera human action video dataset for the evaluation of action recognition methods, in Proc. 17th IEEE Int. Conf. Adv. Video Signal Based Surveill. (AVSS), Sep. 2010, pp. 48 55. [19] A. A. Efros, A. C. Berg, G. Mori, and J. Malik, Recognizing action at a distance, in Proc. IEEE, Oct. 2003, p. 726. [20] A. Fathi and G. Mori, Action recognition by learning mid-level motion features, in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2008, pp. 1 8. [21] C. Rao and M. Shah, View-invariance in action recognition, in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), vol. 2. Jun. 2001, p. II. [22] A. Ali and J. K. Aggarwal, Segmentation and recognition of continuous human activity, in Proc. IEEE Workshop IEEE Detection Recognit. Events Video, Jul. 2001, pp. 28 35. [23] D. Ramanan and D. A. Forsyth, Automatic annotation of everyday move- ments, in Proc. Adv. Neural Inf. Process. Syst., 2004, pp. 1547 1554. [24] Y. Sheikh, M. Sheikh, and M. Shah, Exploring the space of a human action, in Proc. 10th IEEE Int. Conf. Comput. Vis. (ICCV), vol. 1. Oct. 2005, pp. 144 149. [25] Y. Ke, R. Sukthankar, and M. Hebert, Ef cient visual event detec- tion using, volumetric features, in Proc. 10th IEEE Int. Conf. Comput. Vis. (ICCV), vol. 1. Sep. 2005, pp. 166 173. [26] A. Yilmaz and M. Shah, Actions sketch: A novel action representation, in Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. (CVPR), vol. 1. Jun. 2005, pp. 984 989. [27] D. G. Lowe, Distinctive image features from scale-invariant keypoints, Int. J. Comput. Vis., vol. 60, no. 2, pp. 91 110, 2004. [28] P. Scovanner, S. Ali, and M. Shah, A 3-dimensional sift descriptor and its application to action recognition, in Proc. 15th ACM Int. Conf. Multime- dia, 2007, pp. 357 360. [29] G. Willems, T. Tuytelaars, and L. Van Gool, An ef cient dense and scale- invariant spatio-temporal interest point detector, in Computer Vision ECCV. Berlin, Germany: Springer, 2008, pp. 650 663. [30] P. Dollar, V. Rabaud, G. Cottrell, and S. Belongie, Behavior recognition via sparse spatio-temporal features, in Proc. 2nd Joint IEEE Int. Work- shop Vis. Surveill. Perform. Eval. Tracking Surveill., Oct. 2005, pp. 65 72. [31] A. Eweiwi, S. Cheema, C. Thurau, and C. Bauckhage, Temporal key poses for human action recognition, in Proc. IEEE Int. Conf. Comput. Vis. Workshops (ICCV), Sep. 2011, pp. 1310 1317. [32] J. C. Niebles and L. Fei-Fei, A hierarchical model of shape and appear- ance for human action classi cation, in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2007, pp. 1 8. [33] A. Iosi dis, A. Tefas, and I. Pitas, Neural representation and learning for multi-view human action recognition, in Proc. Int. Joint Conf. Neural Netw. (IJCNN), 2012, pp. 1 6. [34] J. Gall, A. Yao, N. Razavi, L. Van Gool, and V. Lempitsky, Hough forests for object detection, tracking, and action recognition, IEEE Trans. Pattern Anal. Mach. Intell., vol. 33, no. 11, pp. 2188 2202, Nov. 2011. [35] Y. Lu et al., A human action recognition method based on Tchebichef moment invariants and temporal templates, in Proc. 4th Int. Conf. Intell. Human-Mach. Syst. Cybern. (IHMSC), vol. 2. Aug. 2012, pp. 76 79. [36] C. Stauffer and W. E. L. Grimson, Adaptive background mixture models for real-time tracking, in Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit., vol. 2. Jun. 1999, pp. 246 252. [37] J. R. Parker, Algorithms for Image Processing and Computer Vision. Hoboken, NJ, USA: Wiley, 2010. [38] M. Bregonzio, S. Gong, and T. Xiang, Recognising action as clouds of space-time interest points, in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2009, pp. 1948 1955. [39] G. R. Shorack and J. A. Wellner, Empirical Processes With Applications to Statistics. Philadelphia, PA, USA: SIAM, 2009. [40] Z. Zhang, Y. Hu, S. Chan, and L.-T. Chia, Motion context: A new representation for human action recognition, in Computer Vision ECCV. Berlin, Germany: Springer, 2008, pp. 817 829. [41] A. Gilbert, J. Illingworth, and R. Bowden, Scale invariant action recogni- tion using compound features mined from dense spatio-temporal corners, in Proc. Eur. Conf. Comput. Vis., 2008, pp. 222 233. [42] S. Savarese, A. DelPozo, J. C. Niebles, and L. Fei-Fei, Spatial-temporal correlatons for unsupervised action classi cation, in Proc. IEEE Work- shop Motion Video Comput. (WMVC), 2008, pp. 1 8. [43] S. Nowozin, G. Bakir, and K. Tsuda, Discriminative subsequence min- ing for action classi cation, in Proc. IEEE 11th Int. Conf. Comput. Vis. (ICCV), Sep. 2007, pp. 1 8. KUANG-PEN CHOU is currently pursuing the Ph.D. degree with the Department of Com- puter Science, National Chiao Tung University, Hsinchu, Taiwan. He has authored several interna- tional conference papers and journal papers. His current research interests include image process- ing, machine learning, and neural networks. VOLUME 6, 2018 15295 K.-P. Chou et al.: Automated Multi-View Human Action Recognition System MUKESH PRASAD received the master s degree in computer application from Jawaharlal Nehru University, New Delhi, India, in 2009, and the Ph.D. degree in computer science from National Chiao Tung University, Hsinchu, Taiwan, in 2015. He is currently a Lecturer with the School of Soft- ware, University of Technology Sydney, Australia. He has authored papers in international journals and conferences, including the IEEE Transactions, ACM, Elsevier, and Springer. His current research interests include machine learning, pattern recognition, fuzzy systems, neural networks, arti cial intelligence, and brain computer interface. DI WU received the B.Sc. and M.Sc. degrees in information technology from Deakin Univer- sity, Australia, in 2010 and 2015, respectively. He is currently pursuing the Ph.D. degree with the Faculty of Engineering and IT, School of Software, University of Technology Sydney, Australia. NABIN SHARMA (SM 18) received the Ph.D. degree from the School of ICT, Grif th University, Queensland, Australia. He is currently a Research Associate with the School of Software, Faculty of Engineering and IT, University of Technology Sydney, Australia. His research area focuses on video and image processing, pattern recognition, and machine learning techniques for object detec- tion and recognition, text classi cation, and doc- ument analysis. He is a member of the ACM, the Australian Water Association, and the Indian Unit for Pattern Recognition and Arti cial Intelligence. DONG-LIN LI received the master s degree in electrical engineering from National Chung Hsing University, Taichung, Taiwan, in 2006, and the Ph.D. degree in electrical engineering from National Chiao Tung University, Hsinchu, Taiwan, in 2013. He is currently a Post-Doctoral Researcher with National Chiao Tung University. He has authored papers in international journal and conferences, including the IEEE Transactions, ACM, Elsevier, and Springer. His current research interests include machine learning, fuzzy systems, neural networks and image processing, and smart living technology. YU-FENG LIN received the master s degree from the Department of Computer Science, National Chiao Tung University, Hsinchu, Taiwan. His research interests include image processing, machine learning, and signal processing. MICHAEL BLUMENSTEIN received the Ph.D. degree in computational intelligence from Grif th University, Queensland, Australia, in 2001. He is currently a Professor and the Associate Dean (Research Strategy and Management), and the Head of the School of Software, University of Technology Sydney, Australia. He is an inter- nationally and nationally renowned expert in the areas of pattern recognition and arti cial intelli- gence (speci cally machine learning and neural networks). He has authored over 170 papers in refereed conferences, jour- nals, and books in these areas. His research also spans various projects, applying arti cial intelligence to the elds of engineering, environmental science, neurobiology, and coastal management. Components of his research into the predictive assessment of beach conditions have been commercialized for use by the local government agencies and coastal management authorities and in commercial applications. WEN-CHIEH LIN received the Ph.D. degree from Carnegie Mellon University, Pittsburgh, PA, USA. He is currently an Associate Professor with the Department of Computer Science, National Chiao Tung University, Hsinchu, Taiwan. He has authored papers in international journal and con- ferences, including the IEEE Transactions, ACM, Elsevier, and Springer. His current research inter- ests include computer graphics, image processing, machine learning, and fuzzy systems. CHIN-TENG LIN (F 05) received the B.S. degree from National Chiao Tung University (NCTU), Taiwan, in 1986, and the master s degree and the Ph.D. degree in electrical engineering from Purdue University, West Lafayette, IN, USA, in 1989 and 1992, respectively. He is currently the Chair Pro- fessor with the Faculty of Engineering and Infor- mation Technology, University of Technology Sydney, the Chair Professor of electrical and com- puter engineering with NCTU, the International Faculty, University of California at San Diego, and the Honorary Professor with the University of Nottingham. He has co-authored the Neural Fuzzy Systems (Prentice-Hall) and has authored the Neural Fuzzy Control Systems with Structure and Parameter Learning (World Scienti c). He contributed to biologically inspired information systems in 2005. He was elevated as an International Fuzzy Systems Association Fellow in 2012. He has been elected as the Editor-in-Chief of the IEEE TRANSACTIONS ON FUZZY SYSTEMS since 2011. He also served on the Board of Governors for the IEEE Circuits and Systems (CAS) Society from 2005 to 2008, the IEEE Systems, Man, Cybernetics Society from 2003 to 2005, and the IEEE Computational Intel- ligence Society (CIS) from 2008 to 2010. He served as the Chair for the IEEE Taipei Section from 2009 to 2010. He is a Distinguished Lecturer of the IEEE CAS Society from 2003 to 2005 and the CIS Society from 2015 to 2017. He served as the Deputy Editor-in-Chief for the IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS-II from 2006 to 2008. He was the Program Chair of the IEEE International Conference on Systems, Man, and Cybernetics in 2005 and the General Chair of the 2011 IEEE International Conference on Fuzzy Systems. He has authored over 200 journal papers (Total Citation: 20,155, H-index: 53, i10-index: 373) in the areas of neural networks, fuzzy systems, multimedia hardware/software, and cognitive neuro-engineering, including approximately 101 IEEE journal papers. 15296 VOLUME 6, 2018