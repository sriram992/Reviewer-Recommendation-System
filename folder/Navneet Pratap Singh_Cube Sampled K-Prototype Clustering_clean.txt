Cube Sampled K-Prototype Clustering for Featured Data Seemandhar Jain Computer Science and Engineering Indian Institute of Technology Indore Indore, India cse170001046@iiti.ac.in Aditya A. Shastri Information Technology NMIMS, Shirpur Shirpur, India aditya.shastri@nmims.edu Kapil Ahuja Math of Data Science & Simulation (MODSS) Lab Indian Institute of Technology Indore Indore, India kahuja@iiti.ac.in Yann Busnel Network Systems, Cybersecurity and Digital Law Department Institut Mines-Telecom Atlantique Rennes, France yann.busnel@imt-atlantique.fr Navneet Pratap Singh Computer Science and Engineering NMIMS Indore Indore, India navneet.diat@gmail.com Abstract Clustering large amount of data is becoming in- creasingly important in the current times. Due to the large sizes of data, clustering algorithm often take too much time. Sampling this data before clustering is commonly used to reduce this time. In this work, we propose a probabilistic sampling technique called cube sampling along with K-Prototype clustering. Cube sampling is used because of its accurate sample selection. K- Prototype is most frequently used clustering algorithm when the data is numerical as well as categorical (very common in today s time). The novelty of this work is in obtaining the crucial inclusion probabilities for cube sampling using Principal Component Analysis (PCA). Experiments on multiple datasets from the UCI repository demonstrate that cube sampled K-Prototype algorithm gives the best clustering accuracy among similarly sampled other popular clustering algorithms (K-Means, Hierarchical Clustering (HC), Spectral Clustering (SC)). When compared with unsampled K- Prototype, K-Means, HC and SC, it still has the best accuracy with the added advantage of reduced computational complexity (due to reduced data size). Index Terms Sampling, Cube Sampling, Clustering, K- Prototype Clustering, Principal Component Analysis, Clustering Accuracy. I. INTRODUCTION Due to the pervasiveness of social media and the sub- sequent transformation of the world digitally, an extensive amount of data is being generated. According to a survey [1], more than 2.5 quintillion bytes of data is produced every day, which is increasing exponentially. Since not all of this data is useful, it is important to obtain some meaning from it to help in decision making. Clustering is the technique to group together the data items having similar behavior. It is often used to get the initial perception of data and is widely used. Some of the pop- ular clustering algorithms include K-Means, K-Modes, K- Prototype, Hierarchical Clustering (HC), Spectral Clustering (SC), Partitioning Around Medoids (PAM) etc. [2] [5]. K- Prototype is a commonly used clustering algorithm because of its ability to handle numerical and categorical data together [6]. To achieve this, it uses a combination of the K-Means and the K-Modes algorithms and is easy to implement as well. Clustering becomes dif cult when the size of the data is very large. One of the ways to develop an ef cient clustering algorithm is to sample this data. There are several sampling techniques that are commonly used. This include random sam- pling, strati ed sampling, Vector Quantization (VQ), pivotal sampling, cube sampling etc. [7] [10]. We focus on cube sampling because rstly it belongs to the class of probabilistic sampling that are accurate. Secondly, it is independent of the order of the data selected to compute the crucial inclusion probabilities [10]. To sum up, we propose cube sampled K- Prototype clustering algorithm with speci c focus on data that contains numerical and categorical features. There have been a few attempts to perform clustering while using a sampling technique to reduce the data [8], [9]. In [8], authors have proposed use of VQ sampling technique along with SC. VQ is a very basic sampling technique with comparatively low accuracy. Moreover, the algorithm in [8] is speci cally designed for genome data. Another work is [9], where a different probabilistic sampling technique called pivotal sampling is used with SC. Although, pivotal sampling is good in terms of accuracy, this algorithm is tested on only plant phenotypic data. This paper also uses pivotal sampling with HC. However, it also has same limitations as pivotal sampling with SC. In another work [11], the authors have used random sampling with the ensemble clustering algorithm. However, they do not show the ef cacy of their approach on large datasets. As evident from above discussion, cube sampling has many advantages and it has not been coupled with any of the commonly used clustering algorithms, which is the focus of this work. As mentioned above, besides combining cube sampling with K-Prototype, we integrate it with K-Means, arXiv:2108.10262v1 [cs.LG] 23 Aug 2021 HC, and SC as well. The most novel contribution of this work is in computing the crucial inclusion probabilities for the cube sampling technique using the concept of Principal Component Analysis (PCA). The proposed algorithm is evaluated with extensive exper- iments that are performed on six labeled datasets from UCI repository with varying sizes. Clustering Accuracy is used as a metric for comparison. We perform four sets of experiments. First, we compare four commonly used clustering algorithms (i.e. unsampled K-Prototype, K-Means [2], HC [4], and SC [5]), where the K-Prototype gives the best accuracy. Second, when using standard random sampling [7], we again compare these algorithms. Here, again, K-Prototype with random sampling gives the best accuracy. Third, we perform same comparison with our cube sampling. Fourth and nally, we compare our cube sampled K-Prototype with unsampled other clustering algorithms mentioned above. Results for both the third and the fourth sets of comparisons show that our proposed algorithm performs the best. Also, out cube sampled K-Prototype has the added advantage of reduced computational complexity (since the input data is reduced by cube sampling before clustering). The rest of this paper is organized as follows. Section II provides a detailed discussion of our proposed algorithm. Experimental setup is presented in Section III. Section IV gives the experimental results. Finally, conclusions and future work are provided in Section V. II. PROPOSED ALGORITHM In this section, we rst explain the concept of cube sam- pling including our novel way of obtaining the inclusion probabilities. Subsequently, we discuss the K-Prototype clus- tering algorithm, followed by our cube sampled K-Prototype clustering. A. Application of Cube Sampling Cube sampling is a type of probabilistic sampling technique that selects a balanced sample [10]. Consider a population U of size n such that U = {x1, x2, .., xn}, where xi Rm for i = 1, 2, ..., n and m is the total number of features. A sample S of size N is said to be balanced if the following equality is satis ed [12]. X j S xj j = X i U xi, (1) where j is the inclusion probability of the jth unit and j = 1, 2, ..., N. Obtaining the inclusion probabilities of all the units in the population, denoted by i, forms an important aspect of this probability based sampling technique. Cube sampling obtains the samples in two phases; the ight phase and the landing phase. The ight phase starts by obtaining the initial values of i for i = 1, 2, ..., n (our unique contribution; discussed in the subsequent paragraphs). These initial probabilities are updated in each iteration of ight phase using a random walk. At each iteration, at least one unit is either included or excluded from the sample. At the end of this phase, inclusion probabilities of all the units are either equal to 0 (eliminated from the sample) or equal to 1 (included in the sample). After the ight phase, if the balancing equation is not exactly satis ed or, inclusion probability of even one unit is neither 0 nor 1, then there is a need for the landing phase. The goal of the landing phase is to nd a sample S such that it is almost balanced (i.e. as close to balanced as possible). Let p be the number of units having inclusion probabilities neither 0 nor 1. Here, we obtain all the possible samples by taking the probability values of these p units both 0 and 1. As there are two possible values for each of these p units, we get a total of 2p possible samples. Finally, we select the sample that closely satis es the balancing condition. Next, the working of cube sampling is explained using an example of a random population of size six. Let the initial probabilities values are 0 = (0.5, 0.5, 0.5, 0.5, 0.5, 0.5), and we need to select three samples. The cube sampling transforms these initial inclusion probabilities as shown in (2). Here, units with the nal probability as 0 are discarded and units with the nal probability as 1 are selected in the sample. 0 = 0.5 0.5 0.5 0.5 0.5 0.5 0.6 0.6 0 0.6 0.6 0.6 0.6 0 0 1 0.8 0.6 1 0 0 1 0.5 0.5 1 0 0 1 1 0 = n (2) Inclusion probability of an unit xi is de ned as the prob- ability with which the unit is selected in the sample S and, as earlier, is denoted by i. Here, we propose a novel and an ef cient way to calculate the initial inclusion probabilities using Principle Component Analysis (PCA) [13]. Although PCA has been used earlier to sample data, it has never been combined with any probabilistic sampling techniques. PCA transforms the data by projecting it onto a single axis where elements are maximally deviated. The process of calculating the inclusion probabilities is described in the following equations [10]: i = min h 1, h 1(n)xi X i , i U (3) where, as earlier, U is the total population and h is de ned as h(z) = X i U min  z xi X , 1  , (4) with P i U i = N and X = P i U xi. B. Our Algorithm The K-Prototype algorithm is an integration of the K- Means algorithm and the K-Modes algorithm to handle mixed data types. Here, the basic idea is similar to K-Means with a different distance criteria to handle numerical and categorical features. The aim is to minimize the cost function by partitioning the dataset into k clusters. This cost function is given as C = k X l=1 N X j=1 pjlDIST (xj, ql) , (5) where pjl {0, 1} denotes the membership of data point xj in cluster l, ql denotes the cluster center for cluster l and DIST(xj, ql) is the distance criteria (dissimilarity measure), which is given as DIST (xj, ql) = mr X r=1 (xjr qlr)2 + l mt X t=1 (xjt, qlt) , (6) where, r is index for numerical features and mr is the total number of numerical features for each data point xj, xir denotes the value of the rth numerical feature of xj, qlr is the mean of the rth numerical feature in the cluster l, l is the weight factor for categorical features for cluster l, t is index for categorical features and mt is the total number of categorical features of xj, xjt denotes the tth categorical feature of xj, qlt is the mode of the tth categorical feature in the cluster l, and (a, b) = 1 for a = b and (a, b) = 0 for a = b. The proposed algorithm (Cube sampled K-Prototype algo- rithm) is given in Algorithm 1 and the broad framework is graphically represented in Fig. 1. Start Dataset New Dataset Results/End Convert into one dimensional data using PCA standardize Find inclusion probabilities using proposed Algorithm Sample using Cube method Apply Clustering Algorithm Remap unselected points Fig. 1: Framework for the proposed algorithm. Algorithm 1: Proposed Algorithm Input: A- Data Matrix (n m rows), N- Sampling size, k- Number of clusters Output: Clusters 1 denotes initial probabilities that is computed as discussed in Section II-A 2 n denotes nal probabilities that is obtained after the ight and the landing phase of cube sampling (also discussed in Section II-A) 3 Generate sample S from n by using units with probability 1 4 Choose k cluster centers from S randomly to form qold 5 Find the distance between elements and cluster centers as described in (6) to form q. 6 while qold = q do 7 qold = q 8 Update q using (6) again 9 Reverse map the remaining points based on their similarity with all clusters. 10 return ClusterDetails III. EXPERIMENTAL SETUP In this section, we rst brie y discuss the datasets used for our experiments and then describe the criteria used to check the goodness of our proposed algorithm. For experiments, we use six datasets with varying sizes. These include labeled datasets downloaded from the UCI repository [14]. Table I provides a brief summary of these datasets, where all the columns are self explanatory. TABLE I: Dataset information from the UCI repository Dataset # of records # of features # of classes German 1000 20 2 Heart 303 13 2 Satellite 6435 36 2 Adult 45222 14 2 Shuttle 43500 9 7 Kddcup99 24701 41 18 Since we use labeled datasets for our experiments, we check the quality of our clustering by using Clustering Ac- curacy (CA), which is calculated as given below [3]. CA = k X l=1 max (Cl | Ll) |U| , (7) where Cl is the set of elements in the lth cluster, Ll is the set of all the class labels in the lth cluster, max(Cl | Ll) is the number of elements with the majority label in the lth cluster, and U is the population size. The experiments are performed on a machine with Intel Core i7-7500U CPU 2.90 GHz and 16 GB RAM. Python 3.7 is used to implement all the algorithms. IV. RESULTS In this section, we show the effectiveness of our proposed algorithm (i.e. Cube sampled K-Prototype). For this, we do four various comparisons here. Without performing any sampling, we rst compare K- Prototype clustering with other commonly used cluster- ing techniques (K-Means, Hierarchical Clustering (HC), Spectral Clustering (SC)). When using frequently used random sampling, we again compare K-Prototype with same set of clustering algo- rithms. Next, when using our proposed cube sampling, we further compare all the above-mentioned clustering al- gorithms. Finally, we compare our cube sampled K-Prototype algorithm with unsampled other clustering algorithms mentioned above. Table II compares four different clustering algorithms (K- Prototype, K-Means, Hierarchical Clustering (HC), and Spec- tral Clustering (SC)) without applying any prior sampling of data. From this table, it is clear that CA values for K- Prototype are greater than or equal to the other algorithms for all the datasets. Thus, K-Prototype performs best among all. TABLE II: CA values for four different clustering algorithms on the six labeled datasets without prior application of sam- pling. Dataset K-Prototype K-Means HC SC German 0.68 0.53 0.62 0.66 Heart 0.59 0.52 0.53 0.52 Satellite 0.79 0.71 0.72 0.75 Adult 0.69 0.61 0.67 0.68 Shuttle 0.82 0.82 0.82 0.82 Kddcup99 0.69 0.59 0.69 0.68 Next, in Table III, we again compare the four clustering algorithms mentioned above but on sampled data (using random sampling). Without loss of generality, we take sample size as 100 for the two relatively smaller datasets (German and Heart), while sample size is taken as 1000 for the other datasets. From this table, we again observe that K-Prototype with random sampling performs best when compared with all other algorithms with random sampling. Use of random sampling deteriorate the CA values substantially, which is not the case with our sampling technique (discussed below). Further, in Table IV, we perform the same set of compar- isons as in Table III except using data obtained after applying cube sampling. The sample sizes here are same as in the previous set of comparisons. We can observe from this table that the CA values for K-Prototype with cube sampling (our proposed algorithm) is greater than all the other algorithms with cube sampling. Finally, we compare our proposed algorithm (cube sam- pled K-Prototype) with the unsampled four clusterings as TABLE III: CA values for four different clustering algorithms on the six labeled datasets with prior application of random sampling. Dataset N K-Proto. K-Means HC SC German 100 0.51 0.39 0.43 0.45 Heart 100 0.48 0.41 0.39 0.42 Satellite 1000 0.63 0.53 0.55 0.56 Adult 1000 0.60 0.54 0.55 0.57 Shuttle 1000 0.69 0.68 0.68 0.69 Kddcup99 1000 0.41 0.37 0.39 0.40 TABLE IV: CA values for four different clustering algorithms on the six labeled datasets with cube sampling. Dataset N K-Proto. K-Means HC SC German 100 0.68 0.52 0.60 0.64 Heart 100 0.59 0.50 0.52 0.51 Satellite 1000 0.78 0.69 0.70 0.73 Adult 1000 0.70 0.60 0.64 0.67 Shuttle 1000 0.82 0.80 0.81 0.82 Kddcup99 1000 0.69 0.56 0.66 0.67 discussed before. For this, we pick the data from the relevant columns of Table II and Table IV. This set of comparisons is given in Table V below. We observe from this table that our algorithm performs better than all others. To demonstrate that our technique gives good result irre- spective of the size of the sample, in Fig. 2, we plot the CA values of our proposed algorithm for different sample sizes and different datasets. On the x-axis, sample size is given, and on the y-axis, CA values are given. From these graphs, it is clear that sample size does not affect the CA values. Thus, use of cube sampling does not deteriorate the quality of clustering as done by K-Prototype. It also has an added advantage of reduced computational complexity (as it works on reduced dataset making it faster). TABLE V: CA values for our cube sampled K-Prototype algorithm and other four unsampled clustering algorithms on the six labeled datasets. Dataset N Our Algo. K- Proto. K- Means HC SC German 100 0.68 0.68 0.53 0.62 0.66 Heart 100 0.59 0.59 0.52 0.53 0.52 Satellite 1000 0.78 0.79 0.71 0.72 0.75 Adult 1000 0.70 0.69 0.61 0.67 0.68 Shuttle 1000 0.82 0.82 0.82 0.83 0.82 Kddcup99 1000 0.69 0.69 0.59 0.69 0.68 V. CONCLUSION In this paper, we propose a cube sampled K-Prototype algorithm for grouping large data. The most innovative aspect of our algorithm is computation of inclusion probabilities in cube sampling using PCA. Experiments on labeled UCI 0 200 400 600 800 1000 Value of N 0.0 0.2 0.4 0.6 0.8 1.0 Cluster Accuracy (a) German Dataset 0 50 100 150 200 250 300 Value of N 0.0 0.2 0.4 0.6 0.8 1.0 Cluster Accuracy (b) Heart Dataset 0 2000 4000 6000 Value of N 0.0 0.2 0.4 0.6 0.8 1.0 Cluster Accuracy (c) Satellite Dataset 0 10000 20000 30000 40000 Value of N 0.0 0.2 0.4 0.6 0.8 1.0 Cluster Accuracy (d) Adult Dataset 0 10000 20000 30000 40000 Value of N 0.0 0.2 0.4 0.6 0.8 1.0 Cluster Accuracy (e) Shuttle Dataset 0 5000 10000 15000 20000 Value of N 0.0 0.2 0.4 0.6 0.8 1.0 Cluster Accuracy (f) Kddcup99 Dataset Fig. 2: CA values of our cube sampled K-Prototype algorithm for different sample sizes and different datasets. datasets with varying sizes show that our proposed algorithm gives best clustering accuracy when compared with other commonly used unsampled clustering algorithms (K-Means, HC, and SC). Also, use of cube sampling does not deteriorate the accuracy of the K-Prototype algorithm. Furthermore, our algorithm also has the added advantage of reduced compu- tational complexity (since the size of the data is reduced by cube sampling). In future, we plan to adapt our algorithm for datasets with millions of units. We also aim to combine cube sampling with other accurate clustering algorithms. We intend to adapt our algorithm for Hadoop framework to make it parallel as well [15]. REFERENCES [1] J. Milenkovic, 30 Eye-opening big data statistics for 2020: pat- terns are everywhere, https://kommandotech.com/statistics/big-data- statistics/, Accessed: June 2020. [2] S. Lloyd, Least squares quantization in PCM, IEEE Transactions on Information Theory, vol. 28, pp. 129 137, 1982. [3] A. Fahad, N. Alshatri, Z. Tari, A. Alamri, I. Khalil, A. Zomaya, et al., A survey of clustering algorithms for big data: taxonomy and empirical analysis, IEEE Transactions on Emerging Topics in Computing, vol. 2, pp. 267 279, 2014. [4] P. Painkra, R. Shrivatava, S. Nag and N. Kumar, Clustering analysis of soybean germplasm (Glycine max L. merrill), The Pharma Innovation Journal, vol. 7, pp. 781 786, 2018. [5] U. Luxburg, A tutorial on spectral clustering, Statistics and Comput- ing, vol. 17, pp. 395 416, 2007. [6] Z. Huang, Extensions to the k-means algorithm for clustering large data sets with categorical values, Data Mining and Knowledge Dis- covery, vol. 2, pp. 283 304, 1998. [7] F. Olken and D. Rotem, Simple random sampling from relational databases, University of California, 1986. [8] A. Shastri, K. Ahuja, M. Ratnaparkhe, A. Shah, A. Gagrani and A. Lal, Vector quantized spectral clustering applied to whole genome sequences of plants, Evolutionary Bioinformatics, vol. 15, pp. 1-7, 2019. [9] A. Shastri, K. Ahuja, M. Ratnaparkhe and Y. Busnel, Probabilistically sampled and spectrally clustered plant species using phenotypic char- acteristics, PeerJ, in press, 2021. [10] Y. Till e, Sampling Algorithms, Springer, 2006. [11] F. Yang, X. Li, Q. Li and T. Li, Exploring the diversity in cluster ensemble generation: random sampling and random projection, Expert Systems with Applications, vol. 41, pp. 4844-4866, 2014. [12] G. Chauvet and Y. Till e, A fast algorithm for balanced sampling, Computational Statistics, vol. 21, pp. 53 62, 2006. [13] S. Karamizadeh, S. Abdullah, A. Manaf, M. Zamani and A. Hooman, An overview of principal component analysis, Journal of Signal and Information Processing, vol. 4, pp. 173, 2013. [14] D. Dua and C. Graff, UCI machine learning repository, http://archive.ics.uci.edu/ml, Accessed: February 2021. [15] V. Nemade, A. Shastri, K. Ahuja and A. Tiwari, Scaled and projected spectral clustering with vector quantization for handling big data , Proc. of the 9th Symposium Series on Computational Intelligence (SSCI), IEEE, Bengaluru, India, pp. 2174 2179, 2018.