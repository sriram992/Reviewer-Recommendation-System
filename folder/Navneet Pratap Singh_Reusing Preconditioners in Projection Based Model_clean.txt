Received July 3, 2020, accepted July 8, 2020, date of publication July 15, 2020, date of current version July 30, 2020. Digital Object Identifier 10.1109/ACCESS.2020.3009456 Reusing Preconditioners in Projection Based Model Order Reduction Algorithms NAVNEET PRATAP SINGH AND KAPIL AHUJA Data and Computational Sciences Laboratory, IIT Indore, Indore 453552, India Corresponding authors: Navneet Pratap Singh (navneet.diat@gmail.com) and Kapil Ahuja (kapsahuja22@gmail.com) The work of Kapil Ahuja was supported by the Mathematical Research Impact Centric Support (MATRICS) Scheme of Department of Science and Technology-Science and Engineering Research Board (DST-SERB), India, under Grant MTR/2017/001023. ABSTRACT Dynamical systems are pervasive in almost all engineering and scienti c applications. Simulat- ing such systems is computationally very intensive. Hence, Model Order Reduction (MOR) is used to reduce them to a lower dimension. Most of the MOR algorithms require solving large sparse sequences of linear systems. Since using direct methods for solving such systems does not scale well in time with respect to the increase in the input dimension, ef cient preconditioned iterative methods are commonly used. In one of our previous works, we have shown substantial improvements by reusing preconditioners for the parametric MOR (Singh et al. 2019). Here, we had proposed techniques for both, the non-parametric and the parametric cases, but had applied them only to the latter. We have three main contributions here. First, we demonstrate that preconditioners can be reused more effectively in the non-parametric case as compared to the parametric one. Second, we show that reusing preconditioners is an art via detailed algorithmic implementations in multiple MOR algorithms. Third and nal, we demonstrate that reusing preconditioners for reducing a real- life industrial problem (of size 1.2 million), leads to relative savings of up to 64% in the total computation time (in absolute terms a saving of 5 days). INDEX TERMS Model order reduction, moment matching, iterative methods, preconditioners, reusing preconditioners. I. INTRODUCTION Dynamical systems arise in many engineering and scienti c applications such as weather prediction, machine design, circuit simulation, biomedical engineering, etc. Generally, dynamical systems corresponding to real-world applications are extremely large in size. A set of equations describing a parametric nonlinear second-order dynamical system is represented as g( x(t), p) = f ( x(t), p) + h(x(t), p, u(t)), y(t) = CT x(t), (1) where t is the time variable, x(t) : R Rn is the state, p = (p1, p2, . . . , pk) is the set of parameters (with pj R; forj = 1, . . . , k), u(t) : R Rm is the input, y(t) : R Rq is the output, CT Rq n is the output matrix, and g( ) : Rn+k Rn, f ( ) : Rn+k Rn and h( ) : Rn+k+m Rn are some nonlinear functions [1] [6]. If m and q both are equal to one, then we have a Single-Input Single-Output (SISO) system. The associate editor coordinating the review of this manuscript and approving it for publication was Yan-Jun Liu. Otherwise, it is called a Multi-Input Multi-Output (MIMO) (m and q > 1) system. The functions g( ), f ( ), and h( ) are usually simpli ed as [2], [6] g( x(t), p) = k X j=1 gj(p)g( x(t)), f ( x(t), p) = k X j=1 fj(p)f( x(t)), h(x(t), p, u(t)) = k X j=1 hj(p)h(x(t), u(t)), (2) where gj( ), fj( ), hj( ) : Rk R are scalar-valued functions while g( ), f( ) : Rn Rn, and h( ) : Rn+m Rn are vector-valued. Next, we look at simpli cations to (1) based upon the three predicates; the presence of parameters; the degree of non-linearity, and the order of the system. If gj(p), fj(p), and hj(p) are independent of the param- eters, then (1) becomes a non-parametric dynamical system. VOLUME 8, 2020 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 133233 N. P. Singh, K. Ahuja: Reusing Preconditioners in Projection Based MOR Algorithms TABLE 1. MOR Algorithms. Bilinear systems are one of the common types of nonlin- ear dynamical systems. Here, there is a product between the state variables and the input variables. Another important class of nonlinear dynamical systems is the quadratic systems. Here, there is product among the state variables. If g( ) and f( ) are linear functions of the state variables, and h( ) is a linear function of the state and the input variables, then (1) is called a linear dynamical system. Finally, if the second derivative term in (1) is not present, then (1) becomes a rst-order dynamical system. Simulation of large dynamical systems can be unman- ageable due to high demands on computational resources. These large systems can be reduced into a smaller dimen- sion by using Model Order Reduction (MOR) techniques [4], [7] [11]. The reduced system has approximately the same characteristics as the original system but it requires signi cantly less computational effort in simulation. MOR can be done in many ways such as balanced trunca- tion, Hankel approximations, and Krylov projection [4], [7], [8], [11]. Among these, the projection methods are quite popular, and hence, we focus on them. Some of the commonly used projection-based MOR algorithms for different types of dynamical systems are sum- marized in Table 1. In the above mentioned MOR algorithms, sequences of very large and sparse linear systems arise during the model reduction process. Solving such linear systems is the main computational bottleneck in ef cient scaling of these MOR algorithms for reducing extremely large dynamical systems. Preconditioned iterative methods are commonly used for solving such linear systems [25], [26]. In most of the above listed MOR algorithms, the change from one linear system to the next is usually very small, and hence, the applied preconditioner could be reused. Next, we brie y summarize the past work that has been done in the eld of reusing preconditioners. References [27] and [28] rst applied this technique in the quantum Monte Carlo context, where it is referred to as recycling precon- ditioners. For the case when the linear system coef cient matrices are perturbed by a varying constant times the identity matrix, ef cient preconditioners have also been developed. These preconditioners are independent of the underlying application and are referred to as preconditioner updates (see [29] for Symmetric Positive De nite (SPD) coef cient matrices and [30] for general coef cient matrices). This approach has been used in the optimization context in [31], where it is again termed as preconditioner updates. In the MOR context, [12] and [32] have used this technique for MOR of non-parametric linear rst-order dynamical sys- tems (part of the rst category above). The main goal of this paper is to demonstrate the reuse of preconditioners in the remainder of the algorithms for the rst category above (MOR of non-parametric linear second-order dynamical systems) as well as the algorithms for the second category above (MOR of non-parametric bilinear/ quadratic- bilinear dynamical systems). In one of our recent works [33], we had proposed a gen- eral framework for reuse of preconditioners during MOR of both non-parametric and parametric dynamical systems. However, in [33] we had demonstrated application of this framework for the parametric case only. That is, the third cat- egory above (MOR of parametric linear dynamical systems). We are currently (and separately) working on the algorithms for the fourth category above as well (MOR of parametric bilinear/quadratic-bilinear dynamical systems). To summarize, in this paper we broadly demonstrate the application of our above mentioned framework for MOR of non-parametric dynamical systems. We have three contribu- tions as below, which have not been catered in any of the above cited papers. (i) We demonstrate that because of the lack of the param- eters in the non-parametric case, the reuse of precondi- tioners here can be done more effectively as compared to the parametric case. (ii) We show that the reuse of preconditioners needs to be ne-tuned for the underlying MOR algorithm. We also highlight that there are multiple pitfalls in the algorith- mic implementation of reusing preconditioners. (iii) We experiment on a massively large and real-life indus- trial problem (BMW disc brake model), which is of size 1.2 million. Here, we are able to reduce the computation time from 197 hours to about 72 hours (relative saving of 64 %). The paper has four more sections. We discuss MOR tech- niques in Section II. The theory of reusing preconditioners is 133234 VOLUME 8, 2020 N. P. Singh, K. Ahuja: Reusing Preconditioners in Projection Based MOR Algorithms described in Section III. We support our theory with numeri- cal experiments in Section IV. Finally, conclusions and future work are discussed in Section V. For the rest of this paper, f denotes the Frobenius norm, denotes the Euclidean norm for vectors and the induced spectral norm for matrices, refers to the Kronecker product (i.e. an operation on two matrices of arbitrary size), vec( ) signi es the vectorization of a matrix, and I denotes the Identity matrix. II. MOR As above, our focus is on MOR of the non-parametric dynamical systems. Hence, we summarize some of the previ- ously listed such algorithms here. Adaptive Iterative Rational Global Arnoldi (AIRGA) [15] is a Ritz-Galerkin projection based algorithm for MOR of linear second-order MIMO dynamical systems with proportional damping, which for the MIMO case are represented as M x(t) = D x(t) Kx(t) + Fu(t), y(t) = CT x(t), (3) where M, D, K Rn n, F Rn m, C Rn q, and D = M + K. Here, , are some scalar values. Let V Rn r and its columns span a r-dimension subspace (r n). In principle, the Ritz-Galerkin projection method involves the steps below. Approximating the reduced state vector x(t) using V as x(t) V x(t) leads to MV x(t) + DV x(t) + KV x(t) Fu(t) = r(t), y(t) = CT V x(t), where r(t) is the residual after projection. Enforcing the residual r(t) to be orthogonal to V or V T r(t) = 0 leads to the reduced system given as follows: M x(t) + D x(t) + K x(t) Fu(t) = 0, y(t) = CT x(t), where M = V T MV, D = V T DV, K = V T KV, F = V T F, and CT = CT V. To compute this projection matrix V, AIRGA matches the moments of the original system transfer function and the reduced system transfer function. We brie y summarize AIRGA in Algorithm 1, where parts relevant to solving linear systems are only listed. Bilinear Iterative Rational Krylov Algorithm (BIRKA) [16] is a Petrov-Galerkin projection based algorithm for MOR of the bilinear rst-order dynamical systems, which for the MIMO case are represented as x(t) = Kx(t) + m X j=1 Njx(t)uj(t) + Fu(t), y(t) = CT x(t), (4) where K, Nj Rn n, F Rn m, C Rn q, and u = [u1, u2, . . . , um] Rm. Let columns of V, W Rn r span two r-dimension subspaces (where, as earlier, r n ). Algorithm 1 AIRGA [15] Input: M, D, K, F, C; S is the set of initial expansion points si, i = 1, . . . , . Output: M, D, K, F, C. 1: z = 1 2: while (no convergence) do 3: for i = 1, . . . , do 4: X(0)(si) = (s2 i M + siD + K) 1F 5: V1 = X(0)(si) X(0)(si) f 6: end for 7: j = 1 8: while (no convergence) do 9: for i = 1, . . . , do 10: X(j)(si) = (s2 i M + siD + K) 1MVj 11: Vj+1 = X(j)(si) X(j)(si) f 12: end for 13: j = j + 1 14: end while 15: All the given set of expansion points (i.e. s1, s2, . . . , s ) are updated 16: z = z + 1 17: end while 18: M = V T MV, D = V T DV, K = V T KV, F = V T F, and CT = CT V In principle, the Petrov-Galerkin projection method involves the steps below. Approximating the reduced state vector x(t) using V as x(t) V x(t) leads to V x(t) KV x(t) m X j=1 NjV x(t)uj(t) Fu(t) = r(t), y(t) = CT V x(t), where r(t) is the residual after projection. Enforcing the residual r(t) to be orthogonal to W or W T r(t) = 0 leads to the reduced system given by x(t) K x(t) m X j=1 Nj x(t)uj(t) Fu(t) = 0, y(t) = CT x(t), where K = (W T V) 1W T KV, Nj = (W T V) 1W T NjV, F = (W T V) 1W T F, CT = CT V, and (W T V) 1 is assumed to be invertible. Here, V and W are computed by using interpolation, where the original system transfer function and its derivative are respectively matched with the reduced system transfer function and its derivative at a set of points. We brie y summarize BIRKA in Algorithm 2, where again, only parts related to solving linear systems are listed. Quadratic Bilinear-Implicit Higher Order Moment Match- ing (QB-IHOMM) [19] is a Petrov-Galerkin projection based VOLUME 8, 2020 133235 N. P. Singh, K. Ahuja: Reusing Preconditioners in Projection Based MOR Algorithms Algorithm 2 BIRKA [16] Input K, N1, . . . , Nm, F, C, and initial guess of the reduced system K, N1, . . . , Nm, F, C Output K, N1, . . . , Nm, F, and C 1: z = 1 2: while (no convergence) do 3: R3R 1 = K, F = FT R T , C = CR, Nj = RT NjR T for j = 1, . . . , m 4: vec (V) = 3 In Ir K mP j=1 N T j Nj ! 1  FT F  vec(Im) 5: vec (W) = 3 In Ir K T mP j=1 Nj N T j ! 1  CT CT  vec(Iq) 6: V = orth (V ) , W = orth (W) 7: K = (W T V) 1W T KV, Nj =  W T V  1 W T NjV, F =  W T V  1 W T F, C = CV 8: z = z + 1 9: end while 10: K = K, Nj = Nj, F = F, and C = C algorithm for MOR of the quadratic-bilinear rst-order dynamical systems, which for the SISO case are repre- sented as1 D x(t) = Kx(t) + Nx(t)u(t) + H (x(t) x(t)) + Fu(t), y(t) = CT x(t), (5) where D, K, N Rn n, H Rn n2, F Rn 1, C Rn 1. Let columns of V, W Rn r span two r-dimension subspaces (where as earlier, r n). In princi- ple, the Petrov-Galerkin projection method involves the steps below. As before, approximating the reduced state vector x(t) using V as x(t) V x(t) leads to DV x(t) KV x(t) NV x(t)u(t) H  V x(t) V x(t)  Fu(t) = r(t), y(t) = CT V x(t), where r(t) is the residual after projection. Enforcing the residual r(t) to be orthogonal to W or W T r(t) = 0 leads to the reduced system 1A variant of BIRKA for MOR of the quadratic-bilinear rst- order dynamical systems also exists. Preconditioned iterative solves and reusing preconditioners can be applied here as done for BIRKA. Hence, we focus on QB-IHOMM that has been developed for the SISO case only. given by D x(t) K x(t) N x(t)u(t) H   x(t) x(t)  Fu(t) = 0, y(t) = CT x(t), where D = W T DV, K = W T KV, N = W T NV, H = W T H(V V), F = W T F, CT = CT V. Here, V and W are computed by matching the moments of the orig- inal system transfer function and the reduced system transfer function. We brie y summarize QB-IHOMM in Algorithm 3, where as earlier, only parts related to solving linear systems are listed. Here, as in [19], the computation is done with the rst two regular transfer function terms. Algorithm 3 QB-IHOMM [19] Input: D, K, N, H, F, C; interpolation points i C for i = 1, . . . , ; higher orders moments numbers P, Q N Output: D, K, N, H, F, C 1: V = [ ] , W = [ ] 2: for j = 0, . . . , P + Q do 3: for i = 1, . . . , do 4: Xj( i) = [( iD K) 1D]j( iD K) 1F 5: V =  V Xj( i)  6: end for 7: end for 8: for j = 0, . . . , Q do 9: for i = 1, . . . , do 10: Xj(2 i)T = [(2 iD K) T DT ]j(2 iD K) T CT 11: W =  W Xj(2 i)T  12: end for 13: end for 14: U = orth([V W]) 15: Construct the reduced system as D = UT DU, K = UT KU, N = UT NU, H = UT H(U U), F = UT F, CT = CT U. III. PROPOSED WORK Here, we discuss preconditioned iterative methods in Section III-A. In Section III-B, we revisit the theory of reusing preconditioners from [33]. Finally, we discuss appli- cation of reusing preconditioners to the earlier discussed algorithms in Section III-C. A. PRECONDITIONED ITERATIVE METHODS Krylov subspace based methods are very popular class of iter- ative methods [34], [35]. Let Ax = b be a linear system, with A Rn n, b Rn, x0 the initial solution and r0 (where r0 = b Ax0) the initial residual. We nd the solution of a linear system in Kk(A, r0) = span{r0, Ar0, A2r0, . . . , Ak 1r0}, where Kk( , ) represents the Krylov subspace. 133236 VOLUME 8, 2020 N. P. Singh, K. Ahuja: Reusing Preconditioners in Projection Based MOR Algorithms Often iterative methods are slow or fail to converge, and hence, preconditioning is used to accelerate them. If P is a non-singular matrix that approximates the inverse of A, then the preconditioned system becomes AP x = b with x = P x. This is termed as right preconditioning. Similarly, left pre- conditioning can also be performed, where the preconditioner is present on the left side of the matrix [36].2 If the linear system coef cient matrices are SPD, then both the types of preconditioning give the same results [36]. For our MOR algorithms under-consideration, the linear system coef cient matrices do not have any special structure. Hence, both these types of preconditioning work differently. In our experiments, we use right preconditioning because it is fairly common [37], [38]. However, to demonstrate that our techniques are independent of the type of preconditioning, for one model, we experiment with left preconditioning in the side as well. We expect that the preconditioned iterative solves would nd a solution in less amount of time as compared to the unpreconditioned ones. For most of the input dynamical systems (as mentioned here), the Krylov subspace methods fail to converge (see Numerical Experiments section). Hence, we use a preconditioner. The goal is to nd a preconditioner that is cheap to compute as well as apply. Preconditioning is of two kinds (implicit and explicit), and we focus on the latter [39]. In case of implicit preconditioners, application of pre- conditioning requires solving linear systems. For example, in factorization based preconditioning A LU, where L and U are sparse triangular matrices approximating exact L and U factors. Here, application of the preconditioner requires only forward and backward solves. This is usually referred to as incomplete LU factorization (ILU) based pre- conditioner. Variations of ILU that exploit certain matrix constructs can also be developed. For example, ILU based upon Schur s complement [40]. Further, ILQ, SSOR and ADI are other kinds of preconditioning that fall under the implicit category [39]. Although implicit preconditioners have been used exten- sively for a very long time, they have their own drawbacks. For example, ILU based preconditioners do not be scale well when the system size becomes very large (computation time becomes prohibitively expensive). This is because, forward and backward solves in such preconditioners are inherently sequential and cannot be easily parallelized. Besides this, the breakdown in the factorization process because of the zero pivoting carries over from the full factorization case to this incomplete factorization case. Explicit preconditioning is one where directly the inverse of the coef cient matrix is approximated or P A 1. Hence, applying the preconditioner just involves performing matrix- vector products [38]. Sparse approximate inverse (SPAI) are 2If the preconditioner is present on both the sides of the coef cient matrix, then it is called split/ center preconditioning. the most commonly used explicit preconditioners, which we use and are discussed in-detail later in this section. Variations of approximate inverse preconditioners also exist. One example, as we have seen in the case of implicit preconditioning, is the Schur s complement based approxi- mate inverse preconditioner [38]. Another example is where the approximate inverse preconditioner is constructed by using a high-order convergent scheme that relies on matrix- matrix multiplications [41], [42]. Hybrid of implicit and explicit preconditioning is also com- mon. Here, combinations of factorizations and approximate inverses are used to compute a preconditioner. An example of this is given in [43], where for a SPD matrix, Cholesky factorization is rst performed. This in-turn is used to obtain a more ef cient approximate inverse preconditioner. Another example is where the approximate inverse of the coef cient matrix is used to compute an approximation to matrix s Schur s complement. This is then used to build an ILU pre- conditioner [40]. Now, we give the details of SPAI. For constructing a preconditioner P corresponding to a coef cient matrix A, we focus on methods for nding approximate inverse of A by minimizing the Frobenius norm of the residual matrix I AP. This minimization problem can be rewritten as [37] min P I AP 2 f . (6) Here, the columns of residual matrix I AP can be computed independently, which is an important property that can be exploited. Hence, the solution of (6) can be separated into n independent least square problems as min P n X i=1 (I AP)ei 2 2, or min pi ei Api 2 2, for i = 1, 2, . . . , n, (7) where ei and pi are the i-th column of I and P, respectively. The above minimization problem can be implemented in par- allel and one can ef ciently obtain the explicit approximate inverse P of A. Usually A is sparse. In this case, we can solve a more ef cient version of the optimization problem given in (7). Here, rst, a good sparsity pattern of P is assumed (usually the Identity matrix). As the solutions of the least squares problems are iteratively computed, this sparsity pattern is updated. One common updating strategy adaptively exploits the number of non-zeros arising in the resulting residuals (ri = ei Api), which requires solving 1D minimization problems [38]. A more sophisticated updating strategy uses a multivariate minimization [44]. Second, now since both A and P are sparse, we solve much smaller least squares problems, and all matrix-vector products are done in a sparse-mode (operations involving a sparse-matrix and a sparse-vector). VOLUME 8, 2020 133237 N. P. Singh, K. Ahuja: Reusing Preconditioners in Projection Based MOR Algorithms B. THEORY OF REUSING PRECONDITIONERS In general, the linear systems of equations generated by lines 4 and 10 of Algorithm 1 (AIRGA); lines 4 and 5 of Algorithm 2 (BIRKA); and lines 4 and 10 of Algorithm 3 (QB-IHOMM) have the following form: A1X1 = F1, A2X2 = F2, ... A X = F , where Ai Rn n, Xi Rn, and Fi Rn; for i = 1, 2, . . . , .3 Let P1 be a good preconditioner for A1 (it is a seed pre- conditioner) that is computed by the theory discussed in the above section ((6)-(7)) or min P1 I A1P1 2 f . Now, we need to nd a good preconditioner P2 corresponding to A2. Using the standard SPAI theory, this means solving min P2 I A2P2 2 f . (8) If we are able to enforce A1P1 = A2 P2, then P2 will be an equally good preconditioner for A2 as much as P1 is a good preconditioner for A1 (since the Spectrum of A2P2 would be same as that of A1P1, on which convergence of any Krylov subspace method depends). Since P2 is unknown here, we have a degree of freedom in choosing how to form it. Without loss of generality, we assume that P2 = Q2P1, where Q2 is an unknown matrix. Here, we need to enforce A1P1 = A2Q2P1. Thus, instead of solving the minimization problem (8), we can solve min Q2 A1 A2 Q2 2 f . Note that P2 here is never explicitly formed by multiplying two matrices Q2 and P1. Rather, always a matrix-vector product is done to apply the preconditioner. Next, we apply a similar argument for nding a good preconditioner Pi corresponding to Ai. For this we refer to one of our recent works [33], which focused on MOR of parametric linear dynamical systems (category three from the Introduction). We can obtain Pi by enforcing either A1P1 = AiPi or Ai 1Pi 1 = AiPi. For these two cases, Pi would be as effective preconditioner for Ai as P1 is for A1 or Pi 1 is for Ai 1, respectively. These two approaches are summarized in Table 2. In [33], we have conjectured (with evidence) the following two results: (a) In the parametric case, the rst approach is more bene cial. This is because, in this case although the two approaches have a similarly hard minimization problem (attributed to slowly varying parameters, and in-turn, slowly changing matrices), the computation of Pi from P1 in the rst 3In-case of BIRKA, the coef cient matrices are of size n2 n2 and solution vectors as well as right-hand sides of size n2. TABLE 2. Cheap preconditioner update approaches [33]. approach leads to a preconditioner with less approximation errors, and hence, a one which is more accurate. (b) In the non-parametric case, the second approach is more suited. This is because in this case the minimization problem of the second approach is much easier to solve as compared to the rst approach (attributed to rapidly changing expansion/ interpolation points, and in-turn, rapidly changing matrices). The computation of Pi from Pi 1 in this case (rather than P1 as above) does have the drawback of accumulated approx- imation errors, however, solving the minimization problem ef ciently is a bigger bottleneck for scaling to large problems. As mentioned in the Introduction, in [33] we have exten- sively experimented for the parametric case (again, category three earlier) using the rst approach. The focus here is to do a similar experimentation for the non-parametric case ( rst two categories earlier) using the second approach. C. APPLICATION OF REUSING PRECONDITIONER Here, we rst discuss the application of the above presented theory of reusing preconditioners to AIRGA. If we closely observe Algorithm 1, as mentioned earlier, linear systems are solved at lines 4 and 10. To solve these system, we can chose any solver from a large pool of available Krylov subspace methods. For example, GMRES [45], BI-CGSTAB [46], IDR(s) [47], etc. Since GMRES is the most popular one among these, we use it inside AIRGA in our result section. If we relook at linear systems at lines 4 and 10 in Algorithm 1, we realize that they have more characteristics. These linear systems can be very easily transformed into general shifted linear systems of the form D + K (see Section 3 of [48]). Therefore, this property can be exploited in solving these sets of linear systems simultaneously [49], [50], which is part of our future work.4 Delving further into the complexity of such linear sys- tems, we observe that the matrices change with the index of outer while loop (line 2) as well as with the index of the for loop corresponding to the expansion points (line 3). Hence, we denote such matrices not only with a subscript as in previous subsection but also with a super- script. That is, A(z) i =  s(z) i 2 M + s(z) i D + K, where z = 1, . . . , z (until covergence) and i = 1, . . . , . As the matrix 4If the linear system coef cient matrices have special properties, then more ef ciency can be incorporated. For example, if the coef cient matrices ( D + K) have D, K as real and as complex, then we can reduce the number of linear systems that are required to be solved. For more details, please see Section 1 of [54]. 133238 VOLUME 8, 2020 N. P. Singh, K. Ahuja: Reusing Preconditioners in Projection Based MOR Algorithms FIGURE 1. Reusing preconditioners in AIRGA. A(z) i changes with respect to two different indices, we can reuse preconditioners in many ways. However, here we use the second approach as discussed in the previous subsection. This approach is diagrammatically represented in Figure 1. Computation of preconditioners is done only at line 4 because at line 10, matrices do not change, only the right- hand sides do. Hence, we only focus on reusing precondi- tioners for line 4. Next, we show how the new preconditioners are computed for both, the horizontal direction and the vertical direction. While looking at the horizontal route, let, A(z) i 1 =  s(z) i 1 2 M + s(z) i 1D + K and A(z) i =  s(z) i 2 M + s(z) i D + K be the two coef cient matri- ces for different expansion points s(z) i 1 and s(z) i , respectively, with i = 2, . . . , . Using the above theory, we enforce A(z) i 1P(z) i 1 = A(z) i P(z) i in Figure 2. Thus, we eventually enforce A(z) i 1P(z) i 1 = A(z) i Q(z) i P(z) i 1 and solve the minimization problem min Q(z) i A(z) i 1 A(z) i Q(z) i 2 f . This gives us the new preconditioner P(z) i = Q(z) i P(z) i 1. This minimization is again performed for n independent least square problems as in (7). Similar steps are followed for reusing preconditioners along the rest of the horizontal direc- tions, i.e. for all z = 1, . . . , z. Now, applying this technique for the vertical direction, we have for z = 2, . . . , z A(z 1) 1 P(z 1) 1 = A(z) 1 P(z) 1 . Following the steps as for the horizontal direction, here, we solve the minimization problem min Q(z) 1 A(z 1) 1 A(z) 1 Q(z) 1 2 f . This gives us the new preconditioner P(z) 1 = Q(z) 1 P(z 1) 1 . Again, this is solved as n independent least square problems as in (7). AIRGA with an ef cient implementation of the above discussed theory of reusing preconditioners is given in Algorithm 4. If we closely look at line 4 of Algorithm 1, the solution vector is denoted by X(0)(si), where the super- script 0 refers to the index of the inner while loop (line 8). We do not bother about this index because, as ear- lier, matrix does not change inside this inner loop. Rather, we need to capture the change because of the outer while loop indexed with z. Hence, we denote the solution vec- tor as X(z)(si) in Algorithm 4 (lines 8, 11, 19 & 22). It is important to emphasize again that preconditioners are never computed explicitly. Rather, they are obtained using matrix- vector products (please see line numbers 11, 19 & 22 of Algorithm 4). Since shift-invariant preconditioners have been proposed for the general shifted linear systems [49], [51], our this reuse FIGURE 2. Expressing one linear system matrix in terms of the other. VOLUME 8, 2020 133239 N. P. Singh, K. Ahuja: Reusing Preconditioners in Projection Based MOR Algorithms TABLE 3. SPAI and reusable SPAI analysis for the academic disk brake model. TABLE 4. SPAI and reusable SPAI computation time for the academic disk brake model. SPAI technique can be coupled with these preconditioners for further ef ciency. We plan to look at this aspect as part of our future work. The next MOR algorithm under-consideration is BIRKA (Algorithm 2). Here, the linear solver would be chosen in a manner similar to AIRGA above. However, the coef cient matrices here have a block form, and hence, instead of stan- dard Krylov subspace methods, their block variants should be used [52]. For the sake of brevity, the reuse of SPAI preconditioners in BIRKA (Algorithm 2) is discussed as part of Appendix A. Here also, the block structure can be exploited in developing a more ef cient preconditioner. An example of this is given in Chapter 11 of [53], where a preconditioner similar to SPAI has been improved upon by utilizing such a structure. This aspect is part of our future work. The third and the nal MOR algorithm under-consideration is QB-IHOMM (Algorithm 3). As for the earlier two algo- rithms, any general Krylov subspace solver can be used here. Algorithm 4 AIRGA With Reuse of SPAI Preconditioner 1: z = 1 2: while no convergence do 3: if z == 1 then 4: for i = 1, . . . , do 5: A(1) i =  s(1) i 2 M +  s(1) i  D + K  6: if i == 1 then 7: Compute initial P(1) 1 by solving min P(1) 1 I A(1) 1 P(1) 1 2 f (First-time; no earlier preconditioner) 8: A(1) 1 P(1) 1 X(1)(s1) = F 9: else 10: Compute Q(1) i by solving min Q(1) i A(1) i 1 A(1) i Q(1) i 2 f (Reuse along horizontal direction) 11: A(1) i [Q(1) i Q(1) 2 P(1) 1 ]X(1)(si) = F 12: end if 13: end for 14: else 15: for i = 1, . . . , do 16: A(z) i =  s(z) i 2 M +  s(z) i  D + K  17: if i == 1 then 18: Compute Q(z) 1 by solving min Q(z) 1 A(z 1) 1 A(z) 1 Q(z) 1 2 f (Reuse along vertical direction) 19: A(z) 1 h Q(z) 1 . . . Q(2) 1 P(1) 1 i X(z)(s1) = F 20: else 21: Compute Q(z) i by solving min Q(z) i A(z) i 1 A(z) i Q(z) i 2 f (Reuse along horizontal direction) 22: A(z) i  Q(z) i Q(z) 2 | {z } 23: Q(z) 1 . . . Q(2) 1 | {z } P(1) 1  X(z)(si) = F 24: end if 25: end for 26: end if 27: All the given set of expansion points (i.e. s1, s2, . . . , s ) are updated 28: z = z + 1 29: end while Note: The minimization problems at lines 7, 10, 18 and 21 are solved as n independent least square problems (see (7)). 133240 VOLUME 8, 2020 N. P. Singh, K. Ahuja: Reusing Preconditioners in Projection Based MOR Algorithms TABLE 5. Condition numbers of the coefficient matrices before and after application of SPAI for the academic disk brake model. Further, as in the case of AIRGA, the linear systems at lines 4 and 10 belong to the class of general shifted linear systems [48] [50] and can be solved simultaneously, which is part of our future work4. For the sake of brevity, the above theory of reusing SPAI preconditioners applied to QB-IHOMM is discussed brie y in Appendix B. Again, our SPAI reuse theory can be coupled with the speci c preconditioners for these kind of systems (e.g., shift-invariant preconditioners [49], [51]) to develop a more ef cient preconditioning strategy. We plan to look at this aspect as part of our future work. IV. NUMERICAL EXPERIMENTS For supporting our proposed preconditioned iterative solver theory using AIRGA [15], we perform experiments on two models. The rst is a macroscopic equations of motion model (i.e. academic disk brake M0) [55], and is discussed in Section IV-A. The second is also a similar model, how- ever, this is a real-life industrial problem (i.e. industrial disk brake M1) [55]. The experiments on this model are discussed in Section IV-B. These models are described by the following set of equations [55]: M x(t) = D x(t) Kx(t) + Fu(t), y(t) = CT x(t), (9) where M = M, K = KE + KR + 2 KG, D = M + K (case of proportionally damped system; as needed for AIRGA) with commonly used parameter values as  = 2 , = 5 10 02, and = 5 10 06. Further, F Rn and CT Rn are taken as [1 0 0]T , which is the most frequently used choice. We take four expansion points linearly spaced between 1 and 500 based upon experience. Although our purpose is to just reuse SPAI in AIRGA (Algorithm 4), we also execute original SPAI in AIRGA (Algorithm 1) for comparison. In Algorithms 1 and 4, at line 2 the overall iteration (while-loop) terminates when the change in the reduced model (computed as H2-error between the reduced models at two consecutive AIRGA iterations) is less than a certain tolerance. We take this tolerance as 10 04 based upon the values in [15]. There is one more stopping criteria in Algorithms 1 at line 8 (also in Algorithm 4 but not listed here). This checks the H2-error between two temporary reduced models. We take this tolerance as 10 06, again based upon the values in [15]. Since this is an adaptive algorithm, the optimal size of the reduced model is determined by the algorithm itself, and is denoted by r. The linear systems that arise here have non-symmetric matrices. There are many iterative methods available for solving such linear systems. We use the Generalized Min- imal Residual (GMRES) method [45] because it is very popular [56]. The stopping tolerance in GMRES is taken as 10 06, which is a common standard. As mentioned in Introduction, for both the given models, we observe that unpreconditioned GMRES fails to converge. Hence, we use the SPAI preconditioner as described above (without and with reuse). As mentioned earlier, without loss of generality, we per- form right preconditioning. To demonstrate the effectiveness of our theory for all types of preconditioning, for the aca- demic disk model, we give data corresponding to left precon- ditioning as well. We use Modi ed Sparse Approximate Inverse (MSPAI 1.0) proposed in [38] as our preconditioner. This is because MSPAI uses a linear algebra library for solving sparse least square problems that arise here. We use standard initial settings of MSPAI  i.e. tolerance (ep) of 10 04 . We perform our numerical experiments on a machine with the following con guration: Intel Xeon (R) CPU E5-1620 V3 @ 3.50 GHz., frequency 1200 MHz., 8 CPU and 64 GB TABLE 6. GMRES computation time for the academic disk brake model. VOLUME 8, 2020 133241 N. P. Singh, K. Ahuja: Reusing Preconditioners in Projection Based MOR Algorithms TABLE 7. GMRES with SPAI and reusable SPAI computation time for the academic disk brake model. RAM. All the codes are written in MATLAB (2016b) (includ- ing AIRGA, GMRES) except SPAI and reusable SPAI. MATLAB is used because of ease of rapid prototyping. Computing SPAI and reusable SPAI in MATLAB is expen- sive, therefore, we use C++ version of these (SPAI is from MSPAI and reusable SPAI is written by us). MSPAI further uses BLAS, LAPACK and ATLAS libraries. It is important to emphasize that we do not integrate our MATLAB code base with the C++ based preconditioner. This is because integrating the two is complicated and is not needed here as well. We compute SPAI and reusable SPAI in-parallel, sepa- rately, and save them on the hard-disk in the standard .mtx les [38]. When we run our MATLAB code base, then these les are read from the hard-disk into the main memory and converted into .mat les for further processing. A. ACADEMIC DISK BRAKE MODEL This model is of size 4, 669. Based upon experience, the maximum reduced system size (rmax) is taken as 20. As mentioned earlier, however, due to the adaptive nature of AIRGA, we obtain a reduced system of size r = 13. For this model, AIRGA takes two outer iterations (line 2 of Algorithms 1 and 4) to converge (i.e. z = 2). Reusing the SPAI preconditioner is bene cial when the values of I A(z) i f / I f is large, and the values of A(z) i 1 A(z) i f / A(z) i 1 f and A(z 1) 1 A(z) 1 f / A(z 1) 1 f are small, which is true in this case (see Table 3). In this table, TABLE 8. SPAI and reusable SPAI computation time for the industrial disk brake model. 133242 VOLUME 8, 2020 N. P. Singh, K. Ahuja: Reusing Preconditioners in Projection Based MOR Algorithms TABLE 9. SPAI and reusable SPAI computation time for the industrial disk brake model. columns 1 and 2 list the AIRGA iterations and the four expansion points, respectively. The above three quantities are listed in columns 3, 4 and 5, respectively. For the rst AIRGA iteration and the rst expansion point, SPAI preconditioner cannot be reused because there is no earlier preconditioner (mentioned as NA in table). From the second expansion point (and the rst AIRGA iteration), we perform horizontal reuse of preconditioner (see Figure 1). This is the same for the sec- ond AIRGA iteration as well. Vertical reuse of preconditioner is done only for the rst expansion point (and the second AIRGA iteration; again see Figure 1). In Table 4, we compare the SPAI and the reusable SPAI timings. As for Table 3, here columns 1 and 2 list the AIRGA iterations and the four expansion points, respectively. SPAI and reusable SPAI computation times are given in columns 3 and 4, respectively. At the rst AIRGA iteration and the rst expansion point, both SPAI and reusable SPAI take the same computation time. This is because, as above, reusing of SPAI preconditioner is not applicable here. From the second expansion point of the rst AIRGA iteration, we see substantial savings because of the reuse of the SPAI preconditioner (approximately 68%). Before presenting GMRES data, we would like to discuss improvements in the condition numbers of the coef cient matrices because of the preconditioning. This data is given in Table 5. As evident, preconditioning does substantially improve the quality of the coef cient matrices. TABLE 10. Condition numbers of the coefficient matrices before and after application of SPAI for the industrial disk brake model. Table 6 provides the iteration count and the computation time of GMRES. Here, we only provide GMRES execu- tion details since the computation time of preconditioner has been discussed above. In this table, column 1 lists the AIRGA iterations. The number of linear solves and average GMRES iterations per linear solve are given in columns 2 and 3, respectively. Finally, columns 4 and 5 list the computation times of GMRES when using SPAI and reusable SPAI, respectively. We notice from this table that solving linear systems by GMRES with SPAI takes less computation time as compared to solving them by GMRES with reusable SPAI. This is because when we reuse the SPAI preconditioner in GMRES, additional matrix-vector products are performed, however, this extra cost is almost negligible when compared to the savings in the preconditioner computation time for the latter case (as evident in Table 3 above; also see total GMRES and preconditioner time below). As earlier, the data in Table 6 is corresponding to right preconditioning. In the case of left preconditioning we see only a modest change in the metrics under-consideration. That is, the total GMRES iterations, the total GMRES plus SPAI time, and the total GMRES plus reusable SPAI time are 6364, 190, and 204, respectively. Table 7 gives the computation time of GMRES plus SPAI (column 2) and GMRES plus reusable SPAI (column 3) at each AIRGA iteration (column 1). As evident from this table, reusing the SPAI preconditioner leads to about 60% savings in total time required for solving all the linear systems. VOLUME 8, 2020 133243 N. P. Singh, K. Ahuja: Reusing Preconditioners in Projection Based MOR Algorithms TABLE 11. GMRES computation time for the industrial disk brake model. B. INDUSTRIAL DISK BRAKE MODEL This model is of size 1.2 million. Based upon experience, the maximum reduced system size (rmax) is taken as 100. As mentioned earlier, however, due to the adaptive nature of AIRGA, we obtain a reduced system of size r = 52. For this model, AIRGA takes four outer iterations (line 2 of Algorithms 1 and 4) to converge (i.e. z = 4). Again, reusing the SPAI preconditioner is bene cial when the value of I A(z) i f / I f is large, and the value of A(z) i 1 A(z) i f / A(z) i 1 f and A(z 1) 1 A(z) 1 f / A(z 1) 1 f are small, which is true in this case (see Table 8). The structure of this table is same as Table 3. As earlier, for the rst AIRGA iteration and the rst expansion point, SPAI preconditioner cannot be reused because there is no earlier preconditioner (mentioned as NA in table). From the second expansion point (and the rst AIRGA iteration), we perform horizontal reuse of preconditioner (see Figure 1). This is the same for the second, the third and the fourth AIRGA iterations as well. Vertical reuse of preconditioner is done only for the rst expansion point (and the second, the third, and the fourth AIRGA iterations; again see Figure 1). In Table 9, we compare the SPAI and the reusable SPAI timings. The structure of this table is same as that of Table 4. As before, at the rst AIRGA iteration and the rst expansion point, both SPAI and reusable SPAI take the same com- putation time. This is because, as above, reusing of SPAI preconditioner is not applicable here. From the second expan- sion point of the rst AIRGA iteration, we see substantial savings because of the reuse of the SPAI preconditioner (from 160 hours to 26 hrs 30 minutes; approximately 83%). As in the case of the academic disk model, here too before presenting GMRES data, we would like to discuss improve- ments in the condition numbers of the coef cient matrix because of the preconditioning. This data is given in Table 10. As evident, preconditioning does substantially improve the quality of the coef cient matrices. Table 11 provides the iteration count and the computa- tion time of GMRES. Here, again we have only provided GMRES execution details since the computation time of the TABLE 12. GMRES with SPAI and reusable SPAI computation time for the industrial disk brake model. preconditioner has already been discussed above. The struc- ture of this table is same as that of Table 6. As earlier, we notice from this table that solving linear systems by GMRES with SPAI takes less computation time as compared to solving them by GMRES with reusable SPAI. This is again because of additional matrix-vector products in the reusable SPAI case. Here also, this extra cost is almost negligible when compared to the savings in the preconditioner computation time (as evident in Table 9; also see the total GMRES and preconditioner time below). Table 12 gives the computation time of GMRES plus SPAI (column 2) and GMRES plus reusable SPAI (column 3) at each AIRGA iteration (column 1). As before, it is evident from this table, reusing the SPAI preconditioner leads to about 64% savings in total time (from 197 hours 28 minutes to 72 hours 06 minutes). To demonstrate the quality of the reduced system, we plot the relative H2 error between the transfer function of the original system and the reduced system with respect to the different expansion points (in Figure 3). The reduced system considered here is obtained by using GMRES with reusable SPAI. These expansion points, denoted by S, are computed as 2 f , where the frequency variable f is linearly spaced between 1 and 500. As evident from this gure, the obtained reduced system is good (the error is very small). Further, we also observe from this gure that the reduced model is most accurate in 7 10 range of the expansion points. This is 133244 VOLUME 8, 2020 N. P. Singh, K. Ahuja: Reusing Preconditioners in Projection Based MOR Algorithms FIGURE 3. Relative error between the original and reduced system for the industrial disk brake model. because the nal expansion points, upon the convergence of AIRGA, lie in this range. V. CONCLUSIONS AND FUTURE WORK In this work, we have focused on MOR of non-parametric dynamical systems, speci cally on the following three algo- rithms: AIRGA, BIRKA, and QB-IHOMM. Since solving large and sparse linear systems is a bottleneck in scaling these MOR algorithms for reduction of large sized dynamical sys- tems, we have proposed reusing of the SPAI preconditioner. Speci cally, we have demonstrated the following: exploitation of the simplicity because of the lack of parame- ters in reusing preconditioners, multiple ways of reusing pre- conditioners within the algorithm, ef cient implementation to ensure that the savings because of reusing preconditioners are not negated by bad coding, and experimentation on a massively large industrial problem. Numerical experiments show the effectiveness of our approach, where for a problem of size 1.2 million, we save up to 64% in the computation time. In absolute terms, this gives a saving of 5 days. Our future work consists of three main directions, focusing on other ef cient MOR algorithms, better linear solvers and enhanced preconditioning techniques. First, we will investigate other important variants of MOR algorithms discussed in our paper (AIRGA, BIRKA, QB-IHOMM). For example, T-BIRKA is a more ef cient version of BIRKA [17], and applying our techniques here would be useful. Second, in our paper, we have used a basic and common linear solver (GMRES). However, as discussed earlier, this aspect can also be optimized. Speci cally, for the class of MOR algorithms to which AIRGA and QB-IHOMM belong, we will investigate the use of linear solvers speci c to general shifted linear systems [48] [50]. Finally and third, we will investigate more sophisticated preconditioning strategies that will further exploit the proper- ties of the underlying MOR algorithms as well as the resulting linear systems. Speci cally, we will explore ve directions as below. (a) Besides the currently used basic SPAI preconditioner, we will investigate the use of high-order convergent approx- imate inverse preconditioners [41], [42] as well as hybrid versions, which use a combination of factorization and approximate inverse techniques [40], [43]. (b) For AIRGA, QB-IHOMM, and related MOR algo- rithms where general shifted linear systems arise, we will investigate the use of our reusable SPAI preconditioner along with shift-invariant preconditioners that have been developed speci cally for such shifted linear systems [49], [51]. (c) We will investigate exploiting the block structure of the linear system coef cient matrices in BIRKA such that the SPAI and its reuse can be done more ef ciently [53]. (d) Since randomized preconditioners have shown promis- ing results in recent years, we will explore their use in the context of linear systems in MOR algorithms. (e) Finally, we would also investigate combining machine learning techniques (e.g., spiking neural networks) to opti- mize the parameters inside the preconditioners. APPENDIX A In the Algorithm 2, we solve linear systems of equations at lines 4 and 5. We rst apply our proposed theory of reusing preconditioners to line 4, which is given as vec (V) = 3 In Ir K m X j=1 N T j Nj 1 FT F  vec(Im). Here, 3 is a diagonal matrix comprising of interpolation points, which is updated at the start of the while loop at line 2. For ease of explanation, we take j = 1 here. Similar steps can be executed for j = 2, . . . , m. Let Az 1 = 3z 1 In Ir K  N T 1  z 1 N1 and Az = 3z In Ir K  N T 1  z N1 be the coef cient matrices corresponding to 3z 1 and 3z, respec- tively (for z = 1, . . . , z (until covergence)). Expressing Az in terms of Az 1, we get Az = Az 1  Inr + A 1 z 1( 3z In) + A 1 z 1(3z 1 In) + A 1 z 1   N T 1  z N1  +A 1 z 1  N T 1  z 1 N1  , where Inr Rn r n r is the Identity matrix. If we de ne Qz =  Inr + A 1 z 1( 3z In) + A 1 z 1(3z 1 In) + A 1 z 1   N T 1  z N1  +A 1 z 1  N T 1  z 1 N1  1 , then above is equivalent to Az = Az 1Q 1 z . (10) Now, we enforce Az 1Pz 1 = AzPz. (11) Using (10), instead we enforce Az 1Pz 1 = Az 1Q 1 z Pz. VOLUME 8, 2020 133245 N. P. Singh, K. Ahuja: Reusing Preconditioners in Projection Based MOR Algorithms If we take Pz = QzPz 1, then we eventually enforce Az 1Pz 1 = Az 1Q 1 z QzPz 1, which is true. Thus, instead of solving for Pz by enforcing (11), which is harder to solve, we obtain the preconditioner at the zth iteration (Pz = QzPz 1) by enforcing Az 1Pz 1 = AzQzPz 1, which is more easily solvable. The remaining derivation here is same as earlier (see Section III-C). We reuse precondition- ers at line 5 similarly. APPENDIX B In the Algorithm 3, we solve linear systems of equations at line 4 and 10. Again, we rst apply our proposed theory of reusing preconditioners to line 4, which is given as Xj( i) = [( iD K) 1D]j( iD K) 1F, for j = 1, . . . , P + Q and i = 1, . . . , . Let Ai 1 = i 1D K and Ai = iD K be the two coef- cient matrices for different interpolation points i 1 and i, respectively (for i = 1, . . . , ). Expressing Ai in terms of Ai 1, we get Ai = Ai 1(I + ( i i 1)A 1 i 1D). If we de ne Qi = (I + ( i i 1)A 1 i 1D) 1, then above is equivalent to Ai = Ai 1Q 1 i . As for AIRGA and BIRKA, instead of obtaining Pi by enforcing Ai 1Pi 1 = AiPi, which is harder to solve, we obtain the preconditioner at the ith iteration (Pi = QiPi 1) by enforcing Ai 1Pi 1 = AiQiPi 1, which is more easily solvable. Again, here also, the remaining derivation is same as earlier (see Section III-C). We reuse preconditioners at line 10 similarly. ACKNOWLEDGMENT The authors would like to deeply thank Prof. Dr. Heike Fa bender (at Institut Computational Mathematics, AG Numerik, Technische Universit t Braunschweig, Germany) for discussions and help regarding different aspects of this project. They would also like to thank Ms. Apoorva Joshi (IIT Indore, India) for help in numerical experiments, which she did as part of her undergraduate thesis. Thanks to the anonymous reviewers that helped them greatly improve the quality of this manuscript. Finally, they would like to thank Dr. Yan-Jun Liu (Editor handling our manuscript) for his tremendous support during the whole reviewing process. REFERENCES [1] O. Katsuhiko, Modern Control Engineering. Upper Saddle River, NJ, USA: Prentice-Hall, 2001. [2] M. Rewienski and J. White, A trajectory piecewise-linear approach to model order reduction and fast simulation of nonlinear circuits and micro- machined devices, IEEE Trans. Comput.-Aided Design Integr. Circuits Syst., vol. 22, no. 2, pp. 155 170, Feb. 2003. [3] A. C. Antoulas, Approximation of large-scale dynamical systems: An overview, IFAC Proc. Volumes, vol. 37, no. 11, pp. 19 28, Jul. 2004. [4] A. C. Antoulas, Approximation of Large-Scale Dynamical Systems. Philadelphia, PA, USA: SIAM, 2005. [5] J. E. S. Socolar, Nonlinear dynamical systems, in Complex Systems Science in Biomedicine, T. S. Deisboeck and J. Y. Kresh Eds. Boston, MA, USA: Springer, 2006, pp. 115 140. [6] B. N. Bond, Parameterized model order reduction for nonlinear dynam- ical systems, M.S. thesis, Dept. Elect. Eng. Comput. Sci., MIT, Cambridge, MA, USA, 2006. [7] E. J. Grimme, Krylov projection methods for model reduction, Ph.D. dissertation, Dept. Elect. Eng., Univ. Illinois Urbana-Champaign, Urbana, IL, USA, 1997. [8] S. Gugercin, Projection methods for model reduction of large-scale dynamical systems, Ph.D. dissertation, Dept. Elect. Comp. Eng., Rice Univ., Houston, TX, USA, 2003. [9] W. H. Schilders, H. A. Van der Vorst, and J. Rommes, Model Order Reduction: Theory, Research Aspects and Applications, vol. 13. Berlin, Germany: Springer, 2008. [10] S. Gugercin, A. C. Antoulas, and C. Beattie, H2 model reduction for large- scale linear dynamical systems, SIAM J. Matrix Anal. Appl., vol. 30, no. 2, pp. 609 638, 2008. [11] T. Breiten, Interpolation methods for model reduction of large-scale dynamical systems, Ph.D. dissertation, Dept. Math., Otto-von-Guericke- Universit t Magdeburg, Magdeburg, Germany, 2013. [12] S. A. Wyatt, Issues in interpolatory model reduction: Inexact solves, second-order systems and DAEs, Ph.D. dissertation, Dept. Math., Virginia Tech, Blacksburg, VA, USA, 2012. [13] Z.-Y. Qiu, Y.-L. Jiang, and J.-W. Yuan, Interpolatory model order reduc- tion method for second order systems, Asian J. Control, vol. 20, no. 1, pp. 312 322, Jan. 2018. [14] Z. Bai and Y. Su, Dimension reduction of large-scale second-order dynamical systems via a second-order Arnoldi method, SIAM J. Sci. Comput., vol. 26, no. 5, pp. 1692 1709, Jan. 2005. [15] T. Bonin, H. Fa bender, A. Soppa, and M. Zaeh, A fully adaptive rational global Arnoldi method for the model-order reduction of second- order MIMO systems with proportional damping, Math. Comput. Simul., vol. 122, pp. 1 19, Apr. 2016. [16] P. Benner and T. Breiten, Interpolation-based H2-model reduction of bilinear control systems, SIAM J. Matrix Anal. Appl., vol. 33, no. 3, pp. 859 885, Aug. 2012. [17] G. M. Flagg, Interpolation methods for the model reduction of bilinear systems, Ph.D. dissertation, Dept. Math., Virginia Tech, Blacksburg, VA, USA, 2012. [18] R. Choudhary and K. Ahuja, Inexact linear solves in model reduction of bilinear dynamical systems, IEEE Access, vol. 7, pp. 72297 72307, May 2019. [19] M. M. A. Asif, M. I. Ahmad, P. Benner, L. Feng, and T. Stykel, Implicit higher-order moment matching technique for model reduction of quadratic-bilinear systems, 2019, arXiv:1911.05400. [Online]. Available: http://arxiv.org/abs/1911.05400 [20] U. Baur, C. Beattie, P. Benner, and S. Gugercin, Interpolatory projec- tion methods for parameterized model reduction, SIAM J. Sci. Comput., vol. 33, no. 5, pp. 2489 2518, Jan. 2011. [21] P. Benner and L. Feng, A robust algorithm for parametric model order reduction based on implicit moment matching, in Reduced Order Meth- ods for Modeling and Computational Reduction, A. Quarteroni and G. Rozza, Eds. Cham, Switzerland: Springer, 2014, pp. 159 185. [22] L. Feng, P. Benner, and J. G. Korvink, Subspace recycling accelerates the parametric macro-modeling of MEMS, Int. J. Numer. Methods Eng., vol. 94, no. 1, pp. 84 110, Apr. 2013. [23] A. C. Rodriguez, S. Gugercin, and J. Borggaard, Interpolatory model reduction of parameterized bilinear dynamical systems, Adv. Comput. Math., vol. 44, no. 6, pp. 1887 1916, Dec. 2018. [24] X. Cao, Optimal model order reduction for parametric nonlinear sys- tems, Ph.D. dissertation, Dept. Math. Comp. Sci., TU Eindhoven, Eindhoven, The Netherlands, 2019. 133246 VOLUME 8, 2020 N. P. Singh, K. Ahuja: Reusing Preconditioners in Projection Based MOR Algorithms [25] K. Ahuja, E. de Sturler, S. Gugercin, and E. R. Chang, Recycling BiCG with an application to model reduction, SIAM J. Sci. Comput., vol. 34, no. 4, pp. A1925 A1949, Jan. 2012. [26] K. Ahuja, P. Benner, E. de Sturler, and L. Feng, Recycling BiCGSTAB with an application to parametric model order reduction, SIAM J. Sci. Comput., vol. 37, no. 5, pp. S429 S446, Jan. 2015. [27] K. Ahuja, Recycling Krylov subspaces and preconditioners, Ph.D. dissertation, Dept. Math., Virginia Tech, Blacksburg, VA, USA, 2011. [28] K. Ahuja, B. K. Clark, E. de Sturler, D. M. Ceperley, and J. Kim, Improved scaling for quantum Monte Carlo on insulators, SIAM J. Sci. Comput., vol. 33, no. 4, pp. 1837 1859, Jan. 2011. [29] S. Bellavia, V. De Simone, D. di Sera no, and B. Morini, Ef cient preconditioner updates for shifted linear systems, SIAM J. Sci. Comput., vol. 33, no. 4, pp. 1785 1809, Jan. 2011. [30] W.-H. Luo, T.-Z. Huang, L. Li, Y. Zhang, and X.-M. Gu, Ef cient preconditioner updates for unsymmetric shifted linear systems, Comput. Math. with Appl., vol. 67, no. 9, pp. 1643 1655, May 2014. [31] A. K. Grim-McNally, E. de Sturler, and S. Gugercin, Preconditioning parametrized linear systems, 2016, arXiv:1601.05883. [Online]. Avail- able: http://arxiv.org/abs/1601.05883 [32] A. K. Grim-McNally, Reusing and updating preconditioners for sequences of matrices, M.S. thesis, Dept. Math., Virginia Tech, Blacksburg, VA, USA, 2015. [33] N. P. Singh and K. Ahuja, Preconditioned linear solves for para- metric model order reduction, Int. J. Comput. Math., vol. 97, no. 7, pp. 1484 1502, Jul. 2020. [34] Y. Saad, Iterative Methods for Sparse Linear Systems. Philadelphia, PA, USA: SIAM, 2003. [35] H.-L. Shen, S.-Y. Li, and X.-H. Shao, The NMHSS iterative method for the standard Lyapunov equation, IEEE Access, vol. 7, pp. 13200 13205, Jan. 2019. [36] M. Benzi, Preconditioning techniques for large linear systems: A survey, J. Comput. Phys., vol. 182, no. 2, pp. 418 477, Nov. 2002. [37] E. Chow and Y. Saad, Approximate inverse preconditioners via sparse- sparse iterations, SIAM J. Sci. Comput., vol. 19, no. 3, pp. 995 1023, May 1998. [38] K. Alexander, Modi ed sparse approximate inverses (MSPAI) for paral- lel preconditioning, Ph.D. dissertation, Dept. Math., TU Munich, Munich, Germany, 2008. [39] M. Benzi and M. Tuma, A sparse approximate inverse preconditioner for nonsymmetric linear systems, SIAM J. Sci. Comput., vol. 19, no. 3, pp. 968 994, May 1998. [40] S. C. Buranay and O. C. Iyikal, Approximate Schur-block ILU preconditioners for regularized solution of discrete ill-posed prob- lems, Math. Problems Eng., pp. 1 18, Apr. 2019, Art. no. 1912535, doi: 10.1155/2019/1912535. [41] S. C. Buranay, D. Subasi, and O. C. Iyikal, On the two classes of high-order convergent methods of approximate inverse preconditioners for solving linear systems, Numer. Linear Algebra Appl., vol. 24, no. 6, p. e2111, Dec. 2017. [42] F. Soleymani, A fast convergent iterative solver for approximate inverse of matrices, Numer. Linear Algebra Appl., vol. 21, no. 3, pp. 439 452, May 2014. [43] L. Y. Kolotilina and A. Y. Yeremin, Factorized sparse approximate inverse preconditionings I. Theory, SIAM J. Matrix Anal. Appl., vol. 14, no. 1, pp. 45 58, Jan. 1993. [44] N. I. M. Gould and J. A. Scott, Sparse approximate-inverse precondition- ers using norm-minimization techniques, SIAM J. Sci. Comput., vol. 19, no. 2, pp. 605 625, Mar. 1998. [45] Y. Saad and M. H. Schultz, GMRES: A generalized minimal residual algorithm for solving nonsymmetric linear systems, SIAM J. Sci. Stat. Comput., vol. 7, no. 3, pp. 856 869, Jul. 1986. [46] H. A. van der Vorst, Bi-CGSTAB: A fast and smoothly converging variant of bi-CG for the solution of nonsymmetric linear systems, SIAM J. Sci. Stat. Comput., vol. 13, no. 2, pp. 631 644, Mar. 1992. [47] P. Sonneveld and M. B. Van Gijzen, IDR(s): A family of simple and fast algorithms for solving large non-symmetric systems of linear equations, SIAM J. Sci. Comput., vol. 31, no. 2, pp. 1035 1062, Jan. 2009. [48] V. Simoncini, Restarted full orthogonalization method for shifted linear systems, BIT Numer. Math., vol. 43, no. 2, pp. 459 466, Jun. 2003. [49] T. Bakhos, P. K. Kitanidis, S. Ladenheim, A. K. Saibaba, and D. B. Szyld, Multipreconditioned GMRES for shifted systems, SIAM J. Sci. Com- put., vol. 39, no. 5, pp. S222 S247, Jan. 2017. [50] X.-M. Gu, T.-Z. Huang, B. Carpentieri, A. Imakura, K. Zhang, and L. Du, Ef cient variants of the CMRH method for solving a sequence of multi- shifted non-Hermitian linear systems simultaneously, J. Comput. Appl. Math., vol. 375, Sep. 2020, Art. no. 112788. [51] X.-M. Gu, T.-Z. Huang, G. Yin, B. Carpentieri, C. Wen, and L. Du, Restarted Hessenberg method for solving shifted nonsymmetric linear systems, J. Comput. Appl. Math., vol. 331, pp. 166 177, Mar. 2018. [52] V. Simoncini, Computational methods for linear matrix equations, SIAM Rev., vol. 58, no. 3, pp. 377 441, Jan. 2016. [53] C. C. K. Mikkelsen, Numerical methods for Lyapunov equations, Ph.D. dissertation, Dept. Math., Purdue Univ., Lafayette, IN, USA, 2009. [54] O. Axelsson and A. Kucherov, Real valued iterative methods for solving complex symmetric linear systems, Numer. Linear Algebra Appl., vol. 7, no. 4, pp. 197 218, Jun. 2000. [55] N. Gr bner, V. Mehrmann, S. Quraishi, C. Schr der, and U. von Wagner, Numerical methods for parametric model reduction in the simulation of disk brake squeal, J. Appl. Math. Mech., vol. 96, no. 12, pp. 1388 1405, Dec. 2016. [56] T. Han and Y. Han, Numerical solution for super large scale systems, IEEE Access, vol. 1, pp. 537 544, Aug. 2013. NAVNEET PRATAP SINGH received the bachelor s degree in computer science and engi- neering from UPTU, Lucknow, India, and the master s degree in modeling and simulation from the Defence Institute of Advanced Technology, Pune, India. He is currently pursuing the Ph.D. degree with IIT Indore. His thesis focuses on Stable Linear Solves with Preconditioner Updates for Model Reduction. His research interests include intersection of computer science and mathematics, especially numerical linear algebra, optimization, dynamical systems, and machine learning. KAPIL AHUJA received the bachelor s degree from IIT (BHU), India, the double master s and Ph.D. degrees from Virginia Tech, Blacksburg, VA, USA, and the Postdoctoral training from the Max Planck Institute, Germany. He is currently an Associate Professor in com- puter science and engineering with IIT Indore, India. In the past, he was a Visiting Professor at TU Braunschweig, Germany, TU Dresden, Germany, and Sandia National Labs, USA. He is also work- ing on mathematics of data science as well as computational science. His research interests in arti cial intelligence, machine learning, numerical methods, and optimization. VOLUME 8, 2020 133247