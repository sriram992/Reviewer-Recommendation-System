AIAA JOURNAL Vol. 41, No. 4, April 2003 Evolutionary Optimization of Computationally Expensive Problems via Surrogate Modeling Yew S. Ong, Prasanth B. Nair, and Andrew J. Keane University of Southampton,Southampton, England SO17 1BJ, United Kingdom We present a parallel evolutionary optimization algorithm that leverages surrogate models for solving compu- tationally expensive design problems with general constraints, on a limited computational budget. The essential backbone of our framework is an evolutionary algorithm coupled with a feasible sequential quadratic program- ming solver in the spirit of Lamarckian learning. We employ a trust-region approach for interleaving use of exact models for the objectiveandconstraint functionswith computationallycheap surrogatemodels duringlocalsearch. In contrast to earlier work, we construct local surrogate models using radial basis functionsmotivated by the prin- ciple of transductive inference. Further, the present approach retains the intrinsic parallelism of evolutionary algorithms and can hence be readily implemented on grid computing infrastructures. Experimental results are presented for some benchmark test functions and an aerodynamic wing design problem to demonstrate that our algorithm converges to good designs on a limited computational budget. Nomenclature d = number of design variables f .x/ = exact analysis function Of .x/ = approximate analysis function g.x/ = exact constraint function Og.x/ = approximate constraint function K = Gram matrix k = trust-regioniteration number kmax = maximum trust-regioniterations allowed m = number of nearest design points employed mmax = maximum number of nearest design points speci ed mmin = minimum number of nearest design points speci ed n = size of training dataset Pq = polynomialof order q 1 p = number of inequality constraints tc = current time spent tt = computationaltime budget allocated x = design variable vector xl = lower bound of design variable vector xu = upper bound of design variable vector xkc = initial guess at the kth trust-regioniteration xk lo = local optimum at the kth trust-regioniteration y = exact function value Oy = approximate function value = weight vector 1k = trust-regionradius at iteration k = approximationerror = undeterminedcoef cients of polynomial P = update factor for trust region radius k = approximation gure of merit Received 6 May 2002; revision received 28 October 2002; accepted for publication 1 November 2002. Copyright c 2003 by the authors. Published by the American Institute of Aeronautics and Astronautics, Inc., with per- mission. Copies of this paper may be made for personal or internal use, on condition that the copier pay the $10.00 per-copy fee to the Copyright Clearance Center, Inc., 222 Rosewood Drive, Danvers, MA 01923; include the code 0001-1452/03 $10.00 in correspondence with the CCC. Ph.D. Student, Computational Engineering and Design Center, School of Engineering Sciences; currently Assistant Professor, School of Com- puter Engineering, Nanyang Technological University, Nanyang Avenue, Singapore 639798,Republic of Singapore; asysong@ntu.edu.sg. SeniorResearch Fellow, ComputationalEngineeringand Design Center, School of EngineeringSciences, High eld; P.B.Nair@soton.ac.uk.Member AIAA. Professor of Computational Engineering, Director Computational En- gineering and Design Center, School of Engineering Sciences, High eld; Andy.Keane@soton.ac.uk. I. Introduction H IGH computational costs associated with the use of high- delity simulation models pose a serious impediment to the successful application of evolutionary algorithms (EAs) to engi- neering design optimization. A motivating example for us is aero- dynamic wing design, where one function evaluation involving the solution of the Navier Stokes equations can take many hours of computertime. Such computationallyexpensiveproblemsalsoarise inotherareassuchasstructuraldesign,electromagnetics,and design of coupled multidisciplinary systems. In such complex engineer- ing design problems EAs typically require thousands of function evaluations to locate a near-optimal solution. Hence, when com- putationallyexpensive high- delity simulation models are used for predicting design improvements, the use of EAs can be computa- tionally prohibitive. In gradient search it is now standard practice for computation- ally cheap surrogate models to be used in lieu of exact models to reducecomputationalcost.Because gradient-basedoptimizational- gorithms make use of line searches to locate a new iterate, the issue of range of validity of the approximation models or the control of approximation errors can be directly addressed by using ad hoc move limits or a trust-regionframework. As shown by Alexandrov et al.,1 the trust-regionstrategy for adaptively controlling the move limits guaranteesglobalconvergenceunder some mild assumptions on the accuracy of the surrogate model. More general frameworks for managing the use of approximation models in pattern search algorithms have also been proposed in the literature, for example, see Booker et al.2 and Sera ni.3 A more detailed survey of the state of the art can be found in Simpson et al.4 In contrast, because EAs make use of probabilistic recombina- tion operatorscontrollingthe step size of designchanges(to control the accuracy of approximate tness predictions) is not straightfor- ward as in gradient-based optimization algorithms. This dif culty becomes particularly severe when local approximation models are employed during search. In principle, global models can be em- ployed to circumvent this problem. However, in practice, because of the curse of dimensionality, such models become increasingly dif cult to construct for problems with large number of variables. Robinson and Keane5 presented a case for employing variable- delity analysis models and approximation techniques to improve theef ciencyofevolutionaryoptimizationforcomplexdesigntasks. Computationalframeworksforintegratinga classof single-pointap- proximation models with EAs were proposed by Nair and Keane.6 However, such frameworks are restricted to a special class of ap- proximation models that are domain speci c. For more general ap- proximationmodels Ratle7 examined a strategy for integratingevo- lutionary search with Kriging models. This problem was revisited 687 688 ONG, NAIR, AND KEANE by El-Beltagy et al.,8 where it is argued that the issue of balanc- ing the concernsof optimizationwith that of design of experiments must be addressed. Numerical studies were presented for certain pathological cases to show that the idea of constructing an accu- rate globalsurrogatemodelmight be fundamentally awed because of the curse of dimensionality. Liang et al.9 proposed a strategy for coupling EAs with local search and quadratic response surface methods. However, when working with multimodal problems the accuracy of quadratic models can become questionable.Jin et al.10 presenteda framework for couplingEAs and neural-network-based surrogate models. This approach uses both the expensive and ap- proximatemodelsthroughoutthe search,with an empiricalcriterion to decide the frequency at which each model should be used. In spite of extensive work on this topic, existing strategies for integrating approximation models with EAs have met with limited success in applications to real-world problems. Some of the key factors responsiblefor this are as follows: 1) The rstfactoris the curseof dimensionalitythatcausessignif- icant dif culties in constructing a global surrogate model which is capable of accurately predicting tness improvements during the search. This fundamental dif culty arises from the fact that the number of hypercubes required to ll out a compact region of a d-dimensional space grows exponentiallywith d. 2) The inability of most frameworks to handle problems with general nonlinear inequality and equality constraints is the second factor. 3) Most of the proposed strategies for managing the interplay between the exact and approximate models tend to compromise on the intrinsic parallelism of traditional EAs. The objectiveof thepresentpaperistodevelopa generalapproach for integratingsurrogatemodels with EAs, which addressesthe lim- itations of existing strategiesjust outlined. The proposedalgorithm leverages well-established notions in the literature on hybrid evo- lutionaryoptimization techniques,radial basis functions,and trust- region frameworks. The essential backbone of our approach is an EA hybridized with a feasible sequential quadratic programming (SQP) solver. The rationalebehind using a feasibleSQP solver is to exploititswell-knownabilityto ef cientlylocatethelocaloptimaof optimization problems with general constraints.11 Each individual in an EA generation is used as an initial guess for local search in the spirit of Lamarckian learning. We employ a trust-regionframe- work to manage the interplay between the original objective and constraint functions and computationally cheap surrogate models during local search. We propose the idea of employing local surrogate models that are constructedusing data points that lie in the vicinity of an initial guess. This local learning technique is an instance of the transduc- tive inferenceparadigm,which has recentlybeen the focus of recent research in statistical learning theory.12;13 Traditionally, surrogate models are constructed using inductive inference, which involves using a training dataset to estimate a functional dependency and then using the computed model to predict the outputs at the points of interest. However, when constructing surrogate models for opti- mization we are speci cally interested in ensuring that the models predictthe objectiveand constraintfunctionvaluesaccuratelyat the sequenceofiteratesgeneratedduringthesearch;howwell themodel performs at other points in the parameter space is of no concern in this speci c context. Transductive inference offers an elegant solu- tion to this problem by directly estimating the outputs at the point of interest in one step; the reader is referred to Vapnik s text12 (see Chapter 8) for a detailed theoretical analysis of its superior gener- alization capabilitiesover standard inductive inference. In the present work we implement transduction by constructing radial basis networks using data points in the local neighborhood of an optimization iterate. In other words, instead of constructinga global surrogatemodel a local model is created on the y whenever the objectiveand constraintfunctionsmust be estimated at a design point during local search. This idea of constructinglocal models is similarin spiritto the multipointapproximationtechniqueproposed by Toropov et al.14 and the moving least-squares approximation technique.15 It is shown that localized training data can be readily selectedfrom a searchengine databasecontainingpreviousiterates, which is continuouslyupdatedas the searchprogresses.Further, the proposedalgorithmcanbeef cientlyparallelizedongridcomputing architectures because it does not compromise on the intrinsic par- allelism offered by EAs. Extensive numerical studies are presented for some benchmarktest functionsand an aerodynamicwing design problem. We show that the present framework allows for the pos- sibility of converging to good designs on a limited computational budget. This paper is organized as follows: Section II outlines surrogate model constructionusing radial basis functions.Section III presents the proposed algorithm for integrating local surrogate models and trust-regionmethods with EAs. The grid infrastructureemployedto achieve parallelismis also brie y discussed.In Sec. IV, we summa- rize experimentalstudies on some benchmark test functionsand an aerodynamicwing designproblem.Section V summarizesthe main conclusions. II. Surrogate Modeling Surrogate models or metamodels are (statistical) models that are built to approximate computationally expensive simulation codes. Surrogate models are orders of magnitude cheaper to run and can be used in lieu of exact analysis during evolutionary search. Fur- ther, the surrogate model can also yield insights into the functional relationshipbetween the input x and the output y. If the true nature of a computer analysis code is representedas y D f .x/ (1) then a surrogate model is an approximationof the form Oy D Of .x/ (2) such that y D Oy C . There exist a variety of techniques for constructing surrogate models, for example, see the texts by Vapnik12 and Bishop16 for an excellentexpositionof thisarea.One popularapproachin the design optimization literature is least-squares regression using low-order polynomials, also known as response surface methods. A statisti- cally sound alternative for constructing surrogate models of deter- ministic computer models is Kriging, which is referred to as de- sign and analysis of computer experiments models in the statistics literature17 and Gaussian process regression in the neural-network literature.18 A comparison of some surrogate modeling techniques has been presented by Giunta and Watson19 and Jin et al.20 As mentioned earlier, in the present study the use of local sur- rogate models in the spirit of transductive inference is proposed. In particular, a surrogate model is built on the y when the ob- jective and constraint functions at an optimization iterate are to be estimated. This local model is built using only a small set of data points that lie in the local neighborhoodof the design point of in- terest. Because surrogate models will probably be built thousands of times during the search in this fashion, computationalef ciency is a major concern. This consideration motivates the use of radial basis functionnetworks,which can be ef cientlyappliedto approx- imate multiple-inputmultiple-output data, particularly when a few hundred data points are used for training. Let fxi; yi; i D 1; 2; : : : ; ng denote the training dataset, where x 2 Rd is the input vector and y 2 R is the output. Because we are interested in cases where the training data are generated by run- ning deterministiccomputermodels, we will focus on interpolating radial basis function models of the form Oy D n X i D 1 i K.kx xik/ (3) where K.kx xik/: Rd ! R is a radial basis kernel and D f 1; 2; : : : ; ng 2 Rn denotes the vector of weights. Typicalchoicesforthe kernelincludelinearsplines,cubicsplines, multiquadrics, thin-plate splines, and Gaussian functions.16 The structure of some commonly used radial basis kernels and their parameterizationare shown in Table 1. Given a suitable kernel, the weight vector can be computed by solving the linear algebraic sys- tem of equations K D y, where y D fy1; y2; : : : ; yng 2 Rn denotes ONG, NAIR, AND KEANE 689 Table 1 Radial basis kernels Kernel Parameterization Linear splines kx cik Thin-plate splines kx ci kk kx cikk Cubic splines kx cik3 Gaussian exp[ .kx cik2/= i] Multiquadrics p 1 C .kx cik2/= i Inverse multiquadrics [1 C .kx cik2/= i] 1=2 the vectorof outputsand K 2 Rn n denotesthe Gram matrix formed using the training inputs (i.e., the i jth element of K is computed as K.kxi xjk/). Micchelli21 proved that nonsingularityof the Gram matrix K can be theoreticallyguaranteed for a class of kernels only when the set of input vectors in the training dataset is distinct. In many papers in the radial basis function literature, a polynomial term P is often appended to Eq. (3) along with some constraints. In other words, if K is a conditionally positive de nite function of order q then to ensurea uniquesolutionfor the weight vector Eq. (3) is rewrittenas Oy D n X i D 1 i K.x; xi/ C Pq.x/ (4) where Pq is a polynomial of order q 1 and the following homo- geneous constraint equations are imposed: n X i D 1 i Pk.xi/ D 0; 1 k q (5) Thentheweightvectorcanbecomputedbysolvinga linearalgebraic system of equationsof the form Ax D b, where A D K P PT 0 ; x D f ; gT ; b D fy; 0gT (6) where P is a matrix that arises by substituting the input vectors in the training dataset into the polynomial term P. In practice, good approximations can be obtained by using a constant instead of a full-order polynomial. Here, the coef cient matrix A becomes A D K 1 1T 0 2 R.n C 1/ .n C 1/ (7) where 1 2 Rn is a vector of ones. For problems with multiple outputs, the weight vector can be ef- ciently computed for all of the outputs of interest once the matrix K is decomposed. For a typical dataset with 500 training points, 20 inputs, and ve outputs, surrogatemodel constructionusing lin- ear splines takes a fraction of a second on one processor of an SGI Power Challenge. When dealing with computationally expensive problems that cost more than a few minutes of cpu time per func- tion evaluation,this training cost is negligible. In the present study we use linear splines to construct surrogate models because experimentalstudies in the literature20 suggest that this kernel is capable of providingmodels with good generalization capability at a low computational cost. We present next an algo- rithm that integrates a local version of such surrogates in hybrid evolutionarysearch. III. Present Framework In thissectionwe presentthe essentialingredientsof the proposed local surrogate modeling algorithm for parallel evolutionary opti- mization of computationallyexpensive problems. In particular, we considera generalnonlinearprogrammingproblemof the following form. Minimize: f .x/ Subject to: gi.x/ 0; i D 1; 2; : : : ; p xl x xu (8) where x 2 Rd is the vector of design variables. In this paper we are interested in cases where the evaluation of f .x/ and g.x/ is computationallyexpensive,and it desiredto obtain a near-optimalsolutionona limitedcomputationalbudget.The basic steps of the proposed algorithm are outlined here: BEGIN Initialize: Generate a database containing a population of designs. (Optional: upload a historical database if one exists) While (computationalbudget not exhausted) Evaluate all individualsin the population using the exact models. For each nonduplicatedindividualin the population 1) Apply trust-regionenabled feasible SQP solver to each individual in the population by interleaving the exact and local surrogate models for the objective and constraint functions. 2) Update the database with any new design points generated during the trust-regioniterations and their exact objective and constraint function values. 3) Replace the individualsin the population with the locally improved solution in the spirit of Lamarckian learning. End For Apply standard EA operators to create a new population. End While END In the rst step we initialize a database using a population of de- signs, either randomly or using design of experiments techniques such as Latin hypercube sampling. All of the design points thus generated and the associated exact values of the objective and con- straint functions are archived in the database that will be used later for constructinglocalsurrogatemodels.Alternatively,onecoulduse a databasecontainingthe resultsof a previoussearchon the problem or a combination of the two. Subsequently, with ample design points in the database a hy- brid EA is employed, where for each nonduplicated design point or chromosome in the population a local search is conducted using surrogates. The local strategy used here embeds the feasible SQP optimizer within a trust-region framework.22 However, instead of adopting an augmented Lagrangian approach we handle the objec- tiveand constraintfunctionsseparatelyusingthe approachof Giunta and Eldred.23 More speci cally, during local search for each chro- mosome in an EA generation we solve a sequence of trust-region subproblemsof the following form. Minimize: Of k x C xk c Subject to: Ogk i x C xk c 0; i D 1; 2; : : : ; p kxk 1k (9) where k D 0; 1; 2; : : : ; kmax. In practice, the L1 norm can be em- ployed to impose the second constraint in Eq. (9). Hence, this con- straint can be transformed into appropriate bounds on the design variables, which is updated at each trust-region iteration based on the value of 1k. For each subproblem(or duringeachtrust-regioniteration)surro- gate models of the objectiveand constraintfunctions,namely Of k.x/ and Ogk i .x/, are created dynamically.The m nearest neighbors of the initial guess xk c are rst extracted from the archived database of design points evaluated so far using the exact analysis code. The criterion used to determine the similarity between design points is the simple Euclidean distance metric. These points are then used to construct local surrogate models of the objective and constraint functions.Carehasto be takento ensurethatrepetitionsdo notoccur in the training dataset because this might lead to a singular Gram matrix K. The surrogate models thus created are used to facilitate the nec- essary objective and constraint function estimations in the local searches. During local search, we initialize the trust region 1 us- ing the minimum and maximum values of the design points used to 690 ONG, NAIR, AND KEANE construct the surrogate model. We found this initializationstrategy worked well for the problems considered in this paper. After each iteration the trust region radius 1k is updated based on a measure that indicates the accuracy of the surrogate model at the kth local optimum xk lo. After computing the exact values of the objective and constraintfunctionsat this point, the gure of merit k is calculated as k D min k f ; k gi for i D 1; 2; : : : ; p (10) where k f D f xk c f xk lo Of xk c Of xk lo ; k gi D gi xk c gi xk lo Ogi xk c Ogi xk lo (11) Theprecedingequationsprovidea measureoftheactualvspredicted changeinthe objectiveand constraintfunctionvaluesatthe kth local optimum. The value of k is then used to update the trust region radius as follows1: 1k C 1 D 0:251k if k 0:25 D 1k if 0:25 < k 0:75 D 1k if k 0:75 (12) where D 2 if kxk lo xk ck1 D 1k, or D 1 if kxk lo xk ck1 < 1k. The trust regionradius 1k is reduced if the accuracyof the surro- gate, measuredby k, is low. 1k is doubled if the surrogateis found to be accurate and the kth local optimum xk lo lies on the trust-region bounds; otherwise, the trust-regionradius remains unchanged. The exact values of the objective and constraint functions at the optimal solution of the kth subproblem are combined with the m nearestneighboringdesignpointsto generatea newsurrogatemodel for the next iteration. In addition, the initial guess for the (k C 1)th iteration within each local search is determined by xk C 1 c D xk lo if k > 0 D xk c if k 0 (13) The trust-region iterations (for each chromosome) are terminated when k kmax. At the end of kmax trust-regioniterations for a chro- mosome, the exact tness of the locally optimized design point is determined. If it is found to be better than that of the initial guess, then Lamarckianlearningproceeds.Lamarckianlearningforces the genotypeto re ect the result of improvement by placing the locally improved individual back into the population to compete for repro- ductiveopportunities.Inaddition,thelocallyoptimizeddesignpoint and its corresponding objective and constraint function values are added to the database. This process of hybrid EA search is contin- ued until the computationalbudget is exhausted or a user-speci ed termination criterion is met. Apart from the parameters used in standard EAs, our algorithm has two additionaluser-speci ed parameters:kmax and m. In Sec. IV we present experimental studies to investigate the effect of these parameters on the convergencetrends. A. Some Remarks on Global Convergence Globalconvergenceis de ned in the optimizationliteratureas the mathematical assurance that the iterates produced by an algorithm, started from an arbitrary initial guess, will converge to a stationary pointor local optima of the originalhigh- delity expensiveanalysis code.An approachbased on the classicaltrustregionidea from non- linear programming is shown by Alexandrov et al.1 to be probably convergent to a local optima of the original problem. Global convergence results for EAs that make use of approxi- mation models in the search have not appeared in the literature. Nevertheless, it is possible to design EAs that inherit the global convergenceproperties of existing algorithms. The work by Hart24 has shown one such possibility where a provably convergent evo- lutionary pattern search algorithm was proposed that inherits the existing theory for traditional pattern search. To proveglobalconvergencefortrust-regionframeworksthatem- bed surrogatemodels in the local search,Alexandrovet al.1 showed that zero-order and rst-order consistency conditions have to be imposed at the initial guess, that is, Of xk c D f xk c (14) r Of xk c D r f xk c (15) Because we use an interpolatingsurrogate model in the present ap- proach, only the zero-order consistencycondition is satis ed at the initial guess. To satisfy Eq. (15), the exact sensitivities of the ob- jective and constraint functions are required, which would be com- putationally prohibitive for many problems. Convergence analysis of trust-region algorithms when only inexact gradient information is available has been considered by Carter25 and Toint.26 Lever- aging these results, Arian et al.27 presented a theoretical analysis for unconstrainedoptimization using surrogatesto show that under mild assumptions convergence can still be guaranteed. In partic- ular, the condition the surrogate model needs to satisfy is that the predicteddirectionof descentapproximatesthe true directionsuf- cientlywell in the limit. This resultcan be readilyextendedto non- linear programming problems with general constraintsby adopting an augmentedLagrangianformulationon the lines of that presented by Rodriguez et al.22 In summary, global convergencecan be guar- anteedonly when some assumptionsare made regardingthe descent direction computed using the surrogatemodel. The observations made here are of theoretical interest alone because 1) we only carry out a few trust-region iterations during local search for each chromosome and 2) we do not have a theo- retical upper bound on the degree to which the descent direction computed using the surrogatemodel approximatesthe actual direc- tion of descent. B. Parallel Implementation In engineering design optimization evaluation of the objective and constraint functions takes up the overwhelming bulk of the computation. Therefore, a sublinear improvement in design search ef ciency can be achieved via global parallelism, where all de- sign points within a single populationare evaluatedsimultaneously acrossmultiple machines.Parallelismis thus considereda desirable feature of any framework for optimization of computationally ex- pensiveproblems.In thepresentalgorithmitis relativelystraightfor- ward to achieveparallelismbecauselocal searchfor each individual in an EA generationcan be conductedindependently.To ensureload balancing, we only need to specify that the number of trust-region iterations be kept the same for each individual. In the present implementation of our algorithm, we employed NetSolve,28 a computationalplatformthatfacilitatesgrid-basedhet- erogenous computing29 in a transparent and ef cient manner. Par- allelism is achieved by wrapping the local search and surrogate modelingroutineson a NetSolve server.The analysiscodes are also wrapped on NetSolve servers, which can be invoked by the local search and the client routines. Hence, using the farming client ap- plicationprogramming interface local search for each chromosome in an EA generation can be readily conducted in parallel on remote servers. Even though we used a centralized database, NetSolve has capabilitiesfor distributed data storage on remote servers. This ap- proach does not parallelize the SQP steps and thus is only suitable where the SQP updates can be offered as serial processes. IV. Numerical Studies In this section we present numerical results obtained by imple- menting the proposed approach within a standard binary coded ge- netic algorithm (GA). We employed a population size of 50 and uniform crossover and mutation operators at probabilities 0.6 and 0.01, respectively.A linear ranking algorithm is used for selection. The codesimplementingthe objectiveand constraintfunctionswere wrapped on NetSolveserversrunningRed Hat Linux on Pentium III processors.The GA code is linked to the NetSolve client library so that the objective function and constraint evaluation modules, and the local search routines can be invoked remotely. ONG, NAIR, AND KEANE 691 The feasible SQP implementation that is used here is the FFSQP code developed by Lawrence and Tits.11 When started from an in- feasiblepoint(onethatviolatesat leastone of the linear or nonlinear constraints),FFSQP rstdoesanoptimizationin which it minimizes the maximum of the constraint violations. Subsequently, FFSQP minimizes the objective function, while maintaining feasibility of the iterates. A. Results for Benchmark Test Functions Two benchmarkproblemscommonlyusedin the globaloptimiza- tion literature are adopted here for testing the proposed algorithm. Theyrepresentclassesofunconstrainedandconstrainedmultimodal test problems. These problems make it possible to study whether the proposed approach would bring any increase in ef ciency or computationalcost reductionwhen used on complex problems.The rst example considered is minimization of the Rastrigin function de ned here.30 Minimize: 10d C d X i D 1 x2 i 10 cos.2 xi/ Subject to: 5:12 xi 5:12; i D 1; 2; : : : ; d (16) The second problem considered is maximization of the bump test function,which is very hard for most search methods to handle.31 It is quite smooth but contains many peaks, all of similar heights. Its main purpose is to test how methods cope with optima that occur hard up against the constraint boundaries commonly found in en- gineeringdesign. These propertiesmake it suitable for the study of GA performance as well as optimizing control parameters of evo- lutionaryoptimizationmethods.The function is de ned as follows. Maximize: abs " d X i D 1 cos4.xi/ 2 dY i D 1 cos2.xi/ #,v u u t d X i D 1 ix2 i Subject to: dY i D 1 xi > 0:75; d X i D 1 xi < 15d 2 0 xi 10; i D 1; 2; : : : ; d (17) The objective function in the preceding problem gives a highly bumpy surface where the true global optimum is usually de ned by the product constraint. Figure 1 shows both the Rastrigin and the bump objective function for d D 2. A 20-dimensional (d D 20) version of the test functions is used here for numerical studies. For the rst problem involving minimization of the Rastrigin function, there is a unique global optima at which the function value is zero. For the bump test function, even though the global optima is not Fig. 1a Surface plot for d = 2 of Rastrigin function. Fig. 1b Surface plot for d = 2 of bump function. precisely known for d D 20, a value of 0.81 can be obtained after around 100,000 function evaluationsusing a GA. The averaged convergence trends obtained by applying the present algorithm to the benchmark test problems as a function of the total number of function evaluations are shown in Figs. 2 4. The results presented here were averaged over 20 runs for each test function. Also shown in the gures are averaged convergence trends obtained using a standard GA and a global surrogate model- ing strategy. The algorithm based on global surrogate models em- ployed in our numerical studies is based on the approach proposed by Ratle7; see the following for an outline of the steps involved in this algorithm: BEGIN Initialize: Generate a database containing a population of designs. (Optional: upload a historical database if one exists) Construct surrogate model using all available design points in the database. Set tness function :D Surrogate model While (computationalbudget not exhausted) Evaluate all individualsin the population using the tness function. Apply standard EA operators to create a new population. If ( tness function :D Exact model) Update database with any new designs generated using the exact model. Update surrogate model using all designs in the database. End If If (convergenceover surrogate model) tness function :D Exact model Else tness function :D Surrogate model End If End While END The results obtained for the test functions show that the global surrogate framework displays early sign of stalling. This is consis- tent with previous studies in the literature,7;8;10 which suggest that when global surrogate models are applied to high-dimensionaland multimodaltest functionsthe searchgenerallytendsto stallearlyon. Such an effect is a result of the curse of dimensionality,which often leads to early convergence at false global optima of the surrogate model. In contrast, the results obtained using the proposed algo- rithm clearly demonstrate that solutions close to the global optima can be obtained on a limited computational budget. As surrogates are usedonlyforlocalsearches,thatis,as the exactmodelis usedfor all analysis conducted at the EA level, the chances for convergence to false global optima are greatly reduced. In addition, the use of the trust-regionframework maintainsconvergenceclose to the local optima of the original problem during the SQP steps. For these benchmark problems we also studied the effect of in- creasing the maximum number of trust-region iterations and the 692 ONG, NAIR, AND KEANE Fig. 2 Averaged convergence trends for m = 100 and various values of kmax (3, 5, and 8) compared with standard GA and global surrogate modeling framework for the 20-dimensionalRastrigin function. Fig. 3 Averaged convergence trends for kmax = 3 and various values of m (100, 150, and 200) compared with standard GA for the 20-dimensional Rastrigin function. ONG, NAIR, AND KEANE 693 Fig. 4 Averaged convergence trends for kmax = 3 and various values of m (100, 150, and 200) compared with standard GA for the 20-dimensional bump function. Fig. 5 Aircraft wing planform geometry. number of nearest neighbors (employed to construct the local sur- rogatemodel)on the convergencebehavior(Figs. 2 4). A numberof observationscan be made from the convergencetrends. First, it ap- pears that there is not much to be gained by increasingkmax beyond three. Second, it appears that smaller values of m generally lead to faster convergence during the early stages of search, but there is a tendency to stall at later stages. The converse is true for increases in m. This suggests the possibility of adaptively selecting m during the search. We propose the following simple strategy: m D .mmin C mmax/.tc=tt/ (18) where mmin D population size and mmax, which limits the design point size used for local surrogate modeling, is set to 400 here. B. Aerodynamic Wing Design In this section we present the application of the proposed algo- rithm to the transonic civil transport aircraft wing design problem consideredin Keane and Petruzzelli.32 Aerodynamicwing design is an extremely complex task, which is normally undertaken over an extended time period and at different levels of complexity.The pa- rameters used to describe the wing design problem consideredhere consist of the freestream velocity and coef cient of lift of the wing Table 2 Optimization conditions for wing design parameters, constraints, and respective limits Lower limit Upper limit Quantity (unit) 11 wing design variable de nitions 100 250 Wing area (m2), S 6 12 Aspect ratio, W 2 s =S 0.2 0.45 Kink position, 2Wk=Ws 25 45 Sweep angle (deg), 0.4 0.7 Inboard taper ratio, Ck=Cr 0.2 0.6 Outboard taper ratio, Ct =Cr 0.1 0.18 Root thickness/chord, Tr=Cr 0.06 0.14 Kink thickness/chord, Tk=Ck 0.06 0.14 Tip thickness/chord, Tt=Ct 4.0 5.0 Tip wash (deg) 0.65 0.85 Kink washout fraction 4 design constraints 2.5 Undercarriage bay length 135,000 Wing weight (N) 40.0 Wing volume (m3) 5.4 Pitch-up margin together with a small number of overall wing geometry variables. The geometryis characterizedby the planformshapeof the wing to- gether with several span-wise functionssuch as twist and thickness to chord ratio. The planform geometry and design variable de ni- tions are shown in Fig. 5 and Table 2, respectively. To prevent the optimizerfrom driving the designsto unworkableextremes,several constraints are placed on the wings designed. These are the under- carriage bay length (which must be accommodated within the root to kink section of the wing), the fuel tank volume (which must be accommodated between the main spars within the wing), the wing weight, and the pitch-up margin. These four nonlinear inequality constraintsare also listed in Table 2. In the present study we considered the optimization of a civil transportaircraft wing for operation at Mach 0.785 and a Reynolds 694 ONG, NAIR, AND KEANE Fig. 6 Summary of minimum drag values (in counts) using the VSAERO code and surrogate models. Results are average of three runs. Table 3 Summary of minimum drag values (in counts) using the TADPOLE code and surrogate models Mean wing drag values (D=q, m2) at evaluation counts of Approach 250 500 600 800 1200 1500 m D 100 2.823 2.781 2.788 2.788 2.788 2.788 m D 200 3.218 2.788 2.775 2.771 2.768 2.768 m D 300 3.235 3.049 3.049 2.771 2.764 2.761 m D 400 3.265 3.064 3.049 3.020 2.772 2.758 Adaptive m 2.919 2.771 2.771 2.768 2.760 2.758 Standard GA 3.213 3.004 2.983 2.978 2.974 2.961 numberof 7:3 106. Theobjectiveisminimizationofwing D=q m2 as calculatedby using an empirical drag estimation tool TADPOLE and the linearized potential code VSAERO, with target lift, wing weight, volume, pitch-up margin, and root triangle layout chosen to be representative of a 220-seat wide-body airliner. Both codes return the total drag coef cient de ned by the wave drag as a result of the presence of shocks, viscous wake, or pro le drag caused by the boundary layer and vortex or induced drag caused by the tip vortex of the three-dimensionalwing. A common approach to drag recovery is also implemented in the two codes. TADPOLE takes only some 6 s to run and returns drag values based on curve ts to previously analyzed wings making assumptions about the kinds of roof-top pressure pro les now commonly achieved in transonic wing design. VSAERO is a linearized potential code with coupled viscous boundary layer and, as employed here, with added correc- tionfor compressibility.33 It is computationallymore expensivethan TADPOLE and requires approximately11 min of compute time per drag evaluation. However, it has the advantage of providing more accurate drag predictionsprovidedMach numbers are not too high. A summary of the search results obtained for the aerodynamic wing design problem is shown in Table 3 and Figs. 6 and 7. Table 3 shows the search results when using TADPOLE for drag estima- tion. Once again, it is observed from these results that smaller val- ues of m lead to faster convergenceof the wing drag values during the early stages of search, but result in stalling at later stages, and the converse is true for increases in m. These results are in line with those observed earlier for the test functions. Hence, for the computationally expensive VSAERO code m is chosen adaptively using Eq. (18). We also set the maximum number of trust region iterations (kmax) to three. During local search, we constructed sur- rogate models for the objective function and the four inequality constraints. For this problem it has been observed from previous studies32 using the TADPOLE code that a design with a drag value of 2.758 counts can be obtained after 10,000 evaluations. In com- parison, using the proposed approach we are able to converge to this solution on an average after 1500 exact evaluations,when m is chosen adaptively during the search. In Fig. 6, the averaged convergence trends using VSAERO for wing drag estimation also clearly illustrate the ability of the pro- posedalgorithmto arriveatgooddesignson a limitedcomputational budget. The convergencetrends of the best run for the aerodynamic wing design problem using VSAERO are also plotted as a function of wall time in Fig. 7. Because of the availability of only eight li- censes for the VSAERO code, the timing plot obtained in Fig. 7 is based on a total of eight processors being used for parallel com- putations. Previous studies using the VSAERO code for wing drag estimation have revealed that the best design that can be obtained using the standard GA with only the exact analysis model has a drag value of 2.63 after 1800 evaluations.In comparison, using the presentapproachwe were able to obtainthis solutionon averageaf- ter 250exactevaluations.After733exactevaluationsthe bestdesign obtained using our algorithm had a drag value of 2.404, which is the lowest value obtained to date for this problem using various op- timization algorithms.The optimal solutionsreported here satis ed all four of the inequality constraints. ONG, NAIR, AND KEANE 695 Fig. 7 Comparison of best convergence trends as a function of wall time for the aerodynamic wing design problem using the VSAERO code and surrogate models. V. Conclusions In this paper we present a hybrid algorithm that leverages sur- rogate models for evolutionary optimization of computationally expensive constrained design problems. It is argued that for such complex design problems constructingan accurateglobal surrogate model is fraught with fundamentaldif culties because of the curse of dimensionality. A local learning approach in the spirit of trans- ductive inference is employed to construct surrogate models. We show that such local approximation models can be readily incor- porated into hybrid evolutionary-gradientoptimization algorithms. Because our local searchstrategyemploysa trust-regionframework to interleave the exact and approximatemodels, convergenceto the optima of the original expensive problem can be guaranteed under some mild assumptions. Further, it is shown that the present ap- proach retains the intrinsic parallelism of traditional evolutionary algorithms. We presented extensive numerical studies on some benchmark test functions to demonstrate the competitiveness of the proposed algorithm. The results were compared with those obtained using a standardgeneticalgorithmand a globalsurrogatemodelingstrategy. Experimental results are also presented for an aerodynamic wing design problem. These studies indicate that the present approach allows for the possibility of arriving at near-optimal solutions on a limited computationalbudget. Acknowledgments This research was supported by the University Technology Part- nership for Design, a partnership between BAe Systems, Rolls Royce Plc., and the Universities of Southampton, Cambridge, and Shef eld, England, United Kingdom. The authors thank the anony- mous referees and editors for their constructive comments on an earlier draft of this paper. References 1Alexandrov, N., Dennis, J. E., Lewis, R. M., and Torczon, V., A Trust Region Framework for Managing the Use of Approximation Models in Op- timization, Structural Optimization, Vol. 15, No. 1, 1998, pp. 16 23. 2Booker, A. J., Dennis, J. E., Jr., Frank, P. D., Sera ni, D. B., Torczon, V., and Trosset, M. W., A Rigorous Framework for Optimization of Expensive Functions by Surrogates, Structural Optimization, Vol. 17, No. 1, 1998, pp. 1 13. 3Sera ni, D. B., A Framework for Managing Models in Nonlinear Op- timization of Computationally Expensive Functions, Ph.D. Dissertation, Computational and Applied Mathematics Dept., Rice Univ., Houston, TX, Nov. 1998. 4Simpson, T. W., Booker, A. J., Ghosh, D., Giunta, A. A., Koch, P. N., and Yang, R.-J., Approximation Methods in Multidisciplinary Anal- ysis and Optimization: A Panel Discussion, Proceedings of the Third ISSMO/AIAA Internet Conference on Approximationsin Optimization [CD- ROM], Oct. 2002. 5Robinson,G.M.,and Keane, A.J., A Case forMulti-LevelOptimization in Aeronautical Design, Aeronautical Journal, Vol. 103, No. 1028, 1999, pp. 481 485. 6Nair, P. B., and Keane, A. J., Passive Vibration Suppression of Flexible Space Structures via Optimal Geometric Redesign, AIAA Journal, Vol. 39, No. 7, 2001, pp. 1338 1346. 7Ratle, A., Kriging as a Surrogate Fitness Landscape in Evolutionary Optimization, Arti cial Intelligence for Engineering Design Analysis and Manufacturing, Vol. 15, No. 1, 2001, pp. 37 49. 8El-Beltagy, M. A., Nair, P. B., and Keane, A. J., Metamodelling Tech- niques for Evolutionary Optimization of Computationally Expensive Prob- lems: Promises and Limitations, Proceedings of the Genetic and Evolution- ary Computation Conference (GECCO99), edited by W. Banshaf, J. Daida, A. E. Eiben, M. H. Garzon, V. Honavar, M. Jakiela, and R. E. Smith, Morgan Kaufman, San Mateo, CA, 1999, pp. 196 203. 9Liang, K.-H., Yao, X., and Newton, C., EvolutionarySearch of Approx- imated N-dimensional Landscapes, International Journal of Knowledge- Based Intelligent Engineering Systems, Vol. 4, No. 3, 2000, pp. 172 183. 10Jin, Y., Olhofer, M., and Sendhoff, B., A Framework for Evolutionary Optimization with Approximate Fitness Functions, IEEE Transactions on Evolutionary Computation, Vol. 6, No. 5, 2002, pp. 481 494. 696 ONG, NAIR, AND KEANE 11Lawrence, C. T., and Tits, A. L., A ComputationallyEf cient Feasible SequentialQuadratic ProgrammingAlgorithm, SIAMJournalon Optimiza- tion, Vol. 11, No. 4, 2001, pp. 1092 1118. 12Vapnik, V., Statistical Learning Theory, Wiley, New York, 1998. 13Chapelle, O., Vapnik, V., and Weston, J., Transductive Inference for Estimating Values of Functions, Advances in Neural Information Process- ing Systems, Vol. 12, 1999. 14Toropov, V. V., Filatov, A. A., and Polynkin, A. A., Multiparameter Structural Optimization Using FEM and Multipoint Explicit Approxima- tions, Structural Optimization, Vol. 6, No. 1, 1993, pp. 7 14. 15Levin, D., The Approximation Power of Moving Least-Squares, Mathematics of Computation, Vol. 67, No. 224, 1998, pp. 1517 1532. 16Bishop, C., Neural Networks for Pattern Recognition, Oxford Univ. Press, Oxford, 1995. 17Sacks, J., Welch, W. J., Mitchell, T. J., and Wynn, H. P., Design and Analysis of Computer Experiments, StatisticalScience, Vol. 4, No. 4, 1989, pp. 409 435. 18Williams, C. K. I., and Rasmussen, C. E., Gaussian Processes for Re- gression, Advances in Neural Information Processing Systems 8, edited by D. S. Touretsky, M. C. Mozer, and M. E. Hasselmo, MIT Press, Cambridge, MA, 1996, pp. 514 520. 19Giunta, A. A., and Watson, L. T., A Comparison of Approximation Modeling Techniques: Polynomial Versus Interpolating Models, AIAA Paper 98-4758, Sept. 1998. 20Jin, R., Chen, W., and Simpson, T. W., Comparative Studies of Meta- modeling Techniques Under Multiple Modeling Criteria, Structural and MultidisciplinaryOptimization, Vol. 23, No. 1, 2001, pp. 1 13. 21Micchelli, C. A., Interpolation of Scattered Data: Distance Matrices and Conditionally Positive De nite Functions, Constructive Approxima- tion, Vol. 2, No. 1, 1986, pp. 11 22. 22Rodriguez,J. F., Renaud,J.E., and Watson,L. T., Convergenceof Trust Region Augmented Lagrangian Methods Using Variable Fidelity Approxi- mation Data, Structural Optimization, Vol. 15,No. 3 4, 1998,pp.141 156. 23Giunta, A. A., and Eldred, M. S., Implementation of a Trust Region Model Management Strategy in the DAKOTA Optimization Toolkit, Pro- ceedings of the 8th AIAA/USAF/NASA/ISSMO Symposium on Multidisci- plinary Analysis and Optimization [CD-ROM], 2000. 24Hart, W. E., A Convergence Analysis of Unconstrained and Bound Constrained Evolutionary Pattern Search, Evolutionary Computation, Vol. 9, No. 1, 2000, pp. 1 23. 25Carter, R. G., On the Global Convergence of Trust-Region Algorithms Using Inexact Gradient Information, SIAM Journal of Numerical Analysis, Vol. 28, No. 1, 1991, pp. 251 265. 26Toint, P. L., Global Convergence of a Class of Trust Region Methods for Nonconvex Minimization in Hilbert Space, IMA Journal of Numerical Analysis, Vol. 8, No. 2, 1988, pp. 231 252. 27Arian, E., Fahl, M., and Sachs, E. W., Trust-Region Proper Orthogonal Decomposition for Optimal Flow Control, NASA CR-2000-210124,May 2000. 28Casanova, H., and Dongarra, J., NetSolve: A Network Server for Solv- ing Computational Science Problems, International Journal of Supercom- puter Applicationsand High Performance Computing, Vol. 11, No. 3, 1997, pp. 212 223. 29Foster, C., and Kesselman, C. (eds.), The Grid: Blueprint for a New Computing Infrastructure, Morgan Kaufmann, San Mateo, CA, 1999. 30Liang,K.H.,Yao,X.,andNewton,C., EvolutionarySearch ofApprox- imated N-Dimensional Landscapes, International Journal of Knowledge- Based Intelligent Engineering Systems, Vol. 4, No. 3, 2000, pp. 172 183. 31Keane, A. J., Genetic Algorithm Optimization of Multi-Peak Prob- lems: Studies in Convergence and Robustness, Arti cial Intelligence in Engineering, Vol. 9, No. 2, 1995, pp. 75 83. 32Keane, A. J., and Petruzzelli, N., Aircraft Wing Design Using GA-Based Multi-Level Strategies, Proceedings of the 8th AIAA/USAF/NASSA/ISSMO Symposium on Multidisciplinary Analysis and Optimization [CD-ROM], 2000. 33Petruzzelli, N., and Keane, A. J., Wave Drag Estimation for Use with Panel Codes, Journal of Aircraft, Vol. 38, No. 4, 2001, pp. 778 782. A. Messac Associate Editor