Research Article Deformation Adjustment with Single Real Signature Image for Biometric Verification Using CNN Rakesh Kumar ,1 Mala Saraswat ,2 Danish Ather ,3 Muhammad Nasir Mumtaz Bhutta ,4 Shakila Basheer,5 and R. N. Thakur 6 1Department of Computer Engineering & Applications, GLA University Mathura, Mathura-281406, India 2Department of Computer Science and Engineering, ABES Engineering College Ghaziabad, India 3Department of Computer Science & Engineering, School of Engineering & Technology Sharda University, Grater Noida, India 4Computer Science and Information Technology (CSIT), College of Engineering, Abu Dhabi University, P.O. Box 5991, Abu Dhabi, UAE 5Department of Information Systems, College of Computer and Information Science, Princess Nourah Bint Abdulrahman University, P.O. Box 84428, Riyadh 11671, Saudi Arabia 6LBEF Campus, Kathmandu, Nepal Correspondence should be addressed to R. N. Takur; rn.thakur@lbef.edu.np Received 29 March 2022; Revised 23 April 2022; Accepted 16 May 2022; Published 25 June 2022 Academic Editor: Shakeel Ahmad Copyright 2022 Rakesh Kumar et al. Tis is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. Signature veri cation is the widely used biometric veri cation method for maintaining individual privacy. It is generally used in legal documents and in nancial transactions. A vast range of research has been done so far to tackle di erent system issues, but there are various hot issues that remain unaddressed. Te scale and orientation of the signatures are some issues to address, and the deformation of the signature within the genuine examples is the most critical for the veri cation system. Te extent of this deformation is the basis for verifying a given sample as a genuine or forgery signature, but in the case of only a single signature sample for a class, the intra-class variation is not available for decision-making, making the task di cult. Besides this, most real- world signature veri cation repositories have only one genuine sample, and the veri cation system is abiding to verify the query signature with a single target sample. In this work, we utilize a two-phase system requiring only one target signature image to verify a query signature image. It takes care of the target signature s scaling, orientation, and spatial translation in the rst phase. It creates a transformed signature image utilizing the a ne transformation matrix predicted by a deep neural network. Te second phase uses this transformed sample image and veri es the given sample as the target signature with the help of another deep neural network. Te GPDS synthetic and MCYT datasets are used for the experimental analysis. Te performance analysis of the proposed method is carried out on FAR, FRR, and AER measures. Te proposed method obtained leading performance with 3.56 average error rate (AER) on GPDS synthetic, 4.15 AER on CEDAR, and 3.51 AER on MCYT-75 datasets. 1. Introduction Te biometric system utilizes an individual s physiological or behavioural characteristics for identi cation, veri cation, and authentication. Te invariable physiological character- istics include DNA, iris, ngerprint, palm, and facial ex- pression [1, 2], whereas behavioural traits cover voice, signature, and handwriting [1, 3, 4]. Physical characteristics such as ngerprint and iris are often used because of their high performance. However, handwriting signatures are still being used and researched due to their ubiquitous use and cultural acceptance for personal authentication. Over cen- turies, its presence in legal documents, property wills and testaments, agreements, contracts, administrative records, and other legal and nancial documents established it as a valuable trait. In the past, manual signature veri cation systems have substantially been used, but they are time- consuming and error-prone. Hence, research has been Hindawi Computational Intelligence and Neuroscience Volume 2022, Article ID 4406101, 12 pages https://doi.org/10.1155/2022/4406101 carried out on automating the veri cation of handwritten signatures since the decade of 1970 [5]. It justi ed the re- search community s extensive investigation and needs in- dustry e orts to develop better products on researched technologies. Biometric signature systems are involved in two sce- narios, namely identi cation and veri cation. In the case of signature identi cation, the task is to retrieve similar sig- nature samples from a signature repository when a signature is provided as a graphical query. In comparison, the sig- nature veri cation system decides whether the same signer produces a given query signature or not. Tus, the signature veri cation system is used to classify given handwritten signature samples as genuine or forgeries. Te broad cate- gories of forgeries are random, simple, and skilled. Tis categorization is based on the availability of the user s name and signature to the forger. In the rst category, the forger does not have information about both the factors. Due to this reason, the forger presents a signature with a di erent shape and looks very di erent in a holistic view. In contrast, the forger knows the user s name in the category of simple forgery. Hence, the forger can produce a much similar signature compared with a genuine signature if a user uses his name or subpart of it as a signature, whereas in the case of skilled forgery, the forger possesses information about the user name and the signature. It helps the forger practice the genuine signature and produces an almost similar signature to the genuine one. Due to this reason, detecting the forged signature in the case of skilled forgery is challenging. Depending on the signature acquisition method, sig- nature veri cation systems are either online or o ine [6, 7]. If the acquisition method stores the signature as a sequence of pen placement points over time, then the corresponding system is an online signature veri cation system. An ex- ample of such an acquisition device is digitizing tablets. Additional information is also available in digitizing tablets, such as pen s inclination and tip pressure. In contrast, the o ine signature system relied on devices such as digital cameras, in which the signature is considered as an image [8]. Tis work is mainly focused on the o ine signature veri cation system. Te signature image has been considered a static representation of the signature for this work. O ine signature veri cation can follow two di erent approaches namely writer-dependent and writer-independent [9]. In the writer-dependent signature veri cation system, a model has been trained with a genuine and forged signature for a particular writer. During inference, the model has to decide based on the similarity measure between the query signature and the genuine signature. In case, if veri cation is needed for a new writer, a separate model needs to get trained, which is the major drawback of writer-dependent signature veri cation. In comparison, the writer-independent signature veri cation method is a generic system and can be deployed for multiple writers. Tus, the writer-independent signature veri cation is more cost-e cient. In the o ine signature veri cation method, the feature representation is one of the most researched points by the researchers in the past [10]. For feature representation, many handcrafted features have been designed and e ectively used in the case of handwritten signature veri cation [9, 11 21], [5, 21 31], 71, but after the advent of deep convolutional neural networks (CNNs), the manual engineering for the features is no more needed. It can be learned by the neural network with the help of provided data [12, 32 36]. Te learned features rely on the training of CNNs to learn the representation of the signature image by minimizing the loss function during the training phase. Tese deep learning methods have achieved good performance, but still, they are facing some trivial issues in case of signature veri cation. An important issue in the training of deep neural net- works is the capability of discriminating two visually close signatures, especially in the case of skilled forgery. In the case of skilled forgery, two signatures holistically look similar but only su er from local deformation, which makes the two signatures dissimilar. It motivates us to devise a novel semi- synthetic approach to add local deformation on the signa- ture for generating the synthetic forged version of the original image. It helps to train the network, which works e ciently to handle the most di cult case of forgery in the case of signature veri cation. Another fundamental issue is the data-hungry deep learning approaches. Te deep learning methods need millions of images to get trained. Ideally, in the case of signature veri cation, a single genuine image should be present in the repository for veri cation with query signature image, but in most existing methods, a set of signatures has been taken from the user (original signer) to train the deep learning method. However, to get rid of this data need for signature veri cation, we have mixed the signature data with the handwritten data. We consider the handwritten word as a genuine signature by a writer and the same word by another writer as a forged sig- nature. A generic training has been conducted for the com- bined signature and word data. It helps to override the need for the vast amount of signature data for the training of the deep learning model for signature veri cation. Hence, the proposed system is writer-independent, and no separate model has been needed for a particular writer. Te rest of the study is organized into ve sections. Section 2 discusses the work related to the proposed method. In Section 3, the proposed approach is described in detail. In Section 4, the experimental setup has been described. Te results and analysis of the proposed work have been dis- cussed in Section 5. Te conclusions have been drawn in Section 6. 2. Related Work In document analysis research, biometric authentication is referred as the unique identi cation of a person. Tis au- thentication can be categorized based on the behavioural and psychological traits of a person. Another categorization is soft (signature, keystrokes, voice and handwriting, gait, etc.) biometrics and hard (facial expression, ngerprint, palm print-based geometry, etc.) biometrics [37]. Soft biometrics refers to features that change frequently depending on the situation. On the other hand, hard bio- metrics includes most of the features that remain permanent until the particular features meet any serious accident. 2 Computational Intelligence and Neuroscience Signature veri cation and analysis is an important soft biometric feature for person authentication, which can vary in o ine and online modes. From the psychological evi- dence, the signature habit of an individual is a motor plan encoded thought. Te moment of the motor plane at any xed moment of time produces a common trajectory. By considering the trajectory of signature as stable regions, Parziale et al. [38] presented a stability modulated based on dynamic time wrapping (SM-DTW) for dynamic signature veri cation and ensured that the dynamic signature veri - cation is more suitable to detect forgery. DTW is used to compare the string of two signatures with time. 2.1.Online SignatureVeri cation. Porwik et al. [39] used the swarm intelligence technique with the probabilistic neural network (PNN) for signature veri cation. Te dynamic feature of signature is similarity coe cients, which are se- lected during the Hotelling reduction process. PSO is helpful to achieve the similarity coe cients from dynamic features of signature. In the signature veri cation process, PNN is optimized by PSO, which is nicely tuned to the data statics of PNN classi er. Dynamic signature veri cation can closely represent the behavioural biometrics, which can be viewed in signing moments and speaking. For solving the problem of dynamic signature veri cation, Zhang [40] proposed the combination of population-based algorithms and fuzzy set theory. Te evaluation of the scheme is carried out with the ATVSSLTsignature veri cation database. Te research work by the authors is referred as a measure of globally changing features and later concluded that their scheme provides a satisfactory solution for the like dynamic signature veri - cation. Zalasinski et al. [41] also presented the dynamic signature veri cation based on selecting the most main partition. Te key features of dynamic signature may include the change in the pressure of holding the pen and speed at particular word from the initial to middle and middle to nal end of the signature. Te method is primarily focused on the partition of particular parts of the signatures. Terefore, the approach increases the precision of signature processing and adapts the speci c signature by removing redundant in- formation. Dynamic methods and fuzzy set theory are used for weighted part signatures, which is a novel contribution. 2.2. O ine Signature Veri cation. Zouari et al. [42] proposed the o ine signature veri cation on the basis of the algebraic geometry of the signature. Tey used partial order sets of the grids arranged in the form of lattice. Okawa [43] proposed a novel method by the fusion of the Fisher vector and KAZE features for o ine signature veri cation. KAZE features are better to provide background information and remove the noise. Te use of PCA with FV reduces the dimensionality issues and provides security by hiding the original signature. Sharif et al. [44] proposed the o ine signature veri cation using very basic methods of feature extraction and feature processing. Initially, from the signature images binary map is prepared, which is further divided into 16 sub-blocks. By applying GA, at the individual block of signature, the received features were classi ed with SVM. In [45], fuzzy similarity measure and symbolic representation techniques are used for the o ine signature veri cation. Inter-valued symbolic data are created from LBP features of signature images and bitmap images. In general, signature duplication methods can be considered as an initiative towards the improvements in automatic signature veri cation. Duplicate dynamic signature generation methods include several state-of-the-art methods such as kinematic model of motor system regarding neuroscience, nonlinear distortion, and a ne transformation [46]. Research on static signature duplication is limited to achieve the recent ad- vancements in human behaviour modelling. Diaz et al. [47] rstly proposed cognitive duplication of signature behaviour algorithm to develop an o ine duplicate signature generation system. During the signing process, spatial cognitive maps of human behaviour and motor system were generated with the help of linear and nonlinear transformations. Deep convolution neural networks have immensely justi ed its performance in image classi cation, natural language processing, and several social media analytics [48]. Te toughest challenge in o ine signature veri cation is the absence of dynamic features, which can be easily helpful to catch the skill forgery. Hafemann et al. [49] presented broad literature on the problem of o ine signature veri cation and concluded that handcrafted feature extraction methods are super shaded by deep learning. Tey further added better fusion of features, augmentation of datasets, and important analysis of ensemble learning and deep learning. For keeping good features that maintain the system performance, Hafemann et al. [49] proposed learning from signature images with writer-independent mode using CNN. In the experiments, the training sample and generalization samples are kept separate. Hafemann et al. [49] presented a xed-size representation scheme for o ine handwritten signature veri cation of di erent sizes. From evolution in deep learning, it is ensured that handcrafted features have been down-shaded by the features automatically extracted from the deeply stacked layers in neural network. By utilizing pyramidal pooling, Hafemann et al. [49] added xed-size input to network layers during varied range signatures from individual users. From the literature, it has been found that the dynamic signature veri cation is more e cient than o ine signature veri cation and a widely accepted person s authentication method, but the issues with dynamic signature veri cation are plenty of samples required to maintain the performance. For mitigating the issues, Daiz et al. [47] proposed signature ver- i cation with only single reference. Inspired from [47], in this work, we also introduced the method, which only needs a single reference image in the o ine signature veri cation method. 3. Proposed Work Te overall work ow of the proposed signature veri cation system is depicted in Figure 1. Te system has a pre- processing phase followed by an a ne alignment of given query signature images. After the a ne alignment of the query image with a reference image, local features are extracted from both images. Further, the features from the reference signature are matched with their neighbourhood Computational Intelligence and Neuroscience 3 feature in the query image and a similarity score is calcu- lated. Te signature veri cation decision is taken based on this similarity score. 3.1. Conceptual Background. Te basic building block of deep learning frameworks originated from the black-box architecture of deep neural network. A brief idea of the components used for developing the deep neural network model for biometric veri cation system is mandatory to present in the following subsections. 3.1.1. Convolution Neural Layer. Te deep convolutional neural network is multilayered neural network and is re- cently used in various challenging problems [50 52]. Te neurons of a convolution layer are connected to the local section of the input data. Te receptive eld of a neuron is the extent of its scope in input data, and it is increased by stacking the convolution layers. Te convolution operation is given as equation (1), where CBk is the kth convolution kernel weights and its bias term, respectively, and is expressing the convolution method. OpMapk InMapCKk   + CBk. (1) Te operation of convolution is constructed by one or more combination of such kernels. All convolution layers are followed by batch normalization layer and leaky ReLU as activation function in the proposed model. 3.1.2. Batch Normalization Layer. Te work [53, 54] revealed that deep neural networks training is complicated and has di erent hyperparameters. Generally, the compu- tational graph of a deep neural model has higher depth, leading to the convergence problem. Tere are some tech- niques [53 57] suggested to x this issue. Te batch nor- malization (BN) layer [56] is used in the proposed model for handling convergence problem and accelerates the net- work s training. In general, the BN layer is applied just before the activation layer (refer to [56] for details). 3.1.3. Activation Function Layer. Te activation functions in a neural network work as the transfer functions. Tese layers transform the results of the previous layer to map it with the given ground truth. Two kinds of activation functions are the linear activation function and the nonlinear activation function. In deep neural networks, di erent nonlinear functions are employed as the activation. Tese functions are generally introduced to maintain nonlinearity concept in the network. We have adopted various classes of di erent ac- tivation functions as described in the following subsections. (1) Leaky ReLU. It is a linear recti ed function, which is in short recall as ReLU. Te output of ReLU function is zero for negative input, and otherwise, input remains unchanged (refer to equation (2)). In back propagation [58], the model parameters are updated by nonnegative input values. Tis leads to the dying ReLU problem; therefore, the leaky ReLU activation function is applied in our network to address this issue. Here, the negative slope is not zeros but has a small value, which creates its derivative nonzeros for any input data ( 0.01 in our experiments). Te function corre- sponding to mathematical representation is given by equation (2), and its derivatives are given by equation (3). Te corresponding functions are also depicted in Figure 2. ReLU(z) max z, 0 { }, (2) f(z) z, z 0, z, z < 0. (3) (2) Hyperbolic Tangent Activation Function. It is a kind of logistic sigmoid activation function, which has the impor- tant interpretation of the biological neurons. Te main characteristic of hyperbolic tangent (tanh) function is having higher derivatives vanishing near zero. Tis is because the hyperbolic tangent function maintains its suitable property to learn the discriminative features from a higher class of varied data samples. Te range of the tanh function is in the range of [ 1, 1]. Te tanh function and its derivatives are Affine Alignment Local Feature Extraction Local Feature Alignment and Matching Verify NO YES Grey-Scale Conversion Intensity Normalization Preprocessing Figure 1: Overall work ow of proposed signature identi cation and veri cation (SIV) system. 4 Computational Intelligence and Neuroscience dispensed in Figure 2 and obtained by equations (4) and (5), respectively. Tis activation function incorporates the re- current network units (GRU and LSTM). tanhz ez e z ez + e z, (4) tanh z 1 tanh2z. (5) (3) Sigmoid Activation Function. Te property of sigmoid activation function yields its normalized score in the range of [0, 1] attheoutput scale.Te mathematicalexpressionof sigmoid function and its derivatives are explained in the gure below and calculated using equations (6) and (7), respectively. GRU and LTM unit present in recurrent network utilize the activation function for computing the corresponding activation values. (z) 1 1 + ez, (6) (z) 1 (z). (7) 3.1.4. MaxPool Layer. Te MaxPool layer [59] is used to increase the receptive eld of the network. Tis operation reduces (spatial dimensions) the size of the feature maps and decreases the computation cost. Te reduction is applied only to the height and width of input data. Te number of feature channels remains unchanged. It is similar to the sliding window approach with the selection of maximum element operation. Te reduction in the size depends on the stride of the sliding operation. Te proposed network utilizes a pooling size 2 2 and strides 2 2 for the pooling op- eration. Te pooling is a nonparametric layer; therefore, there are no parameters for learning. 3.2. Preprocessing. A preprocessing step is not a vital phase for a convolutional neural network-based system, but it can reduce the total training time and sometimes improve the performance of the system. Besides this, it is also instru- mental in representing the input data appropriately for the subsequent phases of the system. In this work, we are in- corporating greyscale conversion of colour images and their intensity normalization as prepossessing steps. After con- verting a colour image into a greyscale image, it is resized such that its smaller side becomes 80 pixels. Besides this, we rotated the images such that the smaller side of the image becomes its height. Finally, its intensities are normalized such that the background pixels on the image became black or near to black, and the foreground pixels (signature pixels) became white or near to white (refer to Figure 3). Here, we are not converting the signature image into black and white; instead, it is still grayscale, but the background is black as we are using it as the padding in other sections of the system. 3.3. A ne Alignment. To understand the importance of this phase, let us assume that we have two di erent sig- nature images of the same signer and try to nd out their di erences. Tere are two types of di erences between these images: (1) global di erence and (2) local di erence. Te global di erence is caused by the shift in the position of signature, size, and shape variance and the orientation of its principal axis, whereas the local di erence is caused by the deformation of each pixel in the form of its position displacement and colour intensity changes (refer to Figure 4). In this phase, the proposed system analyzes the global di erences by predicting the a ne transformation of query signature image with respect to reference signature image. To predict the a ne transformation of query image, the proposed system utilizes two trainable neural net- works: (1) CNN-1 : convolution neural network and (2) FFNN-1 : feed-forward neural network. Te overview of this phase is depicted in Figure 4 with the CNN-1 and FFNN-1 architecture. Here, rst of all the query and reference signature image are processed with CNN-1. Tis network produced 14 64- 0 0.5 1 1.5 2 2.5 3 -2 -1 0 1 2 3 -3 Leaky-ReLu Activation Function Derivative of Leaky-ReLu (a) -2 -1 0 1 2 3 -3 Tanh Activation Function Derivative of Tanh -1 -0.5 0 0.5 1 1.5 (b) -0.2 0 0.2 0.4 0.6 0.8 1 1.2 -2 -1 0 1 2 3 -3 Sigmoid Activation Function Derivative of Sigmoid (c) Figure 2: Di erent activation functions. Computational Intelligence and Neuroscience 5 dimensional vector for each image. Tese vectors (query image and reference image) are concatenated and passed to the FFNN-1. Te FFNN-1 yields di erent parameters of a ne transformation matrix. Te architectural and para- metric design detail of CNN-1 is given in Figure 4 and Table 1. Similarly, for FFNN-1 they are shown in Figure 4 and Table 2. Te training procedure of this a ne alignment network is explained in Subsection 3.4. 3.4.TrainingofA neAlignmentNetworkwithSemi-Synthetic Dataset. Te training of this network section is also a challenging task as we do not have labelled dataset having the a ne transformation variation with ground truth. Terefore, we decided to go for a semi-synthetic dataset. Original Image Grey Image Inverted Grey Image (a) Global Deformation Before Alignment Local Deformation Afer Alignment (b) Figure 3: Left gure (a) shows the preprocessing steps used in the proposed system. Figure (b) in the right side is exemplifying the global deformation in pair of target signature images. C1 1 C1 2 C2 1 C2 2 C2 3 C3 1 C3 2 C3 3 C4 1 CNN-1 Convolution Layer Batch Normalization Layer ReLU Activation Layer Max-Pooling Layer Spatial Pyramidal Pooling Layer D1 D2 D3 Affine Transformation Matrix Dense Layer ReLU Activation Layer Linear Activation Layer FFNN-1 CNN-1 Concat FFNN-1 Affine Transformation Matrix CNN-1 CNN: Convolution Neural Network FFNN: Feed Forward Neural Network Affine Alignment Reference Signature Query Signature Figure 4: A ne alignment of query signature image with reference signature image. Table 1: Architecture description of the CNN-1: convolutional neural network-1 section of the proposed system. Layer #Kernel Kernel size #Parameter Output size Input 0 0 0 H W 1 C1 1 32 5 5 832 H W 32 C2 1 32 3 3 9248 H W 32 C1 2 64 3 3 18496 H/2 W/2 64 C2 2 48 3 3 27696 H/2 W/2 48 C3 2 64 3 3 27712 H/2 W/2 64 C1 3 128 3 3 73856 H/4 W/4 128 C2 3 64 3 3 73792 H/4 W/4 64 C3 3 128 3 3 73856 H/4 W/4 128 C1 4 64 3 3 73792 H/8 W/8 128 Total number of parameters in CNN-1: 379280 6 Computational Intelligence and Neuroscience Here, we collected signature images from all the datasets under consideration (refer to subsection 4.1) and hand- written word image samples from various datasets such as IAM [60] and CVL [61]. Considering these image samples as reference image, we applied a random a ne transformation to generate query images. We utilized the random rotation with rotation angle rotation [ 45, 45], random shearing with shearing angle shearing [ 20, 20], and random scale Sx, Sy [0.5, 2.0] for random a ne transformation (all the transformations are with respect to the center of the image). In this way, we have collected a pair of reference and query image with their corresponding transformation parameters. Utilizing this information, we have trained the a ne alignment network. A a ne transformation matrix is de ned by equation (8). In our case, it is the combination of di erent elementary transformations such as translation, scale, shear, and rota- tion. Te transformation matrix corresponding to these elementary transformations is given by equation (9). AffineTransformation a1 b1 c1 a2 b2 c2 0 0 1 , (8) Translation 1 0 tx 0 1 ty 0 0 1 , Scale sx 0 0 0 sy 0 0 0 1 , Shear 1 srx 0 sry 1 0 0 0 1 , Rotation cos sin 0 sin cos 0 0 0 1 . (9) 3.5. Local Feature Extraction and Matching. Once the query image and reference image are aligned by transforming the reference image as a ne transformation parameter (or transforming the query image as inverse a ne transfor- mation), we acquire the local features in both signature images. Te local features are acquired by processing these images from the CNN-2. Tis network is a convolutional neural network, its architecture is depicted in Figure 5, and layered description is given in Table 3. 3.6. Local Feature Matching. Tis phase is responsible to handle the local di erences in query image and reference image. Te feature map (output of a CNN) generated by CNN-2 represents the neighbourhood region of size 44 44 pixels of a cell size 4 4 pixels. Tis representation is a 64- dimensional vector for each cell in feature map. Although the a ne alignment phase already tackles major alignment issues, the pixel displacement can cause the local mis- alignment. Terefore, we calculate the Euclidean distance of a cell region in reference image with its 9 corresponding neighbours (3 3 window proximity) in the query image. Te neighbouring cell in query image having the lowest distance is selected as the match for the corresponding cell in reference image. 3.7. Signature Veri cation Decision. Tis is the nal step in the proposed signature veri cation system. Here, the matching distance of a cell in reference images is used in making a decision. It is possible that a genuine signature has some portion of signature extra or lesser with respect to reference signature (generally length of underline). So, here we need two levels of decision. First, we calculate the ratio (we call it DMR: distance matching ratio) of number of cells that have lesser matching distance than a prede ned threshold (ThMD) with respect to number of cells that have it higher. We can further analyze a signature if it has DMR higher than a prede ned threshold (ThDMR). Te selection of ThDMR depends upon the extent of extra signature that is allowed. In the proposed work, we have selected it as 4 (80% of total cell should be lower than ThMD). If a query signature gets DMR lesser than ThDMR (in our case 4), then we simply discard the query signature. If the query signature passes the ThDMR, then we calculate its similarity score with respect to reference signature. Te similarity score is the mean of matching distance of all cell regions, which has matching distance lesser than ThMD. 4. Experimental Setup 4.1. Datasets. MYCT this is o ine signature veri cation dataset consisting of 75 writers. Te name of the dataset is referenced from the project on science and technology under the Ministry of Spanish (Ministerio de Ciencia y Tecnolog a) [62]. Te dataset was prepared from 15 simple signatures and 15 simulated signatures along with corre- sponding gure prints. Te resolution of all images of sig- natures was maintained at 600 dpi. Te dataset is useful to develop the biometric algorithms in several secured Table 2: Architecture description of the FFNN-1: feed-forward neural network-1 section of the proposed system. Layer #Neurons #Parameter Output size Input 0 0 128 D1 t 64 8256 64 D2 t 128 8320 128 D3 t 64 8256 64 D4 t 2 130 2 D1 r 64 8256 64 D2 r 128 8320 128 D3 r 64 8256 64 D4 r 2 130 2 D1 s 64 8256 64 D2 s 128 8320 128 D3 s 64 8256 64 D4 s 2 130 2 Total number of parameters in FFNN-1: 74886 Computational Intelligence and Neuroscience 7 domains. GPDS this is an o ine signature veri cation database developed in signal processing laboratory (Grupo de Procesado Digital de la se al) GPDS at University of Las Palmas de Gran Canaria, Spain [63]. GPDS consists of 24 genuine signatures and 30 forgery signatures from each of 960 individuals. Te signatures are black and white format with 300 dpi. During the collection of samples, two di erent sizes of boxes are chosen, one is 5 cm by 1.8 cm and another is 4.5 cm by 2.5 cm. CEDAR this is an online handwritten text database consisting of the samples of handwritten text of tablet and line of text collected from 200 writers [64]. CEDAR signature recognition dataset was developed at Bu alo University. Te dataset consists of 24 samples for CNN-2 Alignment & Similarity Score Similarity Score CNN-2 Local Features Extraction and Matching Reference Signature Query Signature Feature maps C1 1 C1 2 C2 1 C2 2 C2 3 C3 1 C3 2 C3 3 CNN-2 Convolution Layer Batch Normalization Layer ReLU Activation Layer Max-Pooling Layer Feature map Alignment & Similarity Score Reference Signature s Feature Map Query Signature s Feature Map Extended boundary by Reflection Search for closest feature match in Neighbourhood Similarity Score Figure 5: Local feature extraction and matching. Table 3: Architecture description of the CNN-2 : convolutional neural network-1 section of the proposed system. Layer #Kernel Kernel size #Parameter Output size Input 0 0 0 H W 1 C1 1 32 3 3 320 H W 32 C2 1 32 3 3 9248 H W 32 C1 2 32 3 3 18496 H/2 W/2 32 C2 2 48 3 3 27696 H/2 W/2 48 C3 2 32 3 3 27712 H/2 W/2 32 C1 3 64 3 3 73856 H/4 W/4 64 C2 3 48 3 3 73792 H/4 W/4 48 C3 3 64 3 3 36928 H/4 W/4 64 Total number of parameters in CNN-2: 268048 Table 4: Comparison of the proposed system with other (including current state-of-the-art) methods with MCYT-75 dataset. Author Veri cation type No. of training sample FRR FAR AER [4] WD 5G 32.4 26.84 10G 22.93 22.04 [65] 6.67 12.44 9.56 [66] WD 5G 23.25 4.53 10G 12.61 7.53 [42] WD 5G 4.48 25.19 5.62 10G 4.96 17.21 3.45 [67] WD 10G 12.53 13.16 5G 15.47 13.42 [44] WD 5G 6.67 6.67 6.67 10G 6.25 5.67 5.96 12G 3.67 6.67 5.0 Ours WI 5G 4.12 4.48 4.30 10G 3.68 3.96 3.82 12G 3.49 3.53 3.51 Table 5: Comparison of the proposed system with other (including current state-of-the-art) methods with CEDAR dataset. Author Veri cation type No. of training sample FRR FAR AER [68] WI 24G 8.33 [69] WD 16G 6.36 5.68 6.02 [33] WD 12G 9.36 7.84 8.60 [42] WD 5G 4.44 15.91 3.64 10G 5.83 11.52 2.74 [70] WI 4G 8.70 8G 7.41 8.25 7.83 12G 5.60 [44] WD 5G 12.5 8.33 10.41 10G 8.33 4.17 6.25 12G 4.67 4.67 4.67 Ours WI 5G 4.32 7.84 6.08 10G 5.72 4.12 5.92 12G 3.97 4.33 4.15 8 Computational Intelligence and Neuroscience each genuine and simulated signature from 55 enrolled forgeries. Te simulated signatures include both simple and forgeries. Te dataset is very large as it contains 105,573 numbers of words. 4.2. Evaluation Criteria. Te results obtained from the proposed work are compared with current state-of-the-art methods on di erent standard datasets and with di erent evaluation criteria. We have tested the performance of the system through writer-independent signature veri cation task considering all reference signatures as a separate entity. We are listing the performance of the proposed system with three evaluation measures such as (1) FRR, (2) FAR, and (3) AER. 4.2.1. FRR. It stands for false rejection rate, a very important evaluation parameter in the biometric system to measure the likelihood that the biometric-based security system incor- rectly rejects the access attempt made by the authentic user of the system. Mathematically (equation (10)), FRR is cal- culated as a ratio of the total counts of false rejections and total identi cation attempts. FRR FN TP + FN. (10) 4.2.2. FAR. False acceptance rate or FAR is also a likelihood measure to determine that the biometric system incorrectly accepts the access attempt by the unauthentic user. In terms of mathematical formula, FAR (equation (11)) of a biometric system is the ratio of total counts of false acceptances and total number of identi cation attempts. FAR FP FP + TN. (11) 4.2.3. AER. Te average error rate or AER is termed as the best threshold value at which the curve of FAR and FRR meets at a point. It generally determines the stability of the system. It is mathematically computed as an average of FRR and FAR as follows: AER FAR + FRR 2 . (12) 5. Results and Analysis Te proposed system has been extensively validated on the three public datasets of signature veri cation, namely MCYT-75, CEDAR, and GPDS. Te proposed method is also compared with other state-of-the-art methods. Te evaluation results for the MCYT-75 dataset are summarized in Table 4. From Table 4, it has been observed that for the 5G and 12G training samples, our proposed method has re- ported the least average error rate. Te proposed system has achieved the least false acceptance rate (FAR) and false reject rate for 5G, 10G, and 12G training samples. Tis shows the proposed approach s robustness compared with other state- of-the-art methods for the MCYT-75 dataset. For the CEDAR dataset, the quantitative results, along with the state-of-the-art approaches, are mentioned in Ta- ble 5. From Table 5, it has been found that for independent writer setting, our method is best performing as compared to the other 12G training samples. Even the proposed method has achieved the least average error rate for 12G compared with all methods (writer-independent and writer-depen- dent). Te proposed system also achieves least false rejection rate and false acceptance rate for all settings of training samples. Figure 6 presents the average error rate (AER) for di erent samples taken from all three mentioned datasets. It also presents the comparative results against the mentioned state of the art. Another set of comparisons is shown in Figure 7 against the di erent training samples of indepen- dent and dependent writers with their rate of performance. 0 1 2 3 4 5 6 7 10G 12G 5G MCYT75 WD [61] MCYT75 WD [49] MCYT75 WI [Ours] (a) 0 2 4 6 8 10 12 10G 12G 5G CEDAR WD [6] CEDAR WD [61] CEDAR WI [17] CEDAR WD [49] CEDAR WI [Ours] (b) 0 1 2 3 4 5 6 7 8 9 10 10G 12G G16 5G GPDS WD [49] GPDS WI [Ours] (c) Figure 6: Comparison of AER on MYCT-75 (a) CEDAR (b) and GPDS (c) datasets along with di erent sizes of data samples. Computational Intelligence and Neuroscience 9 Te impact of the proposed method for the GPDS synthetic dataset is summarized in Table 6. Te proposed method has achieved the best results on the AER metric for all training sample settings. Te proposed method has also outperformed [44] on the metric of false rejection rate. From Tables 4 6, it is observed that the robustness of the proposed approach is compared with other existing approaches and has been validated with satisfactory measures. 6. Conclusion Generally, signatures are composed of multiple components, and most of them do not provide the necessary information. For example, the date and curved line used below the sig- nature must be ignored since it does not add any infor- mation for writer identi cation. Alternatively, this may help to remove the processing overheads. Interpersonal similarity and high intrapersonal variability are the challenging factors for achieving satisfactory performance to generalize o ine signature veri cation. Tis may be supposed to extract the most discriminant and stable feature sets from the wide variety of geographical-invariant signers. In this study, we presented a practical veri cation problem against the forgeries. In the context of feature extraction for writer- independent signature veri cation, the line-up future di- rections may be planned to fuse nonhandcrafted features. In the case of adversarial machine learning in the security domain, an interesting future direction can be added to analyze the impact of sharp physical attacks by printing the adversarial noise over the signatures. According to the writer s perspective, another future line can be encouraged to develop a better deep network than the Siamese network and the loss functions to introduce versatile reference sig- natures. Signature localization is also an important domain that can assist in signature veri cation in an image. Data Availability Te data used to support the study are cited within the article and are publicly available. Conflicts of Interest Te authors declared no con icts of interest. Acknowledgments Tis research was supported by Princess Nourah Bint Abdulrahman University Researchers Supporting Project, number (PNURSP2022R195), Princess Nourah Bint Abdulrahman University, Riyadh, Saudi Arabia. References [1] N. Kumar and A. Sharma, A spoo ng security approach for facial biometric data authentication in unconstraint envi- ronment, in Progress in Advanced Computing and Intelligent Engineering, pp. 437 448, Springer, Singapore, 2019. [2] J.-C. Lee, A novel biometric system based on palm vein image, Pattern Recognition Letters, vol. 33, no. 12, pp. 1520 1528, 2012. [3] M. Sharif, S. Anis, M. Raza, and S. Mohsin, Enhanced svd based face recognition, Journal of Applied Computer Science Methods, vol. 6, no. 12, 2012. [4] M. Sharif, S. Mohsin, M. Y. Javed, and M. A. Ali, Single image face recognition using laplacian of Gaussian and discrete cosine transforms, Te International Arab Journal of Infor- mation Technology, vol. 9, no. 6, pp. 562 570, 2012. [5] D. Rivard, E. Granger, and R. Sabourin, Multi-feature ex- traction and selection in writer-independent o -line signature veri cation, International Journal on Document Analysis and Recognition, vol. 16, no. 1, pp. 83 103, 2013. [6] E. Parcham, M. Ilbeygi, and M. Amini, Cbcapsnet: A novel writer-independent o ine signature veri cation model using a cnn-based architecture and capsule neural networks, Expert Systems with Applications, vol. 185, Article ID 115649, 2021. [7] R. Plamondon and S. N. Srihari, Online and o -line handwriting recognition: a comprehensive survey, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 22, no. 1, pp. 63 84, 2000. [8] D. Impedovo and G. Pirlo, Automatic signature veri cation: Te state of the art, IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews), vol. 38, no. 5, pp. 609 635, 2008. 10G 5G 10G 12G 16G 5G 10G 12G 16G WD WD WI Traning Samples 0 5 10 15 20 25 30 35 40 Rate of Performace AER FAR FPR Figure 7: Overall comparative performance in terms of FRR, FAR, and AER on MYCT 75, GPDS, and CEDAR datasets. Table 6: Comparison of the proposed system with other (including current state-of-the-art) methods with GPDS synthetic dataset. Author Veri cation type No. of training sample FRR FAR AER [67] WD 10G 5.80 29.49 [44] WD 5G 8.33 10 9.16 10G 12.5 3.33 7.92 12G 6.67 4.16 5.42 16G 4.16 3.33 3.75 Ours WI 5G 6.12 7.68 6.90 10G 5.96 6.32 6.14 12G 4.84 4.46 4.65 16G 3.65 3.47 3.56 10 Computational Intelligence and Neuroscience [9] D. Bertolini, L. S. Oliveira, E. Justino, and R. Sabourin, Reducing forgeries in writer-independent o -line signature veri cation through ensemble of classi ers, Pattern Recog- nition, vol. 43, no. 1, pp. 387 396, 2010. [10] L. G. Hafemann, R. Sabourin, and L. S. Oliveira, O ine handwritten signature veri cation - Literature review, in Proceedings of the 2017 Seventh International Conference on Image Processing Teory, Tools and Applications (IPTA), pp. 1 8, IEEE, Montreal, QC, Canada, December 2017. [11] H. Baltzakis and N. Papamarkos, A new signature veri ca- tion technique based on a two-stage neural network classi er, Engineering Applications of Arti cial Intelligence, vol.14, no.1, pp. 95 103, 2001. [12] J. Coetzer, O -line signature veri cation, PhD Tesis, University of Stellenbosch, Stellenbosch, 2005. [13] P. S. Deng, H.-Y. M. Liao, C. W. Ho, and H.-R. Tyan, Wavelet-based o -line handwritten signature veri cation, Computer Vision and Image Understanding, vol. 76, no. 3, pp. 173 190, 1999. [14] J.-P. Drouhard, R. Sabourin, and M. Godbout, A neural network approach to o -line signature veri cation using directional pdf, Pattern Recognition, vol. 29, no. 3, pp. 415 424, 1996. [15] A. El-Yacoubi, E. J. R. Justino, R. Sabourin, and F. Bortolozzi, O -line signature veri cation using hmms and cross-vali- dation, in Proceedings of the Neural Networks for Signal Processing X. Proceedings of the 2000 IEEE Signal Processing Society Workshop (Cat. No. 00TH8501), pp. 859 868, IEEE, Sydney, NSW, Australia, December 2000. [16] G. S. Eskander, R. Sabourin, and E. Granger, Hybrid writer- independent-writer-dependent o ine signature veri cation system, IET Biometrics, vol. 2, no. 4, pp. 169 181, 2013. [17] J. Hu and Y. Chen, O ine signature veri cation using real adaboost classi er combination of pseudo-dynamic features, in Proceedings of the 2013 12th International Conference on Document Analysis and Recognition, pp. 1345 1349, IEEE, Washington DC, August 2013. [18] E. J. Justino, A. El Yacoubi, F. Bortolozzi, and R. Sabourin, An o -line signature veri cation system using hmm and graphometric features, in Proceedings of the 4th International Workshop on Document Analysis Systems, pp. 211 222, Citeseer, France, 2000. [19] M. I. Malik, M. Liwicki, A. Dengel, S. Uchida, and V. Frinken, Automatic signature stability analysis and veri cation using local features, in Proceedings of the 2014 14th International Conference on Frontiers in Handwriting Recognition, pp. 621 626, IEEE, Hersonissos, Greece, September 2014. [20] W. F. Nemcek and W. C. Lin, Experimental investigation of automatic signature veri cation, IEEE Transactions on Sys- tems, Man, and Cybernetics, vol. SMC-4, no. 1, pp. 121 126, 1974. [21] L. S. Oliveira, E. Justino, C. Freitas, and R. Sabourin, Te graphology applied to signature veri cation, in Proceedings of the 12th Conference of the International Graphonomics Society, pp. 286 290, Italy, 2005. [22] M. R. Pourshahabi, M. H. Sigari, and H. R. Pourreza, O ine handwritten signature identi cation and veri cation using contourlet transform, in Proceedings of the 2009 Interna- tional Conference of Soft Computing and Pattern Recognition, pp. 670 673, IEEE, Malacca, Malaysia, December 2009. [23] R. Sabourin and J.-P. Drouhard, O -line signature veri - cation using directional pdf and neural networks, in Pro- ceedings of the 11th IAPR International Conference on Pattern Recognition. Vol. II. Conference B: Pattern Recognition Methodology and Systems, pp. 321 325, IEEE, Te Hague, Netherlands, September 1992. [24] R. Sabourin and G. Genest, An extended-shadow-code based approach for o -line signature veri cation. i. evaluation of the bar mask de nition, in Proceedings of the 12th IAPR Inter- national Conference on Pattern Recognition, Vol. 3-Conference C: Signal Processing (Cat. No. 94CH3440-5), pp. 450 453, IEEE, Jerusalem, Israel, October 1994. [25] Y. Serdouk, H. Nemmour, and Y. Chibani, Combination of oc-lbp and longest run features for o -line signature veri - cation, in Proceedings of the 2014 Tenth International Con- ference on Signal-Image Technology and Internet-Based Systems, pp. 84 88, IEEE, Marrakech, Morocco, November 2014. [26] J. Ruiz-del-Solar, C. Devia, P. Loncomilla, and F. Concha, O ine signature veri cation using local interest points and descriptors, in Iberoamerican Congress on Pattern Recogni- tion, pp. 22 29, Springer, Berlin, Heidelberg, 2008. [27] J. F. Vargas, C. M. Travieso, J. B. Alonso, and M. A. Ferrer, O -line signature veri cation based on gray level informa- tion using wavelet transform and texture features, in Pro- ceedings of the 2010 12th International Conference on Frontiers in Handwriting Recognition, pp. 587 592, IEEE, Kolkata, India, November 2010. [28] J. F. Vargas, M. A. Ferrer, C. M. Travieso, and J. B. Alonso, O -line signature veri cation based on grey level informa- tion using texture features, Pattern Recognition, vol. 44, no. 2, pp. 375 385, 2011. [29] M. B. Yilmaz, B. Yanikoglu, C. Tirkaz, and A. Kholmatov, O ine signature veri cation using classi er combination of hog and lbp features, in Proceedings of the 2011 International Joint Conference on Biometrics (IJCB), pp. 1 7, IEEE, Washington, DC, USA, October 2011. [30] M. Zalasi nski and K. Cpa ka, A new method for signature veri cation based on selection of the most important parti- tions of the dynamic signature, Neurocomputing, vol. 289, pp. 13 22, 2018. [31] E. N. Zois, L. Alewijnse, and G. Economou, O ine signature veri cation and quality characterization using poset-oriented grid features, Pattern Recognition, vol. 54, pp. 162 177, 2016. [32] S. Basheer, S. Bhatia, and S. B. Sakri, Computational mod- eling of dementia prediction using deep neural network: Analysis on oasis dataset, IEEE Access, vol. 9, pp. 42449 42462, 2021. [33] R. K. Bharathi and B. H. Shekar, O -line signature veri - cation based on chain code histogram and support vector machine, in Proceedings of the 2013 International Conference on Advances in Computing, Communications and Informatics (ICACCI), pp. 2063 2068, IEEE, Mysore, India, August 2013. [34] A. S. Britto Jr, R. Sabourin, and L. E. S. Oliveira, Dynamic selection of classi ers-A comprehensive review, Pattern Recognition, vol. 47, no. 11, pp. 3665 3680, 2014. [35] S. Siyuan Chen and S. Srihari, A new o -line signature veri cation method based on graph, in Proceedings of the 18th International Conference on Pattern Recognition (ICPR 06), pp. 869 872, IEEE, Hong Kong, August 2006. [36] N. Kumar, Human activity recognition from histogram of spatiotemporal depth features, International Journal of Computational Intelligence Studies, vol. 8, no. 4, pp. 309 329, 2019. [37] S. Barzut, M. Milosavljevi c, S. Adamovi c, M. Sara cevi c, N. Ma cek, and M. Gnjatovi c, A novel ngerprint biometric cryptosystem based on convolutional neural networks, Mathematics, vol. 9, no. 7, p. 730, 2021. Computational Intelligence and Neuroscience 11 [38] A. Parziale, M. Diaz, M. A. Ferrer, and A. Marcelli, Sm-dtw: Stability modulated dynamic time warping for signature veri cation, Pattern Recognition Letters, vol. 121, pp. 113 122, 2019. [39] P. Porwik, R. Doroz, and T. Orczyk, Signatures veri cation based on pnn classi er optimised by pso algorithm, Pattern Recognition, vol. 60, pp. 998 1014, 2016. [40] B. Zhang, O -line signature veri cation and identi cation by pyramid histogram of oriented gradients, International Journal of Intelligent Computing and Cybernetics, vol. 3, no. 4, pp. 611 630, 2010. [41] M. Zalasi nski, K. apa, and K. Cpa ka, Prediction of values of the dynamic signature features, Expert Systems with Appli- cations, vol. 104, pp. 86 96, 2018. [42] R. Zouari, R. Mokni, and M. Kherallah, Identi cation and veri cation system of o ine handwritten signature using fractal approach, in Proceedings of the International Image Processing, Applications and Systems Conference, pp. 1 4, IEEE, Sfax, Tunisia, November 2014. [43] M. Okawa, Synergy of foreground-background images for feature extraction: O ine signature veri cation using Fisher vector with fused KAZE features, Pattern Recognition, vol. 79, pp. 480 489, 2018. [44] M. Sharif, M. A. Khan, M. Faisal, M. Yasmin, and S. L. Fernandes, A framework for o ine signature veri - cation system: Best features selection approach, Pattern Recognition Letters, vol. 139, pp. 50 59, 2018. [45] A. Alaei, S. Pal, U. Pal, and M. Blumenstein, An e cient signature veri cation method based on an interval symbolic representation and a fuzzy similarity measure, IEEE Trans- actions on Information Forensics and Security, vol. 12, no. 10, pp. 2360 2372, 2017. [46] P. Bhowal, D. Banerjee, S. Malakar, and R. Sarkar, A two-tier ensemble approach for writer dependent online signature veri cation, Journal of Ambient Intelligence and Humanized Computing, vol. 13, no. 1, pp. 21 40, 2022. [47] M. Diaz, A. Fischer, M. A. Ferrer, and R. Plamondon, Dy- namic signature veri cation system based on one real sig- nature, IEEE Transactions on Cybernetics, vol. 48, no. 1, pp. 228 239, 2016. [48] K. Chakraborty, S. Bhatia, S. Bhattacharyya, J. Platos, R. Bag, and A. E. Hassanien, Sentiment Analysis of COVID-19 tweets by Deep Learning Classi ers-A study to show how popularity is a ecting accuracy in social media, Applied Soft Computing, vol. 97, Article ID 106754, 2020. [49] L. G. Hafemann, R. Sabourin, and L. S. Oliveira, Learning features for o ine handwritten signature veri cation using deep convolutional neural networks, Pattern Recognition, vol. 70, pp. 163 176, 2017. [50] K. Dev, S. A. Khowaja, A. S. Bist, V. Saini, and S. Bhatia, Triage of potential covid-19 patients from chest x-ray images using hierarchical convolutional networks, Neural Com- puting & Applications, vol. 25, pp. 1 16, 2021. [51] Y. LeCun, L. Bottou, Y. Bengio, and P. Ha ner, Gradient- based learning applied to document recognition, Proceedings of the IEEE, vol. 86, no. 11, pp. 2278 2324, 1998. [52] Y. LeCun, K. Kavukcuoglu, and C. Farabet, Convolutional networks and applications in vision, in Proceedings of the 2010 IEEE International Symposium on Circuits and Systems, pp. 253 256, Paris, France, June 2010. [53] X. Glorot and Y. Bengio, Understanding the di culty of training deep feedforward neural networks, in Proceedings of the thirteenth international conference on arti cial intelligence and statistics, pp. 249 256, Sardinia, Italy, 2010. [54] K. He, X. Zhang, S. Ren, and J. Sun, Delving deep into recti ers: surpassing human-level performance on imagenet classi cation, in Proceedings of the IEEE international con- ference on computer vision, pp. 1026 1034, Santiago, Chile, December 2015. [55] I. Goodfellow, J. Pouget-Abadie, M. Mirza et al., Generative adversarial nets, in Advances in Neural Information Pro- cessing Systems, pp. 2672 2680, 2014. [56] S. Io e and C. Szegedy, Batch normalization: accelerating deep network training by reducing internal covariate shift, arXiv:1502.03167, 2015. [57] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, Dropout: a simple way to prevent neural networks from over tting, Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929 1958, Washington, D.C, USA, 2014. [58] R. Hecht-Nielsen, Teory of the Backpropagation Neural Network, in Proceedings of the International Joint Conference on Neural Networks, pp. 593 611, IEEE, June 1989. [59] A. Krizhevsky, I. Sutskever, and G. E. Hinton, Imagenet clas- si cation with deep convolutional neural networks, Advances in Neural Information Processing Systems, vol. 25, 2012. [60] U.-V. Marti and H. Bunke, Te iam-database: an english sentence database for o ine handwriting recognition, In- ternational Journal on Document Analysis and Recognition, vol. 5, no. 1, pp. 39 46, 2002. [61] F. Kleber, S. Fiel, M. Diem, and R. Sablatnig, Cvl-database: An o -line database for writer retrieval, writer identi cation and word spotting, in Proceedings of the Document Analysis and Recognition (ICDAR), pp. 560 564, IEEE, Washington, DC, USA, August 2013. [62] J. Ortega-Garcia, J. Fierrez-Aguilar, D. Simon et al., Mcyt baseline corpus: a bimodal biometric database, IEE Pro- ceedings - Vision, Image and Signal Processing, vol. 150, no. 6, pp. 395 401, 2003. [63] F. Vargas, M. Ferrer, C. Travieso, and J. Alonso, O -line handwritten signature gpds-960 corpus, in Proceedings of the Ninth International Conference on Document Analysis and Recognition (ICDAR 2007), pp. 764 768, IEEE, Curitiba, Brazil, September 2007. [64] S. Dey, A. Dutta, J. I. Toledo, S. K. Ghosh, J. Llad os, and U. Pal, Signet: Convolutional siamese network for writer indepen- dent o ine signature veri cation, 2017, http://arXiv: 170702131. [65] A. N. Azmi, D. Nasien, and F. S. Omar, Biometric signature veri cation system based on freeman chain code and k-nearest neighbor, Multimedia Tools and Applications, vol. 76, no. 14, pp. 15341 15355, 2017. [66] M. B. Y lmaz and B. Yan ko glu, Score level fusion of clas- si ers in o -line signature veri cation, Information Fusion, vol. 32, pp. 109 119, 2016. [67] A. Soleimani, B. N. Araabi, and K. Fouladi, Deep Multitask Metric Learning for O ine Signature Veri cation, Pattern Recognition Letters, vol. 80, pp. 84 90, 2016. [68] R. Kumar, J. D. Sharma, and B. Chanda, Writer-independent o -line signature veri cation using surroundedness feature, Pattern Recognition Letters, vol. 33, no. 3, pp. 301 308, 2012. [69] M. Manoj Kumar and N. B. Puhan, O -line signature ver- i cation: upper and lower envelope shape analysis using chord moments, IET Biometrics, vol. 3, no. 4, pp. 347 354, 2014. [70] Y. Guerbai, Y. Chibani, and B. Hadjadji, Te e ective use of the one-class svm classi er for handwritten signature veri - cation based on writer-independent parameters, Pattern Recognition, vol. 48, no. 1, pp. 103 113, 2015. 12 Computational Intelligence and Neuroscience