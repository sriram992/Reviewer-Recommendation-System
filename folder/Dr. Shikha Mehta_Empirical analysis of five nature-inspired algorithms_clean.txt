See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/314234026 Empirical analysis of ve nature-inspired algorithms on real parameter optimization problems Article in Artificial Intelligence Review October 2018 DOI: 10.1007/s10462-017-9547-5 CITATIONS 19 READS 290 2 authors: Parul Agarwal Jaypee Institute of Information Technology 24 PUBLICATIONS 287 CITATIONS SEE PROFILE Shikha Mehta Jaypee Institute of Information Technology 47 PUBLICATIONS 561 CITATIONS SEE PROFILE All content following this page was uploaded by Parul Agarwal on 06 February 2018. The user has requested enhancement of the downloaded file. Artif Intell Rev DOI 10.1007/s10462-017-9547-5 Empirical analysis of ve nature-inspired algorithms on real parameter optimization problems Parul Agarwal1 Shikha Mehta1 Springer Science+Business Media Dordrecht 2017 Abstract In the past few years nature-inspired algorithms are seen as potential tools to solve computationally hard problems. Tremendous success of these algorithms in providing near optimal solutions has inspired the researchers to develop new algorithms. However, very limited efforts have been made to identify the best algorithms for diverse classes of problems. This work attempts to assess the ef cacy of ve contemporary nature-inspired algorithms i.e. bat algorithm (BA), arti cial bee colony algorithm (ABC), cuckoo search algorithm (CS), re y algorithm (FA) and ower pollination algorithm (FPA). The work evaluates the performance of these algorithms on CEC2014 30 benchmark functions which include unimodal, multimodal, hybrid and composite problems over 10, 30, 50 and 100 dimensions. Control parameters of all algorithms are self-adapted so as to obtain best results over benchmark functions. The algorithms have been evaluated along three perspectives (a) statistical signi cance using Wilcoxon rank sum test (b) computational time complexity (c) convergence rate of algorithms. Experimental results and analysis revealed that ABC algorithm perform best for majority of the problems on high dimension, while on small dimension, CS is the best choice. FPA attain the next best position follow by BA and FA for all kinds of functions. Self adaptation of above algorithms also revealed the best values of input parameters for various algorithms. This study may aid experts and scientists of computational intelligence to solve intricate optimization problems. Keywords Arti cial bee colony algorithm Computational time complexity Wilcoxon rank sum test Convergence rate 1 Introduction Nature-inspired algorithm is an emerging eld of research since last two decades. These algo- rithms simulate the collective behavior of natural swarms such as echolocation behavior of B Parul Agarwal parul.agarwal@jiit.ac.in 1 Department of Computer Science and Information Technology, Jaypee Institute of Information Technology, Sector-62, Noida 201301, India 123 P. Agarwal, S. Mehta bats, ashing behavior of re ies, foraging behavior of honey bees, etc to solve intricate prob- lems of various domains. Swarm intelligence based algorithms, a subset of nature-inspired algorithms, works on multiple agents (swarms) that are self controlled and work in unison to achieve the desired outcome. These algorithms are classi ed based on their natural source of inspirations such as physical based, chemistry based, bio-inspired etc. (Fister et al. 2013a). Most of the real world optimization problems, applied in engineering disciplines, are NP- hard, that is, the solution for these problems does not exists in polynomial time. Additionally, these optimization problems are too hard to model mathematically. Nature-inspired algo- rithms provide near optimal solutions to such problems using metaheuristic techniques for optimizing complex functions. Due to the enhanced capabilities of nature-inspired algorithms in providing potential solution to hard problems, researchers have developed many new algorithms in this eld such as bat algorithm (BA), cuckoo search algorithm (CS), re y algorithm (FA), ower pollination algorithm (FPA) etc. Various studies are performed to evaluate the performance of these algorithms. Table 1 depicts the concise literature study of ve nature-inspired algorithms. Pham and Castellani (2014) examined four nature-inspired algorithms i.e. the bees algorithm (BEA), evolutionary algorithms (EA), particle swarm optimization (PSO) and the arti cial bee colony algorithm (ABC) on the basis of their evolutionary operators and learning parameters. These algorithms were tested on 25 benchmark continuous optimization problems based on their accuracy and speed for small dimensions only. Khaze et al. (2013) discussed two meta- heuristic algorithms namely: ABC and FA for optimizing continuous problems. The work however compared the ef ciency of just two algorithms on two and three dimension bench- mark problems only. This comparison is made on the basis of their accuracy and reliability of convergence. Hashmi et al. (2013) presented the comparative study of three swarm intelligence based algorithms i.e. FA, CS and BA on ve benchmark functions for population size 10, 20 and 40. Dimension size was kept constant i.e.10 throughout the experiment for evaluating their accuracy and convergence speed. Civicioglu and Besdok (2011) statistically analyzed the performance of four nature-inspired algorithms i.e. CS, PSO, Differential Evolution (DE) and ABC on 50 different benchmark problems over various dimensions. The ef ciency of algorithms was studied on the basis of success rate and time complexity. However, the set of benchmark functions did not include various standard and hybrid (Suganthan et al. 2005) functions. Yang (2010a) assessed the performance of BA with respect to Genetic algorithm (GA) and PSO on only 10 benchmark functions for 16, 128 and 256 dimensions. Yang (2009) introduced another new algorithm, re y algorithm (FA), based on ashing behavior of re- ies aiming to optimize multimodal functions. The performance of FA was evaluated only on 10 benchmark functions on the basis of success rate and ef ciency at 16, 128 and 256 dimen- sions. Moreover, author evaluated their algorithm with respect to GA and PSO but not with their own previous algorithm i.e. BA. Yang and Deb (2009) projected a new metaheuristic CS, motivated by brood parasitic behavior of cuckoo birds. The ef ciency of CS was compared with PSO and GA on 10 benchmark functions only. Yang proposed three nature-inspired algorithms but did not evaluate their performances with each other. Moreover, above men- tioned algorithms were not evaluated on hybrid functions and over large dimensions. Singh and Singh (2014) optimized and compared three swarm intelligence based algorithm on 15 mathematical test functions on the basis of processing time and optimized tness functions. Additionally, few other studies were also introduced. Yang (2014) analyzed the ef ciency of two algorithms i.e. FA and CS in terms of global optimization. The tuning and control parameters were studied in the work. Banati and Mehta (2012) presented a automated tool 123 Empirical analysis of ve nature-inspired algorithms on real Table 1 Brief literature study of contemporary algorithm Algorithm Paper references Algorithm evaluated Performance (best to worst) Evaluation parameters Number of dimen- sion evaluated Number of bench- mark functions ABC Pham and Castellani (2014) BEA, EA, PSO, ABC BEA>ABC>EA>PSO Speed and accuracy 2, 10, 15, 20 25 Khaze et al. (2013) ABC, FA ABC> FA Accuracy and convergence reliability 2, 3 1 Karaboga and Basturk (2008) ABC, DE, PSO, EA ABC>DE>EA>PSO Mean and SD of function values 2, 5, 50 5 FA Hashmi et al. (2013) CS, BA, FA FA>CS>BA Objective function value 10, 20, 40 5 Yang (2009) PSO, GA, FA FA>PSO>GA Success rate and ef ciency 16, 128, 256 10 CS Civicioglu and Besdok (2011) PSO, DE, ABC, CS CS=DE>PSO>ABC SD of mean value 2 and 30 50 Yang and Deb (2009) PSO, GA, CS CS>PSO>GA Global optima with success rate 16, 128, 256 10 Singh and Singh (2014) KHA (Krill Herd algorithm), FA, CS CS>FA>KHA Processing time and tness of optimized function 20 15 BA Yang (2010b) GA, PSO, BA BA>PSO>GA Global optima with success rate 16, 128, 256 10 Yang and Deb (2010) BFOA, BA BA>BFOA Convergence rate 2 12 FPA Yang (2013) GA, PSO, FPA FPA>PSO>GA Success rate and ef ciency 16, 128, 256 10 123 P. Agarwal, S. Mehta (SEVO) for evolutionary algorithms i.e. GA, MA, PSO and SFLA and tested these algo- rithms over sixteen benchmark functions on SEVO. Agarwal and Mehta (2014) highlighted the speci c features such as input parameters and evolutionary operators of 12 nature-inspired algorithms. Author provided the tabular analysis of minimum and maximum dimensions eval- uated as well as automated tools available for these algorithms. Banati and Mehta (2013) presented improved SFLA and tested it against GA, MA, PSO and SFLA for higher dimen- sions. Agarwal and Mehta (2015) had analyzed various nature-inspired algorithms on data clustering. An improved version of FPA algorithm on data clustering was proposed in Agar- wal and Mehta (2016). Theaforementionedliteraturestudyindicatesthatanalyzingtheperformanceofcontempo- rary nature-inspired algorithms such as BA, ABC, CS, FA and FPA is still a research problem. Moreover, in above studies algorithms were evaluated only on few classical benchmark func- tions over small dimensions only. Recent nature-inspired algorithms were not evaluated on standard set of benchmark functions in any of the studies. Hence comparing the basic algo- rithm on standard benchmark problems and nding the suitable algorithm along with control parameters values on speci c class of benchmark functions is a research problem. In present work, experimental analysis of ve contemporary nature-inspired algorithms i.e. BA, ABC, CS, FA and FPA is done on real parameter single objective optimization functions of CEC2014 test suite (Liang et al. 2014). This analysis includes performance evaluation, statistical test, computational complexity and run length distribution of these algorithms. Studies reveal that it is impossible to construct general purpose algorithm that gives optimal solution for all types of optimization problems. Hence, it becomes necessary to know which algorithm performs best on which kind of objective function. This paper aids researchers to understand and identify the con gurations of the best algorithm on speci c class of tness function, analyze computational time complexity and determine the most appropriate algorithm on small as well as high dimensions. The rest of the paper is organized as follows: Sect. 2 reviews the ve contemporary nature-inspired algorithms. Benchmark functions and parameter setting of each algorithm is discussed in Sect. 3. Result analysis of ve nature-inspired algorithms is highlighted in Sect. 4. Section 5 nally concludes the paper. 2 Review of ve contemporary nature-inspired algorithms The algorithms studied in this work are population based belonging to the class of meta- heuristic techniques. The basic processes involved in nding near optimal solution by all nature-inspired algorithms are intensi cation and diversi cation of search space. Diversi - cation explores the new search space to nd global solution and intensi cation exploits the explored search space to obtain optimal solution. The goal of nature-inspired algorithms is to minimize n-dimensional objective function where n is the number of variables to be opti- mized. The work employs standard algorithms and is at high level of abstraction. Detailed explanation of algorithms is given in their respective reference. Brief review of ve nature- inspired algorithms is as follows: 2.1 Arti cial bee colony (ABC) algorithm Arti cial bee colony algorithm is inspired by hunting behavior of honey bees with motive of locating best food sources (Karaboga and Basturk 2008). The paper consider basic version only while modi ed algorithm is de ned in Akay and Karaboga (2012).The honey bees 123 Empirical analysis of ve nature-inspired algorithms on real belonging to different groups communicate with each other to share the information of food sources. In order to get maximum nectar amount, they mutually decide which food source is better to exploit. They follow principles of division of labor and self-organization. There are different groups of bees: rst group consists of employed bees, which sucks nectar from owers and brings it to hive. Half of the colony size contains employed bees while the other half is occupied by second group of bees: onlooker bees. Onlooker bees wait in bee hive area to communicate with other bees and determine which food source has highest amount of nectar. Third category of bees is scout bees, which goes out to explore food sources randomly when no better food source is discovered by employed bees and onlooker bees. This algorithm has been applied on various unimodal and multi-modal optimization func- tions (Karaboga and Basturk 2007). In ABC algorithm, primarily, initial random solution is determined using Eq. (1). Xi j = lb j + rand (0, 1)  ub j lb j  (1) where i = 1. . .FS and j = 1. . .D, where FS is the number of food sources and D is dimension parameter of optimization problem. ub and lb are upper and lower bound param- eters de ning search space of a problem. Initial solution obtained in Eq. (1) is provided to employed bees and onlooker bees. Employed bees randomly choose food source position using Eq. (2) and then these bees determine their nectar amount using Eq. (3). POSi j = Xi j + i j  Xi j Xkj  (2) where j is a random integer in the range [1, D] and k (1, 2, . . . FS) is a randomly chosen index that has to be different from i. FS is the population of food source. i j is a uniformly distributed real random number in the range [ 1, 1]. f denotes objective function value in Eq. (3). Fitness =  1 1+ fi , i f fi > 0 1 + abs ( fi) , i f f < 0 (3) Employed bees communicate with onlooker bees to inform about food source. They nd the new food position in neighborhood of previous position using Eq. (2) and again the tness of their new position is determined by using Eq. (3). Thereafter, greedy selection technique is applied between new and previous nectar amounts ( tness value) and the one with highest amount is chosen. If new calculated amount is less than previous one then position couldn t be improved. Onlooker bees nd the new food source using Eq. (2) depending upon the probability obtained from Eq. (4). Pri = tnessi  f s i=1 tnessi (4) Onlooker bees send employee bees to the selected food source. Bees memorize the best food source position, consisting of highest nectar amount. If throughout the cycle nectar amount doesn t changes then that food source is rejected and corresponding employed bee becomes scout. The algorithm enters into scout bee phase if food source is rejected by certain threshold value. In ABC algorithm, Limit is a parameter de ned for entering into scout bee phase. If the solution in employed and onlooker bee phase does not improve by a threshold value (de ned by Limit parameter) then algorithm enters into scout bee phase where solution is determined randomly. Random solutions are found by scout using Eq. (1) and if its value is equal to colony size then corresponding food source is rejected. The exploitation process of food source (extracting nectar amount) is carried out by employed and onlooker bees while 123 P. Agarwal, S. Mehta exploration process (searching food source) is carried out by scouts (Karaboga and Basturk 2008). Parameters of algorithm are FoodNumber viz. half of population (colony) size, Limit, a trial counter variable initialized to 0. 3. while <termination criteria> //Employed Bee Phase A. For 1 to FoodNumber i. Mutant solution is computed from randomly chosen solution using Equation 2 ii. Evaluate new solution using Equation 3 iii. Apply greedy selection between old solution and its mutant (new solution) obtained in step i. iv. if the mutant solution is better than the current solution replace the solution with the mutant and reset the trial counter of solution to 0 v. else increase trial counter by 1. vi. end if B. end for loop C. Determine probability values using fitness value and normalize it using Equation 4 //Onlooker Bee Phase D. while t< FoodNumber //t is some variable initialized to 0 i. if random number is less than probability (determined in step C) a. t=t+1 b. Mutant solution is computed from randomly chosen solution using Equation 2 c. Evaluate new solution using Equation 3 d. Apply greedy selection between old solution and its mutant (new solution) obtained in step b. e. if the mutant solution is better than the current solution replace the solution with the mutant and reset the trial counter of solution to 0 f. else increase trial counter by 1. g. end if ii. end if E. end while //Scout Bee Phase F. Determine food sources whose trial exceeds Limit G. For such food sources (determined in previous step) reset trial to 0 and obtain a random solution H. Evaluate the new random solution using Equation 3 I. Update the solution if better fitness is found. J. Memorize the best food source found so far. 4. end while 5. Return the best solution Algorithm 1: ABC Algorithm Input: FoodNumber, Limit, trial 1. Initialize all food sources using Equation 1. 2. Compute nectar amount from food source using Equation 3. 2.2 Bat algorithm Bat algorithm was introduced by Yang (2010a,b), Yang and He (2013). Bats have a very strong sense of hearing. They produce loud sound to detect its prey. This sound bounces 123 Empirical analysis of ve nature-inspired algorithms on real back with some frequency and process is called echolocation. Echolocation detects object by re ected sound. It is used to know how far the prey is from background object. Bats can distinguish prey and obstacle through this frequency. They y randomly with some velocity, frequency and sound (loudness) to search for food. By observing the bounced frequency of sound, bats are able to sense the distance of an obstacle or prey in nearby surroundings. Bat algorithm simulates the echolocation behavior of microbats as they are capable of generating high echolocation. Solution of minimization problem is expressed in terms of distance to its prey. The frequency and zooming parameters maintain the balance between exploration and exploitation processes. The parameters of bat algorithm are: position of bat, velocity, frequency,loudnessandemittedpulserate.Eachbati hasacertainpositionvector Xt i ,velocity V t i at tth iteration, frequency Fi which varies between de ned values. When bat y randomly, it produces sound with loudness A0 and pulse emission rate ri. Frequency is tuned to explore potential solution using Eq. (5). Velocity of bats is updated using Eq. (6). Position vector represents potential solution to problem and is updated using Eq. (7). Fitness of solution is computed by automatic zooming. This factor helps in exploiting explored positions. If a new solution (new position of bat) is better than the local best solution computed previously then solution gets updated. Accordingly loudness is reduces by Eq. (8) and pulse rate is increased by Eq. (9). These factors are adjusted automatically depending upon the proximity of prey. Fi = Fmin + (Fmax Fmin) (5) V t i = V t 1 i + (Xt i X )Fii (6) Xt i = Xt 1 i + V t i (7) At+1 = At (8) rt+1 i = r0 i  1 e t (9) where and are random values which lie in the range [0, 1]. Pseudocode of bat algorithm is as follows. Algorithm 2: Bat Algorithm Inputs: population size n, loudness A and pulse rate r. 1. Initialize minimum (Fmin ) and maximum (Fmax ) frequency as 0 and 2 respectively. 2. Initialize loudness A and pulse rate r. 3. Compute initial solutions (positions) of bats randomly. 4. Determine initial best solution x* by computing fitness of each solution. 5. while <termination criteria> A. for i=1 to n i. update the frequency, velocity and position vector using Equations 5 to 7 a. if rand>r b. Generate a local solution around the best solution c. end if ii. Evaluate new solution using fitness function (Fitness(x)) a. if (rand < A & Fitness(x)<Fitness(x*) b. Update the solution c. Reduce A and increase r using Equation 8 and 9 d. end if iii. Renew the current best solution (x*) B. end for 6. end while 7. Display the best solution 123 P. Agarwal, S. Mehta 2.3 Cuckoo search (CS) Cuckoo search algorithm Yang and Deb (2010) is based on reproduction strategy of cuckoo bird. They lay eggs in host bird s nest and remove host bird egg to increase the hatching probability of its own egg. Cuckoo search algorithm assumes following three principles for its implementation: 1. At a particular generation, each cuckoo lays only one egg and places it in randomly selected nest. 2. The current best solution indicates high quality of eggs and is passed to next generation. 3. The probability of alien eggs or destroying the host nest, which are x in number, is pa [0, 1]. For generating new solution at tth iteration following equation is deployed Xt+1 i = Xt i + L evy ( ) (10) where Xt+1 i is new solution (nest) of ith cuckoo, entry wise multiplication of step size and L evy( ) is levy ight with as levy exponent taken as 1.5 (in our simulation). Probability that n nest is replaced by new nest is pa. Pseudo code of CS is as follows: Algorithm 3: Cuckoo Search Algorithm Inputs: population size n. 1. Define the probability of alien eggs pa 2. Compute initial solutions of n host nest randomly 3. while <termination criteria> A. Get new solutions/cuckoo by Levy flight using Equation 10 B. Evaluate the new solutions using fitness function (quality) Fi C. Pick a random solution j (nest) among n solution. D. if (Fi > Fj) replace j by new solution (nest) E. end F. Replace worse nests having probability pa by new nest discovered by random walk G. Update current best solution with quality solution H. Determine best solution found so far 4. end while 5. Display the best solution (nests) 2.4 Fire y algorithm (FA) Fire y algorithm was developed by Yang (2010b), Fister et al. (2013b). It is inspired by ashing behavior of re ies and uses following three rules: 1. All re ies are attracted towards each other as they are unisex creatures. 2. The attractiveness of re y is directly proportional to the light intensity or brightness of another re y in neighborhood. Attractiveness and distance r are inversely proportional and is given by = 0exp  r2 (11) 123 Empirical analysis of ve nature-inspired algorithms on real A less bright re y i moves towards brighter re y j at tth generation. Hence new location (solution) of re y is obtained by following equation xt+1 i = xt i + e r2 i j  xt j xt i + t (12) where second term is representing attractiveness in which is variation of attractiveness, r is distance between two les xi and x j and is the light absorption coef cient. The third term represents random variables with t as randomization parameter determining step size and is Gaussian or uniform distribution. 3. Objective function determines the brightness of re y. Pseudo code of re y algorithm is shown below. Algorithm 4: Firefly Algorithm Inputs: population size n 1. Define the values of and 2. Compute initial solutions/location of n fireflies 3. Define the coefficient of light absorption . 4. Using fitness function, compute light intensity Ii at xi 5. while <termination criteria> a. for i=1 to n i. for j=1 to n ii. if I(i)>I(j) 1. use Equation 12 to attract jth firefly towards ith firefly 2. evaluate the new solution using fitness function 3. Update the light intensity of fireflies iii. end if iv. end for b. end for 6. end while 7. Display the best solution (location) of fireflies 2.5 Flower pollination algorithm (FPA) Flowers reproduce through pollination. Flower pollination takes place by transmission of pollen grains between different owers of same species or other species. The algorithm Yang (2013) assumes that each ower contains only one pollen grain which is associated with the solution of problem. There are two types of pollination namely, biotic and abiotic. 90% of pollination is biotic form, where pollen grains are carried by pollinators such as insects, birds etc, while 10% of pollination is abotic which uses wind and water diffusion for pollination. Pollination can be self pollination, where owers are fertilized with pollens of same plant species, or cross pollination, where owers are fertilized with pollens of different owers species. It is assumed that birds and bees follow Levy s distribution to take distant step for ying or jumping. Hence ower pollination algorithm follows following rules: 1. Flower constancy is the probability of reproduction which is proportional to the owers similarity. 2. Global pollination is carried by biotic and cross pollination procedures. Rule 1 and 2 can be combined to give following equation xt+1 i = xt i + L(xt i g ) (13) 123 P. Agarwal, S. Mehta where xt i is ith solution of tth generation, g is the current best solution at present generation and L represents levy ight distribution (Yang 2013; Pavlyukevich 2007) depicting potency of pollination. 3. Local pollination is carried out by abiotic and self pollination. Rule 1 and 3 are combined to give following equation xt+1 i = xt i + (xt j xt k) (14) where xt j and xt k are owers of same species and is the uniform distribution in range [0, 1]. 4. A switch probability p is used to control the local and global pollination and it ranges from [0, 1]. Pseudo code of FPA is given below. Algorithm 5: Flower Pollination Algorithm Inputs: population size n Define the values of switch probability p 1. Initialize the population with random solutions 2. Determine initial best solution 3. while <termination criteria> A. for i=1 to n i. if rand>p a. L is the step of Levy distribution on d dimension b. perform global pollination using Equation 13 ii. else a. find neighbor flowers randomly b. perform local pollination using Equation 14 iii. end if B. evaluate the new solution C. if solution improves Replace the old solution with improved new solution D. end if E. determine current best global solution F. end for 4. end while 5. Display the best solution 3 Experimental setup 3.1 Evaluation criteria The ve nature-inspired algorithms are assessed on 30 benchmark single objective mini- mization functions of CEC 2014 (Liang et al. 2014) with real parameter optimization. These benchmark problems form test functions that are composed of several new features of basic function for the purpose of optimization. Summary of CEC2014 benchmark functions are given in Table 2. These benchmark functions are evaluated over 10, 30, 50 and 100 dimension problems. Thirty functions are subdivided into four subclasses named as: unimodal functions, simple 123 Empirical analysis of ve nature-inspired algorithms on real Table 2 Summary of CEC2014 benchmark functions Sub group Function no. Function name F i = Fi(x ) Unimodal functions 1 Rotated high conditioned elliptic function 100 2 Rotated bent cigar function 200 3 Rotated discus function 300 Simple multimodal functions 4 Shifted and rotated Rosenbrock s function 400 5 Shifted and rotated Ackley s function 500 6 Shifted and rotated Weierstrass function 600 7 Shifted and rotated Griewank s function 700 8 Shifted Rastrigin s function 800 9 Shifted and rotated Rastrigin s function 900 10 Shifted Schwefel s function 1000 11 Shifted and rotated Schwefel s function 1100 12 Shifted and rotated Katsuura function 1200 13 Shifted and rotated HappyCat function 1300 14 Shifted and rotated HGBat function 1400 15 Shifted and rotated expanded Griewank s plus Rosenbrock s function 1500 16 Shifted and rotated expanded Scaffer s F6 function 1600 Hybrid functions 17 Hybrid function 1 (N = 3) 1700 18 Hybrid function 2 (N = 3) 1800 19 Hybrid function 3 (N = 4) 1900 20 Hybrid function 4 (N = 4) 2000 21 Hybrid function 5 (N = 5) 2100 22 Hybrid function 6 (N = 5) 2200 Composition functions 23 Composition function 1 (N = 5) 2300 24 Composition function 2 (N = 3) 2400 25 Composition function 3 (N = 3) 2500 26 Composition function 4 (N = 5) 2600 27 Composition function 5 (N = 5) 2700 123 P. Agarwal, S. Mehta Table 2 continued Sub group Function no. Function name F i = Fi(x ) 28 Composition function 6 (N = 5) 2800 29 Composition function 7 (N = 3) 2900 30 Composition function 8 (N = 3) 3000 multimodal functions, hybrid functions and composition functions. Search range is de ned from 100 to +100 for all test functions. Bias (F i ) for all test functions ranges from 100 for function F1 with increment of 100 for every next consecutive function upto 30,000 for function F30. Maximum function evaluations (MaxFes) are treated as terminating criteria for all algorithms evaluating CEC2014 benchmark functions. As per Liang et al. (2014) its value is assume to be 10,000*D, where D is the dimension of the problem. Algorithms are executed 51 times independently that means there are in total 51 runs and in each run algorithm is executed by 10,000*D maximum function evaluations. Results are recorded in the form of minimum value and nally error is computed as below: Error = Fmin F i (15) Error value less than 10 8 is taken 0. Performance is assessed on basis of best, worst, mean, median and SD of 51 error values (runs) on each function. In this work all algorithms are executed on Matlab R2013a with hardware con guration as: 64 bit Windows7 operating system, Intel Core i5 processor with 16.0GB RAM. 3.2 Parameter regulations In order to attain good results on simple and complex functions, it was found that small population size is more appropriate (Akay and Karaboga 2009). Hence, in this work standard population size viz. 20 is used in all algorithms of different problem sizes (Yang and Deb 2010; Alsariera et al. 2014). Parameter values of algorithms are self-adapted so as to achieve the best results on CEC 2014 benchmark functions. The parameters are initialized with certain values at the beginning of an algorithm (shown in Table 3) and then self-adapted during the execution of algorithm. This self-adaptation or self-tuning of parameters is made either by choosing random number between certain range (for CS and FPA is shown in Table 3) or by some equations (Eqs. 8, 9 for BA and Eq. (16) for FA). Hence, each parameter tunes itself with different value on each individual function. Thus, self-adaptive parameters help an algorithm to obtain its optimal value with best parameter settings. Mean and SD of parameter values on 30 functions are shown in Appendix and initial values of parameters are shown in Table 3. Initial parameter values of re y algorithm are obtained from Yang (2010b), while the value of randomized parameter is modi ed on basis of Eq. (16) de ned in Fister et al. (2013c). t+1 = (1 ) t, where = 1 10 4 0.9 1/Max_Gen (16) 123 Empirical analysis of ve nature-inspired algorithms on real Table 3 Parameters of algorithms Algorithms Parameters Initial values/range Common parameter Population size (n) 20 FA (attractiveness) 0.10 (light absorption coef cient) 1.0 CS pa [0.15, 0.30] BA A (loud) 0.50 r (pulse) 0.50 FPA p [0.7, 0.9] For cuckoo search algorithm, parameters are tuned (Yang and Deb 2010) ef ciently. It has been observed from studies Mlakar et al. (2015) that population size and pa varies from 15 to 25 and 0.15 to 0.30 respectively. Initial parameter values of bat algorithm are taken from Fister et al. (2013d), Alsariera et al. (2014) for acquiring nearest optimal solution. As number of iterations increases, values of parameters are adapted based on Eqs. 8 and 9. In ower pollination algorithm, optimal range of switch probability between local and global pollination is de ned in Draa (2015). In ABC algorithm, Limit value (Karaboga and Akay 2009) is de ned by product of FoodNumber and dimension D. 4 Experimental results and study The error values of ve contemporary nature-inspired algorithms i.e. ABC, FA, CS, BA and FPA on CEC2014 30 benchmark (Liang et al. 2014) functions are presented in Tables 4, 5 6 and 7. Each and every algorithm starts with same initial population. All functions use real parameters and returns single objective value. Tables 4, 5, 6 and 7 depicts the error values for unimodal, simple multimodal, hybrid and composition functions respectively. Bold values in tables represent minimum mean error value among ve algorithms. Error value is computed from recorded function values using Eq. (15). The results obtained over benchmark test problems are analyzed on the basis of i. Wilcoxon rank sum test performed by taking mean of error values over 51 runs. ii. Computational time complexity performed by computing mean time taken by each algorithm to run F18 function (one of CEC 2014 benchmark function) over 5 independent runs. iii. Convergence speed via run length distribution performed by recording the error values at 14 iterations de ned by (0.01, 0.02, 0.03, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0)*MaxFES (maximum tness function evaluations) for each benchmark function. At each iteration point, mean of 51 run is calculated (Liang et al. 2014). Line graph is plotted between log of error values and 14 iteration points. Subsections are organized as follows: Sect. 4.1 gives Wilcoxon rank sum test analysis on 10, 30, 50 and 100 dimensions. Section 4.2 illustrates the computational time complexity of all ve algorithms on F18 function. Section 4.3 describes the run length distribution of all ve algorithms on 100 dimensions for each class of tness function in its sub section. 123 P. Agarwal, S. Mehta Table 4 Best, worst, median, mean and SD of tness values of unimodal function for 10D, 30D, 50D and 100D 10D 30D Best Worst Median Mean SD Best Worst Median Mean SD F1 ABC 1.21E+04 1.15E+06 1.49E+05 2.34E+05 2.39E+05 1.39E+06 1.06E+07 4.31E+06 4.63E+06 2.23E+06 FA 5.02E+04 1.86E+07 4.13E+06 5.76E+06 5.24E+06 3.17E+05 6.31E+08 1.72E+08 2.19E+08 1.69E+08 CS 0.00E+00 1.08E 05 0.00E+00 3.65E 07 1.96E 06 1.13E+03 6.72E+05 6.43E+04 1.26E+05 1.66E+05 BA 1.06E+07 4.37E+08 7.76E+07 1.20E+08 1.11E+08 4.38E+03 1.81E+09 5.90E+08 7.98E+08 4.91E+08 FPA 7.35E 09 4.72E 02 1.67E 05 4.14E 03 1.04E 02 6.89E+03 1.23E+06 3.48E+05 4.38E+05 3.37E+05 F2 ABC 4.57E+00 2.96E+02 5.83E+01 8.30E+01 8.04E+01 1.06E+00 6.34E+02 9.80E+01 1.85E+02 1.78E+02 FA 7.67E+03 3.85E+09 1.29E+09 1.32E+09 1.04E+09 5.29E+06 5.56E+10 1.71E+10 2.01E+10 1.73E+10 CS 0.00E+00 0.00E+00 0.00E+00 0.00E+00 0.00E+00 1.00E+10 1.00E+10 1.00E+10 1.00E+10 0.00E+00 BA 7.54E+03 1.47E+10 6.88E+09 6.80E+09 3.70E+09 2.59E+10 9.00E+10 4.31E+10 4.92E+10 1.69E+10 FPA 7.72E 10 3.26E 02 7.26E 07 1.82E 03 6.78E 03 9.09E+02 2.62E+06 6.00E+04 3.94E+05 6.15E+05 F3 ABC 5.71E+00 1.20E+03 2.17E+02 2.95E+02 2.56E+02 3.00E+01 5.81E+03 8.21E+02 1.06E+03 1.16E+03 FA 2.92E+01 3.24E+04 1.02E+04 1.18E+04 8.72E+03 1.06E+02 1.48E+05 7.91E+04 7.16E+04 4.71E+04 CS 0.00E+00 0.00E+00 0.00E+00 0.00E+00 0.00E+00 0.00E+00 0.00E+00 0.00E+00 0.00E+00 0.00E+00 BA 1.11E+04 7.69E+05 6.55E+04 1.22E+05 1.51E+05 7.83E+04 1.46E+07 1.84E+05 1.21E+06 2.95E+06 FPA 1.22E 06 4.63E 02 3.61E 04 4.36E 03 9.84E 03 1.66E+01 2.76E+03 3.74E+02 6.56E+02 7.57E+02 123 Empirical analysis of ve nature-inspired algorithms on real Table 4 continued 50D 100D Best Worst Median Mean SD Best Worst Median Mean SD F1 ABC 4.24E+06 1.71E+07 8.08E+06 8.65E+06 3.00E+06 4.59E+06 6.59E+07 4.23E+07 4.20E+07 1.19E+07 FA 1.98E+06 2.14E+09 6.47E+08 7.51E+08 6.03E+08 1.03E+07 8.85E+09 1.38E+09 2.93E+09 2.92E+09 CS 4.31E+05 4.44E+06 1.81E+06 1.67E+06 8.67E+05 3.73E+04 1.00E+10 1.00E+10 5.80E+09 4.94E+09 BA 1.92E+04 3.26E+09 8.79E+08 1.12E+09 8.20E+08 2.43E+04 2.24E+09 3.95E+08 5.98E+08 4.97E+08 FPA 1.51E+05 6.14E+06 2.40E+06 2.66E+06 1.38E+06 5.03E+05 9.18E+07 4.83E+07 4.55E+07 1.94E+07 F2 ABC 1.80E+01 1.39E+04 1.10E+03 1.41E+03 2.44E+03 4.42E+02 2.70E+04 4.01E+03 4.94E+03 5.06E+03 FA 5.15E+06 1.20E+11 4.44E+10 4.42E+10 3.55E+10 1.91E+08 3.99E+11 1.84E+11 1.81E+11 1.29E+11 CS 1.00E+10 1.00E+10 1.00E+10 1.00E+10 0.00E+00 1.00E+10 1.00E+10 1.00E+10 1.00E+10 0.00E+00 BA 2.16E+10 1.28E+11 5.80E+10 6.50E+10 2.85E+10 2.25E+10 2.55E+11 9.87E+10 1.02E+11 4.93E+10 FPA 1.08E+04 7.18E+08 4.33E+06 3.86E+07 1.30E+08 6.18E+06 5.84E+09 1.57E+08 6.77E+08 1.41E+09 F3 ABC 2.12E+03 1.34E+04 6.84E+03 7.14E+03 2.35E+03 1.01E+04 3.09E+04 1.90E+04 1.98E+04 5.14E+03 FA 2.21E+03 2.84E+05 1.59E+05 1.49E+05 8.82E+04 3.36E+03 6.91E+05 3.02E+05 3.28E+05 2.20E+05 CS 1.53E 02 1.37E+01 1.05E+00 3.05E+00 3.85E+00 2.53E+00 2.22E+03 1.20E+02 3.51E+02 5.77E+02 BA 1.28E+05 6.87E+06 2.57E+05 7.81E+05 1.40E+06 1.12E+05 1.51E+06 3.19E+05 4.75E+05 3.75E+05 FPA 4.03E+03 2.31E+04 1.01E+04 1.01E+04 3.95E+03 8.12E+03 3.30E+04 2.22E+04 2.20E+04 6.20E+03 123 P. Agarwal, S. Mehta Table 5 Best, worst, median, mean and SD of tness values of simple multimodal functions for 10D, 30D, 50D and 100D 10D 30D Best Worst Median Mean SD Best Worst Median Mean SD F4 ABC 3.97E 03 4.97E+00 6.18E 02 3.94E 01 9.41E 01 1.25E 01 7.21E+01 4.61E+00 2.27E+01 2.95E+01 FA 3.24E+01 1.85E+02 7.16E+01 8.10E+01 3.95E+01 9.63E+01 7.58E+03 9.44E+02 2.02E+03 2.06E+03 CS 0.00E+00 3.48E+01 0.00E+00 5.22E+00 1.19E+01 6.03E 12 6.34E+01 4.67E 04 2.17E+00 1.16E+01 BA 6.28E+01 6.54E+03 1.26E+03 1.64E+03 1.33E+03 9.34E+02 2.24E+04 6.92E+03 7.14E+03 4.62E+03 FPA 0.00E+00 3.48E+01 0.00E+00 6.37E+00 1.30E+01 1.79E+00 1.71E+02 8.43E+01 9.31E+01 3.92E+01 F5 ABC 2.08E+00 2.01E+01 2.01E+01 1.82E+01 5.13E+00 2.01E+01 2.03E+01 2.02E+01 2.02E+01 3.92E 02 FA 1.38E+01 2.05E+01 2.04E+01 2.00E+01 1.56E+00 2.08E+01 2.10E+01 2.10E+01 2.09E+01 4.83E 02 CS 2.00E+01 2.02E+01 2.01E+01 2.01E+01 3.51E 02 2.06E+01 2.08E+01 2.07E+01 2.07E+01 5.59E 02 BA 2.00E+01 2.00E+01 2.00E+01 2.00E+01 4.10E 04 2.00E+01 2.00E+01 2.00E+01 2.00E+01 5.26E 06 FPA 1.84E+01 2.04E+01 2.02E+01 2.02E+01 3.55E 01 2.04E+01 2.10E+01 2.08E+01 2.08E+01 1.38E 01 F6 ABC 1.20E+00 3.42E+00 2.32E+00 2.29E+00 6.09E 01 1.10E+01 1.69E+01 1.51E+01 1.46E+01 1.64E+00 FA 2.02E+00 9.74E+00 7.30E+00 6.88E+00 2.03E+00 1.70E+01 4.10E+01 3.37E+01 3.07E+01 8.13E+00 CS 8.33E 05 3.37E+00 1.32E+00 1.49E+00 1.08E+00 1.51E+01 2.65E+01 2.31E+01 2.26E+01 2.21E+00 BA 9.69E+00 1.52E+01 1.15E+01 1.22E+01 1.99E+00 4.14E+01 4.66E+01 4.40E+01 4.39E+01 1.82E+00 FPA 3.01E 01 5.03E+00 3.13E+00 3.08E+00 1.23E+00 1.49E+01 2.49E+01 2.23E+01 2.17E+01 2.51E+00 F7 ABC 1.55E 08 2.76E 02 9.93E 03 9.82E 03 9.29E 03 1.59E 12 2.14E 02 5.61E 11 7.12E 04 3.90E 03 FA 9.97E 01 3.55E+01 7.61E+00 1.09E+01 9.98E+00 6.20E+00 4.46E+02 2.03E+02 2.14E+02 1.36E+02 CS 8.17E 03 8.80E 02 3.00E 02 3.29E 02 1.95E 02 0.00E+00 9.86E 03 0.00E+00 1.97E 03 4.01E 03 BA 3.60E+01 2.94E+02 1.11E+02 1.25E+02 6.66E+01 2.85E+01 8.81E+02 4.63E+02 4.77E+02 1.88E+02 FPA 2.05E 02 1.62E 01 6.28E 02 6.58E 02 3.06E 02 1.67E 02 1.27E+00 2.41E 01 4.00E 01 4.04E 01 123 Empirical analysis of ve nature-inspired algorithms on real Table 5 continued 10D 30D Best Worst Median Mean SD Best Worst Median Mean SD F8 ABC 0.00E+00 1.14E 13 0.00E+00 3.79E 15 2.08E 14 1.14E 13 2.27E 13 2.27E 13 1.74E 13 5.77E 14 FA 1.98E+01 6.40E+01 4.01E+01 4.19E+01 1.34E+01 1.26E+02 3.35E+02 2.12E+02 2.24E+02 5.58E+01 CS 0.00E+00 0.00E+00 0.00E+00 0.00E+00 0.00E+00 1.99E+00 2.19E+01 6.47E+00 6.63E+00 4.39E+00 BA 1.39E+01 8.56E+01 6.31E+01 5.93E+01 1.93E+01 1.47E+02 3.84E+02 2.48E+02 2.51E+02 6.20E+01 FPA 2.30E+00 2.53E+01 9.85E+00 9.93E+00 4.30E+00 4.57E+01 9.40E+01 6.70E+01 6.72E+01 1.31E+01 F9 ABC 4.30E+00 1.39E+01 7.84E+00 8.35E+00 2.30E+00 4.98E+01 1.17E+02 8.74E+01 8.67E+01 1.44E+01 FA 1.06E+01 6.98E+01 4.34E+01 4.45E+01 1.42E+01 1.48E+02 3.94E+02 2.29E+02 2.49E+02 7.28E+01 CS 5.33E+00 1.89E+01 9.96E+00 1.06E+01 3.90E+00 6.99E+01 1.56E+02 1.23E+02 1.21E+02 2.13E+01 BA 1.29E+01 1.07E+02 6.12E+01 5.87E+01 2.10E+01 1.86E+02 5.11E+02 2.92E+02 3.20E+02 9.17E+01 FPA 4.08E+00 3.00E+01 1.12E+01 1.17E+01 5.00E+00 6.03E+01 1.89E+02 1.13E+02 1.15E+02 2.59E+01 F10 ABC 2.11E 04 1.89E 01 1.25E 01 1.05E 01 5.24E 02 1.46E 01 1.30E+00 2.29E 01 2.93E 01 2.33E 01 FA 1.59E+02 1.30E+03 1.03E+03 9.37E+02 2.67E+02 3.92E+03 7.12E+03 6.60E+03 6.41E+03 6.74E+02 CS 7.30E 01 1.33E+02 1.28E+01 1.69E+01 2.32E+01 1.66E+01 4.97E+02 1.35E+02 1.38E+02 1.22E+02 BA 5.05E+02 2.15E+03 1.41E+03 1.40E+03 3.90E+02 3.38E+03 6.45E+03 4.44E+03 4.69E+03 7.40E+02 FPA 6.56E+01 4.47E+02 2.45E+02 2.51E+02 9.34E+01 1.85E+03 3.09E+03 2.35E+03 2.36E+03 3.33E+02 123 P. Agarwal, S. Mehta Table 5 continued 10D 30D Best Worst Median Mean SD Best Worst Median Mean SD F11 ABC 3.60E+01 4.60E+02 1.69E+02 1.98E+02 1.10E+02 1.29E+03 2.30E+03 1.96E+03 1.91E+03 2.65E+02 FA 5.37E+02 1.55E+03 1.22E+03 1.13E+03 2.63E+02 3.58E+03 7.54E+03 6.95E+03 6.73E+03 8.67E+02 CS 9.78E+01 8.18E+02 4.81E+02 4.78E+02 1.79E+02 3.01E+03 3.75E+03 3.42E+03 3.41E+03 2.40E+02 BA 5.22E+02 2.30E+03 1.56E+03 1.53E+03 4.20E+02 3.38E+03 6.17E+03 4.76E+03 4.74E+03 6.90E+02 FPA 2.02E+02 9.21E+02 6.51E+02 6.42E+02 1.69E+02 2.68E+03 4.42E+03 3.75E+03 3.65E+03 3.66E+02 F12 ABC 1.44E 01 3.34E 01 2.50E 01 2.37E 01 4.23E 02 1.78E 01 3.02E 01 2.34E 01 2.31E 01 3.20E 02 FA 3.97E 01 1.48E+00 1.09E+00 1.11E+00 2.44E 01 1.89E+00 2.93E+00 2.58E+00 2.52E+00 2.83E 01 CS 7.73E 02 2.23E 01 1.44E 01 1.45E 01 3.89E 02 2.76E 01 6.63E 01 5.48E 01 5.36E 01 1.00E 01 BA 3.68E 01 2.52E+00 1.00E+00 1.06E+00 5.16E 01 6.09E 01 3.88E+00 2.13E+00 2.09E+00 6.87E 01 FPA 2.25E 01 5.35E 01 3.85E 01 3.88E 01 8.04E 02 5.59E 01 1.56E+00 1.02E+00 1.04E+00 2.26E 01 F13 ABC 7.96E 02 1.74E 01 1.25E 01 1.26E 01 2.25E 02 1.80E 01 2.66E 01 2.31E 01 2.30E 01 2.37E 02 FA 9.84E 02 1.89E+00 9.19E 01 9.17E 01 5.27E 01 2.13E 01 5.23E+00 2.92E+00 2.65E+00 1.71E+00 CS 9.95E 02 3.05E 01 1.66E 01 1.74E 01 5.44E 02 2.46E 01 5.14E 01 3.38E 01 3.51E 01 7.29E 02 BA 2.15E 01 5.23E+00 3.29E+00 3.39E+00 1.15E+00 2.45E+00 9.28E+00 5.92E+00 6.06E+00 1.43E+00 FPA 1.46E 01 3.82E 01 2.09E 01 2.24E 01 5.49E 02 2.79E 01 6.85E 01 5.23E 01 5.27E 01 9.82E 02 123 Empirical analysis of ve nature-inspired algorithms on real Table 5 continued 10D 30D Best Worst Median Mean SD Best Worst Median Mean SD F14 ABC 9.68E 02 2.02E 01 1.48E 01 1.48E 01 2.48E 02 1.39E 01 2.08E 01 1.87E 01 1.87E 01 1.45E 02 FA 1.49E 01 8.63E+00 1.54E+00 2.76E+00 2.82E+00 1.74E 01 1.58E+02 6.31E+01 6.30E+01 5.29E+01 CS 6.05E 02 2.36E 01 1.53E 01 1.49E 01 4.39E 02 1.71E 01 3.46E 01 2.56E 01 2.56E 01 4.23E 02 BA 4.14E+00 7.23E+01 3.38E+01 3.31E+01 1.66E+01 5.97E+01 3.05E+02 1.85E+02 1.83E+02 6.24E+01 FPA 7.68E 02 2.52E 01 1.74E 01 1.78E 01 4.47E 02 1.76E 01 3.84E 01 2.92E 01 2.81E 01 4.79E 02 F15 ABC 3.83E 01 1.48E+00 1.03E+00 1.03E+00 2.40E 01 5.05E+00 1.02E+01 7.56E+00 7.53E+00 1.25E+00 FA 1.84E+00 1.31E+03 5.05E+01 1.68E+02 2.76E+02 1.42E+01 1.34E+06 1.03E+04 1.73E+05 3.14E+05 CS 5.09E 01 1.30E+00 9.02E 01 9.07E 01 2.40E 01 6.85E+00 2.27E+01 1.25E+01 1.30E+01 3.68E+00 BA 2.04E+03 1.19E+05 7.30E+03 2.19E+04 2.70E+04 1.99E+04 1.72E+06 1.46E+05 3.18E+05 4.03E+05 FPA 6.02E 01 1.66E+00 1.16E+00 1.17E+00 2.99E 01 1.02E+01 9.24E+01 4.05E+01 4.11E+01 1.94E+01 F16 ABC 1.62E+00 2.64E+00 2.25E+00 2.22E+00 2.71E 01 9.14E+00 1.06E+01 1.02E+01 1.00E+01 4.34E 01 FA 2.14E+00 3.77E+00 3.36E+00 3.31E+00 3.84E 01 1.18E+01 1.33E+01 1.30E+01 1.28E+01 4.42E 01 CS 2.00E+00 3.12E+00 2.75E+00 2.71E+00 2.70E 01 1.12E+01 1.25E+01 1.22E+01 1.21E+01 2.68E 01 BA 3.50E+00 4.79E+00 4.42E+00 4.36E+00 3.14E 01 1.22E+01 1.46E+01 1.39E+01 1.39E+01 5.16E 01 FPA 2.19E+00 3.26E+00 2.92E+00 2.88E+00 2.30E 01 1.17E+01 1.27E+01 1.23E+01 1.23E+01 2.08E 01 123 P. Agarwal, S. Mehta Table 5 continued 50D 100D Best Worst Median Mean SD Best Worst Median Mean SD F4 ABC 6.24E+00 9.29E+01 7.41E+01 5.95E+01 2.98E+01 1.06E+02 2.00E+02 1.56E+02 1.58E+02 2.41E+01 FA 1.43E+02 2.66E+04 1.09E+04 1.21E+04 8.95E+03 2.54E+02 1.07E+05 1.26E+04 2.67E+04 3.33E+04 CS 3.25E 01 1.45E+02 7.36E+01 6.11E+01 3.82E+01 8.01E+01 2.87E+02 1.86E+02 1.89E+02 5.22E+01 BA 2.63E+03 5.18E+04 1.49E+04 1.81E+04 1.21E+04 3.91E+03 4.36E+04 1.10E+04 1.46E+04 1.02E+04 FPA 7.21E+01 4.15E+02 1.74E+02 1.87E+02 7.65E+01 3.29E+02 1.01E+03 5.42E+02 5.63E+02 1.59E+02 F5 ABC 2.02E+01 2.03E+01 2.02E+01 2.02E+01 3.13E 02 2.01E+01 2.03E+01 2.02E+01 2.02E+01 3.42E 02 FA 2.10E+01 2.12E+01 2.11E+01 2.11E+01 3.81E 02 2.10E+01 2.13E+01 2.13E+01 2.13E+01 5.48E 02 CS 2.08E+01 2.11E+01 2.10E+01 2.09E+01 6.85E 02 2.10E+01 2.12E+01 2.12E+01 2.12E+01 5.73E 02 BA 2.00E+01 2.00E+01 2.00E+01 2.00E+01 9.25E 06 2.00E+01 2.00E+01 2.00E+01 2.00E+01 5.98E 06 FPA 2.09E+01 2.12E+01 2.11E+01 2.11E+01 4.95E 02 2.11E+01 2.13E+01 2.13E+01 2.13E+01 4.07E 02 F6 ABC 2.80E+01 3.53E+01 3.14E+01 3.15E+01 1.74E+00 7.76E+01 8.93E+01 8.46E+01 8.45E+01 2.56E+00 FA 2.73E+01 7.45E+01 6.56E+01 5.90E+01 1.45E+01 5.65E+01 1.60E+02 1.44E+02 1.37E+02 2.78E+01 CS 3.87E+01 5.02E+01 4.68E+01 4.63E+01 2.99E+00 1.01E+02 1.20E+02 1.13E+02 1.12E+02 3.88E+00 BA 7.61E+01 8.43E+01 8.08E+01 8.04E+01 2.56E+00 1.61E+02 1.75E+02 1.71E+02 1.69E+02 4.91E+00 FPA 3.51E+01 5.03E+01 4.48E+01 4.48E+01 3.15E+00 9.78E+01 1.21E+02 1.09E+02 1.09E+02 6.24E+00 F7 ABC 4.73E 11 8.90E 03 3.33E 04 1.62E 03 2.61E 03 5.25E 08 3.81E 04 5.94E 06 3.11E 05 7.32E 05 FA 1.55E+01 1.30E+03 5.65E+02 5.69E+02 3.79E+02 3.63E+00 3.37E+03 1.63E+03 1.59E+03 1.16E+03 CS 0.00E+00 1.48E 02 0.00E+00 1.97E 03 4.21E 03 0.00E+00 6.61E 02 0.00E+00 3.52E 03 1.26E 02 BA 5.39E+01 1.68E+03 7.83E+02 7.87E+02 3.06E+02 1.41E+02 1.91E+03 9.88E+02 1.02E+03 4.30E+02 FPA 1.50E 01 1.66E+00 6.61E 01 7.69E 01 4.49E 01 1.05E+00 5.65E+01 2.42E+00 5.87E+00 1.05E+01 123 Empirical analysis of ve nature-inspired algorithms on real Table 5 continued 50D 100D Best Worst Median Mean SD Best Worst Median Mean SD F8 ABC 2.27E 13 4.55E 13 2.27E 13 2.96E 13 8.23E 14 4.55E 13 7.96E 13 6.82E 13 6.48E 13 8.52E 14 FA 2.68E+02 6.92E+02 4.82E+02 4.99E+02 1.20E+02 6.83E+02 1.65E+03 1.25E+03 1.20E+03 2.88E+02 CS 8.95E+00 5.07E+01 3.13E+01 3.14E+01 1.01E+01 1.01E+02 2.81E+02 1.83E+02 1.88E+02 4.28E+01 BA 2.99E+02 6.91E+02 4.53E+02 4.68E+02 1.01E+02 6.97E+02 1.31E+03 9.26E+02 9.80E+02 1.61E+02 FPA 1.06E+02 1.93E+02 1.47E+02 1.48E+02 2.42E+01 3.06E+02 5.33E+02 4.37E+02 4.34E+02 5.38E+01 F9 ABC 1.66E+02 2.36E+02 2.01E+02 1.99E+02 2.04E+01 5.98E+02 8.05E+02 7.28E+02 7.20E+02 5.53E+01 FA 2.45E+02 8.11E+02 4.67E+02 5.12E+02 1.51E+02 7.03E+02 1.90E+03 1.21E+03 1.30E+03 3.63E+02 CS 2.09E+02 3.85E+02 2.84E+02 2.81E+02 4.27E+01 5.90E+02 9.19E+02 7.63E+02 7.57E+02 6.64E+01 BA 3.28E+02 9.27E+02 6.13E+02 6.27E+02 1.52E+02 9.13E+02 1.79E+03 1.21E+03 1.24E+03 2.11E+02 FPA 1.92E+02 2.99E+02 2.63E+02 2.54E+02 2.77E+01 5.53E+02 7.74E+02 6.63E+02 6.66E+02 5.54E+01 F10 ABC 1.58E 01 2.67E+00 9.22E 01 9.69E 01 6.25E 01 5.79E 01 3.07E+00 1.59E+00 1.61E+00 5.82E 01 FA 8.46E+03 1.36E+04 1.30E+04 1.26E+04 1.17E+03 1.65E+04 3.10E+04 2.94E+04 2.77E+04 4.09E+03 CS 1.52E+02 1.62E+03 6.23E+02 7.14E+02 3.97E+02 1.80E+03 6.85E+03 3.90E+03 4.12E+03 1.22E+03 BA 6.26E+03 1.00E+04 8.12E+03 8.19E+03 9.54E+02 1.50E+04 2.14E+04 1.69E+04 1.69E+04 1.40E+03 FPA 3.51E+03 5.85E+03 4.95E+03 4.81E+03 6.91E+02 8.30E+03 1.44E+04 1.15E+04 1.16E+04 1.41E+03 123 P. Agarwal, S. Mehta Table 5 continued 50D 100D Best Worst Median Mean SD Best Worst Median Mean SD F11 ABC 2.80E+03 5.04E+03 4.58E+03 4.47E+03 4.15E+02 1.02E+04 1.30E+04 1.18E+04 1.17E+04 7.61E+02 FA 1.10E+04 1.41E+04 1.32E+04 1.31E+04 7.70E+02 1.54E+04 3.13E+04 3.04E+04 2.96E+04 2.93E+03 CS 5.83E+03 7.56E+03 6.57E+03 6.54E+03 4.54E+02 1.48E+04 1.74E+04 1.61E+04 1.62E+04 6.67E+02 BA 6.41E+03 1.04E+04 8.25E+03 8.22E+03 9.96E+02 1.44E+04 2.17E+04 1.64E+04 1.67E+04 1.39E+03 FPA 5.88E+03 8.30E+03 7.10E+03 7.02E+03 6.84E+02 1.23E+04 2.10E+04 1.47E+04 1.50E+04 1.74E+03 F12 ABC 1.50E 01 2.39E 01 2.03E 01 1.98E 01 2.40E 02 2.08E 01 3.44E 01 2.63E 01 2.60E 01 3.15E 02 FA 2.62E+00 3.87E+00 3.40E+00 3.36E+00 2.92E 01 3.56E+00 4.30E+00 4.06E+00 4.04E+00 1.96E 01 CS 6.78E 01 1.14E+00 8.83E 01 8.92E 01 1.27E 01 1.11E+00 1.93E+00 1.67E+00 1.66E+00 1.76E 01 BA 1.49E+00 4.91E+00 2.64E+00 2.74E+00 7.06E 01 2.49E+00 5.05E+00 3.47E+00 3.58E+00 5.54E 01 FPA 8.88E 01 2.11E+00 1.60E+00 1.62E+00 3.36E 01 1.60E+00 3.12E+00 2.58E+00 2.50E+00 3.37E 01 F13 ABC 2.76E 01 3.76E 01 3.18E 01 3.23E 01 2.67E 02 2.95E 01 3.92E 01 3.59E 01 3.58E 01 1.93E 02 FA 3.59E 01 7.43E+00 4.08E+00 3.75E+00 2.39E+00 4.48E 01 9.83E+00 6.21E+00 5.75E+00 3.42E+00 CS 2.94E 01 7.53E 01 4.62E 01 4.78E 01 9.66E 02 4.04E 01 6.60E 01 5.03E 01 5.19E 01 5.96E 02 BA 1.95E+00 8.04E+00 5.45E+00 5.68E+00 1.16E+00 3.28E+00 6.46E+00 5.33E+00 5.07E+00 9.13E 01 FPA 3.71E 01 7.47E 01 6.26E 01 6.17E 01 9.11E 02 5.03E 01 7.82E 01 6.10E 01 6.19E 01 6.23E 02 F14 ABC 1.90E 01 2.77E 01 2.43E 01 2.41E 01 2.29E 02 2.25E 01 2.93E 01 2.70E 01 2.67E 01 1.79E 02 FA 2.15E 01 3.24E+02 1.62E+02 1.54E+02 9.71E+01 2.16E 01 9.37E+02 2.51E+02 3.27E+02 2.96E+02 CS 2.61E 01 3.73E 01 3.14E 01 3.13E 01 3.16E 02 2.61E 01 3.94E 01 3.23E 01 3.26E 01 3.04E 02 BA 7.27E+01 4.31E+02 1.89E+02 2.05E+02 8.38E+01 8.95E+01 6.35E+02 3.10E+02 2.98E+02 1.35E+02 FPA 2.10E 01 9.88E 01 3.19E 01 3.53E 01 1.71E 01 2.33E 01 4.09E 01 3.24E 01 3.25E 01 3.86E 02 123 Empirical analysis of ve nature-inspired algorithms on real Table 5 continued 50D 100D Best Worst Median Mean SD Best Worst Median Mean SD F15 ABC 1.53E+01 2.21E+01 1.88E+01 1.88E+01 1.78E+00 4.78E+01 7.01E+01 5.70E+01 5.75E+01 6.14E+00 FA 3.49E+01 1.05E+07 2.26E+05 1.58E+06 2.67E+06 7.45E+01 9.66E+07 8.45E+06 2.54E+07 3.22E+07 CS 2.71E+01 7.68E+01 4.69E+01 4.75E+01 1.39E+01 1.12E+02 2.55E+02 1.62E+02 1.73E+02 3.79E+01 BA 1.95E+04 3.94E+06 3.37E+05 8.56E+05 1.09E+06 2.69E+04 5.48E+06 3.53E+05 8.32E+05 1.34E+06 FPA 4.88E+01 2.38E+02 1.00E+02 1.05E+02 3.97E+01 2.32E+02 1.08E+03 4.26E+02 4.68E+02 1.98E+02 F16 ABC 1.65E+01 1.88E+01 1.82E+01 1.81E+01 4.77E 01 3.89E+01 4.15E+01 4.04E+01 4.03E+01 6.97E 01 FA 2.13E+01 2.31E+01 2.26E+01 2.25E+01 4.19E 01 4.58E+01 4.74E+01 4.69E+01 4.67E+01 4.10E 01 CS 2.07E+01 2.20E+01 2.16E+01 2.16E+01 2.90E 01 4.33E+01 4.63E+01 4.54E+01 4.53E+01 5.94E 01 BA 2.21E+01 2.42E+01 2.34E+01 2.33E+01 4.75E 01 4.44E+01 4.84E+01 4.73E+01 4.72E+01 8.70E 01 FPA 2.02E+01 2.23E+01 2.17E+01 2.16E+01 4.90E 01 4.28E+01 4.62E+01 4.55E+01 4.53E+01 7.25E 01 123 P. Agarwal, S. Mehta Table 6 Best, worst, median, mean and SD of tness values of hybrid functions for 10D, 30D, 50D and 100D 10D 30D Best Worst Median Mean SD Best Worst Median Mean SD F17 ABC 2.67E+03 5.86E+05 1.89E+05 2.02E+05 1.55E+05 1.07E+06 4.58E+06 2.69E+06 2.75E+06 8.62E+05 FA 8.37E+02 1.05E+05 1.45E+04 3.12E+04 3.08E+04 2.91E+04 2.41E+07 4.14E+06 6.19E+06 6.14E+06 CS 1.07E+00 2.32E+01 1.17E+01 1.20E+01 6.37E+00 7.23E+02 2.10E+03 1.32E+03 1.37E+03 3.71E+02 BA 2.96E+03 1.02E+08 1.63E+06 9.09E+06 2.17E+07 6.22E+04 1.87E+08 2.30E+07 3.82E+07 4.64E+07 FPA 2.72E+01 1.40E+02 6.81E+01 7.25E+01 2.96E+01 5.74E+02 2.21E+03 1.29E+03 1.34E+03 4.57E+02 F18 ABC 7.90E+01 2.12E+03 3.15E+02 4.83E+02 4.31E+02 6.92E+01 1.24E+03 2.14E+02 3.73E+02 3.23E+02 FA 2.28E+03 4.42E+05 2.42E+04 5.51E+04 9.77E+04 9.42E+05 1.06E+09 2.94E+08 3.69E+08 2.97E+08 CS 1.11E 01 1.36E+00 3.41E 01 4.40E 01 3.14E 01 1.53E+01 1.18E+02 4.89E+01 5.56E+01 2.65E+01 BA 1.26E+02 3.52E+04 4.68E+03 8.57E+03 9.55E+03 4.49E+02 3.32E+09 1.50E+07 3.19E+08 6.80E+08 FPA 2.45E+00 2.01E+01 5.95E+00 6.72E+00 4.17E+00 1.19E+02 5.04E+02 2.51E+02 2.66E+02 8.86E+01 F19 ABC 1.81E 01 9.02E 01 4.53E 01 4.86E 01 1.92E 01 4.88E+00 8.76E+00 6.94E+00 6.86E+00 9.82E 01 FA 3.37E+00 8.18E+00 5.89E+00 5.82E+00 1.41E+00 1.86E+01 2.32E+02 7.65E+01 1.04E+02 7.51E+01 CS 2.46E 01 1.31E+00 6.83E 01 7.20E 01 2.89E 01 4.72E+00 9.32E+00 7.09E+00 7.37E+00 1.14E+00 BA 8.96E+00 2.17E+02 5.86E+01 7.05E+01 5.17E+01 1.27E+02 7.01E+02 3.55E+02 3.73E+02 1.27E+02 FPA 4.61E 01 4.52E+00 1.78E+00 1.94E+00 9.26E 01 6.09E+00 6.96E+01 1.01E+01 1.39E+01 1.52E+01 123 Empirical analysis of ve nature-inspired algorithms on real Table 6 continued 10D 30D Best Worst Median Mean SD Best Worst Median Mean SD F20 ABC 1.56E+01 1.42E+03 2.13E+02 2.47E+02 2.59E+02 2.92E+03 1.24E+04 7.08E+03 7.25E+03 2.48E+03 FA 6.41E+01 5.63E+03 1.20E+03 1.55E+03 1.47E+03 5.26E+02 1.41E+05 2.22E+04 3.76E+04 3.92E+04 CS 8.54E 02 1.48E+00 4.35E 01 5.49E 01 3.66E 01 2.37E+01 1.36E+02 4.91E+01 5.73E+01 2.50E+01 BA 1.63E+03 6.31E+06 2.51E+04 4.83E+05 1.28E+06 2.34E+04 2.35E+06 1.03E+05 2.64E+05 4.82E+05 FPA 3.08E+00 1.50E+01 7.39E+00 7.30E+00 2.86E+00 5.27E+01 5.62E+02 1.98E+02 2.19E+02 1.10E+02 F21 ABC 4.73E+02 7.04E+04 9.48E+03 1.37E+04 1.54E+04 3.46E+04 6.87E+05 2.18E+05 2.89E+05 1.82E+05 FA 5.89E+02 2.42E+04 4.79E+03 6.78E+03 5.95E+03 3.15E+04 6.13E+06 1.23E+06 1.84E+06 1.86E+06 CS 6.29E 02 9.99E 01 4.57E 01 4.65E 01 2.09E 01 1.29E+02 1.08E+03 5.29E+02 5.54E+02 2.23E+02 BA 6.34E+02 1.49E+07 1.44E+05 1.68E+06 3.25E+06 3.98E+04 2.45E+08 9.40E+06 2.59E+07 4.79E+07 FPA 7.92E 01 7.39E+01 6.74E+00 1.15E+01 1.50E+01 6.29E+01 1.27E+03 4.64E+02 4.94E+02 2.91E+02 F22 ABC 9.40E 02 1.25E+01 1.08E+00 2.10E+00 2.50E+00 3.65E+01 4.67E+02 3.12E+02 2.98E+02 9.02E+01 FA 3.36E+01 1.56E+02 6.94E+01 7.55E+01 3.35E+01 3.64E+02 1.34E+03 9.23E+02 8.63E+02 2.79E+02 CS 5.74E 01 2.10E+01 3.89E+00 4.81E+00 5.21E+00 3.03E+01 4.03E+02 2.49E+02 2.33E+02 1.07E+02 BA 1.54E+02 1.32E+03 5.02E+02 4.94E+02 2.27E+02 1.17E+03 1.05E+04 3.40E+03 3.61E+03 1.68E+03 FPA 1.19E+01 2.74E+01 2.43E+01 2.41E+01 2.69E+00 8.76E+01 5.04E+02 2.43E+02 2.54E+02 9.12E+01 123 P. Agarwal, S. Mehta Table 6 continued 50D 100D Best Worst Median Mean SD Best Worst Median Mean SD F17 ABC 1.10E+06 5.74E+06 2.81E+06 3.07E+06 1.11E+06 4.79E+06 1.19E+07 8.97E+06 8.99E+06 1.93E+06 FA 4.66E+05 1.34E+08 2.55E+07 3.64E+07 3.82E+07 9.58E+05 8.90E+08 4.55E+08 4.50E+08 2.62E+08 CS 3.16E+03 6.56E+04 1.19E+04 1.77E+04 1.38E+04 1.59E+05 2.42E+06 8.33E+05 8.62E+05 4.89E+05 BA 6.79E+05 4.79E+08 5.49E+07 8.78E+07 9.96E+07 1.12E+06 1.64E+08 2.48E+07 3.55E+07 3.51E+07 FPA 4.56E+03 5.96E+04 2.50E+04 2.65E+04 1.34E+04 2.51E+05 6.74E+05 4.68E+05 4.65E+05 1.29E+05 F18 ABC 2.28E+02 3.67E+03 1.16E+03 1.75E+03 1.35E+03 4.31E+02 2.21E+03 6.36E+02 8.40E+02 5.00E+02 FA 1.18E+07 6.00E+09 1.88E+09 2.37E+09 2.06E+09 2.26E+08 2.46E+10 8.01E+09 9.72E+09 8.67E+09 CS 1.30E+02 1.00E+10 1.00E+10 8.33E+09 3.79E+09 1.00E+10 1.00E+10 1.00E+10 1.00E+10 0.00E+00 BA 6.88E+02 9.07E+09 2.87E+08 1.48E+09 2.43E+09 1.18E+03 2.18E+09 3.04E+03 2.31E+08 6.53E+08 FPA 2.58E+02 6.85E+03 1.02E+03 1.84E+03 1.54E+03 3.59E+02 2.17E+04 2.15E+03 3.63E+03 5.03E+03 F19 ABC 1.20E+01 2.00E+01 1.68E+01 1.66E+01 1.77E+00 3.40E+01 1.07E+02 7.20E+01 6.80E+01 1.50E+01 FA 2.22E+01 8.84E+02 2.87E+02 3.35E+02 2.72E+02 4.58E+01 3.05E+03 1.43E+03 1.56E+03 1.01E+03 CS 9.29E+00 7.06E+01 1.90E+01 2.55E+01 1.52E+01 2.86E+01 1.70E+02 1.03E+02 1.05E+02 3.57E+01 BA 2.82E+02 1.99E+03 5.58E+02 6.29E+02 2.97E+02 6.04E+02 1.42E+03 8.64E+02 9.10E+02 2.16E+02 FPA 1.41E+01 8.76E+01 2.94E+01 4.53E+01 2.82E+01 7.01E+01 1.87E+02 1.40E+02 1.38E+02 3.66E+01 123 Empirical analysis of ve nature-inspired algorithms on real Table 6 continued 50D 100D Best Worst Median Mean SD Best Worst Median Mean SD F20 ABC 1.08E+04 4.34E+04 3.10E+04 2.98E+04 7.14E+03 5.74E+04 1.60E+05 1.14E+05 1.15E+05 2.52E+04 FA 4.25E+02 3.73E+05 6.31E+04 1.03E+05 1.16E+05 1.69E+03 4.40E+06 5.47E+05 9.71E+05 1.09E+06 CS 1.99E+02 1.13E+03 4.48E+02 5.19E+02 2.51E+02 1.13E+03 5.99E+03 1.94E+03 2.34E+03 1.13E+03 BA 4.27E+04 4.69E+05 9.22E+04 1.25E+05 1.03E+05 6.19E+04 2.49E+05 1.38E+05 1.36E+05 5.24E+04 FPA 4.60E+02 4.44E+03 1.37E+03 1.52E+03 8.96E+02 1.08E+04 3.77E+04 2.17E+04 2.15E+04 6.97E+03 F21 ABC 1.95E+05 6.49E+06 2.16E+06 2.19E+06 1.26E+06 2.33E+06 1.09E+07 5.64E+06 5.94E+06 1.98E+06 FA 2.43E+05 5.05E+07 1.86E+07 1.90E+07 1.49E+07 7.71E+04 3.93E+08 1.32E+08 1.62E+08 1.18E+08 CS 1.29E+03 8.09E+03 2.93E+03 3.30E+03 1.56E+03 2.79E+04 1.19E+06 2.02E+05 3.27E+05 3.04E+05 BA 1.74E+05 7.83E+07 4.32E+06 1.43E+07 2.15E+07 5.51E+05 4.68E+07 5.71E+06 1.14E+07 1.28E+07 FPA 2.12E+03 1.80E+04 6.56E+03 7.83E+03 4.45E+03 8.70E+04 7.89E+05 2.31E+05 2.78E+05 1.71E+05 F22 ABC 4.53E+02 1.09E+03 8.27E+02 8.00E+02 1.58E+02 1.27E+03 2.62E+03 2.08E+03 2.07E+03 2.83E+02 FA 6.91E+02 3.46E+03 2.41E+03 2.30E+03 7.94E+02 2.67E+03 2.34E+04 5.29E+03 6.88E+03 4.27E+03 CS 3.84E+02 1.22E+03 9.60E+02 9.18E+02 2.05E+02 1.24E+03 2.75E+03 2.32E+03 2.21E+03 3.88E+02 BA 4.04E+03 3.77E+04 6.49E+03 7.68E+03 5.95E+03 8.30E+03 1.26E+04 1.06E+04 1.05E+04 1.03E+03 FPA 3.97E+02 1.48E+03 9.04E+02 9.12E+02 2.33E+02 1.47E+03 2.95E+03 2.13E+03 2.13E+03 3.49E+02 123 P. Agarwal, S. Mehta Table 7 Best, worst, median, mean and SD of tness values of composition functions for 10D, 30D, 50D and 100D 10D 30D Best Worst Median Mean SD Best Worst Median Mean SD F23 ABC 1.55E 01 3.29E+02 3.29E+02 2.21E+02 1.39E+02 3.15E+02 3.16E+02 3.15E+02 3.15E+02 1.98E 01 FA 3.30E+02 3.80E+02 3.42E+02 3.45E+02 1.48E+01 3.15E+02 6.90E+02 4.31E+02 4.54E+02 1.21E+02 CS 0.00E+00 3.29E+02 3.29E+02 2.92E+02 1.02E+02 3.15E+02 3.15E+02 3.15E+02 3.15E+02 2.15E 13 BA 3.37E+02 9.98E+02 4.52E+02 4.79E+02 1.40E+02 3.42E+02 1.67E+03 5.91E+02 6.16E+02 2.40E+02 FPA 3.29E+02 3.29E+02 3.29E+02 3.29E+02 2.89E 13 3.15E+02 3.15E+02 3.15E+02 3.15E+02 3.15E 02 F24 ABC 1.12E+02 1.25E+02 1.20E+02 1.20E+02 3.91E+00 2.17E+02 2.29E+02 2.27E+02 2.27E+02 1.94E+00 FA 1.28E+02 2.03E+02 1.55E+02 1.57E+02 1.88E+01 2.18E+02 4.14E+02 3.24E+02 3.27E+02 5.58E+01 CS 1.09E+02 1.25E+02 1.17E+02 1.17E+02 4.17E+00 2.23E+02 2.36E+02 2.25E+02 2.26E+02 2.42E+00 BA 1.30E+02 2.51E+02 2.15E+02 2.12E+02 2.98E+01 3.02E+02 4.62E+02 3.66E+02 3.67E+02 4.02E+01 FPA 1.09E+02 1.32E+02 1.17E+02 1.18E+02 4.85E+00 2.25E+02 2.49E+02 2.30E+02 2.32E+02 6.37E+00 F25 ABC 1.23E+02 1.54E+02 1.38E+02 1.38E+02 6.80E+00 2.05E+02 2.10E+02 2.07E+02 2.07E+02 1.27E+00 FA 1.33E+02 2.04E+02 1.77E+02 1.72E+02 2.44E+01 2.03E+02 2.73E+02 2.37E+02 2.36E+02 2.05E+01 CS 1.09E+02 1.85E+02 1.29E+02 1.30E+02 1.38E+01 2.03E+02 2.08E+02 2.03E+02 2.03E+02 1.21E+00 BA 1.40E+02 2.39E+02 2.04E+02 2.02E+02 1.90E+01 2.26E+02 3.01E+02 2.47E+02 2.53E+02 2.09E+01 FPA 1.10E+02 1.90E+02 1.27E+02 1.32E+02 1.83E+01 2.00E+02 2.12E+02 2.04E+02 2.05E+02 2.93E+00 F26 ABC 1.00E+02 1.00E+02 1.00E+02 1.00E+02 4.30E 02 1.00E+02 1.01E+02 1.00E+02 1.00E+02 7.28E 02 FA 1.00E+02 2.00E+02 1.01E+02 1.04E+02 1.81E+01 1.00E+02 2.05E+02 1.03E+02 1.13E+02 3.06E+01 CS 1.00E+02 1.00E+02 1.00E+02 1.00E+02 5.66E 02 1.00E+02 1.00E+02 1.00E+02 1.00E+02 6.10E 02 BA 1.01E+02 2.02E+02 1.06E+02 1.21E+02 3.34E+01 1.08E+02 4.40E+02 1.97E+02 2.40E+02 1.31E+02 FPA 1.00E+02 1.01E+02 1.00E+02 1.00E+02 8.09E 02 1.00E+02 1.01E+02 1.01E+02 1.01E+02 1.15E 01 123 Empirical analysis of ve nature-inspired algorithms on real Table 7 continued 10D 30D Best Worst Median Mean SD Best Worst Median Mean SD F27 ABC 5.07E+00 4.01E+02 8.57E+00 2.21E+01 7.17E+01 4.03E+02 4.18E+02 4.07E+02 4.08E+02 2.93E+00 FA 1.36E+01 6.30E+02 1.51E+02 2.05E+02 1.62E+02 6.95E+02 1.27E+03 9.89E+02 9.92E+02 1.55E+02 CS 2.17E+00 4.00E+02 4.88E+00 1.66E+02 1.78E+02 4.00E+02 4.04E+02 4.01E+02 4.01E+02 6.59E 01 BA 6.04E+00 7.78E+02 6.92E+02 6.31E+02 1.83E+02 6.30E+02 1.97E+03 1.63E+03 1.53E+03 3.32E+02 FPA 1.81E+00 4.00E+02 4.10E+00 1.36E+02 1.90E+02 4.01E+02 9.00E+02 4.01E+02 4.45E+02 1.36E+02 F28 ABC 1.59E+02 3.84E+02 3.75E+02 3.69E+02 4.00E+01 8.99E+02 1.13E+03 9.93E+02 9.90E+02 4.95E+01 FA 3.92E+02 5.13E+02 4.40E+02 4.43E+02 3.18E+01 1.19E+03 5.03E+03 1.69E+03 1.86E+03 6.90E+02 CS 3.57E+02 3.81E+02 3.72E+02 3.73E+02 6.36E+00 8.55E+02 1.10E+03 9.47E+02 9.57E+02 6.47E+01 BA 4.54E+02 1.87E+03 8.07E+02 9.12E+02 3.81E+02 2.41E+03 6.71E+03 4.81E+03 4.66E+03 1.08E+03 FPA 3.57E+02 5.06E+02 3.78E+02 4.00E+02 4.72E+01 9.37E+02 2.22E+03 1.20E+03 1.34E+03 3.15E+02 F29 ABC 2.31E+02 4.51E+02 3.07E+02 3.12E+02 5.19E+01 8.42E+02 1.26E+03 1.06E+03 1.04E+03 1.04E+02 FA 4.35E+02 4.29E+06 1.87E+03 2.17E+05 8.63E+05 6.29E+03 2.42E+07 6.58E+06 7.79E+06 7.14E+06 CS 1.00E+02 2.24E+02 2.22E+02 1.96E+02 3.77E+01 7.19E+02 8.75E+02 7.53E+02 7.62E+02 4.06E+01 BA 2.02E+03 1.20E+06 7.11E+04 1.84E+05 3.40E+05 5.94E+04 3.00E+07 3.95E+05 2.01E+06 5.82E+06 FPA 1.86E+02 3.45E+02 2.34E+02 2.44E+02 2.92E+01 9.29E+02 2.58E+07 1.44E+03 2.88E+06 6.84E+06 F30 ABC 4.82E+02 8.43E+02 5.86E+02 6.16E+02 1.02E+02 1.54E+03 3.37E+03 2.18E+03 2.28E+03 5.05E+02 FA 7.62E+02 2.14E+03 1.46E+03 1.39E+03 3.70E+02 7.48E+03 6.12E+05 2.01E+05 2.27E+05 1.57E+05 CS 3.82E+02 5.22E+02 4.75E+02 4.71E+02 2.88E+01 6.54E+02 2.60E+03 1.14E+03 1.17E+03 3.56E+02 BA 7.54E+02 1.59E+06 5.24E+03 8.37E+04 2.90E+05 7.61E+03 7.45E+06 3.99E+05 9.35E+05 1.43E+06 FPA 4.67E+02 6.31E+02 5.04E+02 5.25E+02 4.92E+01 8.54E+02 6.30E+03 1.62E+03 2.39E+03 1.46E+03 123 P. Agarwal, S. Mehta Table 7 continued 50D 100D Best Worst Median Mean SD Best Worst Median Mean SD F23 ABC 3.44E+02 3.44E+02 3.44E+02 3.44E+02 3.92E 02 3.48E+02 3.50E+02 3.49E+02 3.49E+02 5.10E 01 FA 3.47E+02 1.54E+03 7.89E+02 8.78E+02 4.21E+02 3.51E+02 4.13E+03 1.81E+03 1.95E+03 1.13E+03 CS 3.44E+02 3.44E+02 3.44E+02 3.44E+02 2.89E 13 3.48E+02 3.48E+02 3.48E+02 3.48E+02 2.83E 13 BA 5.11E+02 1.34E+03 8.52E+02 8.84E+02 1.97E+02 4.64E+02 1.61E+03 7.01E+02 7.53E+02 2.60E+02 FPA 3.44E+02 3.45E+02 3.44E+02 3.44E+02 3.42E 01 3.49E+02 3.66E+02 3.51E+02 3.52E+02 4.01E+00 F24 ABC 2.57E+02 2.61E+02 2.58E+02 2.58E+02 9.49E 01 3.56E+02 3.65E+02 3.61E+02 3.60E+02 2.46E+00 FA 2.60E+02 6.74E+02 4.06E+02 4.56E+02 1.20E+02 2.60E+02 1.31E+03 7.68E+02 8.14E+02 3.03E+02 CS 2.56E+02 2.91E+02 2.77E+02 2.77E+02 6.29E+00 3.90E+02 4.54E+02 4.09E+02 4.12E+02 1.43E+01 BA 4.37E+02 7.82E+02 5.75E+02 5.78E+02 7.75E+01 7.69E+02 1.64E+03 1.07E+03 1.08E+03 1.96E+02 FPA 2.66E+02 3.20E+02 2.92E+02 2.90E+02 1.25E+01 3.97E+02 4.74E+02 4.38E+02 4.36E+02 1.68E+01 F25 ABC 2.10E+02 2.20E+02 2.15E+02 2.15E+02 2.13E+00 2.37E+02 2.60E+02 2.50E+02 2.49E+02 4.97E+00 FA 2.06E+02 4.27E+02 3.08E+02 3.01E+02 6.11E+01 2.05E+02 1.04E+03 6.44E+02 6.41E+02 2.68E+02 CS 2.05E+02 2.24E+02 2.08E+02 2.10E+02 4.93E+00 2.39E+02 2.78E+02 2.61E+02 2.61E+02 9.21E+00 BA 2.52E+02 4.32E+02 2.88E+02 2.92E+02 3.59E+01 3.17E+02 4.55E+02 3.65E+02 3.69E+02 3.66E+01 FPA 2.00E+02 2.38E+02 2.15E+02 2.14E+02 1.01E+01 2.11E+02 2.86E+02 2.50E+02 2.48E+02 1.71E+01 F26 ABC 1.00E+02 1.01E+02 1.01E+02 1.01E+02 1.22E 01 1.01E+02 2.03E+02 2.02E+02 1.89E+02 3.52E+01 FA 1.01E+02 2.04E+02 1.05E+02 1.11E+02 2.49E+01 1.07E+02 6.84E+02 2.23E+02 2.67E+02 1.85E+02 CS 1.00E+02 2.00E+02 1.00E+02 1.20E+02 4.06E+01 1.00E+02 2.00E+02 2.00E+02 1.90E+02 3.04E+01 BA 1.28E+02 6.24E+02 2.76E+02 3.52E+02 1.83E+02 1.23E+02 1.28E+03 2.67E+02 4.32E+02 3.25E+02 FPA 1.00E+02 2.00E+02 1.01E+02 1.07E+02 2.52E+01 1.01E+02 2.01E+02 2.00E+02 1.87E+02 3.40E+01 123 Empirical analysis of ve nature-inspired algorithms on real Table 7 continued 50D 100D Best Worst Median Mean SD Best Worst Median Mean SD F27 ABC 4.10E+02 1.32E+03 1.18E+03 9.66E+02 3.65E+02 4.32E+02 2.59E+03 2.45E+03 2.13E+03 7.63E+02 FA 1.13E+03 2.25E+03 1.86E+03 1.77E+03 3.42E+02 1.97E+03 4.76E+03 3.97E+03 3.60E+03 9.31E+02 CS 4.01E+02 1.63E+03 1.43E+03 1.22E+03 4.50E+02 2.73E+03 3.42E+03 3.08E+03 3.06E+03 1.28E+02 BA 2.39E+03 3.81E+03 2.61E+03 2.66E+03 2.56E+02 4.64E+03 1.11E+04 5.12E+03 5.36E+03 1.15E+03 FPA 4.03E+02 1.78E+03 1.56E+03 1.50E+03 2.42E+02 2.94E+03 3.72E+03 3.30E+03 3.31E+03 2.09E+02 F28 ABC 1.43E+03 2.38E+03 1.86E+03 1.85E+03 2.17E+02 4.07E+03 7.58E+03 6.27E+03 6.23E+03 7.27E+02 FA 1.62E+03 7.45E+03 3.71E+03 4.27E+03 1.69E+03 3.69E+03 1.99E+04 1.35E+04 1.32E+04 5.43E+03 CS 1.28E+03 2.95E+03 1.52E+03 1.68E+03 4.09E+02 2.85E+03 8.49E+03 5.61E+03 5.45E+03 1.39E+03 BA 6.19E+03 1.50E+04 1.02E+04 1.05E+04 2.35E+03 1.83E+04 3.51E+04 2.50E+04 2.58E+04 4.47E+03 FPA 1.84E+03 3.85E+03 2.51E+03 2.61E+03 5.55E+02 5.41E+03 1.11E+04 7.21E+03 7.26E+03 1.42E+03 F29 ABC 9.56E+02 2.64E+03 1.60E+03 1.61E+03 3.48E+02 1.65E+03 5.51E+03 4.15E+03 4.01E+03 1.08E+03 FA 3.00E+05 2.88E+08 5.60E+07 9.97E+07 1.06E+08 5.02E+05 1.56E+09 4.92E+08 6.68E+08 5.87E+08 CS 9.07E+02 4.32E+07 1.29E+03 2.78E+06 1.06E+07 1.36E+03 1.83E+08 9.63E+07 7.03E+07 6.10E+07 BA 1.69E+05 8.15E+08 2.44E+06 4.80E+07 1.65E+08 4.55E+05 1.28E+08 1.90E+06 7.87E+06 2.35E+07 FPA 1.39E+03 4.70E+08 8.70E+07 9.36E+07 1.03E+08 4.32E+07 1.24E+09 2.63E+08 3.50E+08 2.63E+08 F30 ABC 8.52E+03 1.34E+04 1.01E+04 1.01E+04 1.09E+03 1.24E+04 3.20E+04 1.99E+04 2.03E+04 4.19E+03 FA 2.71E+04 4.32E+06 6.24E+05 1.27E+06 1.39E+06 4.49E+05 6.36E+07 2.58E+07 2.20E+07 2.06E+07 CS 7.88E+03 1.05E+04 9.08E+03 8.98E+03 7.40E+02 6.05E+03 1.19E+04 9.37E+03 9.26E+03 1.72E+03 BA 2.51E+05 2.03E+07 4.61E+06 6.09E+06 5.98E+06 1.96E+05 7.92E+07 6.81E+06 1.44E+07 1.91E+07 FPA 8.76E+03 3.28E+04 1.34E+04 1.46E+04 4.82E+03 8.58E+03 4.70E+04 1.69E+04 1.93E+04 9.36E+03 123 P. Agarwal, S. Mehta Table 8 Wilcoxon rank sum test analysis: (a) ABC, (b) CS 10D 30D 50D 100D 10D 30D 50D 100D (a) Versus ABC (b) Versus CS FA FA Better (+) 1 0 0 0 Better (+) 1 0 2 0 Worse ( ) 28 30 29 29 Worse ( ) 28 29 28 27 No sig diff 1 0 1 1 No sig diff 1 1 0 3 CS ABC Better (+) 14 16 9 7 Better (+) 10 13 19 21 Worse ( ) 10 13 19 21 Worse ( ) 14 16 9 7 No sig diff 6 1 2 2 No sig diff 6 1 2 3 BA BA Better (+) 0 1 2 2 Better (+) 1 1 3 3 Worse ( ) 29 28 28 26 Worse ( ) 28 28 27 24 No sig diff 1 1 0 2 No sig diff 1 1 0 3 FPA FPA Better (+) 11 7 4 5 Better (+) 0 1 5 8 Worse ( ) 15 20 23 20 Worse ( ) 25 23 21 17 No sig diff 4 3 3 5 No sig diff 5 6 4 5 4.1 Wilcoxon rank sum test analysis Wilcoxon rank sum was performed to validate the statistical signi cance of results over the mean error values of each algorithm. For each function, rank sum p is computed for two compared algorithms over 51 runs. Accordingly, p value de nes the probability of difference between mean values of algorithms that occur purely by chance. In this work, the difference between the errors is considered to be signi cant at 5% level of con dence. Mean of 30 error values is calculated for compared algorithm (mean_error_value1) versus ABC/CS (mean_error_value2). Pseudocode of computing statistical difference is given below: Algorithm 6: Algorithm for Wilcoxon rank sum test 1. Input: p value, mean_error_value1, mean_error_value2 2. if p < 0.05 and mean_error_value1 < mean_error_value2 3. return significantly better (+) 4. else if p < 0.05 and mean_error_value1> mean_error_value2 5. return significantly worse ( ) 6. else 7. return no significant difference ( ) Wilcoxon rank sum test shown in Table 8a, b summarize the experimental results of every algorithm over 10, 30, 50 and 100 dimensions. It was observed from Tables 4, 5, 6 and 7 that ABC and CS gives best mean error values for majority of the functions. Hence, Wilcoxon rank sum test is rst performed against ABC (Table 8a) and then against CS algorithm (Table 8b). When the statistical evaluation is made versus ABC algorithm, it can be noticed that ABC perform better than FA for 28, 30, 29 and 29 functions on 10, 30, 50 and 100 dimensions respectively out of total 30 CEC2014 benchmark functions. Similarly, ABC also gives superior performance against BA and FPA algorithms. However, when comparison is 123 Empirical analysis of ve nature-inspired algorithms on real made against CS, ABC gives better performance on 50 and 100 dimensions but worse at 10 and 30 dimensions. This fact is observed from Table 8a It shows CS is better on total 14 and 16 functions at 10 and 30 dimensions respectively. However it is worse than ABC algorithm on total 19 and 21 functions at 50 and 100 dimensions respectively. Hence this test was performed with respect to CS (as shown in Table 8b). It can be observed that for 10 and 30 dimensions CS is better than FA (28 and 29 functions), ABC (14 and 16 functions), BA (28 and 28 functions) and FPA (25 and 23 functions). However, when CS was compared with ABC on 50 and 100 (high) dimensions, ABC was found better on 19 and 21 functions respectively. Hence, from Wilcoxon rank sum test analysis, it can be observed that both ABC and CS depict relatively competitive performance against BA, FPA and FA. However for small dimensions, CS seems to be the better choice and for large dimensions ABC may suit better. In order to further substantiate these results, time complexity of these algorithms is analyzed in the next section. 4.2 Time complexity analysis Time complexity is the computational time taken by each algorithm to execute a particular benchmark function. As per CEC2014, computational time complexity of F18 benchmark function is computed using Eq. (17) Time complexity = T 2 T 1/T 0 (17) where T0 is the time required for executing the test program (given below) de ned in CEC2014 (Liang et al. 2014), T1 is the time required for executing F18 function, is the mean time of 5 independent runs taken by each algorithm to execute F18 function. The pseudo code of test program is shown below: Algorithm 7: Test program for time complexity 1. for i=1:106 2. x = 0.55 + i; x = x + x; x = x = x/2; x=x2; 3. x = sqrt(x); x = log(x); x = exp(x); x = x/(x+2); 4. end for For both T1 and T2, number of iterations at each dimension is xed to 200,000 (Liang et al. 2014). Table 9 shows computational complexity of all ve algorithms viz. measured in terms of T0, T1, and at 10, 30, 50 and 100 dimensions. RuntimegiveninTable9ismeasuredinseconds.Ithasbeenobservedthattimecomplexity ofBAisminimumamongall vealgorithms.ABCtakessecondposition,FPAthird,CSfourth and FA fth. Though BA completes its iterations in least time but it is unable to attain optimal value.ABCtakessecondpositionintermsoftimecomplexityandattainsbestoptimalsolution as compared to other algorithms (as seen in previous section). Since computational time of CS is quite high (approximately 70% more than ABC) and also escalates with the increase in number of dimensions; it can be inferred that ABC is the most appropriate algorithm for complex optimization problems. FPA is next to ABC. These results are further established using run length distribution in the next section. 123 P. Agarwal, S. Mehta Table 9 Comparison of 5 nature-inspired algorithms on the basis of run time complexity T0 T1 ABC CS FPA FA BA T 2 T 2 T 1 T 0 T 2 T 2 T 1 T 0 T 2 T 2 T 1 T 0 T 2 T 2 T 1 T 0 T 2 T 2 T 1 T 0 D = 10 0.14 1.49 4.11 18.21 5.52 27.94 4.12 18.21 7.68 42.95 2.51 7.05 D = 30 1.65 4.31 18.48 6.63 34.56 4.51 19.87 8.18 45.37 2.85 8.37 D = 50 1.97 4.7 18.96 8.21 43.31 5.03 21.25 9.27 50.72 3.28 9.08 D = 100 3.5 6.25 19.15 13.12 66.82 6.81 23.01 12.55 62.86 4.94 10.03 123 Empirical analysis of ve nature-inspired algorithms on real Table 10 Result analysis of minimum mean error value on unimodal function Functions Best algorithm on lower dimension Best algorithm on higher dimension (100D) 10D 30D 50D F1 CS CS CS ABC F2 CS ABC ABC ABC F3 CS CS CS CS 4.3 Run length distribution analysis Performance of ABC, FA, CS, BA and FPA were compared in terms of their convergence speed. For visualizing the convergence rate of algorithms, convergence graphs are plotted in the form of line graph at 100 dimensions representing mean error value of each algorithm over 51 runs on xed number of iterations. The x axis represents iteration number which varies from 0 to 1,000,000. The y axis represent log of mean error value. Since CEC2014 benchmark functions are divided into four sub groups, the analysis in sub sections are made on basis of these subgroups. 4.3.1 Unimodal functions Unimodal functions are the functions with one global optimum. Results of algorithms includ- ing ABC, FA, CS, BA and FPA are compared on the basis of their best, worst, mean, median and standard deviation error values on unimodal functions as shown in Table 4. Table 10 presents the algorithms achieving minimum mean error values at lower and higher dimen- sions. F1, F2 and F3 are unimodal non-separable functions where F1 has very high number of quadratic conditions, F2 has polished but slender ridge and F3 is narrow sensitivity at one direction. Results depict that CS gives better outcome among all ve algorithms on small dimensions for majority of the functions. Additionally, CS has high ef cacy on the function which is sensitive at one direction only i.e. F3. However on higher dimension ABC gives best values for majority of functions. At higher dimension, further analysis of all ve algorithms is made on basis of convergence graph. For an algorithm to be best, it should have overall better convergence speed. This parameter is analyzed on basis of run length distribution (RLD). RLD of unimodal functions F1, F2 and F3 are represented in Figs 1, 2 and 3 respectively. Fig. 1 RLD of algorithms for F1 123 P. Agarwal, S. Mehta Fig. 2 RLD of algorithms for F2 Fig. 3 RLD of algorithms for F3 On F1 and F2, ABC algorithm depicts fast convergence speed as compared to other algorithms as it achieves its minima in 1.00E+06 and 2.00E+05 iterations respectively. Although on F2 function, algorithm attempts to diversify its search space but reach at same optima over the time. On F3 function, CS shows a linear decrement in error value i.e. good convergence speed among all ve algorithms and reaches minimum error value in 1.00E+06 iterations. Though difference in minimum error value of ABC and CS is quite high but ABC reaches its global solution at 3.00E+05 iterations. This might be due to function s property that is skewed in one direction. Also on F1 and F2 function CS converges at a very early stage but its minima is far from global minimum value. FPA is the second best algorithm after ABC. At F1 function, FPA convergence speed is approximately same as that of ABC while on F2 and F3 function, FPA is the second best in terms of convergence speed and minimum error value. It has been observed from above analysis that ABC algorithm is the best algorithm in terms of mean error value at high dimension. Next to ABC, FPA shows uniform behavior throughout all unimodal function. While at lower dimension, CS is the best algorithm. Thus, depending on requirement of application appropriate algorithm can be chosen for achieving desired result. 4.3.2 Simple multimodal functions Multimodal functions are the functions having one or more than one global optima. Table 5 represents error values of ve nature-inspired algorithms on simple multimodal functions of CEC2014 benchmark suite. Best, mean, median, worst and SD values for 51 runs are depicted. Analysis of mean values on simple multimodal functions at lower and higher dimension is mentioned in Table 11. ABC algorithm proved to be the best algorithm on majority of simple multimodal functions. 123 Empirical analysis of ve nature-inspired algorithms on real Table 11 Result analysis of minimum mean error value on simple multimodal function Functions Best algorithm on lower dimension Best algorithm on higher dimension (100D) 10D 30D 50D F4 ABC CS ABC ABC F5 ABC BA BA BA F6 BA ABC ABC ABC F7 ABC ABC ABC ABC F8 ABC/CS ABC ABC ABC F9 ABC ABC ABC FPA F10 ABC ABC ABC ABC F11 ABC ABC ABC ABC F12 CS ABC ABC ABC F13 ABC ABC ABC ABC F14 ABC ABC ABC ABC F15 CS ABC ABC ABC F16 ABC ABC ABC ABC It can be precisely observed that ABC algorithm is the best performer on small dimensions. While on higher dimension further analysis of algorithms is made on the basis of RLD. RLD is represented by line graphs shown in Figs. 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15 and 16. It can be observed that in most of the cases convergence speed of ABC algorithm in achieving minimum error value is faster than any other algorithm. It attains global optima at 1.00E+06 and 7.00E+05 iterations on F4 and F6 functions respectively. ABC algorithm tries to explore more search space after achieving best value at 5.00E+05 and 7.00E+05 iterations on F7 and F8 functions respectively. ABC algorithm has ability to explore more search space for obtaining global optima. On F10 function, performance of ABC algorithm improves in steps over the iterations. It attains its minima at 9.00E+05 iterations and settles down after that. This might be due to some signi cant characteristics of functions such as F8 and F10 which are shifted and separable functions with large number of local optima. The behavior of ABC algorithm may be due to the use of real random integer variable in exploiting neighbor food sources or type of objective function used. On F11, F13, F14, F15 and F16 functions, i.e. multimodal, shifted, rotated and non separable, ABC algorithm achieves minimum mean error value in early stages i.e. at 6.00E+05, 2.00E+05, 3.00E+04, 2.00E+05 and 7.00E+05 iterations respectively. On F5 function, ABC performs next to BA though difference in mean error value is very small i.e. ABC lags by 1.5% only. ABC achieves its minima at 8.00E+05 iterations. On F9 function viz. non separable with large number local optima, though FPA achieves best mean error value but ABC converges earlier than FPA. ABC attains its minima at 6.00E+05 iterations while FPA at 1.00E+06 iterations. ABC lags FPA by approximately 15% in terms of minimum mean error value. After ABC, it is CS which achieves minimum mean error value on majority of multimodal functions. Thus, it can be observed from above analysis that for simple multimodal functions, ABC algorithm is the best algorithm for attaining minimum error value. CS takes second position while FPA is on third position. 123 P. Agarwal, S. Mehta Fig. 4 RLD of algorithms for F4 Fig. 5 RLD of algorithms for F5 Fig. 6 RLD of algorithms for F6 Fig. 7 RLD of algorithms for F7 123 Empirical analysis of ve nature-inspired algorithms on real Fig. 8 RLD of algorithms for F8 Fig. 9 RLD of algorithms for F9 Fig. 10 RLD of algorithms for F10 Fig. 11 RLD of algorithms for F11 123 P. Agarwal, S. Mehta Fig. 12 RLD of algorithms for F12 Fig. 13 RLD of algorithms for F13 Fig. 14 RLD of algorithms for F14 Fig. 15 RLD of algorithms for F15 123 Empirical analysis of ve nature-inspired algorithms on real Fig. 16 RLD of algorithms for F16 Table 12 Result analysis of minimum mean error value on hybrid function Hybrid function Best algorithm on lower dimension Best algorithm on higher dimension (100D) 10D 30D 50D F17 CS FPA CS FPA F18 CS CS ABC ABC F19 ABC ABC ABC ABC F20 CS CS CS CS F21 CS FPA CS FPA F22 ABC CS ABC ABC 4.3.3 Hybrid functions In hybrid functions, variables are decomposed into several subcomponents. Each subcompo- nent is associated with different basic function. Generally, hybrid functions are unimodal or multimodal constituting non separable functions with speci c traits. Table 6 represents best, mean, median, worst and standard deviation values of hybrid function on 51 runs. Table 12 scrutinize Table 6 and represents algorithms having minimum mean error values on lower and higher dimension on each function. It has been observed from Table 12 that CS is the better performer as compared to other algorithms on small dimensions while on high dimensions results need more analysis. To illustrate algorithms reliability on high dimension, further analysis is made through conver- gence graphs. Run length distribution of all ve algorithms on hybrid functions at high dimensions is de ned by line graphs as shown in Figs. 17, 18, 19, 20, 21 and 22. It has been analyzed that ABC is the best performer on F18, F19 and F22 functions. ABC portrays fast convergence speed and attains minima at 9.00E+05 iterations on F18 and F19 functions. It shows early convergenceat7.00E+05iterationsonF22function.OnF17andF21functionsviz.separable, non separable, ill conditioned and huge number of local optima, though FPA depicts best performance, CS converges very close to FPA. ABC converges at around 7.00E+05 iterations while FPA and CS both converge at 1.00E+06 iterations. On F20 function, CS algorithm performance continuously improves throughout the iterations and reaches its global minima at 1.00E+06 iterations while ABC attains minima at 4.00E+05 iterations. CS stagnates to local optima on majority of hybrid functions. Thus, it can be observed that due to different 123 P. Agarwal, S. Mehta Fig. 17 RLD of algorithms for F17 Fig. 18 RLD of algorithms for F18 Fig. 19 RLD of algorithms for F19 phases of bees in optimizing objective function, ABC algorithm obtain fast convergence rate on most of the functions. Hence from above investigation and analysis it can be established that ABC is the best algorithm at higher dimensions on hybrid functions in terms convergence speed and minimum mean error value. FPA is the second best while CS attains next position after FPA. 4.3.4 Composition function Composition functions (F23 F30) are composed of various basic functions. These functions consist of various components of variables possessing different basic functions. Table 13 investigates Table 7 for acquiring best algorithm at each function on the basis of mean error values obtained over 51 runs. It has been observed that for small dimensions CS gives the most promising result on majority of composition functions. While on high dimension, ABC and CS both depict good performances. Hence there is tradeoff between ABC and CS on high dimension. In order to establish the results at high dimension and obtain the most suitable 123 Empirical analysis of ve nature-inspired algorithms on real Fig. 20 RLD of algorithms for F20 Fig. 21 RLD of algorithms for F21 Fig. 22 RLD of algorithms for F22 algorithm for composition functions, further analysis is made on run length distribution (RLD). RLD of algorithms on composition functions are analyzed for determining the most tted algorithm among the existing algorithms in terms of convergence speed. RLD of composition functions are represented by line graphs shown in Figs. 23, 24, 25, 26, 27, 28, 29 and 30. On functions F24, F27 and F29, having different properties at different subcomponents, ABC depicts minimum mean error value as compared to other algorithms. However, on F27 andF29,convergenceofABCalgorithmisat7.00E+05and6.00E+05iterationsrespectively. ABC is second best with small difference on F23, F25, F26, F28 and F30 functions. On F23 and F28 functions viz. is composed of different functions having asymmetry, multimodal and non separable properties at various local optima, CS attains best mean error value at 1.00E+06 iterations while ABC achieves its global optima at 9.00E+05 and 8.00E+05 iterations respectively. Moreover, in terms of mean error values ABC lags by 0.3 and 14% only on F23 and F28 functions respectively. On F25 and F26 functions, FPA shows minimum mean error value and succeeds ABC by 4.4 and 1.5% only. The performance of ABC, CS and 123 P. Agarwal, S. Mehta Table 13 Result analysis of minimum mean error value on composition function Composition function Best algorithm on lower dimension Best algorithm on higher dimension (100D) 10D 30D 50D F23 ABC ABC/CS/FPA ABC/CS/FPA CS F24 CS CS ABC ABC F25 CS CS CS FPA F26 ABC/CS/FPA ABC/CS ABC FPA F27 ABC CS ABC ABC F28 ABC CS CS CS F29 CS CS ABC ABC F30 CS CS CS CS Fig. 23 RLD of algorithms for F23 Fig. 24 RLD of algorithms for F24 FPA on these functions is almost same. Hence, it can be inferred that ABC portrays better performance for majority of composition functions and CS is the second best algorithm. This is due to the fact that CS uses some probability to abandon host nest and its update equations gives excellent exploration and exploitation capabilities to avoid local optima. ABC uses 3 stages i.e. employed, onlooker and scout bee phase to explore quality solutions. Therefore, it can be substantiated from above study that ABC algorithm is the most appropriate algorithm for composition functions on high dimensions. On the whole, analysis of contemporary nature-inspired algorithms on CEC2014 bench- mark functions establishes that ABC algorithm performs best for more or less all kinds of functions. Both CS and FPA attain the second position as FPA performed better than CS on unimodal and hybrid functions while CS performed better for multimodal and composite functions at high dimension. Hence, ABC, FPA and CS all perform better than BA and FA 123 Empirical analysis of ve nature-inspired algorithms on real Fig. 25 RLD of algorithms for F25 Fig. 26 RLD of algorithms for F26 Fig. 27 RLD of algorithms for F27 Fig. 28 RLD of algorithms for F28 for all kinds of functions. However, when analysis is made dimension wise, then CS is the best choice for small dimension while ABC should be chosen for high dimension problems. From time complexity analysis, it has been observed that ABC takes least time as compared 123 P. Agarwal, S. Mehta Fig. 29 RLD of algorithms for F29 Fig. 30 RLD of algorithms for F30 to CS and FPA. FPA is second to ABC and CS takes third position. The study also presented the best value of input parameters required for various algorithms. 5 Conclusion The work presents the comprehensive analysis of ve contemporary self-adaptive nature- inspired algorithms i.e. ABC, FA, FPA, CS and BA on standard CEC2014 benchmark functions. The algorithms are compared on the basis of error values computed from t- ness function values on unimodal, simple multimodal, hybrid and composition functions at 10, 30, 50 and 100 dimensions. Analysis over minimum error value attained, computational time complexity and convergence speed depicts that ABC is the most suitable algorithm for majority of the functions. ABC is followed by CS and FPA in terms of minimum error value attained. FA and BA attain second last and last position in terms of ef ciency. This study also provides the best values of control parameters on each function for various algorithms. Future work relies on numerous routes for ful lling objective of developing best nature- inspired algorithm. The paper is helpful to design hybrid nature-inspired algorithm by combining strengths of various nature-inspired algorithms i.e. arti cial bee colony algorithm, cuckoo search algorithm and ower pollination algorithm. Algorithms can also be tested on higher dimensions (200, 500 1000) to overcome the problem of curse of dimensionality. Future work could be evaluating the ef cacy these algorithms on real world problems. Appendix See Tables 14 and 15. 123 Empirical analysis of ve nature-inspired algorithms on real Table 14 Parameter values of FA, CS, BA and FPA on 10, 30, 50 and 100 dimensions 10D 30D FA CS BA FPA FA CS BA FPA Alpha pa Pulse Loud p Alpha pa Pulse Loud p F1 Mean 0.2722759 0.2203437 1.87E 01 1.67E 05 0.799698 0.2708551 0.209241 1.16E 01 4.12E 06 0.7908566 SD 0.1709817 0.0446601 1.19E 01 2.48E 05 0.0599293 0.1552514 0.041758 1.08E 01 3.74E 06 0.0582901 F2 Mean 0.3464869 0.2294014 2.23E 01 1.27E 05 0.7954489 0.3123352 0.2349083 1.80E 01 3.95E 06 0.795433 SD 0.1972742 0.0486367 1.39E 01 1.21E 05 0.0580095 0.19803 0.043733 1.30E 01 4.91E 06 0.0565739 F3 Mean 0.3472604 0.2217781 1.76E 01 1.27E 05 0.8094212 0.3104254 0.2335315 1.99E 01 4.11E 06 0.8278387 SD 0.1791352 0.0484594 1.09E 01 1.21E 05 0.0538456 0.1829587 0.0375302 1.38E 01 3.05E 06 0.0522796 F4 Mean 0.3324723 0.2252056 1.85E 01 2.00E 05 0.8087294 0.3118529 0.2208041 1.79E 01 4.39E 06 0.7890279 SD 0.1744634 0.0448074 1.38E 01 2.32E 05 0.0541805 0.1781723 0.0426966 1.53E 01 4.18E 06 0.0601704 F5 Mean 0.3135091 0.2209748 1.89E 01 1.25E 05 0.7960855 0.3395496 0.2241715 2.18E 01 3.45E 06 0.7812525 SD 0.1867189 0.0432804 1.58E 01 1.28E 05 0.0598864 0.1437103 0.047512 1.54E 01 3.47E 06 0.0602944 F6 Mean 0.2916939 0.2349488 1.56E 01 1.36E 05 0.8108871 0.2833266 0.2200663 1.55E 01 4.20E 06 0.8001092 SD 0.1883532 0.0452396 1.39E 01 1.14E 05 0.0609888 0.176607 0.0410609 1.46E 01 4.24E 06 0.0588005 F7 Mean 0.2845395 0.2409324 1.59E 01 1.05E 05 0.7900857 0.3740304 0.2064326 2.23E 01 5.96E 06 0.8038736 SD 0.1900076 0.0466053 1.07E 01 9.93E 06 0.0536948 0.1642933 0.0403357 1.54E 01 6.61E 06 0.0601343 123 P. Agarwal, S. Mehta Table 14 continued 10D 30D FA CS BA FPA FA CS BA FPA Alpha pa Pulse Loud p Alpha pa Pulse Loud p F8 Mean 0.3341264 0.2353938 1.94E 01 1.25E 05 0.7996712 0.2776111 0.2261034 2.26E 01 4.38E 06 0.7922069 SD 0.1959416 0.0397055 1.50E 01 1.08E 05 0.0509077 0.1785557 0.0416991 1.32E 01 4.59E 06 0.0487556 F9 Mean 0.3335126 0.2361646 2.37E 01 1.70E 05 0.7862518 0.2593791 0.2267338 1.95E 01 4.70E 06 0.7904442 SD 0.1887614 0.047618 1.53E 01 1.71E 05 0.0515379 0.1716209 0.0450413 1.51E 01 4.85E 06 0.0598589 F10 Mean 0.3047874 0.2217943 2.09E 01 1.47E 05 0.7978236 0.3294502 0.2153027 2.06E 01 4.39E 06 0.8009308 SD 0.1878902 0.0386907 1.41E 01 1.64E 05 0.0580561 0.1784438 0.0409442 1.44E 01 3.54E 06 0.0641542 F11 Mean 0.3119762 0.229196 2.43E 01 1.22E 05 0.8062877 0.3438108 0.2042404 2.28E 01 5.02E 06 0.8014943 SD 0.1739064 0.0434677 1.25E 01 1.57E 05 0.0566122 0.1832204 0.0412947 1.54E 01 5.74E 06 0.0570324 F12 Mean 0.4053215 0.2225225 2.41E 01 1.20E 05 0.8040599 0.3083517 0.2328392 2.34E 01 4.04E 06 0.8028371 SD 0.1739284 0.0480133 1.72E 01 1.57E 05 0.0589029 0.19227 0.03875 1.41E 01 4.66E 06 0.0596218 F13 Mean 0.3523106 0.230307 1.79E 01 9.44E 06 0.8059356 0.2992677 0.2200634 2.17E 01 4.80E 06 0.8084678 SD 0.1789443 0.0465486 1.35E 01 6.21E 06 0.0604162 0.1774537 0.0435522 1.56E 01 3.59E 06 0.051094 F14 Mean 0.2894146 0.2280216 1.95E 01 1.53E 05 0.7871595 0.3257827 0.2292328 2.13E 01 5.57E 06 0.7987165 SD 0.1792255 0.0339184 1.36E 01 1.42E 05 0.0540209 0.2128183 0.0421436 1.56E 01 4.09E 06 0.0555462 123 Empirical analysis of ve nature-inspired algorithms on real Table 14 continued 10D 30D FA CS BA FPA FA CS BA FPA Alpha pa Pulse Loud p Alpha pa Pulse Loud p F15 Mean 0.3472586 0.2219359 2.23E 01 1.32E 05 0.8000088 0.2924333 0.2169551 2.12E 01 5.35E 06 0.7932415 SD 0.1918128 0.0466316 1.37E 01 1.22E 05 0.0544308 0.1842373 0.0416351 1.41E 01 7.03E 06 0.0567152 F16 Mean 0.3148748 0.2303006 1.91E 01 1.13E 05 0.7981206 0.314106 0.2287634 2.25E 01 4.25E 06 0.8066692 SD 0.1992437 0.0390951 1.03E 01 7.04E 06 0.0598458 0.1941745 0.0325763 1.57E 01 5.62E 06 0.060194 F17 Mean 0.283616 0.2209095 1.91E 01 1.84E 05 0.8134361 0.2655929 0.2172352 2.36E 01 4.70E 06 0.8011771 SD 0.1786621 0.0403428 1.44E 01 1.70E 05 0.0470319 0.1832261 0.049514 1.48E 01 4.69E 06 0.062107 F18 Mean 0.2936042 0.2294665 1.54E 01 1.09E 05 0.776639 0.341312 0.2253135 1.67E 01 3.92E 06 0.7897658 SD 0.1717042 0.0437783 1.22E 01 1.20E 05 0.0531085 0.1607757 0.0427463 1.33E 01 4.42E 06 0.0490891 F19 Mean 0.3373163 0.2248058 2.08E 01 1.48E 05 0.7801876 0.2891295 0.2256333 2.03E 01 5.87E 06 0.7853949 SD 0.1657161 0.0387517 1.50E 01 1.78E 05 0.0593308 0.1793093 0.0430601 1.33E 01 6.63E 06 0.0457032 F20 Mean 0.3225599 0.2182741 2.10E 01 1.30E 05 0.7955768 0.3263194 0.2237305 2.16E 01 3.52E 06 0.7953552 SD 0.1691737 0.042149 1.45E 01 1.28E 05 0.061918 0.1801289 0.0509083 1.46E 01 3.89E 06 0.0542799 F21 Mean 0.3515683 0.2220562 1.44E 01 1.32E 05 0.7851544 0.283982 0.2286066 2.45E 01 4.07E 06 0.7965594 SD 0.1789988 0.0402909 1.23E 01 1.20E 05 0.060318 0.2037364 0.0463232 1.54E 01 4.99E 06 0.0566243 123 P. Agarwal, S. Mehta Table 14 continued 10D 30D FA CS BA FPA FA CS BA FPA Alpha pa Pulse Loud p Alpha pa Pulse Loud p F22 Mean 0.3301514 0.2200404 2.01E 01 9.07E 06 0.78487 0.3255258 0.223818 1.98E 01 3.56E 06 0.7988331 SD 0.1782583 0.0439009 1.39E 01 7.51E 06 0.0534909 0.215953 0.0418699 1.68E 01 3.36E 06 0.0684991 F23 Mean 0.2750546 0.23059 1.61E 01 9.56E 06 0.7843265 0.291003 0.2253814 2.00E 01 3.64E 06 0.7890458 SD 0.1700449 0.0396458 1.18E 01 7.41E 06 0.0566802 0.1881174 0.0466609 1.40E 01 3.62E 06 0.0549653 F24 Mean 0.3113061 0.2311845 1.80E 01 1.29E 05 0.8043415 0.3381634 0.2274586 2.03E 01 4.59E 06 0.7933631 SD 0.1615548 0.047452 1.23E 01 1.62E 05 0.0585762 0.1823287 0.0480847 1.57E 01 4.88E 06 0.0594301 F25 Mean 0.3256166 0.2343457 1.51E 01 1.11E 05 0.7928865 0.3474868 0.2134066 2.06E 01 4.23E 06 0.7886438 SD 0.1920092 0.043449 1.13E 01 1.00E 05 0.047478 0.2091014 0.0447611 1.39E 01 3.79E 06 0.057137 F26 Mean 0.3646074 0.224008 1.78E 01 1.08E 05 0.7838864 0.3190865 0.2330302 2.23E 01 4.37E 06 0.7968719 SD 0.1977082 0.0496045 1.32E 01 1.20E 05 0.0641727 0.1838445 0.0381587 1.56E 01 4.33E 06 0.0594243 F27 Mean 0.338128 0.2309029 1.83E 01 1.37E 05 0.7990631 0.2831152 0.2157726 2.07E 01 4.10E 06 0.7915868 SD 0.1992252 0.0382321 1.47E 01 1.22E 05 0.0570528 0.1668755 0.038593 1.63E 01 4.61E 06 0.0632203 F28 Mean 0.3203911 0.2370114 1.50E 01 1.08E 05 0.8122299 0.354919 0.2406205 2.29E 01 5.09E 06 0.8034302 SD 0.1878174 0.034479 1.16E 01 1.28E 05 0.0597167 0.1624517 0.0404844 1.52E 01 4.46E 06 0.0636212 123 Empirical analysis of ve nature-inspired algorithms on real Table 14 continued 10D 30D FA CS BA FPA FA CS BA FPA Alpha pa Pulse Loud p Alpha pa Pulse Loud p F29 Mean 0.3447222 0.219595 1.71E 01 1.85E 05 0.800136 0.3387822 0.2415259 1.92E 01 4.26E 06 0.784098 SD 0.1913485 0.0405908 1.41E 01 1.89E 05 0.0524954 0.2098357 0.0368161 1.42E 01 3.51E 06 0.0549937 F30 Mean 0.3669368 0.2189679 2.24E 01 1.37E 05 0.80537 0.3541147 0.2254758 1.70E 01 4.77E 06 0.8128823 SD 0.1722933 0.0480944 1.47E 01 1.26E 05 0.0556391 0.1777563 0.0422896 1.24E 01 3.72E 06 0.0593142 50D 100D FA CA BA FPA FA CS BA FPA Alpha pa Pulse Loud p Alpha pa Pulse Loud p F1 Mean 0.2904196 0.223075 2.02E 01 2.66E 06 0.7926243 2.50E 01 2.16E 01 2.15E 01 1.28E 06 8.01E 01 SD 0.1843142 0.0419657 1.41E 01 4.23E 06 0.0535002 1.94E 01 4.95E 02 1.59E 01 1.25E 06 5.26E 02 F2 Mean 0.2835688 0.2227919 1.66E 01 2.37E 06 0.7886816 3.28E 01 2.22E 01 2.29E 01 1.22E 06 7.86E 01 SD 0.1668758 0.0442377 1.53E 01 2.46E 06 0.0630196 1.89E 01 3.56E 02 1.47E 01 1.83E 06 6.09E 02 F3 Mean 0.324954 0.2258939 2.33E 01 2.28E 06 0.7905698 2.89E 01 2.19E 01 1.56E 01 9.72E 07 8.06E 01 SD 0.1924751 0.0410649 1.53E 01 2.14E 06 0.0609512 2.06E 01 4.55E 02 1.58E 01 7.83E 07 5.57E 02 F4 Mean 0.3918619 0.2257163 1.72E 01 3.16E 06 0.7869417 2.70E 01 2.18E 01 2.07E 01 1.41E 06 8.09E 01 SD 0.187597 0.0438638 1.53E 01 3.69E 06 0.0634608 1.86E 01 4.58E 02 1.46E 01 1.62E 06 6.58E 02 123 P. Agarwal, S. Mehta Table 14 continued 50D 100D FA CA BA FPA FA CS BA FPA Alpha pa Pulse Loud p Alpha pa Pulse Loud p F5 Mean 0.3448382 0.2211629 2.11E 01 1.45E 06 0.797565 2.92E 01 2.20E 01 2.32E 01 1.12E 06 7.98E 01 SD 0.1893626 0.0334834 1.41E 01 1.74E 06 0.0578288 1.65E 01 4.75E 02 1.75E 01 1.14E 06 5.48E 02 F6 Mean 0.3442015 0.2288192 1.80E 01 2.27E 06 0.8001104 3.26E 01 2.31E 01 2.08E 01 1.76E 06 8.23E 01 SD 0.1930434 0.0449192 1.39E 01 2.25E 06 0.0543768 1.76E 01 5.00E 02 1.32E 01 1.72E 06 4.93E 02 F7 Mean 0.340297 0.2301417 2.69E 01 3.62E 06 0.781685 3.30E 01 2.33E 01 2.13E 01 1.20E 06 7.89E 01 SD 0.1681938 0.040029 1.50E 01 3.93E 06 0.0471419 1.87E 01 4.87E 02 1.50E 01 1.28E 06 5.32E 02 F8 Mean 0.3578346 0.2315954 1.84E 01 1.74E 06 0.7963005 3.29E 01 2.15E 01 1.71E 01 1.23E 06 7.96E 01 SD 0.1969852 0.0401637 1.33E 01 1.76E 06 0.0554815 2.01E 01 4.45E 02 1.41E 01 1.76E 06 5.49E 02 F9 Mean 0.2651149 0.2333678 2.01E 01 3.01E 06 0.7897363 3.07E 01 2.27E 01 1.92E 01 1.31E 06 7.94E 01 SD 0.1826381 0.041928 1.45E 01 3.93E 06 0.0577762 1.84E 01 4.56E 02 1.49E 01 1.42E 06 5.72E 02 F10 Mean 0.3556653 0.2195773 1.84E 01 1.95E 06 0.8233599 2.31E 01 2.33E 01 2.05E 01 1.63E 06 7.95E 01 SD 0.1773669 0.0428415 1.32E 01 1.43E 06 0.0519767 1.70E 01 4.52E 02 1.43E 01 1.32E 06 5.58E 02 123 Empirical analysis of ve nature-inspired algorithms on real Table 14 continued 50D 100D FA CA BA FPA FA CS BA FPA Alpha pa Pulse Loud p Alpha pa Pulse Loud p F11 Mean 0.3490255 0.2325594 1.79E 01 2.21E 06 0.8088158 3.61E 01 2.37E 01 1.98E 01 8.24E 07 7.92E 01 SD 0.184142 0.0349732 1.67E 01 2.64E 06 0.0587813 1.60E 01 4.48E 02 1.27E 01 9.46E 07 5.65E 02 F12 Mean 0.2851578 0.2427527 1.62E 01 2.54E 06 0.8154915 3.16E 01 2.19E 01 1.92E 01 1.03E 06 8.06E 01 SD 0.1794463 0.0372223 1.22E 01 2.59E 06 0.0656154 1.70E 01 4.91E 02 1.46E 01 1.10E 06 6.16E 02 F13 Mean 0.2924779 0.2191099 1.69E 01 2.56E 06 0.8091659 3.18E 01 2.13E 01 1.87E 01 1.06E 06 8.11E 01 SD 0.1775395 0.0420278 1.42E 01 2.74E 06 0.0561586 1.88E 01 4.73E 02 1.18E 01 9.31E 07 5.56E 02 F14 Mean 0.3406516 0.2343432 1.96E 01 2.61E 06 0.8041085 2.55E 01 2.36E 01 2.33E 01 1.40E 06 8.04E 01 SD 0.1669629 0.0437034 1.61E 01 2.23E 06 0.0567334 1.64E 01 4.20E 02 1.38E 01 1.15E 06 5.38E 02 F15 Mean 0.2879238 0.2295798 2.26E 01 2.83E 06 0.7834836 3.37E 01 2.24E 01 1.94E 01 1.10E 06 8.04E 01 SD 0.1756314 0.0419672 1.53E 01 3.42E 06 0.051931 1.99E 01 4.78E 02 1.55E 01 1.09E 06 6.98E 02 F16 Mean 0.3531768 0.2153667 2.31E 01 2.31E 06 0.7832423 3.37E 01 2.26E 01 2.56E 01 1.67E 06 7.91E 01 SD 0.1929632 0.0397243 1.69E 01 1.67E 06 0.0600518 1.74E 01 4.55E 02 1.30E 01 1.50E 06 5.39E 02 F17 Mean 0.2532004 0.2336338 1.93E 01 2.20E 06 0.7944526 3.95E 01 2.24E 01 1.75E 01 1.15E 06 7.94E 01 SD 0.1834398 0.0421329 1.16E 01 2.03E 06 0.0606691 1.65E 01 4.43E 02 1.39E 01 1.16E 06 5.60E 02 123 P. Agarwal, S. Mehta Table 14 continued 50D 100D FA CA BA FPA FA CS BA FPA Alpha pa Pulse Loud p Alpha pa Pulse Loud p F18 Mean 0.318662 0.2310373 1.95E 01 3.18E 06 0.8017671 2.79E 01 2.34E 01 2.19E 01 9.64E 07 7.99E 01 SD 0.1912575 0.0532354 1.58E 01 2.51E 06 0.0495826 1.85E 01 4.54E 02 1.34E 01 8.24E 07 5.45E 02 F19 Mean 0.3111258 0.2347913 1.96E 01 2.37E 06 0.7862302 3.50E 01 2.37E 01 1.69E 01 1.63E 06 7.92E 01 SD 0.2128124 0.0417872 1.62E 01 2.20E 06 0.0574931 1.82E 01 4.43E 02 1.46E 01 2.28E 06 6.25E 02 F20 Mean 0.3007105 0.2235241 1.90E 01 2.49E 06 0.788603 3.47E 01 2.26E 01 1.67E 01 8.47E 07 7.93E 01 SD 0.173824 0.0352048 1.17E 01 2.21E 06 0.0607615 2.06E 01 4.72E 02 1.38E 01 1.13E 06 6.26E 02 F21 Mean 0.3508223 0.2149572 1.74E 01 3.04E 06 0.80915 3.52E 01 2.25E 01 2.03E 01 1.98E 06 8.09E 01 SD 0.2028324 0.0396753 1.39E 01 2.88E 06 0.0517638 1.93E 01 3.92E 02 1.53E 01 1.79E 06 5.26E 02 F22 Mean 0.3586696 0.2161282 2.24E 01 3.03E 06 0.7977888 2.62E 01 2.26E 01 2.56E 01 1.58E 06 7.94E 01 SD 0.1962307 0.0489156 1.44E 01 2.56E 06 0.0557054 1.59E 01 4.36E 02 1.58E 01 1.80E 06 5.94E 02 F23 Mean 0.3391531 0.2384328 2.04E 01 3.07E 06 0.8050244 3.08E 01 2.18E 01 2.01E 01 1.24E 06 7.97E 01 SD 0.2029713 0.046192 1.36E 01 2.74E 06 0.0598797 1.77E 01 4.41E 02 1.61E 01 1.51E 06 5.54E 02 F24 Mean 0.3039263 0.2312114 2.15E 01 2.55E 06 0.8122195 2.65E 01 2.33E 01 2.50E 01 1.26E 06 7.88E 01 SD 0.1755055 0.046302 1.55E 01 2.57E 06 0.0601582 1.61E 01 3.81E 02 1.48E 01 1.33E 06 5.77E 02 123 Empirical analysis of ve nature-inspired algorithms on real Table 14 continued 50D 100D FA CA BA FPA FA CS BA FPA Alpha pa Pulse Loud p Alpha pa Pulse Loud p F25 Mean 0.2816541 0.2245474 1.96E 01 1.80E 06 0.8042364 3.12E 01 2.25E 01 2.04E 01 1.14E 06 7.95E 01 SD 0.1700208 0.0397898 1.57E 01 2.00E 06 0.0569569 1.85E 01 3.99E 02 1.36E 01 1.08E 06 5.40E 02 F26 Mean 0.3472982 0.2514349 2.06E 01 2.31E 06 0.7922071 3.52E 01 2.21E 01 1.74E 01 1.20E 06 8.21E 01 SD 0.1583693 0.0354047 1.23E 01 1.94E 06 0.0665101 1.81E 01 4.33E 02 1.33E 01 1.13E 06 5.35E 02 F27 Mean 0.2903625 0.2192183 2.24E 01 3.18E 06 0.7932016 2.97E 01 2.20E 01 1.94E 01 1.48E 06 8.03E 01 SD 0.1719552 0.04212 1.29E 01 3.00E 06 0.0592081 2.11E 01 3.48E 02 1.47E 01 1.12E 06 6.01E 02 F28 Mean 0.3158466 0.2355729 2.07E 01 3.36E 06 0.7975199 3.48E 01 2.11E 01 2.46E 01 1.23E 06 8.03E 01 SD 0.2103601 0.0472814 1.33E 01 2.91E 06 0.0455554 1.70E 01 4.55E 02 1.44E 01 1.49E 06 5.14E 02 F29 Mean 0.2953038 0.2235519 2.00E 01 2.23E 06 0.79894 3.21E 01 2.14E 01 2.18E 01 1.51E 06 8.11E 01 SD 0.2144624 0.0471642 1.39E 01 2.12E 06 0.0644896 1.95E 01 4.71E 02 1.60E 01 1.84E 06 5.56E 02 F30 Mean 0.2875417 0.2158555 2.26E 01 3.78E 06 0.782582 3.15E 01 2.30E 01 2.25E 01 1.25E 06 7.87E 01 SD 0.2057834 0.0453937 1.46E 01 4.18E 06 0.1562969 2.12E 01 6.18E 02 1.41E 01 1.46E 06 4.59E 02 123 P. Agarwal, S. Mehta Table 15 List of nomenclature Original name Nomenclature Arti cial bee colony algorithm ABC Cuckoo search algorithm CS Bat algorithm BA Fire y algorithm FA Flower pollination algorithm FPA Run length distribution RLD Bees algorithm BEA Evolutionary algorithms EA Particle swarm optimization PSO Differential evolution DE Genetic algorithm GA References Agarwal P, Mehta S (2014) Nature-inspired algorithms: state-of-art, problems and prospects. Int J Comput Appl 100(14):14 21. http://research.ijcaonline.org/volume100/number14/pxc3898331.pdf Agarwal P, Mehta S (2015) Comparative analysis of nature inspired algorithms on data clustering. In: IEEE international conference on research in computational intelligence and communication networks (ICR- CICN), pp 119 124 Agarwal P, Mehta S (2016) Enhanced ower pollination algorithm on data clustering. Int J Comput Appl 7074(August):1 12 Akay B, Karaboga D (2009) Parameter tuning for the arti cial bee colony algorithm. In: International confer- ence on computational collective intelligence. Springer, Heidelberg, pp 608 619 Akay B, Karaboga D (2012) A modi ed arti cial bee colony algorithm for real-parameter optimization. Inf Sci 192:120 142. doi:10.1016/j.ins.2010.07.015 Alsariera YA et al (2014) Comparative performance analysis of bat algorithm and bacterial foraging opti- mization algorithm using standard benchmark functions. In: Software engineering conference (MySEC), 2014 8th Malaysian, pp 295 300 Banati H, Mehta S (2012) SEVO: bio-inspired analytical tool for uni-modal and multimodal optimization. In: Proceedings of the international conference, pp 557 566 Banati H, Mehta S (2013) Improved shuf ed frog leaping algorithm for continuous optimisation adapted SEVO toolbox. Int J Adv Intell Paradig 5(1/2):31 44 Civicioglu P, Besdok E (2011) A conception comparison of the cuckoo search, particle swarm optimization, differential evolution and arti cial bee colony algorithms. Artif Intell Rev. doi:10.1007/s10462-011- 92760 Draa A (2015) On the performances of the ower pollination algorithm qualitative and quantitative analyses. Appl Soft Comput J 34(October):349 371 Fister I, Yang X-S, Fister I et al (2013a) A brief review of nature-inspired algorithms for optimization. Elektrotehni Ski Vestnik 80(3):1 7 Fister I, Fister D, Yang XS (2013c) A hybrid bat algorithm. Electrotech Rev 80(1 2):1 7 Fister I, Yang XS, Brest J (2013d) A comprehensive review of re y algorithms. Swarm Evol Comput 13:34 46 Fister I, Yang X-S, Brest J et al (2013b) Memetic self-adaptive re y algorithm. In: Swarm intelligence and bio-inspired computation, pp 73 101 Hashmi A et al (2013) Comparative study of bio-inspired algorithms for unconstrained optimization problems. In: International conference on advances in electronics, electrical and computer engineering, pp 978 981 Karaboga D, Akay B (2009) A comparative study of arti cial bee colony algorithm. Appl Math Comput 214(1):108 132. doi:10.1016/j.amc.2009.03.090 KarabogaD,BasturkB(2007)Apowerfulandef cientalgorithmfornumericalfunctionoptimization:arti cial bee colony (ABC) algorithm. J Glob Optim 39(3):459 471 Karaboga D, Basturk B (2008) On the performance of arti cial bee colony (ABC) algorithm. Appl Soft Comput J 8(1):687 697 123 Empirical analysis of ve nature-inspired algorithms on real Khaze S, Hojjatkhah S, Bagherinia A (2013) Evaluation the ef ciency of arti cial bee colony and the re y algorithm in solving the continuous optimization problem 3(4):23 35. arXiv:1310.7961 Liang JJ et al (2014) Problem de nitions and evaluation criteria for the CEC 2014 special session and competition on single objective real-parameter numerical optimization. http://web.mysites.ntu.edu.sg/ epnsugan/PublicSite/SharedDocuments/CEC-2015/Learning-BasedSingleObjectiveOptimization/De n itionsofCEC2015benchmarklearning-based20141228.pdf, http://web.mysites.ntu.edu.sg/epnsugan/ PublicSite/SharedDocuments/Form Mlakar U, Fister I, Fister I (2015) Hybrid self-adaptive cuckoo search for global optimization. Swarm Evol Comput. doi:10.1016/j.swevo.2016.03.001 Pavlyukevich I (2007) L vy ight, non local search and simulated annealing. J Comput Phys 226:1830 1844 Pham DT, Castellani M (2014) Benchmarking and comparison of nature-inspired population-based continuous optimisation algorithms. Soft Comput 18(5):871 903 Singh GP, Singh A (2014) Comparative study of Krill Herd, re y and cuckoo search algorithms for unimodal and multimodal optimization. Int J Intell Syst Appl 6(3):35 49 Suganthan PN et al (2005) Problem de nitions and evaluation criteria for the CEC 2005 special session on real- parameter optimization. Technical report, Nanyang Technological University, Singapore And KanGAL report number 2005005 (Kanpur Genetic Algorithms Laboratory, IIT Kanpur), (May), pp 251 256 Yang X (2010a) A new metaheuristic bat-inspired algorithm. In: Nicso 2010, studies in computational intel- ligence. Springer Berlin, vol 284, pp 65 74 Yang X-S. Cuckoo search (CS) algorithm Yang XS, Deb S (2009) Cuckoo search via L evy ights. In: Proc. of World Congress on Nature & Biologically Inspired Computing (NaBIC 2009), December 2009, India. IEEE Publications, USA, pp. 210 214 Yang X-S (2009) Fire y algorithms for multimodal optimization. In: Proceedings of the 5th international conference on stochastic algorithms: foundations and applications, vol 5792, pp 169 178 Yang X-S (2013) Flower pollination algorithm for global optimization. In: Unconventional computation and natural computation 2012. Lecture notes in computer science, vol 7445, pp 240 249. http://arxiv.org/ abs/1312.5673 Yang X-S (2010b) Fire y algorithm, stochastic test functions and design optimization. Int J Bio-Inspired Comput 2(2):78 84 Yang X-S (2014) Cuckoo search and re y algorithm, vol 516. Springer, New York, pp 315 330 Yang X-S, Deb S (2010) Engineering optimisation by cuckoo search. Int J Math Model Numer Optim 1(4):330 343 Yang X-S, He X (2013) Bat algorithm: literature review and applications. Int J Bio-Inspired Comput 5(3):141 149 123 View publication stats