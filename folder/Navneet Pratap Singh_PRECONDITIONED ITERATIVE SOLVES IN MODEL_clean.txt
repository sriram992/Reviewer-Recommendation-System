arXiv:1606.01216v3 [math.NA] 14 Feb 2017 PRECONDITIONED ITERATIVE SOLVES IN MODEL REDUCTION OF SECOND ORDER LINEAR DYNAMICAL SYSTEMS NAVNEET PRATAP SINGH, KAPIL AHUJA, AND HEIKE FASSBENDER Abstract. Recently a new algorithm for model reduction of second order linear dynamical systems with proportional damping, the Adaptive Iterative Rational Global Arnoldi (AIRGA) algorithm [8], has been proposed. The main computational cost of the AIRGA algorithm is in solving a sequence of linear systems. These linear systems do change only slightly from one iteration step to the next. Here we focus on e ciently solving these systems by iterative methods and the choice of an appropriate preconditioner. We propose the use of relevant iterative algorithm and the Sparse Approximate Inverse (SPAI) preconditioner. A technique to cheaply update the SPAI preconditioner in each iteration step of the model order reduction process is given. Moreover, it is shown that under certain conditions the AIRGA algorithm is stable with respect to the error introduced by iterative methods. Our theory is illustrated by experiments. It is demonstrated that SPAI preconditioned Conjugate Gra- dient (CG) works well for model reduction of a one dimensional beam model with AIRGA algorithm. Moreover, the computation time of preconditioner with update is on an average 2 3-rd of the computation time of preconditioner without update. With average timings running into hours for very large sys- tems, such savings are substantial. 1. Introduction A continuous time-invariant second order linear dynamical system is of the form M x(t) = D x(t) Kx(t) + Fu(t), y(t) = Cpx(t) + Cv x(t), (1.1) where M, D, K Rn n are mass, damping and sti ness matrices, respectively, F Rn m, Cp, Cv Rq n are constant matrices. In (1.1), x(t) Rn is the state, u(t) Rm is the input, and y(t) Rq is the output. If m and q both are one, then we have a Single-Input Single-Output (SISO) system. Otherwise (m and q > 1) the system is called Multi-Input Multi-Output (MIMO). We assume the case of proportional damping, i.e., D = M + K, where the coe cients and are chosen based on experimental results [4, 8]. For our derivations, the system matrices M, D and K need not hold any speci c property (e.g., symmetry, positive de niteness etc.). 2010 Mathematics Subject Classi cation. Primary 34C20, 65F10, 65L20. Key words and phrases. Model Order Reduction, Global Arnoldi Algorithm, Moment Match- ing, Iterative Methods, Preconditioner and Stability Analysis. This work was supported by DAAD grant number A/14/04422 under the IIT-TU9 exchange of faculty program. 1 2 NAVNEET PRATAP SINGH, KAPIL AHUJA, AND HEIKE FASSBENDER It is assumed that the order n of the system (1.1) is extremely high. The sim- ulation of large dynamical systems can be unmanageable due to high demands on computational resources, which is the main motivation for model reduction. The goal of model reduction is to produce a low dimensional system that has, as best as possible, the same characteristics as the original system but whose simulation requires signi cantly less computational e ort. The reduced system of (1.1) is de- scribed by M x(t) = D x(t) K x(t) + Fu(t), y(t) = Cp x(t) + Cv x(t), (1.2) where M, K, D Rr r, F Rr m, Cp, Cv Rq rand r n. The damping property of the original system needs to be re ected in the reduced system. That is, D = M + K is required, where and remain unchanged from the original system. Model reduction can be done in many ways, see, e.g., [2]. We will focus on a projection based method, speci cally Galerkin projection [8]. For this a matrix V Rn r with orthonormal columns is chosen and the system (1.1) is projected V T (MV x(t) + DV x(t) + KV x(t) Fu(t)) = 0, y(t) = CpV x(t) + CvV x(t). (1.3) Comparing (1.3) with (1.2) yields M = V T MV, D = V T DV, K = V T KV, F = V T F, Cp = CpV and Cv = CvV. (1.4) The matrix V can be obtained in many ways, see, e.g., [2]. The focus in this paper will be on the Adaptive Iterative Rational Global Arnoldi (AIRGA) algorithm [8] in which V is generated by an Arnoldi based approach. The main contributions of this paper are as follows: Section 2 summarizes the AIRGA model reduction process which uses a direct solver for solving the linear systems arising in each iteration step. In Section 3, we discuss the use of itera- tive solvers and preconditioners for these linear systems. Preconditioned iterative solvers are a good choice here since they scale well. They have time complexity O(n nnz), where n is the size of the system and nnz is the number of nonzeros in system matrices as compared to O(n3) for direct solvers [17]. The choice of iterative algorithm is problem dependent. We show that Sparse Approximate Inverse (SPAI) preconditioners are well suited for solving the linear systems arising in the model reduction process. These linear systems change at each model reduction iteration, but this change is small. Exploiting this fact we propose a cheap preconditioner update. Using an iterative solver introduces additional errors in the computation since the linear systems are not solved exactly. Hence, we discuss the stability of AIRGA in Section 4. In Section 5, an numerical experiment is given to support our preconditioned iterative solver theory. The cheap updates to the SPAI precondi- tioner, with CG as the underlying iterative algorithm, leads to about 1 3-rd savings in time. Finally, we give some conclusions and point out future work in Section 6. For the rest of this paper, || ||F denotes the Frobenius norm, || || the 2-norm, || ||H2 the H2-norm, and || ||H the H -norm [2]. Also, qr denotes the QR factorization [21]. PRECONDITIONED ITERATIVE SOLVES IN MODEL REDUCTION 3 2. Arnoldi Based Projection Method In this section, we rst describe how to obtain V such that the rst few moments of the transfer functions of the original and the reduced order transfer function are matched. We then state the AIRGA algorithm [8] based on this approach. The transfer function of (1.1) is given by H(s) = (Cp + sCv)(s2M + sD + K) 1F = (Cp + sCv)X(s), where X(s) = (s2M + sD + K) 1F is the state variable in frequency domain. The power series expansion of state variable X(s) around expansion point s0 R is given as [24] X(s) = X j=0 X(j)(s0)(s s0)j, (2.1) where, X(0)(s0) = (s2 0M + s0D + K) 1F, X(1)(s0) = (s2 0M + s0D + K) 1( (2s0M + D))X(0)(s0), X(2)(s0) = (s2 0M + s0D + K) 1[ (2s0M + D)X(1)(s0) MX(0)(s0)], ... X(j)(s0) = (s2 0M + s0D + K) 1[ (2s0M + D)X(j 1)(s0) MX(j 2)(s0)]. (2.2) Here, X(j)(s0) is called the jth-order system moment of X(s) at s0. Similarly, the transfer function of the reduced system (1.2) is given by H(s) = ( Cp + s Cv) X(s), where X(s) = (s2 M + s D + K) 1 F. The power series expansion of the reduced state space X(s) around expansion point s0 R is X(s) = X j=0 X(j)(s0)(s s0)j. (2.3) Here, X(j)(s0) is de ned analogoulsy to the X(j)(s0). It is called the jth-order system moment of X(s) at s0. The goal of moment-matching approaches is to nd a reduced order model such that the rst few moments of (2.1) and (2.3) are matched, that is, X(j)(s0) = X(j)(s0) for j = 0, 1, 2, . . ., t for some t. De ne P1 = (s2 0M + s0D + K) 1(2s0M + D), P2 = (s2 0M + s0D + K) 1M, Q = (s2 0M + s0D + K) 1F, then from (2.2) we have X(0)(s0) = Q, X(1)(s0) = P1X(0)(s0), and X(j)(s0) = P1X(j 1)(s0) + P2X(j 2)(s0) 4 NAVNEET PRATAP SINGH, KAPIL AHUJA, AND HEIKE FASSBENDER for j 2. The second order Krylov subspace [3] is de ned as Gj(P1, P2, Q) = span{Q, P1Q, (P2 1 + P2)Q, . . . , Sj(P1, P2)Q}, where Sj(P1, P2) = P1 Sj 1(P1, P2) + P2 Sj 2(P1, P2) for j 2. Let K = (s2 0M + s0D + K). For the special case of proportionally damped second-order systems, it has been observed in [4] Gj(P1, P2, Q) = Gj( K 1(2s0M + D), K 1M, K 1F), = Gj( K 1((2s0 + )M + K), K 1M, K 1F), = Kj(P1, Q), where Kj(P1, Q) is the standard block Krylov subspace Kj(P1, Q) = span{Q, P1Q, P2 1Q, . . . , Pj 1 1 Q}. Thus, we need a good basis of Kj(P1, Q). This can be obtained e ciently by, e.g., the block or the global Arnoldi algorithm [17, 14, 18]. The AIRGA algorithm, as proposed in [8], is one of the latest methods based on the global Arnoldi method. It is given in Algorithm 1. In this method, moment matching is done at multiple expansion points si, i = {1, . . . , l}, rather than just at s0 as earlier. This ensures a better reduced model in the entire frequency domain of interest. The initial selection and further the computation of expansion points has been discussed in [8] and [13]. We adopt the choices described in Section 5.0.1 of [8]. The initial expansion points could be either real or imaginary, both of which have their merits. This is problem dependent and discussed in results section. After the rst AIRGA iteration, the expansion points are chosen from the eigenvalues of the quadratic eigenvalue problem 2 M + D + K (at line 33). The method is adaptive, i.e., it automatically chooses the number of moments to be matched at each expansion point si. This is controlled by the while loop at line 9. The variable j stores the number of moments matched. The upper bound on j is rmax/m , where rmax is the maximum dimension to which we want to reduce the state variable (input from the user), and m is the dimension of the input; see [8] for a detailed discussion. At exit of this while loop, J = j. At line 3, no convergence implies that the H2 norm of the di erence between two consecutive reduced systems, computed at line 34, is greater than a certain tolerance. Similarly, at line 9, no convergence implies that the H2 norm of the di erence between two consecutive intermediate reduced systems, computed at line 26, is greater than a certain tolerance. This algorithm requires solving a linear system at line 5 and 14. As the si change in each iteration step, the linear systems to be solved change in each iteration step. As discussed in Section 1, since solving such systems by direct methods is quite expensive, we propose to use iterative methods. As the change in the si will be small (at least after the rst iteration step), we can develop a cheap update of the necessary preconditioner. 3. Preconditioned Iterative Method There are two types of methods for solving linear systems of equations; a) direct methods and b) iterative methods. For large systems, direct methods are not PRECONDITIONED ITERATIVE SOLVES IN MODEL REDUCTION 5 Algorithm 1 Adaptive Iterative Rational Global Arnoldi Algorithm [8] 1: Input: {M, D, K, F, Cp, Cv, rmax; S is the set initial expansion points si, i = 1, . . . , l} 2: z = 1 3: while no convergence do 4: for each si S do 5: X(0)(si) = (s2 i M + siD + K) 1F 6: Compute QR = qr(X(0)(si)), X(0)(si) = Q 7: end for 8: j = 1 9: while no convergence and j rmax/m do 10: Let j be expansion point corresponding to maximum moment error of reduced system at si 11: Vj = X(j 1)( j)/||X(j 1)( j)||F 12: for i = 1, . . . , l do 13: if (si == j) then 14: X(j)(si) = (s2 i M + siD + K) 1MVj 15: else X(j)(si) = X(j 1)(si) 16: end if 17: for t = 1, 2, . . . , j do 18: t,j(si) = trace(V H t X(j)(si)) 19: X(j)(si) = X(j)(si) t,j(si)Vt 20: end for 21: end for 22: Wi = X(j)(si)/||X(j)(si)||F for i = {1, . . . , l}. 23: W = [W1, W2, . . . , Wl]. 24: Compute WY = qr( W ), W = W 25: Compute reduced system matrices M, D, and K with V = W as in (1.4) 26: HInt = ( Cp + j Cv)( 2 j M + j D + K) 1 F 27: j = j+1 28: end while 29: Set J = j and pick J corresponding to maximum moment error of reduced system at si 30: VJ = X(J 1)( J)/||X(J 1)( J)||F and V = [V1, V2, . . . , VJ]. 31: Compute V Y = qr( V ), V = V 32: Compute reduced system matrices M, D, and K with V as in (1.4), and take M = M, D = D, K = K, for the next iteration. 33: Choose new expansion points si 34: H = ( Cp + J Cv)( 2 J M + J D + K) 1 F 35: z = z + 1 36: end while 37: Compute the remaining reduced system matrices F, Cp, Cv with V as in (1.4) preferred because they are too expensive in terms of storage and operation. On the other hand, iterative methods require less storage and operations than direct methods. For a large linear system Ax = b, with A Rn n and b Rn, an iterative 6 NAVNEET PRATAP SINGH, KAPIL AHUJA, AND HEIKE FASSBENDER method nds a sequence of solution vectors x0, x1, . . . , xk which (hopefully) converges to the desired solution. Krylov subspace based methods are an important and popular class of iterative methods. If x0 is the initial solution and r0 = b Ax0 is the initial residual, then Krylov subspace methods nd the approximate solution by projecting onto the Krylov subspace Kk(A, r0) = span{r0, Ar0, A2r0, . . . , Ak 1r0}. There are many types of Krylov subspace algorithms [17]. Some popular ones include Conjugate Gradient (CG), Generalized Minimal Residual (GMRES), Min- imum Residual (MINRES), and BiConjugate Gradient (BiCG). Block versions of these algorithms do exist. The choice of algorithm is problem dependent. In results section (Section 5), the coe cient matrices arising from the problem are symmet- ric positive de nite (SPD). Hence, we use CG algorithm, which is ideal for such systems. In Krylov subspace methods, the conditioning of the system is very impor- tant. Conditioning pertains to the perturbation behavior of a mathematical prob- lem [21] . For example, in a well-conditioned problem, a small perturbation of the input leads to a small change in the output. This is not guaranteed for an ill-conditioned problem, where a small perturbation in the input may change the output drastically [21]. Preconditioning is a technique to well-condition an ill- conditioned problem. We discuss that next. Preconditioning is a technique for improving the performance of iterative meth- ods. It transforms a di cult system (ill-conditioned system) to another system with more favorable properties for iterative methods. For example, a preconditioned ma- trix may have eigenvalues clustered around one. This means that the preconditioned matrix is close to the identity matrix, and hence, the iterative method will converge faster. For a symmetric positive de nite (SPD) system, the convergence rate of it- erative methods depends on the distribution of the eigenvalues of the coe cient matrix. However, for a non-symmetric system, the convergence rate may depend on pseudo-spectra as well [20, 16]. If M is a nonsingular matrix which approximates A; that is, M A 1, then the system MAx = Mb may be faster to solve than the original one. The above system represents pre- conditioning from left. Similarly, right and split preconditioning is given by two equations below, respectively. AM x = b, x = M x and M1AM2 x = M1b, x = M2 x. The type of preconditioning technique to be used depends on the problem prop- erties as well as on the choice of the iterative solver. For example, for SPD systems, MA, AM, M1AM2 all have same eigenvalue spectrum, and hence, left, right and split preconditioners behave the same way, respectively. For a general system, this need not be true [6]. Besides making the system easier to solve by an iterative method, a precon- ditioner should be cheap to construct and apply. Some existing preconditioning techniques include Successive Over Relaxation, Polynomial, Incomplete Factoriza- tions, Sparse Approximate Inverse (SPAI), and Algebraic Multi-Grid [6, 7, 9, 10]. PRECONDITIONED ITERATIVE SOLVES IN MODEL REDUCTION 7 We use SPAI preconditioner here since these (along with incomplete factoriza- tions) are known to work in the most general setting. Also, SPAI preconditioners are easily parallelizable, hence, have an edge over incomplete factorization based preconditioners [23]. In Section 3.1 we summarize the SPAI preconditioner from [10] and we discuss the use of SPAI in the AIRGA algorithm. Since the change in the coe cient matrix of the linear system to be solved is small from one step of AIRGA to the next, we update the preconditioner from one step to the next. This aspect is covered in Section 3.2. 3.1. Sparse Approximate Inverse (SPAI) Preconditioner. In constructing a preconditioner P (z) i for a coe cient matrix K(z) i at the zth outer AIRGA iteration (i.e., K(z) i = s2 i M + siD + K), we would like P (z) i K(z) i I (for left precondition- ing) and K(z) i P (z) i I (for right preconditioning). SPAI preconditioners nd P (z) i by minimizing the associated error norm I P (z) i K(z) i or I K(z) i P (z) i for a given sparsity pattern. If the norm used is Frobenius norm, then the minimization function will be min P (z) i S I K(z) i P (z) i F , where S is a set of certain sparse matrices. The above approach produces a right approximate inverse. Similarly, a left approximate inverse can be computed by solving the minimization problem I P (z) i K(z) i F . For non-symmetric matrices, the distinction between left and right approximate inverses is important. There are some situations where it can be di cult to nd a right approximate inverse but nding a left approximate inverse can be easy. Whether left or right precondition- ing should be used is problem dependent [17]. Since the SPAI preconditioner was originally proposed for right preconditioning [10], we focus on the same here. Sim- ilar derivation can be done for the left preconditioning as well. The minimization problem can be rewritten as min I K(z) i P (z) i 2 F = min n X j=1 e(j) K(z) i p(j) i 2 2 , (3.1) where p(j) i and e(j) are jth columns of the P (z) i matrix and I (identity matrix), respectively. The minimization problem (3.1) is essentially just one least squares problem, to be solved for n di erent right-hand sides. Here it is solved iteratively. The algorithm for computing a SPAI preconditioner for right preconditioning is given in Algorithm 2. The inputs to this algorithm is K(z) i (coe cient matrix) and tol (stopping residual of the minimization problem for each column); tol is picked based on experience. This is ALGORITHM 2.5 of [10] with two minor di erences. First, we do not list the code related to sparsity pattern matching (for obtaining a sparse preconditioner) because the goal here is to motivate SPAI update, and for our problems the original matrix is very sparse so the preconditioner stays sparse any ways. This aspect can be easily incorporated. Second, we use a While loop at line 6 of Algorithm 2 instead of a For loop. This is because with a For loop one has to decide the stopping count in advance (which is chosen heuristically). We use a more certain criteria. That is, residual of the minimization problem for each 8 NAVNEET PRATAP SINGH, KAPIL AHUJA, AND HEIKE FASSBENDER Algorithm 2 : Sparse Approximate Inverse (SPAI) Preconditioner [10] 1: Input: {K(z) i , tol} 2: P (z) i = I where = trace  K(z) i  trace  K(z) i  K(z) i T  and n is the dimension of K(z) i 3: for j = 1, . . . , n do 4: De ne p(j) i = P (z) i e(j) 5: r = e(j) K(z) i p(j) i 6: while ||r|| > tol do 7: d = P (z) i r 8: w = K(z) i d 9: = (r, w) (w, w) 10: p(j) i = p(j) i + d 11: r = r w 12: end while 13: end for column less than tol. This is linear cost (for each column) and we are doing such computation anyways. The initial guess for approximate inverse P (z) i is usually taken as I where = trace  K(z) i  /trace  K(z) i  K(z) i T  (see line 2). This initial scaling factor is minimizes the spectral radius of (I K(z) i ) [7, 10, 15]. The AIRGA algorithm with the SPAI preconditioner is given in Algorithm 3. Here, we only show those parts of AIRGA algorithm that require changes. Algorithm 3 : AIRGA Algorithm with SPAI Preconditioner 1: while no convergence do 2: for i = 1, . . . , l do 3: Let K(z) i = (s2 i M + siD + K) 4: Compute preconditioner P (z) i by solving min I K(z) i P (z) i 2 F 5: Solve K(z) i P (z) i X(0)(si) = F with X(0)(si) = P (z) i X(0)(si) 6: end for 7: j = 1 8: while no convergence and j rmax/m do 9: for i = 1, . . . , l do 10: Only right hand sides are changing, so above preconditioner P (z) i can be applied as it is, i.e., Solve K(z) i P (z) i X(j)(si) = MVj with X(j)(si) = P (z) i X(j)(si) 11: end for 12: end while 13: j = j+1 14: end while PRECONDITIONED ITERATIVE SOLVES IN MODEL REDUCTION 9 3.2. SPAI Update Preconditioner. Let Kold = s2 oldM + soldD + K and Knew = s2 newM + snewD + K be two coe cient matrices for di erent expansion points sold and snew, respectively. These expansion points can be at the same or di erent AIRGA iteration. If the di erence between Kold and Knew is small, then one can exploit this while building preconditioners for this sequence of matrices. This has been considered in the quantum Monte Carlo setting [1] and for model reduction of rst order linear dynamical systems [12, 25]. Let Pold be a good initial preconditioner for Kold. As will be seen, a cheap preconditioner update can be obtained by asking for KoldPold KnewPnew, where old, new = {1, . . . , l} and, as earlier, l denotes the number of expansion points. Expressing Knew in terms of Kold, we get Knew = Kold(I + (s2 new s2 old)K 1 oldM + (snew sold)K 1 oldD). Now we enforce KoldPold = KnewPnew or KoldPold = Kold(I + (s2 new s2 old)K 1 oldM + (snew sold)K 1 oldD) (I + (s2 new s2 old)K 1 oldM + (snew sold)K 1 oldD) 1Pold = KnewPnew, where Pnew = (I + (s2 new s2 old)K 1 oldM + (snew sold)K 1 oldD) 1Pold. Let Qnew (I + (s2 new s2 old)K 1 oldM + (snew sold)K 1 oldD) 1, then the above implies KoldPold KnewQnewPold or Kold KnewQnew. This leads us to the following idea: instead of solving for Pnew from KoldPold = KnewPnew, we solve a simpler problem min ||Kold KnewQnew||2 F = min n X j=1 k(j) old Knewq(j) new 2 2 , where k(j) old and q(j) new denote the jth columns of Kold and Qnew, respectively. Com- pare this minimization problem with the one in SPAI (Equation (3.1) in Sec- tion 3.1). Earlier, we were nding the preconditioner Pnew for Knew by solv- ing min ||I KnewPnew||2 F . Here, we are nding the preconditioner Pnew (i.e., Pnew = QnewPold) by solving min ||Kold KnewQnew||2 F . The second formulation is much easier to solve since in the rst Knew could be very di erent from I, while in the second Knew and Kold are similar (change only in the expansion points). The SPAI algorithm (Algorithm 2) adapted for nding the preconditioner by minimizing this new expression is given in Algorithm 4. The inputs to this algorithm include Kold, Knew, and tol (stopping residual of the minimization problem for each column); tol is picked based on experience. The initial guess for the approximate inverse Qnew is usually taken as I. Similar to before, is chosen to minimize the 10 NAVNEET PRATAP SINGH, KAPIL AHUJA, AND HEIKE FASSBENDER Algorithm 4 : SPAI Update Preconditioner 1: Input: {Kold, Knew, tol} 2: Qnew = I where = 1 2 trace(KT oldKnew + KT newKold) trace(KTnewKnew) and n is the dimension of Knew 3: for j = 1, . . . , n do 4: De ne q(j) = Qnewe(j) 5: r = k(j) old Knewq(j) 6: while ||r|| > tol do 7: d = Qnewr 8: w = Knewd 9: = (r, w) (w, w) 10: q(j) = q(j) + d 11: r = r w 12: end while 13: end for spectral radius of (Kold Knew) : ||Kold Knew||2 F = 0 or ||Kold Knew||2 F = trace(Kold Knew)T (Kold Knew) = 0 or trace[KT oldKold KT oldKnew KT newKold + 2KT newKnew] = 0 or 2 trace(KT newKnew) = trace(KT oldKnew + KT newKold) or = 1 2 trace(KT oldKnew + KT newKold) trace(KTnewKnew) . In AIRGA, this update to the preconditioner can be done in two ways. To understand these ways, let us look at how expansion points change in AIRGA. Table 1 shows changing expansion points (labeled as Exp Pnt) for two iterations of the while loop at line 3 of Algorithm 1. First, we can update the preconditioner when expansion points change from s(1) 1 to s(1) 2 , s(1) 2 to s(1) 3 , s(1) 3 to s(1) 4 and so on (horizontal update). Second, we can update the preconditioner when expansion points change from s(1) 1 to s(2) 1 , s(1) 2 to s(2) 2 , s(1) 3 to s(2) 3 and so on (vertical update). Since in AIRGA, vertical change in expansion points is less (which means vertically the coe cient matrices are close), we use this strategy. Thus, we are updating Table 1. Change in Expansion Points Outer AIRGA Itn (z) Exp Pnt 1 (i = 1) Exp Pnt 2 (i = 2) Exp Pnt 3 (i = 3) Exp Pnt l (i = l) 1 s(1) 1 s(1) 2 s(1) 3 s(1) l 2 s(2) 1 s(2) 2 s(2) 3 s2 l PRECONDITIONED ITERATIVE SOLVES IN MODEL REDUCTION 11 Algorithm 5 : AIRGA with SPAI Update Preconditioner 1: z = 1 2: while no convergence do 3: for i = 1 to l do 4: if z == 1 then 5: K(1) i = (s2 i M + siD + K) 6: Compute initial P (1) i by solving min I K(1) i P (1) i 2 F 7: Solve K(1) i P (1) i X(0)(si) = F with X(0)(si) = P (1) i X(0)(si) 8: else 9: K(z) i = (s2 i M + siD + K) 10: Compute Q(z) i by solving min K(z 1) i K(z) i Q(z) i 2 F 11: Solve K(z) i h Q(z) i Q(z 1) i . . . Q(2) i P (1) i i X(0)(si) = F with X(0)(si) = h Q(z) i Q(z 1) i . . . Q(2) i P (1) i i X(0)(si) 12: end if 13: end for 14: j = 1 15: while no convergence and j rmax/m do 16: for i = 1 to l do 17: Only right hand sides are changing, so above preconditioners can be applied as it is, i.e., Solve K(z) i h Q(z) i Q(z 1) i . . . Q(2) i P (1) i i X(j)(si) = MVj with X(j)(si) = h Q(z) i Q(z 1) i . . . Q(2) i P (1) i i X(j)(si) 18: end for 19: end while 20: j = j + 1 21: end while 22: z = z + 1 preconditioner from K(z 1) i =  s(z 1) i 2 M +s(z 1) i D+K (which is Kold) to K(z) i =  s(z) i 2 M + s(z) i D + K (which is Knew). The AIRGA algorithm with the SPAI update preconditioner is given in Algo- rithm 5. Here, we only show those parts of the AIRGA algorithm that require changes. At line 10 of Algorithm 5, after solving for Q(z) i , ideally one would ob- tain the preconditioner at the zth AIGRA iteration as P (z) i = Q(z) i P (z 1) i . Since this involves a matrix-matrix multiplication, it would be expensive and defeat the purpose of using a SPAI update. Instead, we never explicitly build P (z) i except at the rst AIRGA iteration where we directly obtain P (1) i (see line 6 of Algorithm 5). From the second AIRGA iteration onwards, we pass the all Qi s (until one reaches P (1) i ) and P (1) i to the Krylov solver so that we only require matrix-vector products (see line 11 of Algorithm 5). 12 NAVNEET PRATAP SINGH, KAPIL AHUJA, AND HEIKE FASSBENDER 4. Stability Analysis of AIRGA An algorithm f for computing the solution of a continuous problem f on a digital computer is said to be stable [21] if f(x) = f( x) for some x with || x x|| ||x|| = O( machine), where machine is the machine precision. Here, we study the stability of AIRGA algorithm with respect to the errors introduced by iterative methods. Suppose X(j)(si) at line 5 and 14 in AIRGA algorithm (Algorithm 1) are com- puted using a direct method of solving linear system. This gives us the matrix V at line 31 in Algorithm 1. Let f be the functional representation of the moment matching process that uses V in AIRGA (i.e., exact AIRGA). Similarly, suppose X(j)(si) at line 5 and 14 in Algorithm 1 are computed using an iterative method of solving linear systems. Since iterative methods are inexact, i.e., they solve the linear systems upto a certain tolerance, we denote the resulting matrix V as V . Let f be the functional representation of the moment matching process that uses V in AIRGA (i.e., inexact AIRGA). Then, we will say that AIRGA is stable with respect to iterative solvers if f(H(s)) = f( H(s)) for some H(s) with (4.1) H(s) H(s) H2 or H H(s) H2 or H = O(||Z||), (4.2) where H(s) is a perturbed original full model corresponding to the error in the linear solves for computing V in inexact AIRGA. This perturbation is denoted by Z. Further, we denote f(H(s)) = H(s) and f(H(s)) = H(s). In Algorithm 1, the linear systems at line 5 are computed for di erent expansion points as (s2 i M + siD + K)X(0)(si) = F, where si {s1, s2, . . . , sl}. As discussed earlier, we solve these linear systems inexactly (i.e., by an iterative method). Let the residual associated with inexact linear solves for computing X(0)(si) be 0i for i = 1, . . . , l (s2 i M + siD + K)X(0)(si) = F + 0i. (4.3) Further, in Algorithm 1 at line 11, V1 is computed as V1 = [X(0)(st0)/||X(0)(st0)||], (4.4) where st0 is the expansion point corresponding to the maximum moment error of the reduced system. Solving the linear systems for X(j), j = 1, . . . , J 1 at line 14 in Algorithm 1 inexactly yields (s2 i M + siD + K)X(j)(si) = M Vj + ji for i = 1, . . . , l. (4.5) Next, Vj+1 is computed as Vj+1 = [X(j)(stj)/||X(j)(stj)||], (4.6) where stj is the expansion point corresponding to the maximum moment error of the reduced system. PRECONDITIONED ITERATIVE SOLVES IN MODEL REDUCTION 13 Finally, Galerkin projection is used to generate the reduced model (obtained by inexact AIRGA) M = V T M V , D = V T D V , K = V T K V , F = V T F, Cp = Cp V , and Cv = Cv V , (4.7) where V = [ V1, V2, . . . , VJ]. (4.7) states f(H(s)). Now we have to nd a per- turbed original model whose exact solution, f( H(s)), will give the reduced model as obtained by the inexact solution of the original full model, f(H(s)). That is, nd H(s) such that f(H(s)) = f( H(s)). This would satisfy the rst condition of stability (4.1). Assume that H is given by the original matrices M and D and a perturbed matrix K = K + Z. Then, for H we have (s2 i M + siD + (K + Z))X(0)(si) = F for i = 1, . . . , l. (4.8) Further, assume that the linear systems can be solved exactly as (s2 i M + siD + (K + Z))X(j)(si) = M Vj for j = 1, . . . , J 1 and i = 1, . . . , l. (4.9) Again, V = [ V1, V2, . . . , VJ] where V1 and Vj+1 for j = 1, . . . , J 1 are given by (4.4) and (4.6) since X(0)(si) and X(j)(si) for j = 1, . . . , J 1 and i = 1, . . . , l are kept same as in (4.3) and (4.5), respectively. As earlier, applying Galerkin projection to the perturbed original system gives M = V T M V , D = V T D V , K = V T (K + Z) V = K + V T Z V , F = V T F, Cp = Cp V , and Cv = Cv V . (4.10) Our goal now is to nd Z such that K = K (recall, that we assumed that the error can be attributed solely to K; M and D do not change). Comparing (4.3) with (4.8), and (4.5) with (4.9), we get Z X(0)(st0) = 0t0 and Z X(j)(stj) = jtj for j = 1, . . . , J 1, where 0t0 and jtj are residuals corresponding to the maximum moment error of the reduced system in inexact AIRGA. We can rewrite Z as Z X = , where Z Rn n, X = [X(0)(st0), . . . , X(J 1)(st(J 1))] Rn mJ, and = [ 0t0, . . . , (J 1)t(J 1))] Rn mJ. As discussed in Section 2, the upper bound for J is r/m , and hence, mJ < r. Using the fact that r n, we have mJ < n. Thus, we have an under-determined system of equations. One solution of this is Z = XT (XXT ) 1. (4.11) Multiplying both sides of (4.11) with V , we get V T Z V = V T XT (XXT ) 1 V . (4.12) 14 NAVNEET PRATAP SINGH, KAPIL AHUJA, AND HEIKE FASSBENDER For Ritz-Galerkin based iterative solvers, the solution space of linear systems is or- thogonal to the residuals, i.e., V1 0t0, V2 1t1, . . . , and VJ (J 1)t(J 1) [23]. Hence, V T = V T 1 V T 2 ... V T J 1 V T J  0t0 1t1 . . . (J 1)t(J 1)  = 0 V T 1 1t1 . . . V T 1 (J 2)t(J 2) V T 1 (J 1)t(J 1) V T 2 0t0 0 . . . V T 2 (J 2)t(J 2) V T 2 (J 1)t(J 1) ... ... ... ... ... V T J 1 0t0 V T J 1 1t1 . . . 0 V T J 1 (J 1)t(J 1) V T J 0t0 V T J 1t1 . . . V T J (J 2)t(J 2) 0 . Further, V T XT = 0 V T 1 1t1 . . . V T 1 (J 2)t(J 2) V T 1 (J 1)t(J 1) V T 2 0t0 0 . . . V T 2 (J 2)t(J 2) V T 2 (J 1)t(J 1) ... ... ... ... ... V T J 1 0t0 V T J 1 1t1 . . . 0 V T J 1 (J 1)t(J 1) V T J 0t0 V T J 1t1 . . . V T J (J 2)t(J 2) 0 X(0)(st0)T X(1)(st1)T ... X(J 2)(stJ 2)T X(J 1)(stJ 1)T = 0 X(0)(st0)T + V T 1 1t1X(1)(st1)T + + V T 1 (J 1)t(J 1)X(J 1)(st(J 1))T V T 2 0t0X(0)(st0)T + 0 X(1)(st1)T + + V T 2 (J 1)t(J 1)X(J 1)(st(J 1))T ... V T J 0t0X(0)(st0)T + + V T J (J 2)t(J 2)X(J 2)(st(J 2))T + 0 X(J 1)(st(J 1))T Since V1 0t0, V2 1t1, . . . , VJ (J 1)t(J 1), and, due to (4.6), Vj+1 is just the normalized X(j)(stj), we have X(0)(st0) 0t0, X(1)(st1) 1t1, . . . , and X(J 1)(st(J 1)) (J 1)t(J 1). Therefore from (4.12), we get V T Z V = 0. Thus, K = K or f(H(s)) = f( H(s)) = H(s), where H(s) = (Cp + sCv)(s2M + sD + K) 1F, H(s) = (Cp + sCv)(s2M + sD + (K + Z)) 1F, and H(s) = ( Cp + s Cv)(s2 M + s D + K) 1 F = ( Cp + s Cv)(s2 M + s D + K) 1 F. Thus, we have satis ed the rst condition of stability. According to the second condition of stability, given in (4.2), the di erence be- tween the original full model and the perturbed full model should be of the order of the perturbation [21]. This can be easily shown (Theorem 4.3 from [5]). Theorem 4.1. If ||Z|| < 1 ||K(s) 1||H then ||H(s) H(s)||H2 ||C(s)K(s) 1||H2||K(s) 1F||H 1 ||K(s) 1||H ||Z|| ||Z||, PRECONDITIONED ITERATIVE SOLVES IN MODEL REDUCTION 15 where K(s) = (s2M + sD + K) and C(s) = (Cp + sCv). Hence, H(s) H(s) H2 = O( Z ). The above result holds in a relative sense too. This proves the stability of AIRGA. The next theorem summarizes this. Theorem 4.2. If the linear systems arising in AIRGA are solved by a Ritz-Galerkin based solver (i.e., the residual is orthogonal to the generated Krylov subspace) and ||(s2M + sD + K) 1||H ||Z|| < 1, where Z given by (4.11), then AIRGA is stable. 5. Numerical results Consider a one dimensional beam model [4], which is of the form (1) M x(t) + D x(t) + Kx(t) = Fu(t), y(t) = Cpx(t), where m = q = 1, F Rn 1 and Cp R1 n. The model has proportional damping, i.e., D = M + K, where the damping coe cients and belong to (0, 1) [4]. We consider the model with two di erent sizes, n = 2000 and n = 10000. We compute a reduced order model by the AIRGA algorithm given in Algo- rithm 1. We implement AIRGA in MATLAB (2014a). We take rmax, i.e., the maximum dimension to which we want to reduce the system, as 30 for model size 2000 and 150 for model size 10000 based on [4]. We take three expansion points that are linearly spaced between 1 and 100 based on initial data used in [4]. As discussed in Section 3, we use iterative methods to solve the linear systems at lines 5 and 14 of Algorithm 1 instead of a direct method. Since the direct method (LU factorization) runs out of memory for large problems (> 50000) even on a high con guration server (64 GB RAM), we did not pursue it further for comparison. For the model under consideration, M, D and K matrices are symmetric positive de nite. Hence, with the initial expansion points all taken as real and positive, the coe cient matrices of the linear systems to solved s2 i M +siD+K are also symmetric positive de nite initially. As discussed in Section 2, after the rst AIRGA iteration, the expansion points are chosen from the eigenvalues of the quadratic eigenvalue problem 2 M + D + K. For our example, after the rst AIRGA iteration, the eigenvalues of this quadratic eigenvalue problem turn out to be complex (see Table 1.1 in [19] that describes why this is supposed to happen even when all matrices are symmetric positive de nite). Thus, we get complex expansion points. Real, imaginary, or complex expansion points, each have their own merits (see Chapter 6 of [13]). After the rst AIRGA iteration, we use real parts of the com- plex eigenvalues as the expansion points. This is because of three reasons. First, real expansion points give good approximation for general frequency response [13]. Second, using real or complex expansion points has no e ect on the execution of the AIRGA algorithm as well as the accuracy of the reduced system. Third, real expansion points are computationally easier to implement (the di erence between K(z 1) i and K(z) i is more easily quanti able; see Section 3.2). For ensuring stable iterative solves in AIRGA, from Theorem 4.2 we know that we need to use a Ritz-Galerkin based solver. Conjugate Gradient (CG) is the most 16 NAVNEET PRATAP SINGH, KAPIL AHUJA, AND HEIKE FASSBENDER Table 2. CG iterations and computation time for model size 2000 AIRGA Iteration# CG using SPAI CG using SPAI Update Iter Time (secs) Iter Time (secs) 1 4 0.07 4 0.07 4 0.07 4 0.07 4 0.07 4 0.07 2 3 0.06 3 0.06 3 0.06 3 0.06 4 0.07 4 0.07 3 4 0.07 4 0.11 4 0.07 4 0.11 4 0.07 4 0.11 4 4 0.07 4 0.18 4 0.07 4 0.18 4 0.07 4 0.18 popular solver based on this theory. Moreover, since CG is ideal for SPD linear systems and we obtain such linear systems here, we use CG as the underlying iterative solver. Since the unpreconditioned CG required twice as many iterations as the preconditioned CG (for almost all problem sizes), and we did not pursue it further. As discussed in Section 3, preconditioning has to be employed when iterative methods fail or have very slow convergence. We use SPAI and SPAI update as discussed in Section 3.1 and 3.2, respectively1. That is, we use Algorithm 3 with Algorithm 2 and Algorithm 5 with Algorithm 4. The input to Algorithm 2 and Algorithm 4 is tol (besides the coe cient matrices), which we take as 0.01 based upon experience. In Algorithms 3 and 5, at lines 1 and 2, respectively, the overall iteration (while- loop) terminates when the change in the reduced model (computed as H2-error between the reduced models at two consecutive AIRGA iterations) is less than a certain tolerance. We take this tolerance to be 10 06 based on values in [8]. There is one more stopping criteria in these algorithms, at lines 8 and 15, respectively. This checks the H2-error between two temporary reduced models. We take this tolerance to be 10 06 based on values in [8]. Since this is an adaptive algorithm, the optimal size of the reduced model is determined by the algorithm itself, and is denoted by r. As discussed in Section 3.2, SPAI update in AIRGA is done vertically (see Al- gorithm 5 and Table 1). Ideally, this update should be done from AIRGA iteration 2 (since at iteration 1, there is no preceding set of expansion points). However, for 1For SPD linear systems, incomplete Cholesky factorization based preconditioners are quite popular. However, as discussed in Section 3, these preconditioners are not easily parallelizable. Since for larger model sizes (> 10000; e.g., 100,000 and so on) parallelization would be needed and incomplete Cholesky preconditioners will not be able to compete with SPAI then, we do not use them here. Moreover, SPAI preconditioners are popular even for SPD systems (see [10, 11]). PRECONDITIONED ITERATIVE SOLVES IN MODEL REDUCTION 17 Table 3. CG iterations and computation time for model size 10000 AIRGA Iteration# CG using SPAI CG using SPAI update Iter Time (secs) Iter Time (secs) 1 3 1.4 3 1.4 4 1.4 4 1.5 4 1.5 4 1.5 2 4 1.5 4 1.5 3 1.4 3 1.4 3 1.4 3 1.4 3 4 1.5 4 2.4 3 1.4 3 2.3 4 1.5 4 2.4 4 4 1.4 4 4.1 3 1.4 3 3.7 4 1.4 4 4.1 Table 4. SPAI and SPAI update computation time for model size 2000 AIRGA Iteration# SPAI (secs) SPAI with update (secs) 1 10.0 10.0 35.0 35.0 36.0 36.0 2 10.0 10.0 11.0 11.0 10.0 10.0 3 10.0 0.8 11.0 6.0 11.0 1.1 4 11.0 0.6 10.0 4.0 11.0 0.8 this problem change in expansion points from iteration 1 to 2 is fairly large (imply- ing the corresponding coe cient matrices are far). Hence, we implement update from AIRGA iteration 3 onwards. Table 2 gives the CG iteration count and time when using basic SPAI (that is without SPAI update) and SPAI with update for model size 2000. Table 3 gives the same data for model size 10000. From Tables 2 and 3 it is observed that the CG computation time for both variants of preconditioners is almost same. The computation time for CG using SPAI update is slightly higher than CG using basic SPAI from AIRGA iteration 3 onwards (for both model sizes). This is because from AIRGA iteration 3 onwards, we apply SPAI update and hence, the number of matrix-vector products increase (see lines 11 and 17 of Algorithm 5). However, 18 NAVNEET PRATAP SINGH, KAPIL AHUJA, AND HEIKE FASSBENDER Table 5. SPAI and SPAI update computation time for model size 10000 AIRGA Iteration# SPAI (secs) SPAI with Update (secs) 1 282.0 282.0 476.0 476.0 530.0 530.0 2 177.0 177.0 276.0 276.0 170.0 170.0 3 172.0 28.0 272.0 146.0 280.0 37.0 4 179.0 27.0 300.0 65.0 179.0 22.0 Table 6. SPAI and SPAI update analysis for model size 2000 (a) AIRGA Iteration (z) Expansion Points 1 Value I K(z) 1 F K(z 1) 1 K(z) 1 F 3 0.2681 110.5137 0.1124 4 0.2682 110.5170 0.0046 (b) AIRGA Iteration (z) Expansion Point 2 Value I K(z) 2 F K(z 1) 2 K(z) 2 F 3 1.9948 701.4626 40.2943 4 1.8491 631.3391 9.6520 (c) AIRGA Iteration (z) Expansion Point 3 Value I K(z) 3 F K(z 1) 3 K(z) 3 F 3 0.2700 110.6989 0.0398 4 0.2699 110.7225 0.0068 this slight increase in the time for SPAI with update fades when one takes the preconditioner computation time into account (see the discussion below). Table 4 gives the time for computing the basic SPAI preconditioner and SPAI with update preconditioner for model size 2000. Table 5 gives the same data for model size 10000. From Tables 4 and 5 it is observed that considerable amount of time is saved by using SPAI with updates. As discussed in earlier paragraphs, PRECONDITIONED ITERATIVE SOLVES IN MODEL REDUCTION 19 Table 7. SPAI and SPAI update analysis for model size 10000 (a) AIRGA Iteration (z) Expansion Points 1 Value I K(z) 1 F K(z 1) 1 K(z) 1 F 3 0.2681 247.18 0.4279 4 0.2682 247.17 0.0066 (b) AIRGA Iteration (z) Expansion Point 2 Value I K(z) 2 F K(z 1) 2 K(z) 2 F 3 1.5037 1.0e + 03 416.5 4 1.9415 1.5e + 03 116.8 (c) AIRGA Iteration (z) Expansion Point 3 Value I K(z) 3 F K(z 1) 3 K(z) 3 F 3 0.2698 247.6 3.9324 4 0.2696 247.5 0.0602 Table 8. Total computation time of CG with preconditioners Size CG and SPAI (Min) CG and SPAI update (Min) 2000 3.1 2.1 10000 61.2 40.7 Table 9. Accuracy of reduced system Problem n Method Error r 1-D Beam Model 2000 LU 1.2e 06 27 CG with SPAI 3.0e 06 27 CG with SPAI update 2.2e 06 27 10000 LU 1.1e 06 128 CG with SPAI 3.6e 06 128 CG with SPAI update 2.7e 06 128 since SPAI update is being done only from AIRGA iteration 3, saving in time is observed from this step onwards only. We also analyze why SPAI update takes less time. As discussed in Section 3.2, SPAI update is useful when I K(z) i F is large and K(z 1) i K(z) i F is small. This data for three expansion points for model size 2000 is given in Tables 6(A), 6(B) and 6(C). Similar data for model size 10000 is given in Tables 7(A), 7(B) and 20 NAVNEET PRATAP SINGH, KAPIL AHUJA, AND HEIKE FASSBENDER 7(C). For model size 2000, in Table 4 we see that at AIRGA iterations 3 and 4 computation time for SPAI update is almost one fourth of the SPAI time. This is because K(z 1) i K(z) i F is very small as compared to I K(z) i F (see Tables 6(A), 6(B) and 6(C)). Similar pattern is observed for model size 10000 (see Table 5 and Tables 7(A), 7(B) and 7(C)). Table 8 shows the total computation time for iterative solves (i.e., CG time plus the preconditioner time) when using basic SPAI preconditioner and when using SPAI with update preconditioner for model sizes 2000 and 10000. We can notice from this table that computation time of iterative solves with SPAI update is on an average 2 3-rd of the computation time of iterative solves with SPAI. This saving is larger for model size 10000 (we go from around 1 hour to 40 Minutes). Hence, larger the problem more the saving. Table 9 lists the relative error between the original model and the reduced model as well as the size to which the model is reduced (r) when using LU factorization (direct method), CG with SPAI, and CG with SPAI updates. Since the error and r values for all the above three cases are almost the same, we can conclude that by using iterative solves in AIRGA, the quality of reduced system is not compromised. 6. Conclusion and Future work We discussed the application of preconditioned iterative methods for solving large linear systems arising in AIRGA. The SPAI preconditioner works well here and SPAI update (where we reuse the preconditioner) leads to substantial savings. This is demonstrated by experiments on two di erent sizes of one dimensional beam model. We also presented conditions under which the AIRGA algorithm is stable with respect to the errors introduced by iterative methods. Future work includes applying preconditioned iterative methods in other model reduction algorithms for second order dynamical systems (besides AIRGA). For example, Alternate Direction Implicit (ADI) methods for model reduction of second order linear dynamical systems [22]. Based upon our studies on AIRGA and ADI based methods, we also plan to propose a class of preconditioners that would work for most model reduction algorithms for second order linear dynamical systems. Acknowledgement We would like to thank Prof. Eric de Sturler (at Department of Mathematics, Virginia Tech, Blacksburg, VA, USA) for stimulated discussion regarding stability of AIRGA. References 1. K. Ahuja, B. K. Clark, E. de Sturler, D. M. Ceperley, and J. Kim, Improved scaling for quantum Monte Carlo on insulators, SIAM Journal on Scienti c Computing, 33 (2011), no. 4, 1837 1859. 2. A. Antoulas, Approximation of large-scale dynamical systems, Society for Industrial and Ap- plied Mathematics, Philadelphia, PA, USA, 2005. 3. Z. Bai and Y. Su, Dimension reduction of large-scale second-order dynamical systems via a second-order Arnoldi method, SIAM Journal on Scienti c Computing, 26 (2005), no. 5, 1692 1709. 4. C. Beattie and S. Gugercin, Krylov-based model reduction of second-order systems with pro- portional damping, Proceedings of the 44th IEEE Conference on Decision and Control, 2005, pp. 2278 2283. PRECONDITIONED ITERATIVE SOLVES IN MODEL REDUCTION 21 5. C. Beattie, S. Gugercin, and S. A. Wyatt, Inexact solves in interpolatory model reduction, Elsevier Journal of Linear Algebra and its Applications, 436 (2012), no. 8, 2916 2943. 6. M. Benzi, Preconditioning techniques for large linear systems: A survey, Elsevier Journal of Computational Physics, 182 (2002), no. 2, 418 477. 7. M. Benzi and M. Tuma, A comparative study of sparse approximate inverse preconditioners, Applied Numerical Mathematics, 30 (1999), no. 2, 305 340. 8. T. Bonin, H. Fassbender, A. Soppa, and M. Zaeh, A fully adaptive rational global Arnoldi method for the model-order reduction of second-order MIMO systems with proportional damp- ing, Elsevier Mathematics and Computers in Simulation, 122 (2016), 1 19. 9. E. Chow and Y. Saad, Approximate inverse techniques for block-partitioned matrices, SIAM Journal on Scienti c Computing, 18 (1997), no. 6, 1657 1675. 10. , Approximate inverse preconditioners via sparse-sparse iterations, SIAM Journal on Scienti c Computing, 19 (1998), no. 3, 995 1023. 11. T. George, A. Gupta, and V. Sarin, An empirical analysis of the performance of precondi- tioners for SPD systems, ACM Transactions on Mathematical Software, 38 (2012), no. 4, 24:1 24:30. 12. A. K. Grim-McNally, Reusing and updating preconditioners for sequences of matrices, Mas- ter s thesis, Virginia Tech, USA, 2015. 13. E. J. Grimme, Krylov projection methods for model reduction, Ph.D. thesis, University of Illinois at Urbana-Champaign, Urbana, IL, USA, 1997. 14. K. Jbilou, A. Messaoudi, and H. Sadok, Global FOM and GMRES algorithms for matrix equations, Applied Numerical Mathematics 31 (1999), no. 1, 49 63. 15. C. D. Meyer, Matrix analysis and applied linear algebra, Society for Industrial and Applied Mathematics, Philadelphia, PA, USA, 2000. 16. N. M. Nachtigal, S. C. Reddy, and L. N. Trefethen, How fast are nonsymmetric matrix iterations?, SIAM Journal on Matrix Analysis and Applications, 13 (1992), no. 3, 778 795. 17. Y. Saad, Iterative methods for sparse linear systems, Society for Industrial and Applied Math- ematics, Philadelphia, PA, USA, 2003. 18. M. Sadkane, Block-Arnoldi and Davidson methods for unsymmetric large eigenvalue problems, Numerische Mathematik 64 (1993), no. 1, 195 211. 19. F. Tisseur and K. Meerbergen, The quadratic Eigenvalue problem, SIAM Review 43 (2001), no. 2, 235 286. 20. L. N. Trefethen, Pseudospectra of matrices, Numerical Analysis, 91 (1991), 234 266. 21. L. N. Trefethen and D. Bau, Numerical linear algebra, Society for Industrial and Applied Mathematics, Philadelphia, PA, USA, 1997. 22. M. M. Uddin, J. Saak, B. Kranz, and P. Benner, Computation of a compact state space model for an adaptive spindle head con guration with piezo actuators using balanced truncation, Springer Production Engineering, 6 (2012), no. 6, 577 586. 23. H. A. Van der Vorst, Iterative krylov methods for large linear systems, Cambridge University Press, New York, USA, 2003. 24. J. M. Wang, C. C. Chu, Q. Yu, and E. S. Kuh, On projection-based algorithms for model- order reduction of interconnects, IEEE Transactions on Circuits and Systems I: Fundamental Theory and Applications 49 (2002), no. 11, 1563 1585. 25. S. A. Wyatt, Issues in interpolatory model reduction: Inexact solves, second-order systems and DAEs, Ph.D. thesis, Virginia Tech, USA, 2012. Discipline of Computer Science and Engineering, Indian Institute of Technology Indore, India E-mail address: phd1301201002@iiti.ac.in Discipline of Computer Science and Engineering, Indian Institute of Technology Indore, India E-mail address: kahuja@iiti.ac.in Institut Computational Mathematics, Technische Universit at Braunschweig, Ger- many E-mail address: h.fassbender@tu-braunschweig.de