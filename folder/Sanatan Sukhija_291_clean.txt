Supervised Heterogeneous Domain Adaptation via Random Forests Sanatan Sukhija1, Narayanan C Krishnan1, Gurkanwal Singh2 1Department of Computer Science and Engineering, Indian Institute of Technology Ropar, Punjab, India sanatan@iitrpr.ac.in, ckn@iitrpr.ac.in 2Department of Computer Science and Engineering, PEC University of Technology, Chandigarh, India gurkanwal.singh7@gmail.com Abstract Heterogeneity of features and lack of correspon- dence between data points of different domains are the two primary challenges while performing fea- ture transfer. In this paper, we present a novel su- pervised domain adaptation algorithm (SHDA-RF) that learns the mapping between heterogeneous features of different dimensions. Our algorithm uses the shared label distributions present across the domains as pivots for learning a sparse fea- ture transformation. The shared label distributions and the relationship between the feature spaces and the label distributions are estimated in a supervised manner using random forests. We conduct exten- sive experiments on three diverse datasets of vary- ing dimensions and sparsity to verify the superi- ority of the proposed approach over other baseline and state of the art transfer approaches. 1 Introduction The key to success of many supervised learning algorithms is the availability of abundant labeled training data. How- ever, for many real-world problems, collecting labeled data is often very expensive and cumbersome. Transfer learning algorithms help to overcome the scarcity of labeled data in a domain (often referred to as the target domain) by util- ising information about the task, and data from single or multiple auxiliary domains (referred to as source domains). Transfer learning approaches have found success in many applications including activity recognition [Hu et al., 2011; Cook et al., 2013c], sentiment classi cation [Zhou et al., 2014], document analysis and indoor localization [Pan and Yang, 2010]. A popular setting for performing transfer is when the source and the target domains are represented by the same set of features. The goal in this setting is to minimise the dif- ferences in the data distribution of the source and target do- mains. However, for applications such as sentiment analysis across different languages [Pan, 2010], and activity recogni- tion across different domains [Cook et al., 2013b], the source The author contributed to this work during his internship at IIT Ropar. and target data are represented using heterogeneous features of different dimensions that may or may not overlap. Transfer learning for such heterogeneous domains can be performed by rst bridging the gap between the features characterising the different domains. This is the principle behind feature- based transfer learning approaches. The feature-based transfer approach proposed in this pa- per is motivated by the application of activity recognition in a smart home. Smart home based activity recognition deals with learning the daily activities of smart home resident(s), captured through a series of sensor observations. Transfer learning algorithms can be used to overcome the scarcity of labeled data of a new target smart home by utilising the la- beled data of other source smart homes. However, different layouts and types of sensors deployed at different places lead to heterogeneous feature spaces [Hu and Yang, 2011] neces- sitating transfer methodologies. Figure 1 illustrates the layout and sensor locations for three smart homes from the CASAS datasets [Cook et al., 2013c] used in this paper. Given only a few labeled instances in the target we leverage the common labels in the source and target domains to derive the relation- ship between the corresponding feature spaces. The key as- sumption of our algorithm is that features in both source and target domains that characterise data partitions with similar label distribution, must be related to each other. The shared label distributions across the two domains act as the pivot for learning the mapping between the feature spaces. The gen- erated sparse mapping represents a target feature as a linear combination of source features. This mapping is estimated without assuming any correspondence between source and target data points. 1.1 Problem De nition Let {XS, YS}m i=1 and {XT , YT }n j=1 represent the set of la- beled instances in the source domain S and target domain T respectively, where m o n. xS 2 RdS is a source data point with yS 2 Y the corresponding class label. Similarly, xT 2 RdT is a target data point and yT 2 Y is its associated label. The features that describe xS and xT are completely different and dS 6= dT . However, we assume that the source and target domains share a common label space. Let the num- ber of shared labels be k. Our goal is to learn a mapping f : RdS ! RdT such that the data from the source domain can be mapped to the target domain. This mapped source data Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence (IJCAI-16) 2039 hh102 hh113 hh118 Figure 1: Layouts of the three CASAS smart homes that differ in terms the layout, and count of the sensors deployed. The black squares represent the location of sensors. can then be used in conjunction with the target data to learn the hypothesis h : RdT ! Y. 1.2 Contributions The contributions of this paper can be summarised as follows: 1. The proposed algorithm yields a heterogeneous feature- space class-invariant mapping, assuming no correspon- dence between the data-points of the domains that share no overlapping features. 2. Our algorithm does not require the computation of an optimal code matrix for error correcting output code, which is a challenging task, that is a requirement for the current supervised state of the art feature transfer al- gorithm [Zhou et al., 2014]. The proposed algorithm utilises naturally occurring label distributions at leaf nodes of a decision tree model as pivots to generate the mapping PS 2 RdS dT . 3. The experiments conducted on diverse datasets indicate the effectiveness of the algorithm even if very few la- beled instances are available in the target domain. 2 Related Work Bridging features across heterogeneous spaces for domain adaptation is a challenging problem. The approaches for het- erogeneous domain adaptation can be broadly split into two categories based on the type of mapping learned, namely, Feature Remapping and Latent Space Transformation. Feature Remapping approaches determine the transforma- tion for converting source features to target features or vice- versa. It can in turn be of two types: one in which there is an explicit correspondence between the individual features of source and target domain such as the ith source feature be- ing mapped to the jth target feature, and second in which a source or target feature is represented as a combination (of- ten linear) of features from the other domain. Approaches for one-to-one source to target feature remapping have used ge- netic algorithms and other greedy methods to obtain an op- timal mapping, using classi cation accuracy as the perfor- mance measure [Feuz and Cook, 2014]. Alternate approaches rely on domain independent features known as pivots that can be utilised to align the feature spaces [Blitzer et al., 2006; He et al., 2014]. These approaches partition the features of a domain into independent and dependent sets. The domain independent features are present across different domains, while the dependent features are speci c to a domain. The goal is to learn the mapping between the dependent features by using the independent features. Spectral clustering algo- rithm can be used to obtain the feature-clusters from the co- aligned bipartite graph of domain independent and dependent features that acts as the common subspace [Pan, 2010]. In the absence of explicit domain independent features, statistical properties of domain speci c features can be used to derive meta features to bridge the domains [Feuz, 2014]. A recent work on feature remapping for feature transfer constructs a class-invariant sparse transformation matrix by mapping the weight vectors of SVM classi er trained on labeled data from the domains [Zhou et al., 2014]. Synthetically generated er- ror correcting output codes (ECOC) are used to train the SVM model so as to estimate accurate transformations. We com- pare the performance of our algorithm against this supervised heterogeneous feature remapping approach (SHFR-ECOC). Latent Space Transformation approaches determine trans- formations to project the data of different domains onto a common latent space. Speci cally, these approaches compute two projection matrices PS and PT for source and target do- mains respectively, such that the difference between the pro- jected source space BS : PS XS and projected target space BT : PT XT is minimised while trying to preserve the char- acteristics of the original feature spaces. Approaches in this category can be summarised as unsupervised Eigen transfer frameworks. Manifold alignment based approaches [Wang and Mahadevan, 2009] determine the transformed space by making the manifold assumption, where the mapping brings data distributions of the domains closer to each other while preserving the local geometric structure and maximising the alignment. However, these approaches only work for data that exhibit strong manifold property and therefore are not applicable to other scenarios. Heterogeneous spectral map- ping (HeMap) [Shi and Yu, 2012] optimises the difference in the latent space in a general setting by learning two transfor- mation matrices using spectral embedding without using any label information. The algorithm implicitly tries to discover the correspondence between the data points of source and tar- get domain through an optimisation framework. Often, in sit- uations where there is no explicit data correspondences, the recovered transformations are noisy. Since these approaches directly estimate the projected data, estimating the projection for out-of-sample data is a challenging problem. We com- pare the performance of our algorithm against the HeMap ap- proaches that compute linear and non-linear transformations. In the context of smart home activity recognition, heteroge- neous layouts can be manually uni ed by de ning a common meta-feature space that is shared by all smart apartments. These meta-features can be manually speci ed by a domain expert or can be derived from structural, temporal, spatial or 2040 XS1<x1 XS3<x3 XS2<x2 XS4<x4 XT2<y1 XT1<y2 XT3<y3 XT4<y4 WS WT Target Domain Source Domain Shared label distribution Labels Features Features Contribution Distribution Figure 2: Illustration of the relationship between the features of the two domains based on a single pivot label distribution. functional similarities of original features [Rashidi and Cook, 2010]. A manual mapping is not optimal and a poor map- ping can drastically hinder the recognition performance of the model. The proposed algorithm intends to exploit label space distributions of data partitions to identify a mapping across the domains for addressing the problem of heterogeneous do- main adaptation. 3 Proposed Methodology The task of determining common features between hetero- geneous source and target feature spaces for knowledge trans- fer is a challenging problem [Li et al., 2014]. Our novel so- lution to this problem leverages the common label informa- tion between the source and target domains as the pivot for knowledge transfer. The proposed algorithm determines the mapping PS between source and target features based on the estimate of the contribution of the features towards creating data partitions having similar label distributions. 3.1 Estimating Pivots Across the Domains The rst step in our proposed approach is to derive the piv- ots that are used to construct the bridge across heterogeneous feature spaces. We de ne the pivots in terms of the shared labels between the source and target domains. In the sim- plest scenario each shared label is a pivot. When the num- ber of shared labels between the domains is small, learning the feature mapping is a challenging problem [Zhou et al., 2014]. The SHFR ECOC approach overcomes this limita- tion by using synthetically generated error-correcting output codes (ECOC) for representing each shared label. It is desir- able that the class labels are independent as the relationships between the different labels are not effectively captured due to the randomness of the ECOC generation process [Rajan and Ghosh, 2004]. Thus selecting the optimal code matrix is a challenging problem for SHFR ECOC. Our approach over- comes this limitation by relying on naturally occurring label distributions in the complex data space. To arrive at these label distributions, our approach looks at the leaf nodes of a decision tree modeled on the dataset. A decision tree follows a greedy strategy to recursively parti- tion the data based on some feature value test. A leaf node in a decision tree holds a distribution L of class labels that is associated with a set of data partition. A path in a decision tree from the root to a leaf node contains a sequence of fea- tures chosen as split functions. The candidate split at a node involves a locally optimal partitioning based on some metric like Gini Impurity, Information Gain or Gain Ratio. To ensure a suf cient number of pivotal label distributions for learning the mapping between the domains, we train a random forest, which also helps to reduce over tting. Every tree in the forest is constructed using a random subset of fea- tures [Breiman, 2001]. Our algorithm rst constructs ns and nt trees from S and T respectively. Every path in a decision tree leading to a partition of data is associated with a certain label distribution. Label distributions that appear both in the source and target random forest models are the pivots that are used for bridging the two domains. 3.2 Estimating Feature Relationships WS and WT The next step in the algorithm computes the relationship ma- trices WS and WT between the domain dependent features and the shared pivots. Since our pivots are label distributions, we de ne this relationship as the contribution of the domain dependent features towards creating the pivot label distribu- tion. This relationship can be easily extracted from the deci- sion tree structure. The boundary of the data partition at the leaf node can be identi ed with the feature splits along the path to the leaf node from the root of the tree. A simple approach to compute WS and WT would be to give equal importance to all the features that were used as split nodes in the path. Thus for a path, the ith entry in the corresponding feature relationship vector would contain the frequency of the ith feature getting selected as a split node. Another approach would be to give higher priority to a feature used at parent node compared to the features chosen as split nodes at its descendants. For every path, each entry in the feature contribution vector is given by Pc i=1(1/2)v(i) where v(i) denotes the decision tree depth at which the split was made and c represents the frequency of the feature being used as a candidate split in the path. In practice, it is common to have duplicate label distribu- tions at leaf nodes, i.e., different data partitions correspond- ing to the same label distribution. The feature contribution vectors for these data partitions are averaged. Thus at the end of this process, for each shared pivotal label distribution between S and T, we also have the domain dependent fea- ture relationships to these pivots. Based on the similar source and target class label distributions, the estimated feature con- tribution matrices WS 2 RNp dS and WT 2 RNp dT are mapped to yield the source projection matrix PS, where Np is the number of pivots. This process is illustrated in Figure 2. The advantage with using a random forest model is that the pivots and the relationship between the domain dependent features and pivots across source and target can be estimated from a single model reducing the complexity of the transfer approach. 2041 Algorithm 1 Supervised HDA via Random Forests (SHDA- RF) Input: Source data: S 2 RM dS and Target data: T 2 RN dT Output: PS 2 RdS dT 1. Build a random forest with ns trees from source features XS. 2. For every path from the root to a leaf node in a tree, the con- tribution of a feature is estimated as W(xS(v)) = W(xS(v)) + (1/2)v where v denotes the level at which the feature x 2 XS was selected as a candidate split. The corresponding label distri- bution L is acquired from the leaf node. 3. Similarly construct the target features contribution matrix WT using nt trees created from T. 4. Remove duplicates from LS and LT . For every duplicate entry in LS and LT , the corresponding feature vector entries in WS and WT are averaged. 5. Return the corresponding WS and WT for the identical class label distributions. 6. The mapping PS can be obtained by running LASSO dT times on obtained WS and WT from Step 5. Table 1: Summary of CASAS-HH datasets. Dataset Feature count Activity count hh102 43 29 hh113 48 30 hh118 44 32 3.3 Deriving the Feature Transformation The last step in our algorithm derives a sparse transformation PS between the two domains. Our objective is to represent each target feature as a linear combination of a small set of source features. The Least Absolute Shrinkage and Selection Operator (LASSO) is used to learn PS from WS and WT . It is de ned as: min PS 1 Np Np X i=1 k WT WSPS k2 2 + dT X i i k PSi k1, s.t. PSi 0 The rst part of the optimisation problem minimises the dif- ference between the projected source feature contribution ma- trix PS WS and target feature contribution matrix WT . The second part is the L1 regularisation term to obtain a sparse transformation matrix. The regularisation parameter con- trols the size of this subset. There are dT minimisation prob- lems that are solved using Least Angle Regression (LARS) [Hastie et al., 2001]. The proposed methodology is summarised in Algorithm 1. Once the mapping PS 2 RdS dT is obtained, the tar- get model is retrained along with the projected source data (S PS). The SHFR-ECOC approach does not retrain the model after nding the transformation. It uses the source model to predict the class labels of transformed target in- stances. In contrast, our approach utilizes the bene ts of ran- domization and implicit feature selection of RF to retrain the model attuned for target domain. 4 Experiments We compare the performance of the proposed algorithm against other baseline classi ers and approaches that perform transfer. Random forests (BRF) and SVM that uses ECOC (SVM ECOC) were chosen as the baseline classi ers. Trans- fer approaches include SHFR ECOC [Zhou et al., 2014] and HeMAP (linear and non-linear) [Shi and Yu, 2012]. The num- ber of trees in the random forest was set to 100. The number of bagged features for learning in a tree in the forest was set to p d + 5, where d is the total number of features. The parameters for the SVM model with RBF kernel were ne-tuned using grid search. Based on cross validation ex- periments, the length of ECOC was set to 35, beyond which the performance plateaued. We choose three diverse datasets, varying in the size and sparsity of the features, for investigat- ing the performance of the different algorithms. The CASAS dataset [Cook et al., 2013a] is a collection of smart home datasets that are widely used for investigating ac- tivity recognition algorithms. We use the horizon house (HH) datasets from this collection, which are records of sensor data from single resident smart homes. Sensor data from one smart home serves as the source and another acts as the target. A sliding window of 20 sensor events is used to build the fea- ture vector that consists of counts of sensor events within the sliding window, along with temporal features such as time of the day and day of the week [Cook and Krishnan, 2015; Feuz, 2014]. The feature vector is annotated with the activ- ity label associated with the last sensor event in the sliding window. The number of features and activity labels in each dataset are presented in Table 1. The feature values of the sen- sors in close vicinity appear to be mutually related. This mo- tivates learning a sparse feature mapping instead of a dense mapping. The target training set consists of approximately 7000 samples that preserve the original class distribution. 16 such random subsets are used for evaluating the performance of the different algorithms. The 20 Newsgroups [Lang, 1995] text collection is a sparse dataset of approximately 19000 documents belonging to 20 classes that follow a label hierarchy. The transfer ex- periments were performed on two datasets each containing the subcategories falling under rec and talk, and rec and sci respectively. There are a total of 8 classes in each dataset with a vocabulary spanning over 26000 words. We consid- ered only the rst 10000 features that contributed the most towards the classi cation task. For each dataset, two transfer settings were created. In the rst setting, the source and target consisted of random and mutually exclusive partition of 5000 features. Target training data is created by randomly selecting 10 samples per class. In the second setting, the roles of the source and target dataset were reversed. Since the baseline SVM ECOC model was unable to handle the high dimen- sional features, PCA was performed while preserving 75% variance on the TF-IDF feature values. Dimensionality re- duction is not performed as a pre-processing step for the other approaches. The prede ned test partitions of the dataset are used for testing the approaches. The Statlog (Landsat Satellite) [Lichman, 2013] image dataset comprises of 6 classes and 36 real-valued features. It 2042 Table 2: Performance comparison is depicted in terms of mean error(%). Statistically signi cant SHDA-RF results against BRF and SHFR-RF are highlighted in bold and indicated by respectively. CASAS HH datasets Baseline Results Transfer Results S!T BRF SVM- ECOC SHFR- ECOC HeMap Linear HeMap Non-Linear FA SHFR-RF SHDA-RF hh102!hh113 30.49 2.58 47.14 1.00 39.22 1.63 51.06 1.53 52.97 0.97 34.71 1.55 28.68 1.14 27.93 2.54 hh102!hh118 28.6 1.07 57.74 1.84 43.52 1.18 59.6 0.89 61.8 0.87 37.7 2.38 27.89 0.95 26.97 1.15* hh113!hh102 28.44 1.54 37.54 1.60 38.70 1.50 41.41 1.92 43.47 2.53 38.64 2.68 25.97 1.69 25.97 1.00 hh113!hh118 21.6 0.45 54.97 1.13 36.7 1.41 58.4 1.26 63 1.39 31 2.7 19.47 1.07 18.38 1.29* hh118!hh102 29.6 1.86 39.99 1.59 39.28 1.88 43 0.99 45.7 0.9 37.4 2.67 29.54 1.88 27.83 2.64* hh118!hh113 23.5 1.21 36.3 0.67 32.35 1.1 40.3 0.72 41.4 0.53 31 7.38 21.69 0.68 21.54 1.47 20 Newsgroups dataset Baseline results Transfer Results S ! T BRF SVM-ECOC (PCA) SHFR-ECOC (PCA) HeMap Linear HeMap Non-Linear SHFR-RF SHDA-RF rec v/s sci F1:F5000!F5001:F10000 51.91 2.3 50.49 4.1 48.01 3.5 63.6 3.62 63.22 4.1 46.61 1.36 40.06 2.9* F5001:F10000!F1:F5000 68.41 3.6 67.09 4.0 60.23 6.6 73.1 3.9 72.8 4.6 58.12 2.13 56.81 4.1* rec v/s talk F1:F5000!F5001:F10000 55.79 1.1 56.12 1.6 51.55 2.5 66.2 3.9 66.0 3.55 49.99 0.12 48.82 3.3* F5001:F10000!F1:F5000 68.63 2.4 66.16 3.8 52.92 3.1 70.44 3.0 70.2 6.11 44.67 0.23 35.51 5.2* Satellite Statlog dataset S!T BRF SVM ECOC SHFR-ECOC HeMap- Linear HeMap Non-Linear SHFR-RF SHDA-RF F1:F18!F19:F36 19.30 0.9 21.45 1.3 22.20 2.13 33.31 5.8 33.18 5.1 19.42 1.65 18.58 1.6 F19:F36!F1:F18 20.05 1.00 21.50 1.67 22.45 1.1 31.57 6.1 32.16 4.2 19.73 1.45 18.66 0.78* Table 3: Performance of SVM and SHFR ECOC models on original features of 20 Newsgroups dataset. rec v/s sci S ! T SVM ECOC SHFR ECOC F1:F5000!F5001:F10000 79.48 3.39 87.09 4.1 F5001:F10000!F1:F5000 85.01 3.76 88.22 3.12 rec v/s talk F1:F5000!F5001:F10000 83.12 3.6 85.16 4.23 F5001:F10000!F1:F5000 89.55 4.1 86.92 3.75 consists of 4435 examples in the training set and 2000 exam- ples in the test set. The 36 features were split randomly into two equal groups for creating the source and target domains. To evaluate different algorithms, we used multiple sets of 10 labeled samples per class to create target training data. 5 Results and Discussion The performance of different classi ers on the datasets is re- ported in Table 2. The common observation across all the datasets is the superior performance of baseline random forest (BRF) model over other baseline and some transfer learning approaches. This is another motivation behind adopting ran- dom forest model for performing transfer. The performance of the SHDA-RF algorithm on the CASAS-HH dataset is sig- ni cantly better than all the other approaches by about 2-3% (p-value< 0.05). Among the baseline classi ers, it is evi- dent that the BRF models perform better than SVM ECOC. This can be explained by considering that the activity labels in the dataset are annotated by humans using rule based heuris- tics. It can be also noted that SHFR ECOC, a transfer strat- egy based on SVM ECOC, performs better than SVM ECOC Table 4: Performance of HeMap (Linear and Non-Linear) on 20 Newsgroups dataset without explicit correspondence be- tween source and target data rec v/s sci S ! T HeMap Linear HeMap Non-Linear F1:F5000!F5001:F10000 83.75 4.23 93.75 4.79 F5001:F10000!F1:F5000 85.01 3.76 88.22 3.12 rec v/s talk F1:F5000!F5001:F10000 87.5 6.45 86.44 6.52 F5001:F10000!F1:F5000 83.75 4.79 85.0 7.07 signi cantly. This suggests that the possibility of knowledge transfer between the two domains, which is further reinforced by the performance improvement obtained by SHDA-RF over BRF model. Another common strategy that is used for per- forming transfer on activity recognition datasets is by de n- ing a mapping that aggregates sensors to form layout inde- pendent functional areas (FA) [van Kasteren et al., 2010] as an explicit meta-feature space. For example, individual sen- sors in the bedroom are all clubbed together under a single feature. It can be observed from Table 2 that this approach performs worse than the BRF model. This suggests poten- tial loss in information due to aggregation of different sen- sor events that is critical for differentiating activities happen- ing in the same functional area. The FA approach, an unsu- pervised transfer approach, on the other hand performs sig- ni cantly better than other unsupervised transfer approaches namely HeMAP (linear and non-linear). On the high dimensional 20 Newsgroups dataset, SHDA- RF results in superior performance as compared to all the other approaches. The difference in the performance of 2043 7000 samples 1 week 2 weeks 3 weeks 0 10 20 30 Target Data Size > Mean Error > BRF SHDA RF hh102 >hh113 7000 samples 1 week 2 weeks 3 weeks 0 5 10 15 20 25 Target Data Size > Mean Error > BRF SHDA RF hh113 >hh118 7000 samples 1 week 2 weeks 3 weeks 0 10 20 30 Target Data Size > Mean Error > BRF SHDA RF hh118 >hh102 Figure 3: Availability of labeled target data helps to reduce mean error by learning a better mapping SHDA-RF and the next best classi er SHFR-ECOC is on an average 7-8% (p-value< 0.05). Handling high dimen- sional sparse data with only a few samples available per class necessitated the use of dimensionality reduction techniques for SVM ECOC and SHFR ECOC approaches. This can be observed by comparing the results of the SVM ECOC and SHFR ECOC models trained and tested on the origi- nal features (Table 3) and on transformed features (Table 2). However, the proposed approach does not require such a pre- processing step and is able to learn well in the original high dimensional space. The HeMAP approaches attempt to es- timate a direct mapping between the source and target data. Learning this mapping in the presence of explicit correspon- dence between source and target data is easier than in the gen- eral case. As depicted in Table 4, the performance of HeMap suffers without explicit correspondence between source and target data. Even with explicit correspondence between the data points, the performance of the unsupervised transfer ap- proaches are not at par with the other techniques. SHDA-RF Identical >=95% >=90% >=85% 16 18 20 22 24 26 28 30 32 Similarity of label distributions > Mean Error > hh102 >hh113 hh113 >hh118 hh118 >hh102 Figure 4: Effect of the number of pivots on the mean error performs marginally better than BRF on the Statlog dataset. It is able to signi cantly outperform the BRF model only in one out of the two settings. Though the difference in the per- formances of SVM ECOC and SHFR ECOC is not signi - cant, however if considered, it performs better than the SHFR ECOC transfer model. The smaller margin of improvement could be attributed to the property of the dataset, which is dense and real-valued. Random Forest is used as the nal model for comparing the different transfer mappings. The performance of the transfer mapping learned from SHFR-ECOC that uses random for- est as the nal model (SHFR-RF) is signi cantly poorer than SHDA-RF on the 20 Newsgroups dataset. On the CASAS- HH datasets, the results are only marginally poorer. Thus overall the results do seem to suggest that the transfer map- ping learned through SHDA-RF is better than SHFR-ECOC. Figure 3 presents the results for BRF and SHDA-RF mod- els on a few CASAS-HH datasets with increasing target train- ing data. It can be observed that the mean error for both the approaches reduces with increase in the number of labeled target domain data. However, the transfer approach performs marginally better than the baseline when number of target training examples is close to 50%. The SHDA-RF algorithm uses only identical label distri- butions across the domains as pivots. We conducted experi- ments to study the effect of increasing the shared label dis- tributions between the domains by relaxing the similarity be- tween the distributions. We used Jensen-Shannon divergence [Lin, 1991] to determine the similarity between two label dis- tributions. Figure 4 reports the mean error for models with in- creasing similarity relaxation on three CASAS-HH datasets. It can be observed that the mean error reduces only till about 90% relaxation beyond which the error increases marginally. 6 Summary and Future Work In this paper we present a novel supervised heterogeneous do- main adaptation technique that learns the mapping between heterogeneous feature spaces of different dimensions. Our algorithm uses the shared label distributions across the do- mains as the pivots for learning the feature transformation. We estimate the pivots using random forest models trained both on source and a small part of target labeled data. The experiments conducted on diverse datasets suggest the supe- riority of the proposed algorithm over other baseline and fea- ture transfer approaches. The SHDA-RF algorithm establishes the mapping between the feature spaces. A natural extension will be to consider in- stance transfer approaches to reduce the marginal and condi- tional probability distributions between the target and trans- formed source data. The proposed algorithm utilises a sin- gle source domain for knowledge transfer. Another direction that we would like to explore is how to effectively combine labeled data from multiple source domains to make an im- proved nal prediction on the target. Finally, variants of ran- dom forests and sampling techniques can be used to improve upon on the random forest model that is the foundation of the SHDA-RF approach. Acknowledgments The authors are grateful to the anonymous reviewers for their valuable comments. This research is partially supported by the ISIRD grant from IIT Ropar. 2044 References [Blitzer et al., 2006] John Blitzer, Ryan McDonald, and Fer- nando Pereira. Domain adaptation with structural corre- spondence learning. In Proceedings of the 2006 Confer- ence on Empirical Methods in Natural Language Process- ing, pages 120 128, 2006. [Breiman, 2001] Leo Breiman. Random forests. Machine learning, pages 5 32, 2001. [Cook and Krishnan, 2015] Diane J. Cook and Narayanan C. Krishnan. Activity Learning:Discovering, Recognizing, and Predicting Human Behavior from Sensor Data. John Wiley and Sons Inc., 2015. [Cook et al., 2013a] Diane J. Cook, Aaron S. Crandall, Brian L. Thomas, and Narayanan C. Krishnan. Casas: A smart home in a box. Computer, 46(7):62 69, 2013. [Cook et al., 2013b] Diane J. Cook, Kyle Dillon Feuz, and Narayanan C. Krishnan. Transfer learning for activity recognition: a survey. Journal of Knowledge and Infor- mation Systems, pages 537 556, 2013. [Cook et al., 2013c] Diane J. Cook, Narayanan C. Krishnan, and Parisa Rashidi. Activity discovery and activity recog- nition: A new partnership. IEEE Transactions on Systems Man and Cybernetics, pages 820 828, 2013. [Feuz and Cook, 2014] Kyle Dillon Feuz and Diane J. Cook. Heterogeneous transfer learning for activity recognition using heuristic search techniques. International Journal of Pervasive Computing and Communications, pages 393 418, 2014. [Feuz, 2014] Kyle Dillon Feuz. Preparing smart environ- ments for life in the wild: Feature-space and Multi-view hetrogeneous learning. PhD thesis, Washington State Uni- versity, 2014. [Hastie et al., 2001] Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical Learning. Springer Series in Statistics. 2001. [He et al., 2014] Jingrui He, Yan Liu, and Qiang Yang. Link- ing heterogeneous input spaces with pivots for multi-task learning. In Proceedings of the SIAM International Con- ference on Data Mining, pages 181 189, 2014. [Hu and Yang, 2011] Derek Hao Hu and Qiang Yang. Trans- fer learning for activity recognition via sensor mapping. In Proceedings of the 22nd International Joint Conference on Arti cial Intelligence, pages 1962 1967, 2011. [Hu et al., 2011] Derek Hao Hu, Vincent Wenchen Zheng, and Qiang Yang. Cross-domain activity recognition via transfer learning. Journal of Pervasive and Mobile Com- puting, pages 344 358, 2011. [Lang, 1995] Ken Lang. Newsweeder: Learning to lter net- news. In Proceedings of the Twelfth International Confer- ence on Machine Learning, pages 331 339, 1995. [Li et al., 2014] Wen Li, Lixin Duan, Dong Xu, and Ivor W. Tsang. Learning with augmented features for supervised and semi-supervised heterogeneous domain adaptation. IEEE Transactions on Pattern Analysis and Machine In- telligence, pages 1134 1148, 2014. [Lichman, 2013] M. Lichman. UCI machine learning repos- itory, 2013. [Lin, 1991] Jianhua Lin. Divergence measures based on the shannon entropy. IEEE Transactions on Information The- ory, pages 145 151, 1991. [Pan and Yang, 2010] Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on Knowl- edge and Data Engineering, pages 1345 1359, 2010. [Pan, 2010] Jialin Pan. Feature based transfer learning with real-world applications. PhD thesis, Hong Kong Univer- sity of Science and Technology, 2010. [Rajan and Ghosh, 2004] Suju Rajan and Joydeep Ghosh. An empirical comparison of hierarchical vs. two-level ap- proaches to multiclass problems. In Multiple Classi er Systems, Lecture Notes in Computer Science, pages 283 292. 2004. [Rashidi and Cook, 2010] Parisa Rashidi and Diane J. Cook. Multi home transfer learning for resident activity discov- ery and recognition. In Proceedings of the International Workshop on Knowledge Discovery from Sensor Data, pages 56 63, 2010. [Shi and Yu, 2012] Xiaoxiao Shi and Philip Yu. Dimension- ality reduction on heterogeneous feature space. In Pro- ceedings of the 12th IEEE International Conference on Data Mining, pages 635 644, 2012. [van Kasteren et al., 2010] T. L. M. van Kasteren, G. En- glebienne, and B. J. A. Kr ose. Transferring knowledge of activity recognition across sensor networks. In Pro- ceedings of the 8th International Conference on Pervasive Computing, pages 283 300, 2010. [Wang and Mahadevan, 2009] Chang Wang and Sridhar Ma- hadevan. Manifold alignment without correspondence. In Proceedings of the 21st International Joint Conference on Arti cial Intelligence, pages 1273 1278, 2009. [Zhou et al., 2014] Joey Tianyi Zhou, Ivor W. Tsang, Sinno Jialin Pan, and Mingkui Tan. Heterogeneous do- main adaptation for multiple classes. In Proceedings of the 17th International Conference on Arti cial Intelligence and Statistics, pages 1095 1103, 2014. 2045